\documentclass[a4paper,12pt]{book}

%%% Packages %%%
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{url}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage[utf8]{inputenc}

%%% Settings %%%
\setlength{\parindent}{0pt}
\raggedbottom
\frenchspacing
\urlstyle{same}
\MakeOuterQuote{"}
\setlist{nolistsep}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

%%% Format %%%
\renewcommand\thechapter{\Roman{chapter}}
\renewcommand\thesection{\arabic{section}}
\renewcommand\theequation{\arabic{equation}}
\let\Chaptermark\chaptermark
\def\chaptermark#1{\def\Chaptername{#1}\Chaptermark{#1}}
\let\Sectionmark\sectionmark
\def\sectionmark#1{\def\Sectionname{#1}\Sectionmark{#1}}

%%% Title %%%
\title{\Huge{The Book of Statistical Proofs}}
\author{https://statproofbook.github.io/ \\ StatProofBook@gmail.com}
\date{2020-02-13, 17:11}

\begin{document}


%%% Title %%%
\maketitle

%%% Contents %%%
\pagebreak
\pagenumbering{roman}
\tableofcontents

%%% Text %%%
\newpage
\pagenumbering{arabic}


% Chapter 1 %
\chapter{General Theorems} \label{sec:General Theorems} \newpage

\pagebreak
\section{Probability theory}

\subsection{Probability distributions}

\subsubsection[\textit{Moment-generating function}]{Moment-generating function} \label{sec:mgf}

\vspace{1em}
\textbf{Definition:}

1) The moment-generating function of a random variable ($\rightarrow$ Definition "rvar") $X \in \mathbb{R}$ is

\begin{equation} \label{eq:mgf-mgf-var}
M_X(t) = \mathrm{E} \left[ e^{tX} \right], \quad t \in \mathbb{R} \; .
\end{equation}

2) The moment-generating function of a random vector ($\rightarrow$ Definition "rvec") $X \in \mathbb{R}^n$ is

\begin{equation} \label{eq:mgf-mgf-vec}
M_X(t) = \mathrm{E} \left[ e^{t^\mathrm{T}X} \right], \quad t \in \mathbb{R}^n \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Moment-generating function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-22; URL: \url{https://en.wikipedia.org/wiki/Moment-generating_function#Definition}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: D2 | shortcut: mgf | author: JoramSoch | date: 2020-01-22, 10:58.


\subsection{Bayesian inference}

\subsubsection[\textbf{Bayes' theorem}]{Bayes' theorem} \label{sec:bayes-th}

\vspace{1em}
\textbf{Theorem:} Let $A$ and $B$ be two arbitrary statements about random variables ($\rightarrow$ Definition "rvar"), such as statements about the presence or absence of an event or about the value of a scalar, vector or matrix. Then, the conditional probability that $A$ is true, given that $B$ is true, is equal to

\begin{equation} \label{eq:bayes-th-BT}
p(A|B) = \frac{p(B|A) \, p(A)}{p(B)} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The conditional probability ($\rightarrow$ Definition "cp") is defined as the ratio of joint probability ($\rightarrow$ Definition "jp"), i.e. the probability of both statements being true, and marginal probability ($\rightarrow$ Definition "mp"), i.e. the probability of only the second one being true:

\begin{equation} \label{eq:bayes-th-LCP}
p(A|B) = \frac{p(A,B)}{p(B)} \; .
\end{equation}

It can also be written down for the reverse situation, i.e. to calculate the probability that $B$ is true, given that $A$ is true:

\begin{equation} \label{eq:bayes-th-LCP-rev}
p(B|A) = \frac{p(A,B)}{p(A)} \; .
\end{equation}

Both equations can be rearranged for the joint probability

\begin{equation} \label{eq:bayes-th-JP}
p(A|B) \, p(B) \overset{\eqref{eq:bayes-th-LCP}}{=} p(A,B) \overset{\eqref{eq:bayes-th-LCP-rev}}{=} p(B|A) \, p(A)
\end{equation}

from which Bayes' theorem can be directly derived:

\begin{equation} \label{eq:bayes-th-BT-proof}
p(A|B) \overset{\eqref{eq:bayes-th-JP}}{=} \frac{p(B|A) \, p(A)}{p(B)} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Koch, Karl-Rudolf (2007): "Rules of Probability"; in: \textit{Introduction to Bayesian Statistics}, Springer, Berlin/Heidelberg, 2007, pp. 6/13, eqs. 2.12/2.38; URL: \url{https://www.springer.com/de/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P4 | shortcut: bayes-th | author: JoramSoch | date: 2019-09-27, 16:24.


\subsubsection[\textbf{Bayes' rule}]{Bayes' rule} \label{sec:bayes-rule}

\vspace{1em}
\textbf{Theorem:} Let $A_1$, $A_2$ and $B$ be arbitrary statements about random variables ($\rightarrow$ Definition "rvar") where $A_1$ and $A_2$ are mutually exclusive. Then, Bayes' rule states that the posterior odds ($\rightarrow$ Definition "post-odd") are equal to the Bayes factor ($\rightarrow$ Definition "bf") times the prior odds ($\rightarrow$ Definition "prior-odd"), i.e.

\begin{equation} \label{eq:bayes-rule-bayes-rule}
\frac{p(A_1|B)}{p(A_2|B)} = \frac{p(B|A_1)}{p(B|A_2)} \cdot \frac{p(A_1)}{p(A_2)} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Using Bayes' theorem ($\rightarrow$ Proof \ref{sec:General Theorems}/\ref{sec:bayes-th}), the conditional probabilities ($\rightarrow$ Definition "cp") on the left are given by

\begin{equation} \label{eq:bayes-rule-bayes-th-A1}
p(A_1|B) = \frac{p(B|A_1) \cdot p(A_1)}{p(B)}
\end{equation}

\begin{equation} \label{eq:bayes-rule-bayes-th-A2}
p(A_2|B) = \frac{p(B|A_2) \cdot p(A_2)}{p(B)} \; .
\end{equation}

Dividing the two conditional probabilities by each other

\begin{equation} \label{eq:bayes-rule-bayes-rule-qed}
\begin{split}
\frac{p(A_1|B)}{p(A_2|B)} &= \frac{p(B|A_1) \cdot p(A_1) / p(B)}{p(B|A_2) \cdot p(A_2) / p(B)} \\
&= \frac{p(B|A_1)}{p(B|A_2)} \cdot \frac{p(A_1)}{p(A_2)} \; ,
\end{split}
\end{equation}

one obtains the posterior odds ratio as given by the theorem.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2019): "Bayes' theorem"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-06; URL: \url{https://en.wikipedia.org/wiki/Bayes%27_theorem#Bayes%E2%80%99_rule}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P12 | shortcut: bayes-rule | author: JoramSoch | date: 2020-01-06, 20:55.


\pagebreak
\section{Estimation theory}

\subsection{Point estimates}

\subsubsection[\textbf{Partition of the mean squared error into bias and variance}]{Partition of the mean squared error into bias and variance} \label{sec:mse-bnv}

\vspace{1em}
\textbf{Theorem:} The mean squared error ($\rightarrow$ Definition "mse") can be partitioned into variance and squared bias

\begin{equation} \label{eq:mse-bnv-MSE}
\mathrm{MSE}(\hat{\theta}) = \mathrm{Var}(\hat{\theta}) - \mathrm{Bias}(\hat{\theta},\theta)^2
\end{equation}

where the variance ($\rightarrow$ Definition "var") is given by

\begin{equation} \label{eq:mse-bnv-Var}
\mathrm{Var}(\hat{\theta}) = \mathbb{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \mathbb{E}_{\hat{\theta}}(\hat{\theta}) \right)^2 \right]
\end{equation}

and the bias ($\rightarrow$ Definition "bias") is given by

\begin{equation} \label{eq:mse-bnv-Bias}
\mathrm{Bias}(\hat{\theta},\theta) = \left( \mathbb{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The mean squared error (MSE) is defined as ($\rightarrow$ Definition "mse") the expected value ($\rightarrow$ Definition "ev") of the squared deviation of the estimated value $\hat{\theta}$ from the true value $\theta$ of a parameter, over all values $\hat{\theta}$:

\begin{equation} \label{eq:mse-bnv-MSE-def}
\mathrm{MSE}(\hat{\theta}) = \mathbb{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \theta \right)^2 \right] \; .
\end{equation}

This formula can be evaluated in the following way:

\begin{equation} \label{eq:mse-bnv-MSE-ref1}
\begin{split}
\mathrm{MSE}(\hat{\theta}) &= \mathbb{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \theta \right)^2 \right] \\
&= \mathbb{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \mathbb{E}_{\hat{\theta}}(\hat{\theta}) + \mathbb{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right)^2 \right] \\
&= \mathbb{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \mathbb{E}_{\hat{\theta}}(\hat{\theta}) \right)^2 + 2 \left( \hat{\theta} - \mathbb{E}_{\hat{\theta}}(\hat{\theta}) \right) \left( \mathbb{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right) + \left( \mathbb{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right)^2 \right] \\
&= \mathbb{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \mathbb{E}_{\hat{\theta}}(\hat{\theta}) \right)^2 \right] + \mathbb{E}_{\hat{\theta}}\left[ 2 \left( \hat{\theta} - \mathbb{E}_{\hat{\theta}}(\hat{\theta}) \right) \left( \mathbb{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right) \right] + \mathbb{E}_{\hat{\theta}}\left[ \left( \mathbb{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right)^2 \right] \; . \\
\end{split}
\end{equation}

Because $\mathbb{E}_{\hat{\theta}}(\hat{\theta}) - \theta$ is constant as a function of $\hat{\theta}$, we have:

\begin{equation} \label{eq:mse-bnv-MSE-ref2}
\begin{split}
\mathrm{MSE}(\hat{\theta}) &= \mathbb{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \mathbb{E}_{\hat{\theta}}(\hat{\theta}) \right)^2 \right] + 2  \left( \mathbb{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right) \mathbb{E}_{\hat{\theta}}\left[ \hat{\theta} - \mathbb{E}_{\hat{\theta}}(\hat{\theta}) \right] + \left( \mathbb{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right)^2 \\
&= \mathbb{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \mathbb{E}_{\hat{\theta}}(\hat{\theta}) \right)^2 \right] + 2  \left( \mathbb{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right) \left( \mathbb{E}_{\hat{\theta}}(\hat{\theta}) - \mathbb{E}_{\hat{\theta}}(\hat{\theta}) \right) + \left( \mathbb{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right)^2 \\
&= \mathbb{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \mathbb{E}_{\hat{\theta}}(\hat{\theta}) \right)^2 \right] + \left( \mathbb{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right)^2 \; . \\
\end{split}
\end{equation}

This proofs the partition given by \eqref{eq:mse-bnv-MSE}.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2019): "Mean squared error"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2019-11-27; URL: \url{https://en.wikipedia.org/wiki/Mean_squared_error#Proof_of_variance_and_bias_relationship}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P5 | shortcut: mse-bnv | author: JoramSoch | date: 2019-11-27, 14:26.


\pagebreak
\section{Information theory}

\subsection{Discrete mutual information}

\subsubsection[\textbf{Relation to marginal and conditional entropy}]{Relation to marginal and conditional entropy} \label{sec:dmi-mce}

\vspace{1em}
\textbf{Theorem:} Let $X$ and $Y$ be discrete random variables ($\rightarrow$ Definition "rvar") with the joint probability ($\rightarrow$ Definition "jp") $p(x,y)$ for $x \in \mathcal{X}$ and $y \in \mathcal{Y}$. Then, the mutual information ($\rightarrow$ Definition "mi") of $X$ and $Y$ can be expressed as

\begin{equation} \label{eq:dmi-mce-dmi-mce}
\begin{split}
\mathrm{I}(X,Y) &= \mathrm{H}(X) - \mathrm{H}(X|Y) \\
&= \mathrm{H}(Y) - \mathrm{H}(Y|X)
\end{split}
\end{equation}

where $\mathrm{H}(X)$ and $\mathrm{H}(Y)$ are the marginal entropies ($\rightarrow$ Definition "ent-marg") of $X$ and $Y$ and $\mathrm{H}(X \mid Y)$ and $\mathrm{H}(Y \mid X)$ are the conditional entropies ($\rightarrow$ Definition "ent-cond").


\vspace{1em}
\textbf{Proof:} The mutual information ($\rightarrow$ Definition "mi") of $X$ and $Y$ is defined as

\begin{equation} \label{eq:dmi-mce-MI}
\mathrm{I}(X,Y) = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)\,p(y)} \; .
\end{equation}

Separating the logarithm, we have:

\begin{equation} \label{eq:dmi-mce-MI-s1}
\mathrm{I}(X,Y) = \sum_x \sum_y p(x,y) \log \frac{p(x,y)}{p(y)} - \sum_x \sum_y p(x,y) \log p(x) \; .
\end{equation}

Applying the law of conditional probability ($\rightarrow$ Proof "lcp"), i.e. $p(x,y) = p(x \mid y) \, p(y)$, we get:

\begin{equation} \label{eq:dmi-mce-MI-s2}
\mathrm{I}(X,Y) = \sum_x \sum_y p(x|y) \, p(y) \log p(x|y) - \sum_x \sum_y p(x,y) \log p(x) \; .
\end{equation}

Regrouping the variables, we have:

\begin{equation} \label{eq:dmi-mce-MI-s3}
\mathrm{I}(X,Y) = \sum_y p(y) \sum_x p(x|y) \log p(x|y) - \sum_x \left( \sum_y p(x,y) \right) \log p(x) \; .
\end{equation}

Applying the law of marginal probability ($\rightarrow$ Proof "lmp"), i.e. $p(x) = \sum_y p(x,y)$, we get:

\begin{equation} \label{eq:dmi-mce-MI-s4}
\mathrm{I}(X,Y) = \sum_y p(y) \sum_x p(x|y) \log p(x|y) - \sum_x p(x) \log p(x) \; .
\end{equation}

Now considering the definitions of marginal ($\rightarrow$ Definition "ent-marg") and conditional ($\rightarrow$ Definition "ent-cond") entropy

\begin{equation} \label{eq:dmi-mce-ME-CE}
\begin{split}
\mathrm{H}(X) &= - \sum_{x \in \mathcal{X}} p(x) \log p(x) \\
\mathrm{H}(X|Y) &= \sum_{y \in \mathcal{Y}} p(y) \, \mathrm{H}(X|Y=y) \; ,
\end{split}
\end{equation}

we can finally show:

\begin{equation} \label{eq:dmi-mce-MI-qed}
\begin{split}
\mathrm{I}(X,Y) &= - \mathrm{H}(X|Y) + \mathrm{H}(X) \\
&= \mathrm{H}(X) - \mathrm{H}(X|Y) \; .
\end{split}
\end{equation}

The conditioning of $X$ on $Y$ in this proof is without loss of generality. Thus, the proof for the expression using the reverse conditional entropy of $Y$ given $X$ is obtained by simply switching $x$ and $y$ in the derivation.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Mutual information"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-13; URL: \url{https://en.wikipedia.org/wiki/Mutual_information#Relation_to_conditional_and_joint_entropy}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P19 | shortcut: dmi-mce | author: JoramSoch | date: 2020-01-13, 18:20.


\subsubsection[\textbf{Relation to marginal and joint entropy}]{Relation to marginal and joint entropy} \label{sec:dmi-mje}

\vspace{1em}
\textbf{Theorem:} Let $X$ and $Y$ be discrete random variables ($\rightarrow$ Definition "rvar") with the joint probability ($\rightarrow$ Definition "jp") $p(x,y)$ for $x \in \mathcal{X}$ and $y \in \mathcal{Y}$. Then, the mutual information ($\rightarrow$ Definition "mi") of $X$ and $Y$ can be expressed as

\begin{equation} \label{eq:dmi-mje-dmi-mje}
\mathrm{I}(X,Y) = \mathrm{H}(X) + \mathrm{H}(Y) - \mathrm{H}(X,Y)
\end{equation}

where $\mathrm{H}(X)$ and $\mathrm{H}(Y)$ are the marginal entropies ($\rightarrow$ Definition "ent-marg") of $X$ and $Y$ and $\mathrm{H}(X,Y)$ is the joint entropy ($\rightarrow$ Definition "ent-joint").


\vspace{1em}
\textbf{Proof:} The mutual information ($\rightarrow$ Definition "mi") of $X$ and $Y$ is defined as

\begin{equation} \label{eq:dmi-mje-MI}
\mathrm{I}(X,Y) = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)\,p(y)} \; .
\end{equation}

Separating the logarithm, we have:

\begin{equation} \label{eq:dmi-mje-MI-s1}
\mathrm{I}(X,Y) = \sum_x \sum_y p(x,y) \log p(x,y) - \sum_x \sum_y p(x,y) \log p(x) - \sum_x \sum_y p(x,y) \log p(y) \; .
\end{equation}

Regrouping the variables, this reads:

\begin{equation} \label{eq:dmi-mje-MI-s2}
\mathrm{I}(X,Y) = \sum_x \sum_y p(x,y) \log p(x,y) - \sum_x \left( \sum_y p(x,y) \right) \log p(x) - \sum_y \left( \sum_x p(x,y) \right) \log p(y) \; .
\end{equation}

Applying the law of marginal probability ($\rightarrow$ Proof "lmp"), i.e. $p(x) = \sum_y p(x,y)$, we get:

\begin{equation} \label{eq:dmi-mje-MI-s3}
\mathrm{I}(X,Y) = \sum_x \sum_y p(x,y) \log p(x,y) - \sum_x p(x) \log p(x) - \sum_x p(y) \log p(y) \; .
\end{equation}

Now considering the definitions of marginal ($\rightarrow$ Definition "ent-marg") and joint ($\rightarrow$ Definition "ent-joint") entropy

\begin{equation} \label{eq:dmi-mje-ME-JE}
\begin{split}
\mathrm{H}(X) &= - \sum_{x \in \mathcal{X}} p(x) \log p(x) \\
\mathrm{H}(X,Y) &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) \; ,
\end{split}
\end{equation}

we can finally show:

\begin{equation} \label{eq:dmi-mje-MI-qed}
\begin{split}
\mathrm{I}(X,Y) &= - \mathrm{H}(X,Y) + \mathrm{H}(X) + \mathrm{H}(Y) \\
&= \mathrm{H}(X) + \mathrm{H}(Y) - \mathrm{H}(X,Y) \; .
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Mutual information"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-13; URL: \url{https://en.wikipedia.org/wiki/Mutual_information#Relation_to_conditional_and_joint_entropy}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P20 | shortcut: dmi-mje | author: JoramSoch | date: 2020-01-13, 21:53.


\subsubsection[\textbf{Relation to joint and conditional entropy}]{Relation to joint and conditional entropy} \label{sec:dmi-jce}

\vspace{1em}
\textbf{Theorem:} Let $X$ and $Y$ be discrete random variables ($\rightarrow$ Definition "rvar") with the joint probability ($\rightarrow$ Definition "jp") $p(x,y)$ for $x \in \mathcal{X}$ and $y \in \mathcal{Y}$. Then, the mutual information ($\rightarrow$ Definition "mi") of $X$ and $Y$ can be expressed as

\begin{equation} \label{eq:dmi-jce-dmi-jce}
\mathrm{I}(X,Y) = \mathrm{H}(X,Y) - \mathrm{H}(X|Y) - \mathrm{H}(Y|X)
\end{equation}

where $\mathrm{H}(X,Y)$ is the joint entropy ($\rightarrow$ Definition "ent-joint") of $X$ and $Y$ and $\mathrm{H}(X \mid Y)$ and $\mathrm{H}(Y \mid X)$ are the conditional entropies ($\rightarrow$ Definition "ent-cond").


\vspace{1em}
\textbf{Proof:} The existence of the joint probability function ensures that the mutual information ($\rightarrow$ Definition "mi") is defined:

\begin{equation} \label{eq:dmi-jce-MI}
\mathrm{I}(X,Y) = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)\,p(y)} \; .
\end{equation}

The relation of mutual information to conditional entropy ($\rightarrow$ Proof \ref{sec:General Theorems}/\ref{sec:dmi-mce}) is:

\begin{equation} \label{eq:dmi-jce-dmi-mce1}
\mathrm{I}(X,Y) = \mathrm{H}(X) - \mathrm{H}(X|Y)
\end{equation}

\begin{equation} \label{eq:dmi-jce-dmi-mce2}
\mathrm{I}(X,Y) = \mathrm{H}(Y) - \mathrm{H}(Y|X)
\end{equation}

The relation of mutual information to joint entropy ($\rightarrow$ Proof \ref{sec:General Theorems}/\ref{sec:dmi-mje}) is:

\begin{equation} \label{eq:dmi-jce-dmi-mje}
\mathrm{I}(X,Y) = \mathrm{H}(X) + \mathrm{H}(Y) - \mathrm{H}(X,Y) \; .
\end{equation}

It is true that

\begin{equation} \label{eq:dmi-jce-MI-s1}
\mathrm{I}(X,Y) = \mathrm{I}(X,Y) + \mathrm{I}(X,Y) - \mathrm{I}(X,Y) \; .
\end{equation}

Plugging in \eqref{eq:dmi-jce-dmi-mce1}, \eqref{eq:dmi-jce-dmi-mce2} and \eqref{eq:dmi-jce-dmi-mje} on the right-hand side, we have

\begin{equation} \label{eq:dmi-jce-MI-s2}
\begin{split}
\mathrm{I}(X,Y) &= \mathrm{H}(X) - \mathrm{H}(X|Y) + \mathrm{H}(Y) - \mathrm{H}(Y|X) - \mathrm{H}(X) - \mathrm{H}(Y) + \mathrm{H}(X,Y) \\
&= \mathrm{H}(X,Y) - \mathrm{H}(X|Y) - \mathrm{H}(Y|X)
\end{split}
\end{equation}

which proves the identity given above.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Mutual information"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-13; URL: \url{https://en.wikipedia.org/wiki/Mutual_information#Relation_to_conditional_and_joint_entropy}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P21 | shortcut: dmi-jce | author: JoramSoch | date: 2020-01-13, 22:17.




% Chapter 2 %
\chapter{Probability Distributions} \label{sec:Probability Distributions} \newpage

\pagebreak
\section{Univariate discrete distributions}

\subsection{Bernoulli distribution}

\subsubsection[\textbf{Mean}]{Mean} \label{sec:bern-mean}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following a Bernoulli distribution ($\rightarrow$ Definition "bern"):

\begin{equation} \label{eq:bern-mean-bern}
X \sim \mathrm{Bern}(p) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ Definition "ev") of $X$ is

\begin{equation} \label{eq:bern-mean-bern-mean}
\mathrm{E}(X) = p \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The expected value ($\rightarrow$ Definition "ev") is the probability-weighted average of all possible values:

\begin{equation} \label{eq:bern-mean-mean}
\mathrm{E}(X) = \sum_{x \in \mathcal{X}} x \cdot \mathrm{Pr}(X = x) \; .
\end{equation}

Since there are only two possible outcomes for a Bernoulli random variable ($\rightarrow$ Proof "bern-pmf"), we have:

\begin{equation} \label{eq:bern-mean-bern-mean-qed}
\begin{split}
\mathrm{E}(X) &= 0 \cdot \mathrm{Pr}(X = 0) + 1 \cdot \mathrm{Pr}(X = 1) \\
&= 0 \cdot (1-p) + 1 \cdot p \\
&= p \; . \\
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Bernoulli distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-16; URL: \url{https://en.wikipedia.org/wiki/Bernoulli_distribution#Mean}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P22 | shortcut: bern-mean | author: JoramSoch | date: 2020-01-16, 10:58.


\subsection{Binomial distribution}

\subsubsection[\textbf{Mean}]{Mean} \label{sec:bin-mean}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following a binomial distribution ($\rightarrow$ Definition "bin"):

\begin{equation} \label{eq:bin-mean-bin}
X \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ Definition "ev") of $X$ is

\begin{equation} \label{eq:bin-mean-bin-mean}
\mathrm{E}(X) = n p \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} By definition, a binomial random variable ($\rightarrow$ Definition "bin") is the sum of $n$ independent and identical Bernoulli trials ($\rightarrow$ Definition "bern") with success probability $p$. Therefore, the expected value is

\begin{equation} \label{eq:bin-mean-bin-mean-s1}
\mathrm{E}(X) = \mathrm{E}(X_1 + \ldots + X_n)
\end{equation}

and because the expected value is a linear operator ($\rightarrow$ Proof "ev-lin"), this is equal to

\begin{equation} \label{eq:bin-mean-bin-mean-s2}
\begin{split}
\mathrm{E}(X) &= \mathrm{E}(X_1) + \ldots + \mathrm{E}(X_n) \\
&= \sum_{i=1}^{n} \mathrm{E}(X_i) \; .
\end{split}
\end{equation}

With the expected value of the Bernoulli distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:bern-mean}), we have:

\begin{equation} \label{eq:bin-mean-bin-mean-s3}
\mathrm{E}(X) = \sum_{i=1}^{n} p = n p \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-16; URL: \url{https://en.wikipedia.org/wiki/Binomial_distribution#Expected_value_and_variance}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P23 | shortcut: bin-mean | author: JoramSoch | date: 2020-01-16, 11:06.


\pagebreak
\section{Multivariate discrete distributions}

\subsection{Categorical distribution}

\subsubsection[\textbf{Mean}]{Mean} \label{sec:cat-mean}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ Definition "rvec") following a categorical distribution ($\rightarrow$ Definition "cat"):

\begin{equation} \label{eq:cat-mean-cat}
X \sim \mathrm{Cat}(\left[p_1, \ldots, p_k \right]) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ Definition "ev") of $X$ is

\begin{equation} \label{eq:cat-mean-cat-mean}
\mathrm{E}(X) = \left[p_1, \ldots, p_k \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} If we conceive the outcome of a categorical distribution ($\rightarrow$ Definition "cat-pmf") to be a $1 \times k$ vector, then the elementary row vectors $e_1 = \left[1, 0, \ldots, 0 \right]$, ..., $e_k = \left[0, \ldots, 0, 1 \right]$ are all the possible outcomes and they occur with probabilities $\mathrm{Pr}(X = e_1) = p_1$, ..., $\mathrm{Pr}(X = e_k) = p_k$. Consequently, the expected value ($\rightarrow$ Definition "ev") is

\begin{equation} \label{eq:cat-mean-cat-mean-qed}
\begin{split}
\mathrm{E}(X) &= \sum_{x \in \mathcal{X}} x \cdot \mathrm{Pr}(X = x) \\
&= \sum_{i=1}^k e_i \cdot \mathrm{Pr}(X = e_i) \\
&= \sum_{i=1}^k e_i \cdot p_i \\
&= \left[p_1, \ldots, p_k \right] \; .
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P24 | shortcut: cat-mean | author: JoramSoch | date: 2020-01-16, 11:17.


\subsection{Multinomial distribution}

\subsubsection[\textbf{Mean}]{Mean} \label{sec:mult-mean}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ Definition "rvec") following a multinomial distribution ($\rightarrow$ Definition "mult"):

\begin{equation} \label{eq:mult-mean-mult}
X \sim \mathrm{Mult}(n,\left[p_1, \ldots, p_k \right]) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ Definition "ev") of $X$ is

\begin{equation} \label{eq:mult-mean-bin-mean}
\mathrm{E}(X) = \left[n p_1, \ldots, n p_k \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} By definition, a multinomial random variable ($\rightarrow$ Definition "mult") is the sum of $n$ independent and identical categorical trials ($\rightarrow$ Definition "cat") with category probabilities $p_1, \ldots, p_k$. Therefore, the expected value is

\begin{equation} \label{eq:mult-mean-mult-mean-s1}
\mathrm{E}(X) = \mathrm{E}(X_1 + \ldots + X_n)
\end{equation}

and because the expected value is a linear operator ($\rightarrow$ Proof "ev-lin"), this is equal to

\begin{equation} \label{eq:mult-mean-mult-mean-s2}
\begin{split}
\mathrm{E}(X) &= \mathrm{E}(X_1) + \ldots + \mathrm{E}(X_n) \\
&= \sum_{i=1}^{n} \mathrm{E}(X_i) \; .
\end{split}
\end{equation}

With the expected value of the categorical distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:cat-mean}), we have:

\begin{equation} \label{eq:mult-mean-mult-mean-s3}
\mathrm{E}(X) = \sum_{i=1}^{n} \left[p_1, \ldots, p_k \right] = n \cdot \left[p_1, \ldots, p_k \right] = \left[n p_1, \ldots, n p_k \right] \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P25 | shortcut: mult-mean | author: JoramSoch | date: 2020-01-16, 11:26.


\pagebreak
\section{Univariate continuous distributions}

\subsection{Continuous uniform distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:cuni}

\vspace{1em}
\textbf{Definition:} Let $X$ be a continuous random variable ($\rightarrow$ Definition "rvar"). Then, $X$ is said to be uniformly distributed with minimum $a$ and maximum $b$

\begin{equation} \label{eq:cuni-cuni}
X \sim \mathcal{U}(a, b) \; ,
\end{equation}

if and only if each value between and including $a$ and $b$ occurs with the same probability.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Uniform distribution (continuous)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-27; URL: \url{https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: D3 | shortcut: cuni | author: JoramSoch | date: 2020-01-27, 14:05.


\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:cuni-pdf}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following a continuous uniform distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:cuni}):

\begin{equation} \label{eq:cuni-pdf-cuni}
X \sim \mathcal{U}(a, b) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ Definition "pdf") of $X$ is

\begin{equation} \label{eq:cuni-pdf-cuni-pdf}
f_X(x) = \left\{
\begin{array}{rl}
\frac{1}{b-a} \; , & \text{if} \; a \leq x \leq b \\
0 \; , & \text{otherwise} \; .
\end{array}
\right.
\end{equation}


\vspace{1em}
\textbf{Proof:} A continuous uniform variable is defined as ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:cuni}) having a constant probability density between minimum $a$ and maximum $b$. Therefore,

\begin{equation} \label{eq:cuni-pdf-cuni-pdf-s1}
\begin{split}
f_X(x) &\propto 1 \quad \text{for all} \quad x \in [a,b] \quad \text{and} \\
f_X(x) &= 0, \quad\!\! \text{if} \quad x < a \quad \text{or} \quad x > b \; .
\end{split}
\end{equation}

To ensure that $f_X(x)$ is a proper probability density function ($\rightarrow$ Definition "pdf"), the integral over all non-zero probabilities has to sum to $1$. Therefore,

\begin{equation} \label{eq:cuni-pdf-cuni-pdf-s2}
f_X(x) = \frac{1}{c(a,b)} \quad \text{for all} \quad x \in [a,b]
\end{equation}

where the normalization factor $c(a,b)$ is specified, such that

\begin{equation} \label{eq:cuni-pdf-cuni-pdf-s3}
\frac{1}{c(a,b)} \int_{a}^{b} 1 \, \mathrm{d}x = 1 \; .
\end{equation}

Solving this for $c(a,b)$, we obtain:

\begin{equation} \label{eq:cuni-pdf-cuni-pdf-s4}
\begin{split}
\int_{a}^{b} 1 \, \mathrm{d}x &= c(a,b) \\
[x]_a^b &= c(a,b) \\
c(a,b) &= b-a \; .
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P37 | shortcut: cuni-pdf | author: JoramSoch | date: 2020-01-31, 15:41.


\subsubsection[\textbf{Cumulative distribution function}]{Cumulative distribution function} \label{sec:cuni-cdf}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following a continuous uniform distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:cuni}):

\begin{equation} \label{eq:cuni-cdf-cuni}
X \sim \mathcal{U}(a, b) \; .
\end{equation}

Then, the cumulative distribution function ($\rightarrow$ Definition "cdf") of $X$ is

\begin{equation} \label{eq:cuni-cdf-cuni-cdf}
F_X(x) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < a \\
\frac{x-a}{b-a} \; , & \text{if} \; a \leq x \leq b \\
1 \; , & \text{if} \; x > b \; .
\end{array}
\right.
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density function of the continuous uniform distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:cuni-pdf}) is:

\begin{equation} \label{eq:cuni-cdf-cuni-pdf}
\mathcal{U}(z; a, b) = \left\{
\begin{array}{rl}
\frac{1}{b-a} \; , & \text{if} \; a \leq x \leq b \\
0 \; , & \text{otherwise} \; .
\end{array}
\right.
\end{equation}

Thus, the cumulative distribution function ($\rightarrow$ Definition "cdf") is:

\begin{equation} \label{eq:cuni-cdf-cuni-cdf-s1}
F_X(x) = \int_{-\infty}^{x} \mathcal{U}(z; a, b) \, \mathrm{d}z
\end{equation}

First of all, if $x < a$, we have

\begin{equation} \label{eq:cuni-cdf-cuni-cdf-s2a}
F_X(x) = \int_{-\infty}^{x} 0 \, \mathrm{d}z = 0 \; .
\end{equation}

Moreover, if $a \leq x \leq b$, we have using \eqref{eq:cuni-cdf-cuni-pdf}

\begin{equation} \label{eq:cuni-cdf-cuni-cdf-s2b}
\begin{split}
F_X(x) &= \int_{-\infty}^{a} \mathcal{U}(z; a, b) \, \mathrm{d}z + \int_{a}^{x} \mathcal{U}(z; a, b) \, \mathrm{d}z \\
&= \int_{-\infty}^{a} 0 \, \mathrm{d}z + \int_{a}^{x} \frac{1}{b-a} \, \mathrm{d}z \\
&= 0 + \frac{1}{b-a} [z]_a^x \\
&= \frac{x-a}{b-a} \; .
\end{split}
\end{equation}

Finally, if $x > b$, we have

\begin{equation} \label{eq:cuni-cdf-cuni-cdf-s2c}
\begin{split}
F_X(x) &= \int_{-\infty}^{b} \mathcal{U}(z; a, b) \, \mathrm{d}z + \int_{b}^{x} \mathcal{U}(z; a, b) \, \mathrm{d}z \\
&= F_X(b) + \int_{b}^{x} 0 \, \mathrm{d}z \\
&= \frac{b-a}{b-a} + 0 \\
&= 1 \; .
\end{split}
\end{equation}

This completes the proof.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P38 | shortcut: cuni-cdf | author: JoramSoch | date: 2020-01-02, 18:05.


\subsubsection[\textbf{Quantile function}]{Quantile function} \label{sec:cuni-qf}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following a continuous uniform distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:cuni}):

\begin{equation} \label{eq:cuni-qf-cuni}
X \sim \mathcal{U}(a, b) \; .
\end{equation}

Then, the quantile function ($\rightarrow$ Definition "qf") of $X$ is

\begin{equation} \label{eq:cuni-qf-cuni-qf}
Q_X(p) = bp + a(1-p) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The cumulative distribution function of the continuous uniform distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:cuni-cdf}) is:

\begin{equation} \label{eq:cuni-qf-cuni-cdf}
F_X(x) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < a \\
\frac{x-a}{b-a} \; , & \text{if} \; a \leq x \leq b \\
1 \; , & \text{if} \; x > b \; .
\end{array}
\right.
\end{equation}

Thus, the quantile function ($\rightarrow$ Definition "qf") is:

\begin{equation} \label{eq:cuni-qf-cuni-qf-s1}
Q_X(p) = F_X^{-1}(x) \; .
\end{equation}

This can be derived by rearranging equation \eqref{eq:cuni-qf-cuni-cdf}:

\begin{equation} \label{eq:cuni-qf-cuni-cdf-s2}
\begin{split}
p &= \frac{x-a}{b-a} \\
x &= p(b-a) + a \\
x &= bp + a(1-p) = Q_X(p) \; .
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P39 | shortcut: cuni-qf | author: JoramSoch | date: 2020-01-02, 18:27.


\subsection{Normal distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:norm}

\vspace{1em}
\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar"). Then, $X$ is said to be normally distributed with mean $\mu$ and variance $\sigma^2$ (or, standard deviation $\sigma$)

\begin{equation} \label{eq:norm-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; ,
\end{equation}

if and only if its probability density function ($\rightarrow$ Definition "pdf") is given by

\begin{equation} \label{eq:norm-norm-pdf}
\mathcal{N}(x; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right]
\end{equation}

where $\mu \in \mathbb{R}$ and $\sigma^2 > 0$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-27; URL: \url{https://en.wikipedia.org/wiki/Normal_distribution}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: D4 | shortcut: norm | author: JoramSoch | date: 2020-01-27, 14:15.


\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:norm-pdf}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following a normal distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-pdf-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ Definition "pdf") of $X$ is

\begin{equation} \label{eq:norm-pdf-norm-pdf}
f_X(x) = \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the normal distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:norm}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P33 | shortcut: norm-pdf | author: JoramSoch | date: 2020-01-27, 15:15.


\subsubsection[\textbf{Mean}]{Mean} \label{sec:norm-mean}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following a normal distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-mean-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ Definition "ev") of $X$ is

\begin{equation} \label{eq:norm-mean-norm-mean}
\mathrm{E}(X) = \mu \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The expected value ($\rightarrow$ Definition "ev") is the probability-weighted average over all possible values:

\begin{equation} \label{eq:norm-mean-mean}
\mathrm{E}(X) = \int_{\mathbb{R}} x \cdot f_X(x) \, \mathrm{d}x \; .
\end{equation}

With the probability density function of the normal distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}), this reads:

\begin{equation} \label{eq:norm-mean-norm-mean-s1}
\begin{split}
\mathrm{E}(X) &= \int_{-\infty}^{+\infty} x \cdot \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \, \mathrm{d}x \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{+\infty} x \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \, \mathrm{d}x \; .
\end{split}
\end{equation}

Substituting $z = x -\mu$, we have:

\begin{equation} \label{eq:norm-mean-norm-mean-s2}
\begin{split}
\mathrm{E}(X) &= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty-\mu}^{+\infty-\mu} (z + \mu) \cdot \exp \left[ -\frac{1}{2} \left( \frac{z}{\sigma} \right)^2 \right] \, \mathrm{d}(z + \mu) \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{+\infty} (z + \mu) \cdot \exp \left[ -\frac{1}{2} \left( \frac{z}{\sigma} \right)^2 \right] \, \mathrm{d}z \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \left( \int_{-\infty}^{+\infty} z \cdot \exp \left[ -\frac{1}{2} \left( \frac{z}{\sigma} \right)^2 \right] \, \mathrm{d}z + \mu \int_{-\infty}^{+\infty} \exp \left[ -\frac{1}{2} \left( \frac{z}{\sigma} \right)^2 \right] \, \mathrm{d}z \right) \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \left( \int_{-\infty}^{+\infty} z \cdot \exp \left[ -\frac{1}{2 \sigma^2} \cdot z^2 \right] \, \mathrm{d}z + \mu \int_{-\infty}^{+\infty} \exp \left[ -\frac{1}{2 \sigma^2} \cdot z^2 \right] \, \mathrm{d}z \right) \; .
\end{split}
\end{equation}

The general antiderivatives are

\begin{equation} \label{eq:norm-mean-exp-erf-anti-der}
\begin{split}
\int x \cdot \exp \left[ -a x^2 \right] \mathrm{d}x &= -\frac{1}{2a} \cdot \exp \left[ -a x^2 \right] \\
\int \exp \left[ -a x^2 \right] \mathrm{d}x &= \frac{1}{2} \sqrt{\frac{\pi}{a}} \cdot \mathrm{erf} \left[ \sqrt{a} x \right]
\end{split}
\end{equation}

where $\mathrm{erf}(x)$ is the error function. Using this, the integrals can be calculated as:

\begin{equation} \label{eq:norm-mean-norm-mean-s3}
\begin{split}
\mathrm{E}(X) &= \frac{1}{\sqrt{2 \pi} \sigma} \left( \left[ -\sigma^2 \cdot \exp \left[ -\frac{1}{2 \sigma^2} \cdot z^2 \right] \right]_{-\infty}^{+\infty} + \mu \left[ \sqrt{\frac{\pi}{2}} \sigma \cdot \mathrm{erf} \left[ \frac{1}{\sqrt{2} \sigma} z \right] \right]_{-\infty}^{+\infty} \right) \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \left( \left[ \lim_{z \to \infty} \left( -\sigma^2 \cdot \exp \left[ -\frac{1}{2 \sigma^2} \cdot z^2 \right] \right) - \lim_{z \to -\infty} \left( -\sigma^2 \cdot \exp \left[ -\frac{1}{2 \sigma^2} \cdot z^2 \right] \right) \right] \right. \\
&\hphantom{\sqrt{2 \pi}\sigma \;} + \mu \left. \left[ \lim_{z \to \infty} \left( \sqrt{\frac{\pi}{2}} \sigma \cdot \mathrm{erf} \left[ \frac{1}{\sqrt{2} \sigma} z \right] \right) - \lim_{z \to -\infty} \left( \sqrt{\frac{\pi}{2}} \sigma \cdot \mathrm{erf} \left[ \frac{1}{\sqrt{2} \sigma} z \right] \right) \right] \right) \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \left( [0 - 0] + \mu \left[ \sqrt{\frac{\pi}{2}} \sigma - \left(- \sqrt{\frac{\pi}{2}} \sigma \right) \right] \right) \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \cdot \mu \cdot 2 \sqrt{\frac{\pi}{2}} \sigma \\
&= \mu \; .
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Papadopoulos, Alecos (2013): "How to derive the mean and variance of Gaussian random variable?"; in: \textit{StackExchange Mathematics}; URL: \url{https://math.stackexchange.com/questions/518281/how-to-derive-the-mean-and-variance-of-a-gaussian-random-variable}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P15 | shortcut: norm-mean | author: JoramSoch | date: 2020-01-09, 15:04.


\subsubsection[\textbf{Median}]{Median} \label{sec:norm-med}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following a normal distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-med-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the median ($\rightarrow$ Definition "med") of $X$ is

\begin{equation} \label{eq:norm-med-norm-median}
\mathrm{median}(X) = \mu \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The median ($\rightarrow$ Definition "med") is the value at which the cumulative distribution function ($\rightarrow$ Definition "cdf") is $1/2$:

\begin{equation} \label{eq:norm-med-median}
F_X(\mathrm{median}(X)) = \frac{1}{2} \; .
\end{equation}

The cumulative distribution function of the normal distribution ($\rightarrow$ Proof "norm-cdf") is

\begin{equation} \label{eq:norm-med-norm-cdf}
F_X(x) = \frac{1}{2} \left[ 1 + \mathrm{erf} \left( \frac{x-\mu}{\sqrt{2}\sigma} \right) \right]
\end{equation}

where $\mathrm{erf}(x)$ is the error function. Thus, the inverse CDF is

\begin{equation} \label{eq:norm-med-norm-cdf-inv}
x = \sqrt{2}\sigma \cdot \mathrm{erf}^{-1}(2p-1) + \mu
\end{equation}

where $\mathrm{erf}^{-1}(x)$ is the inverse error function. Setting $p = 1/2$, we obtain:

\begin{equation} \label{eq:norm-med-norm-med-qed}
\mathrm{median}(X) = \sqrt{2}\sigma \cdot \mathrm{erf}^{-1}(0) + \mu = \mu \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P16 | shortcut: norm-med | author: JoramSoch | date: 2020-01-09, 15:33.


\subsubsection[\textbf{Mode}]{Mode} \label{sec:norm-mode}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following a normal distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-mode-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the mode ($\rightarrow$ Definition "mode") of $X$ is

\begin{equation} \label{eq:norm-mode-norm-mode}
\mathrm{mode}(X) = \mu \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The mode ($\rightarrow$ Definition "mode") is the value which maximizes the probability density function ($\rightarrow$ Definition "pdf"):

\begin{equation} \label{eq:norm-mode-mode}
\mathrm{mode}(X) = \operatorname*{arg\,max}_x f_X(x) \; .
\end{equation}

The probability density function of the normal distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) is:

\begin{equation} \label{eq:norm-mode-norm-pdf}
f_X(x) = \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \; .
\end{equation}

The first two deriatives of this function are:

\begin{equation} \label{eq:norm-mode-norm-pdf-der1}
f'_X(x) = \frac{\mathrm{d}f_X(x)}{\mathrm{d}x} = \frac{1}{\sqrt{2 \pi} \sigma^3} \cdot (-x + \mu) \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right]
\end{equation}

\begin{equation} \label{eq:norm-mode-norm-pdf-der2}
f''_X(x) = \frac{\mathrm{d}^2f_X(x)}{\mathrm{d}x^2} = -\frac{1}{\sqrt{2 \pi} \sigma^3} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] + \frac{1}{\sqrt{2 \pi} \sigma^5} \cdot (-x + \mu)^2 \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \; .
\end{equation}

We now calculate the root of the first derivative \eqref{eq:norm-mode-norm-pdf-der1}:

\begin{equation} \label{eq:norm-mode-norm-mode-s1}
\begin{split}
f'_X(x) = 0 &= \frac{1}{\sqrt{2 \pi} \sigma^3} \cdot (-x + \mu) \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \\
0 &= -x + \mu \\
x &= \mu \; .
\end{split}
\end{equation}

By plugging this value into the second deriative \eqref{eq:norm-mode-norm-pdf-der2},

\begin{equation} \label{eq:norm-mode-norm-mode-s2}
\begin{split}
f''_X(\mu) &= -\frac{1}{\sqrt{2 \pi} \sigma^3} \cdot \exp(0) + \frac{1}{\sqrt{2 \pi} \sigma^5} \cdot (0)^2 \cdot \exp(0) \\
&= -\frac{1}{\sqrt{2 \pi} \sigma^3} < 0 \; ,
\end{split}
\end{equation}

we confirm that it is in fact a maximum which shows that

\begin{equation} \label{eq:norm-mode-norm-mode-qed}
\mathrm{mode}(X) = \mu \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P17 | shortcut: norm-mode | author: JoramSoch | date: 2020-01-09, 15:58.


\subsubsection[\textbf{Variance}]{Variance} \label{sec:norm-var}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following a normal distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-var-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the variance ($\rightarrow$ Definition "var") of $X$ is

\begin{equation} \label{eq:norm-var-norm-mode}
\mathrm{Var}(X) = \sigma^2 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ Definition "var") is the probability-weighted average of the squared deviation from the mean ($\rightarrow$ Definition "ev"):

\begin{equation} \label{eq:norm-var-var}
\mathrm{Var}(X) = \int_{\mathbb{R}} (x - \mathrm{E}(X))^2 \cdot f_\mathrm{X}(x) \, \mathrm{d}x \; .
\end{equation}

With the expected value ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:norm-mean}) and probability density function ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) of the normal distribution, this reads:

\begin{equation} \label{eq:norm-var-norm-var-s1}
\begin{split}
\mathrm{Var}(X) &= \int_{-\infty}^{+\infty} (x - \mu)^2 \cdot \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \, \mathrm{d}x \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{+\infty} (x - \mu)^2 \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \, \mathrm{d}x \; .
\end{split}
\end{equation}

Substituting $z = x -\mu$, we have:

\begin{equation} \label{eq:norm-var-norm-var-s2}
\begin{split}
\mathrm{Var}(X) &= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty-\mu}^{+\infty-\mu} z^2 \cdot \exp \left[ -\frac{1}{2} \left( \frac{z}{\sigma} \right)^2 \right] \, \mathrm{d}(z + \mu) \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{+\infty} z^2 \cdot \exp \left[ -\frac{1}{2} \left( \frac{z}{\sigma} \right)^2 \right] \, \mathrm{d}z \; .
\end{split}
\end{equation}

Now substituting $z = \sqrt{2} \sigma x$, we have:

\begin{equation} \label{eq:norm-var-norm-var-s3}
\begin{split}
\mathrm{Var}(X) &= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{+\infty} (\sqrt{2} \sigma x)^2 \cdot \exp \left[ -\frac{1}{2} \left( \frac{\sqrt{2} \sigma x}{\sigma} \right)^2 \right] \, \mathrm{d}(\sqrt{2} \sigma x) \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \cdot 2 \sigma^2 \cdot \sqrt{2} \sigma \int_{-\infty}^{+\infty} x^2 \cdot \exp \left[ -x^2 \right] \, \mathrm{d}x \\
&= \frac{2 \sigma^2}{\sqrt{\pi}} \int_{-\infty}^{+\infty} x^2 \cdot e^{-x^2} \, \mathrm{d}x \; .
\end{split}
\end{equation}

Since the integrand is symmetric with respect to $x = 0$, we can write:

\begin{equation} \label{eq:norm-var-norm-var-s4}
\mathrm{Var}(X) = \frac{4 \sigma^2}{\sqrt{\pi}} \int_{0}^{\infty} x^2 \cdot e^{-x^2} \, \mathrm{d}x \; .
\end{equation}

If we define $z = x^2$, then $x = \sqrt{z}$ and $\mathrm{d}x = 1/2 \, z^{-1/2} \, \mathrm{d}z$. Substituting this into the integral

\begin{equation} \label{eq:norm-var-norm-var-s5}
\mathrm{Var}(X) = \frac{4 \sigma^2}{\sqrt{\pi}} \int_{0}^{\infty} z \cdot e^{-z} \cdot \frac{1}{2} z^{-\frac{1}{2}} \, \mathrm{d}z = \frac{2 \sigma^2}{\sqrt{\pi}} \int_{0}^{\infty} z^{\frac{3}{2}-1} \cdot e^{-z} \, \mathrm{d}z
\end{equation}

and using the definition of the gamma function

\begin{equation} \label{eq:norm-var-gam-fct}
\Gamma(x) = \int_{0}^{\infty} z^{x-1} \cdot e^{-z} \, \mathrm{d}z \; ,
\end{equation}

we can finally show that

\begin{equation} \label{eq:norm-var-norm-var-s6}
\mathrm{Var}(X) = \frac{2 \sigma^2}{\sqrt{\pi}} \cdot \Gamma\!\left(\frac{3}{2}\right) = \frac{2 \sigma^2}{\sqrt{\pi}} \cdot \frac{\sqrt{\pi}}{2} = \sigma^2 \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Papadopoulos, Alecos (2013): "How to derive the mean and variance of Gaussian random variable?"; in: \textit{StackExchange Mathematics}; URL: \url{https://math.stackexchange.com/questions/518281/how-to-derive-the-mean-and-variance-of-a-gaussian-random-variable}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P18 | shortcut: norm-var | author: JoramSoch | date: 2020-01-09, 22:47.


\subsection{Gamma distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:gam}

**Definition**: Let $X$ be a random variable ($\rightarrow$ Definition "rvar"). Then, $X$ is said to follow a gamma distribution with shape $a$ and rate $b$

\begin{equation} \label{eq:gam-gam}
X \sim \mathrm{Gam}(a, b) \; ,
\end{equation}

if and only if its probability density function ($\rightarrow$ Definition "pdf") is given by

\begin{equation} \label{eq:gam-gam-pdf}
\mathrm{Gam}(x; a, b) = \frac{b^a}{\Gamma(a)} x^{a-1} \exp[-b x], \quad x > 0
\end{equation}

where $a > 0$ and $b > 0$, and the density is zero, if $x \leq 0$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Koch, Karl-Rudolf (2007): "Gamma Distribution"; in: \textit{Introduction to Bayesian Statistics}, Springer, Berlin/Heidelberg, 2007, p. 47, eq. 2.172; URL: \url{https://www.springer.com/de/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: D7 | shortcut: gam | author: JoramSoch | date: 2020-02-08, 23:29.


\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:gam-pdf}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a positive random variable ($\rightarrow$ Definition "rvar") following a gamma distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:gam-pdf-gam}
X \sim \mathrm{Gam}(a, b) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ Definition "pdf") of $X$ is

\begin{equation} \label{eq:gam-pdf-gam-pdf}
f_X(x) = \frac{b^a}{\Gamma(a)} x^{a-1} \exp[-b x] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the gamma distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:gam}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P45 | shortcut: gam-pdf | author: JoramSoch | date: 2020-02-08, 23:41.


\subsection{Exponential distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:exp}

**Definition**: Let $X$ be a random variable ($\rightarrow$ Definition "rvar"). Then, $X$ is said to be exponentially distributed with rate (or, inverse scale) $\lambda$

\begin{equation} \label{eq:exp-exp}
X \sim \mathrm{Exp}(\lambda) \; ,
\end{equation}

if and only if its probability density function ($\rightarrow$ Definition "pdf") is given by

\begin{equation} \label{eq:exp-exp-pdf}
\mathrm{Exp}(x; \lambda) = \lambda \exp[-\lambda x], \quad x \geq 0
\end{equation}

where $\lambda > 0$, and the density is zero, if $x < 0$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Exponential distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-08; URL: \url{https://en.wikipedia.org/wiki/Exponential_distribution#Definitions}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: D8 | shortcut: exp | author: JoramSoch | date: 2020-02-08, 23:48.


\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:exp-pdf}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a non-negative random variable ($\rightarrow$ Definition "rvar") following an exponential distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation} \label{eq:exp-pdf-exp}
X \sim \mathrm{Exp}(\lambda) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ Definition "pdf") of $X$ is

\begin{equation} \label{eq:exp-pdf-gam-pdf}
f_X(x) = \lambda \exp[-\lambda x] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the exponential distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:exp}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P46 | shortcut: exp-pdf | author: JoramSoch | date: 2020-02-08, 23:53.


\subsubsection[\textbf{Cumulative distribution function}]{Cumulative distribution function} \label{sec:exp-cdf}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following an exponential distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation} \label{eq:exp-cdf-exp}
X \sim \mathrm{Exp}(\lambda) \; .
\end{equation}

Then, the cumulative distribution function ($\rightarrow$ Definition "cdf") of $X$ is

\begin{equation} \label{eq:exp-cdf-exp-cdf}
F_X(x) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < 0 \\
1 - \exp[-\lambda x] \; , & \text{if} \; x \geq 0 \; .
\end{array}
\right.
\end{equation}


\vspace{1em}
\textbf{Proof:}  The probability density function of the exponential distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:exp-pdf}) is:

\begin{equation} \label{eq:exp-cdf-exp-pdf}
\mathrm{Exp}(x; \lambda) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < 0 \\
\lambda \exp[-\lambda x] \; , & \text{if} \; x \geq 0 \; .
\end{array}
\right.
\end{equation}

Thus, the cumulative distribution function ($\rightarrow$ Definition "cdf") is:

\begin{equation} \label{eq:exp-cdf-exp-cdf-s1}
F_X(x) = \int_{-\infty}^{x} \mathrm{Exp}(z; \lambda) \, \mathrm{d}z \; .
\end{equation}

If $x < 0$, we have:

\begin{equation} \label{eq:exp-cdf-exp-cdf-s2a}
F_X(x) = \int_{-\infty}^{x} 0 \, \mathrm{d}z = 0 \; .
\end{equation}

If $x \geq 0$, we have using \eqref{eq:exp-cdf-exp-pdf}:

\begin{equation} \label{eq:exp-cdf-exp-cdf-s2b}
\begin{split}
F_X(x) &= \int_{-\infty}^{0} \mathrm{Exp}(z; \lambda) \, \mathrm{d}z + \int_{0}^{x} \mathrm{Exp}(z; \lambda) \, \mathrm{d}z \\
&= \int_{-\infty}^{0} 0 \, \mathrm{d}z + \int_{0}^{x} \lambda \exp[-\lambda z] \, \mathrm{d}z \\
&= 0 + \lambda \left[ -\frac{1}{\lambda} \exp[-\lambda z] \right]_{0}^{x} \\
&= \lambda \left[ \left( -\frac{1}{\lambda} \exp[-\lambda x] \right) - \left( -\frac{1}{\lambda} \exp[-\lambda \cdot 0] \right) \right] \\
&= 1 - \exp[-\lambda x] \; .
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P48 | shortcut: exp-cdf | author: JoramSoch | date: 2020-02-11, 14:48.


\subsubsection[\textbf{Quantile function}]{Quantile function} \label{sec:exp-qf}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following an exponential distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation} \label{eq:exp-qf-exp}
X \sim \mathrm{Exp}(\lambda) \; .
\end{equation}

Then, the quantile function ($\rightarrow$ Definition "qf") of $X$ is

\begin{equation} \label{eq:exp-qf-exp-qf}
Q_X(p) = -\frac{\ln(1-p)}{\lambda} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The cumulative distribution function of the exponential distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:exp-cdf}) is:

\begin{equation} \label{eq:exp-qf-exp-cdf}
F_X(x) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < 0 \\
1 - \exp[-\lambda x] \; , & \text{if} \; x \geq 0 \; .
\end{array}
\right.
\end{equation}

Thus, the quantile function ($\rightarrow$ Definition "qf") is:

\begin{equation} \label{eq:exp-qf-exp-qf-s1}
Q_X(p) = F_X^{-1}(x) \; .
\end{equation}

This can be derived by rearranging equation \eqref{eq:exp-qf-exp-cdf}:

\begin{equation} \label{eq:exp-qf-exp-qf-s2}
\begin{split}
p &= 1 - \exp[-\lambda x] \\
\exp[-\lambda x] &= 1-p \\
-\lambda x &= \ln(1-p) \\
x &= -\frac{\ln(1-p)}{\lambda} \; .
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P50 | shortcut: exp-qf | author: JoramSoch | date: 2020-02-12, 15:48.


\subsubsection[\textbf{Mean}]{Mean} \label{sec:exp-mean}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following an exponential distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation} \label{eq:exp-mean-exp}
X \sim \mathrm{Exp}(\lambda) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ Definition "ev") of $X$ is

\begin{equation} \label{eq:exp-mean-exp-mean}
\mathrm{E}(X) = \frac{1}{\lambda} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The expected value ($\rightarrow$ Definition "ev") is the probability-weighted average over all possible values:

\begin{equation} \label{eq:exp-mean-mean}
\mathrm{E}(X) = \int_{\mathbb{R}} x \cdot f_\mathrm{X}(x) \, \mathrm{d}x \; .
\end{equation}

With the probability density function of the exponential distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:exp-pdf}), this reads:

\begin{equation} \label{eq:exp-mean-exp-mean-s1}
\begin{split}
\mathrm{E}(X) &= \int_{0}^{+\infty} x \cdot \lambda \exp(-\lambda x) \, \mathrm{d}x \\
&= \lambda \int_{0}^{+\infty} x \cdot \exp(-\lambda x) \, \mathrm{d}x \; .
\end{split}
\end{equation}

Using the following anti-deriative

\begin{equation} \label{eq:exp-mean-exp-mean-s2}
\int x \cdot \exp(-\lambda x) \, \mathrm{d}x = \left( - \frac{1}{\lambda} x - \frac{1}{\lambda^2} \right) \exp(-\lambda x) \; ,
\end{equation}

the expected value becomes

\begin{equation} \label{eq:exp-mean-exp-mean-s3}
\begin{split}
\mathrm{E}(X) &= \lambda \left[ \left( - \frac{1}{\lambda} x - \frac{1}{\lambda^2} \right) \exp(-\lambda x) \right]_{0}^{+\infty} \\
&= \lambda \left[ \lim_{x \to \infty} \left( - \frac{1}{\lambda} x - \frac{1}{\lambda^2} \right) \exp(-\lambda x) - \left( - \frac{1}{\lambda} \cdot 0 - \frac{1}{\lambda^2} \right) \exp(-\lambda \cdot 0) \right] \\
&= \lambda \left[ 0 + \frac{1}{\lambda^2} \right] \\
&= \frac{1}{\lambda} \; .
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Koch, Karl-Rudolf (2007): "Expected Value"; in: \textit{Introduction to Bayesian Statistics}, Springer, Berlin/Heidelberg, 2007, p. 39, eq. 2.142a; URL: \url{https://www.springer.com/de/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P47 | shortcut: exp-mean | author: JoramSoch | date: 2020-02-10, 21:57.


\subsubsection[\textbf{Median}]{Median} \label{sec:exp-med}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following an exponential distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation} \label{eq:exp-med-exp}
X \sim \mathrm{Exp}(\lambda) \; .
\end{equation}

Then, the median ($\rightarrow$ Definition "med") of $X$ is

\begin{equation} \label{eq:exp-med-exp-med}
\mathrm{median}(X) = \frac{\ln 2}{\lambda} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The median ($\rightarrow$ Definition "med") is the value at which the cumulative distribution function ($\rightarrow$ Definition "cdf") is $1/2$:

\begin{equation} \label{eq:exp-med-median}
F_X(\mathrm{median}(X)) = \frac{1}{2} \; .
\end{equation}

The cumulative distribution function of the exponential distribution ($\rightarrow$ Definition "exp-cdf") is

\begin{equation} \label{eq:exp-med-exp-cdf}
F_X(x) = 1 - \exp[-\lambda x], \quad x \geq 0 \; .
\end{equation}

Thus, the inverse CDF is

\begin{equation} \label{eq:exp-med-exp-cdf-inv}
x = -\frac{\ln(1-p)}{\lambda}
\end{equation}

and setting $p = 1/2$, we obtain:

\begin{equation} \label{eq:exp-med-exp-med-qed}
\mathrm{median}(X) = -\frac{\ln(1-\frac{1}{2})}{\lambda} = \frac{\ln 2}{\lambda} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P49 | shortcut: exp-med | author: JoramSoch | date: 2020-02-11, 15:03.


\subsubsection[\textbf{Mode}]{Mode} \label{sec:exp-mode}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ Definition "rvar") following an exponential distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation} \label{eq:exp-mode-exp}
X \sim \mathrm{Exp}(\lambda) \; .
\end{equation}

Then, the mode ($\rightarrow$ Definition "mode") of $X$ is

\begin{equation} \label{eq:exp-mode-exp-mode}
\mathrm{mode}(X) = 0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:}  The mode ($\rightarrow$ Definition "mode") is the value which maximizes the probability density function ($\rightarrow$ Definition "pdf"):

\begin{equation} \label{eq:exp-mode-mode}
\mathrm{mode}(X) = \operatorname*{arg\,max}_x f_X(x) \; .
\end{equation}

The probability density function of the exponential distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:exp-pdf}) is:

\begin{equation} \label{eq:exp-mode-exp-pdf}
f_X(x) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < 0 \\
\lambda \exp[-\lambda x] \; , & \text{if} \; x \geq 0 \; .
\end{array}
\right.
\end{equation}

Since

\begin{equation} \label{eq:exp-mode-exp-pdf-eq0}
\lim_{x \to 0} f_X(x) = \infty
\end{equation}

and

\begin{equation} \label{eq:exp-mode-exp-pdf-neq0}
f_X(x) < \infty \quad \text{for any} \quad x \neq 0 \; ,
\end{equation}

it follows that

\begin{equation} \label{eq:exp-mode-exp-mode-qed}
\mathrm{mode}(X) = 0 \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P51 | shortcut: exp-mode | author: JoramSoch | date: 2020-02-12, 15:53.


\pagebreak
\section{Multivariate continuous distributions}

\subsection{Multivariate normal distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:mvn}

\vspace{1em}
\textbf{Definition:} Let $X$ be an $n \times 1$ random vector ($\rightarrow$ Definition "rvec"). Then, $X$ is said to be multivariate normally distributed with mean $\mu$ and covariance $\Sigma$

\begin{equation} \label{eq:mvn-mvn}
X \sim \mathcal{N}(\mu, \Sigma) \; ,
\end{equation}

if and only if its probability density function ($\rightarrow$ Definition "pdf") is given by

\begin{equation} \label{eq:mvn-mvn-pdf}
\mathcal{N}(x; \mu, \Sigma) = \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right]
\end{equation}

where $\mu$ is an $n \times 1$ real vector and $\Sigma$ is an $n \times n$ positive definite matrix.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Koch KR (2007): "Multivariate Normal Distribution"; in: \textit{Introduction to Bayesian Statistics}, ch. 2.5.1, pp. 51-53, eq. 2.195; URL: \url{https://www.springer.com/gp/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: D1 | shortcut: mvn | author: JoramSoch | date: 2020-01-22, 05:20.


\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:mvn-pdf}

\vspace{1em}
\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ Definition "rvec") following a multivariate normal distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:mvn}):

\begin{equation} \label{eq:mvn-pdf-mvn}
X \sim \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ Definition "pdf") of $X$ is

\begin{equation} \label{eq:mvn-pdf-mvn-pdf}
f_X(x) = \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the multivariate normal distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:mvn}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P34 | shortcut: mvn-pdf | author: JoramSoch | date: 2020-01-27, 15:23.


\subsubsection[\textbf{Linear transformation theorem}]{Linear transformation theorem} \label{sec:mvn-ltt}

\vspace{1em}
\textbf{Theorem:} Let $x$ follow a multivariate normal distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:mvn}):

\begin{equation} \label{eq:mvn-ltt-mvn}
x \sim \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Then, any linear transformation of $x$ is also multivariate normally distributed:

\begin{equation} \label{eq:mvn-ltt-mvn-lt}
y = Ax + b \sim \mathcal{N}(A\mu + b, A \Sigma A^\mathrm{T}) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The moment-generating function of a random vector ($\rightarrow$ Definition \ref{sec:General Theorems}/\ref{sec:mgf}) $x$ is

\begin{equation} \label{eq:mvn-ltt-vect-mgf}
M_x(t) = \mathbb{E} \left( \exp \left[ t^\mathrm{T} x \right] \right)
\end{equation}

and therefore the moment-generating function of the random vector $y$ is given by

\begin{equation} \label{eq:mvn-ltt-y-mgf-s1}
\begin{split}
M_y(t) &= \mathbb{E} \left( \exp \left[ t^\mathrm{T} (Ax + b) \right] \right) \\
&= \mathbb{E} \left( \exp \left[ t^\mathrm{T} A x \right] \cdot \exp \left[ t^\mathrm{T} b \right] \right) \\
&= \exp \left[ t^\mathrm{T} b \right] \cdot \mathbb{E} \left( \exp \left[ t^\mathrm{T} A x \right] \right) \\
&= \exp \left[ t^\mathrm{T} b \right] \cdot M_x(At) \; .
\end{split}
\end{equation}

The moment-generating function of the multivariate normal distribution ($\rightarrow$ Proof "mvn-mgf") is

\begin{equation} \label{eq:mvn-ltt-mvn-mgf}
M_x(t) = \exp \left[ t^\mathrm{T} \mu + \frac{1}{2} t^\mathrm{T} \Sigma t \right]
\end{equation}

and therefore the moment-generating function of the random vector $y$ becomes

\begin{equation} \label{eq:mvn-ltt-y-mgf-s2}
\begin{split}
M_y(t) &= \exp \left[ t^\mathrm{T} b \right] \cdot M_x(At) \\
&= \exp \left[ t^\mathrm{T} b \right] \cdot \exp \left[ t^\mathrm{T} A \mu + \frac{1}{2} t^\mathrm{T} A \Sigma A^\mathrm{T} t \right] \\
&= \exp \left[ t^\mathrm{T} \left( A \mu + b \right) + \frac{1}{2} t^\mathrm{T} A \Sigma A^\mathrm{T} t \right] \; .
\end{split}
\end{equation}

Because moment-generating function and probability density function of a random variable are equivalent, this demonstrates that $y$ is following a multivariate normal distribution with mean $A \mu + b$ and covariance $A \Sigma A^\mathrm{T}$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2010): "Linear combinations of normal random variables"; in: \textit{Lectures on probability and statistics}; URL: \url{https://www.statlect.com/probability-distributions/normal-distribution-linear-combinations}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P1 | shortcut: mvn-ltt | author: JoramSoch | date: 2019-08-27, 12:14.


\subsubsection[\textbf{Marginal distributions}]{Marginal distributions} \label{sec:mvn-marg}

\vspace{1em}
\textbf{Theorem:} Let $x$ follow a multivariate normal distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:mvn}):

\begin{equation} \label{eq:mvn-marg-mvn}
x \sim \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Then, the marginal distribution ($\rightarrow$ Definition "md") of any subset vector $x_s$ is also a multivariate normal distribution

\begin{equation} \label{eq:mvn-marg-mvn-marg}
x_s \sim \mathcal{N}(\mu_s, \Sigma_s)
\end{equation}

where $\mu_s$ drops the irrelevant variables (the ones not in the subset, i.e. marginalized out) from the mean vector $\mu$ and $\Sigma_s$ drops the corresponding rows and columns from the covariance matrix $\Sigma$.


\vspace{1em}
\textbf{Proof:} Define an $m \times n$ subset matrix $S$ such that $s_{ij} = 1$, if the $j$-th element in $\mu_s$ corresponds to the $i$-th element in $x$, and $s_{ij} = 0$ otherwise. Then,

\begin{equation} \label{eq:mvn-marg-xs}
x_s = S x
\end{equation}

and we can apply the linear transformation theorem ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}) to give

\begin{equation} \label{eq:mvn-marg-mvn-marg-qed}
x_s \sim \mathcal{N}(S \mu, S \Sigma S^\mathrm{T}) \; .
\end{equation}

Finally, we see that $S \mu = \mu_s$ and $S \Sigma S^\mathrm{T} = \Sigma_s$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P35 | shortcut: mvn-marg | author: JoramSoch | date: 2020-01-29, 15:12.


\subsection{Normal-gamma distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:ng}

**Definition**: Let $X$ be an $n \times 1$ random vector ($\rightarrow$ Definition "rvec") and let $Y$ be a positive random variable ($\rightarrow$ Definition "rvar"). Then, $X$ and $Y$ are said to follow a normal-gamma distribution

\begin{equation} \label{eq:ng-ng}
X,Y \sim \mathrm{NG}(\mu, \Lambda, a, b) \; ,
\end{equation}

if and only if their joint probability ($\rightarrow$ Definition "jp") density function ($\rightarrow$ Definition "pdf") is given by

\begin{equation} \label{eq:ng-ng-pdf}
f_{X,Y}(x,y) = \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \cdot \mathrm{Gam}(y; a, b)
\end{equation}

where $\mathcal{N}(x; \mu, \Sigma)$ is the probability density function of the multivariate normal distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}) with mean $\mu$ and covariance $\Sigma$ and $\mathrm{Gam}(x; a, b)$ is the probability density function of the gamma distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}) with shape $a$ and rate $b$. The $n \times n$ matrix $\Lambda$ is referred to as the precision matrix of the normal-gamma distribution.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Koch KR (2007): "Normal-Gamma Distribution"; in: \textit{Introduction to Bayesian Statistics}, ch. 2.5.3, pp. 55-56, eq. 2.212; URL: \url{https://www.springer.com/gp/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: D5 | shortcut: ng | author: JoramSoch | date: 2020-01-27, 14:28.


\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:ng-pdf}

\vspace{1em}
\textbf{Theorem:} Let $x$ and $y$ follow a normal-gamma distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:ng}):

\begin{equation} \label{eq:ng-pdf-ng}
x,y \sim \mathrm{NG}(\mu, \Lambda, a, b) \; .
\end{equation}

Then, the joint probability ($\rightarrow$ Definition "jp") density function ($\rightarrow$ Definition "pdf") of $x$ and $y$ is

\begin{equation} \label{eq:ng-pdf-ng-pdf}
p(x,y) = \sqrt{\frac{|\Lambda|}{(2 \pi)^n}} \frac{b^a}{\Gamma(a)} \cdot y^{a+\frac{n}{2}-1} \exp \left[ -\frac{y}{2} \left( (x-\mu)^\mathrm{T} \Lambda (x-\mu) + 2b \right) \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density of the normal-gamma distribution is defined as ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:ng}) as the product of a multivariate normal distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:mvn}) over $x$ conditional on $y$ and a univariate gamma distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:gam}) over $y$:

\begin{equation} \label{eq:ng-pdf-ng-pdf-w1}
p(x,y) = \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \cdot \mathrm{Gam}(y; a, b)
\end{equation}

With the probability density function of the multivariate normal distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}) and the probability density function of the gamma distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), this becomes:

\begin{equation} \label{eq:ng-pdf-ng-pdf-s2}
p(x,y) = \sqrt{\frac{|y \Lambda|}{(2 \pi)^n}} \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} (y \Lambda) (x-\mu) \right] \cdot \frac{b^a}{\Gamma(a)} y^{a-1} \exp\left[-by\right] \; .
\end{equation}

Using the relation $\lvert y A \rvert = y^n \lvert A \rvert$ for an $n \times n$ matrix $A$ and rearranging the terms, we have:

\begin{equation} \label{eq:ng-pdf-ng-pdf-qed}
p(x,y) = \sqrt{\frac{|\Lambda|}{(2 \pi)^n}} \frac{b^a}{\Gamma(a)} \cdot y^{a+\frac{n}{2}-1} \exp \left[ -\frac{y}{2} \left( (x-\mu)^\mathrm{T} \Lambda (x-\mu) + 2b \right) \right] \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Koch KR (2007): "Normal-Gamma Distribution"; in: \textit{Introduction to Bayesian Statistics}, ch. 2.5.3, pp. 55-56, eq. 2.212; URL: \url{https://www.springer.com/gp/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P44 | shortcut: ng-pdf | author: JoramSoch | date: 2020-02-07, 20:44.


\subsubsection[\textbf{Kullback-Leibler divergence}]{Kullback-Leibler divergence} \label{sec:ng-kl}

\vspace{1em}
\textbf{Theorem:} Let $x \in \mathbb{R}^k$ be a random vector ($\rightarrow$ Definition "rvec") and $y > 0$ be a random variable ($\rightarrow$ Definition "rvar"). Assume two normal-gamma distributions ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:ng}) $P$ and $Q$ specifying the joint distribution of $x$ and $y$ as

\begin{equation} \label{eq:ng-kl-NGs}
\begin{split}
P: \; (x,y) &\sim \mathrm{NG}(\mu_1, \Lambda_1^{-1}, a_1, b_1) \\
Q: \; (x,y) &\sim \mathrm{NG}(\mu_2, \Lambda_2^{-1}, a_2, b_2) \; . \\
\end{split}
\end{equation}

Then, the Kullback-Leibler divergence ($\rightarrow$ Definition "kl") of $P$ from $Q$ is given by

\begin{equation} \label{eq:ng-kl-NG-KL}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \frac{1}{2} \frac{a_1}{b_1} \left[ (\mu_2 - \mu_1)^T \Lambda_2 (\mu_2 - \mu_1) \right] + \frac{1}{2} \, \mathrm{tr}(\Lambda_2 \Lambda_1^{-1}) - \frac{1}{2} \ln \frac{|\Lambda_2|}{|\Lambda_1|} - \frac{k}{2} \\
&+ a_2 \, \ln \frac{b_1}{b_2} - \ln \frac{\Gamma(a_1)}{\Gamma(a_2)} + (a_1 - a_2) \, \psi(a_1) - (b_1 - b_2) \, \frac{a_1}{b_1} \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} The probabibility density function of the normal-gamma distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:ng-pdf}) is

\begin{equation} \label{eq:ng-kl-NG-pdf}
p(x,y) = p(x|y) \cdot p(y) = \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \cdot \mathrm{Gam}(y; a, b)
\end{equation}

where $\mathcal{N}(x; \mu, \Sigma)$ is a multivariate normal density with mean $\mu$ and covariance $\Sigma$ (hence, precision $\Lambda$) and $\mathrm{Gam}(y; a, b)$ is a univariate gamma density with shape $a$ and rate $b$. The Kullback-Leibler divergence of the multivariate normal distribution ($\rightarrow$ Proof "mvn-kl") is

\begin{equation} \label{eq:ng-kl-mvn-KL}
\mathrm{KL}[P\,||\,Q] = \frac{1}{2} \left[ (\mu_2 - \mu_1)^T \Sigma_2^{-1} (\mu_2 - \mu_1) + \mathrm{tr}(\Sigma_2^{-1} \Sigma_1) - \ln \frac{|\Sigma_1|}{|\Sigma_2|} - k \right]
\end{equation}

and the Kullback-Leibler divergence of the univariate gamma distribution ($\rightarrow$ Proof "gam-kl") is

\begin{equation} \label{eq:ng-kl-gam-KL}
\mathrm{KL}[P\,||\,Q] = a_2 \, \ln \frac{b_1}{b_2} - \ln \frac{\Gamma(a_1)}{\Gamma(a_2)} + (a_1 - a_2) \, \psi(a_1) - (b_1 - b_2) \, \frac{a_1}{b_1}
\end{equation}

where $\Gamma(x)$ is the gamma function and $\psi(x)$ is the digamma function.

\vspace{1em}
The KL divergence for a continuous random variable ($\rightarrow$ Definition "kl") is given by 

\begin{equation} \label{eq:ng-kl-KL-cont}
\mathrm{KL}[P\,||\,Q] = \int_{\mathcal{Z}} p(z) \, \ln \frac{p(z)}{q(z)} \, \mathrm{d}z
\end{equation}

which, applied to the normal-gamma distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:ng}) over $x$ and $y$, yields

\begin{equation} \label{eq:ng-kl-NG-KL0}
\mathrm{KL}[P\,||\,Q] = \int_{0}^{\infty} \int_{\mathbb{R}^k} p(x,y) \, \ln \frac{p(x,y)}{q(x,y)} \, \mathrm{d}x \, \mathrm{d}y \; .
\end{equation}

Using the law of conditional probability ($\rightarrow$ Proof "lcp"), this can be evaluated as follows:

\begin{equation} \label{eq:ng-kl-NG-KL1}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \int_{0}^{\infty} \int_{\mathbb{R}^k} p(x|y) \, p(y) \, \ln \frac{p(x|y) \, p(y)}{q(x|y) \, q(y)} \, \mathrm{d}x \, \mathrm{d}y \\
&= \int_{0}^{\infty} \int_{\mathbb{R}^k} p(x|y)\, p(y) \, \ln \frac{p(x|y)}{q(x|y)} \, \mathrm{d}x \, \mathrm{d}y + \int_{0}^{\infty} \int_{\mathbb{R}^k} p(x|y)\, p(y) \, \ln \frac{p(y)}{q(y)} \, \mathrm{d}x \, \mathrm{d}y \\
&= \int_{0}^{\infty} p(y) \int_{\mathbb{R}^k} p(x|y) \, \ln \frac{p(x|y)}{q(x|y)} \, \mathrm{d}x \, \mathrm{d}y + \int_{0}^{\infty} p(y) \, \ln \frac{p(y)}{q(y)} \int_{\mathbb{R}^k} p(x|y) \, \mathrm{d}x \, \mathrm{d}y \\
&= \left\langle \mathrm{KL}[p(x|y)\,||\,q(x|y)] \right\rangle_{p(y)} + \mathrm{KL}[p(y)\,||\,q(y)] \; .
\end{split}
\end{equation}

In other words, the KL divergence between two normal-gamma distributions over $x$ and $y$ is equal to the sum of a multivariate normal KL divergence regarding $x$ conditional on $y$, expected over $y$, and a univariate gamma KL divergence regarding $y$.

\vspace{1em}
From equations \eqref{eq:ng-kl-NG-pdf} and \eqref{eq:ng-kl-mvn-KL}, the first term becomes

\begin{equation} \label{eq:ng-kl-exp-mvn-KL-s1}
\begin{split}
&\left\langle \mathrm{KL}[p(x|y)\,||\,q(x|y)] \right\rangle_{p(y)} \\
&= \left\langle \frac{1}{2} \left[ (\mu_2 - \mu_1)^T (y \Lambda_2) (\mu_2 - \mu_1) + \mathrm{tr}\left( (y \Lambda_2) (y \Lambda_1)^{-1} \right) - \ln \frac{|(y \Lambda_1)^{-1}|}{|(y \Lambda_2)^{-1}|} - k \right] \right\rangle_{p(y)} \\
&= \left\langle \frac{y}{2} (\mu_2 - \mu_1)^T \Lambda_2 (\mu_2 - \mu_1) + \frac{1}{2} \, \mathrm{tr}(\Lambda_2 \Lambda_1^{-1}) - \frac{1}{2} \ln \frac{|\Lambda_2|}{|\Lambda_1|} - \frac{k}{2} \right\rangle_{p(y)} \\
\end{split}
\end{equation}

and using the relation ($\rightarrow$ Proof "gam-mean") $y \sim \mathrm{Gam}(a,b) \Rightarrow \left\langle y \right\rangle = a/b$, we have

\begin{equation} \label{eq:ng-kl-exp-mvn-KL-s2}
\begin{split}
\left\langle \mathrm{KL}[p(x|y)\,||\,q(x|y)] \right\rangle_{p(y)} = \frac{1}{2} \frac{a_1}{b_1} (\mu_2 - \mu_1)^T \Lambda_2 (\mu_2 - \mu_1) + \frac{1}{2} \, \mathrm{tr}(\Lambda_2 \Lambda_1^{-1}) - \frac{1}{2} \ln \frac{|\Lambda_2|}{|\Lambda_1|} - \frac{k}{2} \; .
\end{split}
\end{equation}

By plugging \eqref{eq:ng-kl-exp-mvn-KL-s2} and \eqref{eq:ng-kl-gam-KL} into \eqref{eq:ng-kl-NG-KL1}, one arrives at the KL divergence given by \eqref{eq:ng-kl-NG-KL}.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Soch \& Allefeld (2016): "Kullback-Leibler Divergence for the Normal-Gamma Distribution"; in: \textit{arXiv math.ST}, 1611.01437; URL: \url{https://arxiv.org/abs/1611.01437}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P6 | shortcut: ng-kl | author: JoramSoch | date: 2019-12-06, 09:35.


\subsubsection[\textbf{Marginal distributions}]{Marginal distributions} \label{sec:ng-marg}

\vspace{1em}
\textbf{Theorem:} Let $x$ and $y$ follow a normal-gamma distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:ng}):

\begin{equation} \label{eq:ng-marg-ng}
x,y \sim \mathrm{NG}(\mu, \Lambda, a, b) \; .
\end{equation}

Then, the marginal distribution ($\rightarrow$ Definition "md") of $y$ is a gamma distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:gam})

\begin{equation} \label{eq:ng-marg-ng-marg-y}
y \sim \mathrm{Gam}(a, b)
\end{equation}

and the marginal distribution ($\rightarrow$ Definition "md") of $x$ is a multivariate t-distribution ($\rightarrow$ Definition "mvt")

\begin{equation} \label{eq:ng-marg-ng-marg-x}
x \sim \mathrm{t}\left( \mu, \left(\frac{a}{b} \Lambda \right)^{-1}, 2a \right) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density function of the normal-gamma distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:ng-pdf}) is given by

\begin{equation} \label{eq:ng-marg-ng-pdf}
\begin{split}
p(x,y) &= p(x|y) \cdot p(y) \\
p(x|y) &= \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \\
p(y) &= \mathrm{Gam}(y; a, b) \; .
\end{split}
\end{equation}

\vspace{1em}
Using the law of marginal probability ($\rightarrow$ Proof "lmp"), the marginal distribution of $y$ can be derived as

\begin{equation} \label{eq:ng-marg-ng-marg-y-qed}
\begin{split}
p(y) &= \int p(x,y) \, \mathrm{d}x \\
&= \int \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \, \mathrm{Gam}(y; a, b) \, \mathrm{d}x \\
&= \mathrm{Gam}(y; a, b) \int \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \, \mathrm{d}x \\
&= \mathrm{Gam}(y; a, b)
\end{split}
\end{equation}

which is the probability density function of the gamma distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}) with shape parameter $a$ and rate parameter $b$.

\vspace{1em}
Using the law of marginal probability ($\rightarrow$ Proof "lmp"), the marginal distribution of $x$ can be derived as

\begin{equation} \label{eq:ng-marg-ng-marg-x-qed}
\begin{split}
p(x) &= \int p(x,y) \, \mathrm{d}y \\
&= \int \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \, \mathrm{Gam}(y; a, b) \, \mathrm{d}y \\
&= \int \sqrt{\frac{|y \Lambda|}{\sqrt{(2 \pi)^n}}} \, \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} (y \Lambda) (x-\mu) \right] \cdot \frac{b^a}{\Gamma(a)} \, y^{a-1} \exp[-b y] \, \mathrm{d}y \\
&= \int \sqrt{\frac{y^n |\Lambda|}{\sqrt{(2 \pi)^n}}} \, \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} (y \Lambda) (x-\mu) \right] \cdot \frac{b^a}{\Gamma(a)} \, y^{a-1} \exp[-b y] \, \mathrm{d}y \\
&= \int \sqrt{\frac{|\Lambda|}{\sqrt{(2 \pi)^n}}} \cdot \frac{b^a}{\Gamma(a)} \cdot y^{a+\frac{n}{2}-1} \cdot \exp \left[ -\left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right) y \right] \mathrm{d}y \\
&= \int \sqrt{\frac{|\Lambda|}{\sqrt{(2 \pi)^n}}} \cdot \frac{b^a}{\Gamma(a)} \cdot \frac{\Gamma\left( a+\frac{n}{2} \right)}{\left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{a+\frac{n}{2}}} \cdot \mathrm{Gam}\left( y; a+\frac{n}{2}, b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right) \mathrm{d}y \\
&= \sqrt{\frac{|\Lambda|}{\sqrt{(2 \pi)^n}}} \cdot \frac{b^a}{\Gamma(a)} \cdot \frac{\Gamma\left( a+\frac{n}{2} \right)}{\left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{a+\frac{n}{2}}} \int \mathrm{Gam}\left( y; a+\frac{n}{2}, b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right) \mathrm{d}y \\
&= \sqrt{\frac{|\Lambda|}{\sqrt{(2 \pi)^n}}} \cdot \frac{b^a}{\Gamma(a)} \cdot \frac{\Gamma\left( a+\frac{n}{2} \right)}{\left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{a+\frac{n}{2}}} \\
&= \frac{\sqrt{|\Lambda|}}{(2 \pi)^\frac{n}{2}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot b^a \cdot \left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{-\left( a+\frac{n}{2} \right)} \\
&= \frac{\sqrt{|\Lambda|}}{\pi^\frac{n}{2}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot \left( \frac{1}{b} \right)^{-a} \cdot \left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{-a} \cdot 2^{-\frac{n}{2}} \cdot \left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{-\frac{n}{2}} \\
&= \frac{\sqrt{|\Lambda|}}{\pi^\frac{n}{2}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot \left( 1 + \frac{1}{2b} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{-a} \cdot \left( 2b + (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{-\frac{n}{2}} \\
&= \frac{\sqrt{|\Lambda|}}{\pi^\frac{n}{2}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot \left( \frac{1}{2a} \right)^{-a} \cdot \left( 2a + (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-a} \cdot \left( \frac{b}{a} \right)^{-\frac{n}{2}} \cdot \left( 2a + (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-\frac{n}{2}} \\
&= \frac{\sqrt{\left( \frac{a}{b} \right)^n |\Lambda|}}{(2a)^{-a}\,\pi^\frac{n}{2}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot \left( 2a + (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-a} \cdot \left( 2a + (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-\frac{n}{2}} \\
&= \frac{\sqrt{\left( \frac{a}{b} \right)^n |\Lambda|}}{(2a)^{-a}\,\pi^\frac{n}{2}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot (2a)^{-a} \cdot \left( 1 + \frac{1}{2a} (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-a} \cdot (2a)^{-\frac{n}{2}} \cdot \left( 1 + \frac{1}{2a} (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-\frac{n}{2}} \\
&= \frac{\sqrt{\left( \frac{a}{b} \right)^n |\Lambda|}}{(2a)^\frac{n}{2}\,\pi^\frac{n}{2}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot \left( 1 + \frac{1}{2a} (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-\frac{2a+n}{2}} \\
&= \sqrt{\frac{\left| \frac{a}{b}\,\Lambda \right|}{(2a\,\pi)^n}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot \left( 1 + \frac{1}{2a} (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-\frac{2a+n}{2}} \\
\end{split}
\end{equation}

which is the probability density function of a multivariate t-distribution ($\rightarrow$ Proof "mvt-pdf") with mean vector $\mu$, shape matrix $\left( \frac{a}{b}\Lambda \right)^{-1}$ and $2a$ degrees of freedom.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P36 | shortcut: ng-marg | author: JoramSoch | date: 2020-01-29, 21:42.


\pagebreak
\section{Matrix-variate continuous distributions}

\subsection{Matrix-normal distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:matn}

**Definition**: Let $X$ be an $n \times p$ random matrix ($\rightarrow$ Definition "rmat"). Then, $X$ is said to be matrix-normally distributed with mean $M$, covariance across rows $U$ and covariance across columns $V$

\begin{equation} \label{eq:matn-matn}
X \sim \mathcal{MN}(M, U, V) \; ,
\end{equation}

if and only if its probability density function ($\rightarrow$ Definition "pdf") is given by

\begin{equation} \label{eq:matn-matn-pdf}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( V^{-1} (X-M)^\mathrm{T} \, U^{-1} (X-M) \right) \right]
\end{equation}

where $\mu$ is an $n \times p$ real matrix, $U$ is an $n \times n$ positive definite matrix and $V$ is a $p \times p$ positive definite matrix.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Matrix normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-27; URL: \url{https://en.wikipedia.org/wiki/Matrix_normal_distribution#Definition}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: D6 | shortcut: matn | author: JoramSoch | date: 2020-01-27, 14:37.


\subsubsection[\textbf{Equivalence to multivariate normal distribution}]{Equivalence to multivariate normal distribution} \label{sec:matn-mvn}

\vspace{1em}
\textbf{Theorem:} The matrix $X$ is matrix-normally distributed ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:matn})

\begin{equation} \label{eq:matn-mvn-matn}
X \sim \mathcal{MN}(M, U, V) \; ,
\end{equation}

if and only if $\mathrm{vec}(X)$ is multivariate normally distributed ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:mvn})

\begin{equation} \label{eq:matn-mvn-mvn}
\mathrm{vec}(X) \sim \mathcal{MN}(\mathrm{vec}(M), V \otimes U)
\end{equation}

where $\mathrm{vec}(X)$ is the vectorization operator and $\otimes$ is the Kronecker product.


\vspace{1em}
\textbf{Proof:} The probability density function of the matrix-normal distribution ($\rightarrow$ Proof "matn-pdf") with $n \times p$ mean $M$, $n \times n$ covariance across rows $U$ and $p \times p$ covariance across columns $V$ is

\begin{equation} \label{eq:matn-mvn-matn-pdf}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( V^{-1} (X-M)^\mathrm{T} \, U^{-1} (X-M) \right) \right] \; .
\end{equation}

Using the trace property $\mathrm{tr}(ABC) = \mathrm{tr}(BCA)$, we have:

\begin{equation} \label{eq:matn-mvn-matn-mvn-s1}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( (X-M)^\mathrm{T} \, U^{-1} (X-M) \, V^{-1} \right) \right] \; .
\end{equation}

Using the trace-vectorization relation $\mathrm{tr}(A^\mathrm{T} B) = \mathrm{vec}(A)^\mathrm{T} \, \mathrm{vec}(B)$, we have:

\begin{equation} \label{eq:matn-mvn-matn-mvn-s2}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{vec}(X-M)^\mathrm{T} \, \mathrm{vec}\left( U^{-1} (X-M) \, V^{-1} \right) \right] \; .
\end{equation}

Using the vectorization-Kronecker relation $\mathrm{vec}(ABC) = \left( C^\mathrm{T} \otimes A \right) \mathrm{vec}(B)$, we have:

\begin{equation} \label{eq:matn-mvn-matn-mvn-s3}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{vec}(X-M)^\mathrm{T} \, \left( V^{-1} \otimes U^{-1} \right) \mathrm{vec}(X-M) \right] \; .
\end{equation}

Using the Kronecker product property $\left( A^{-1} \otimes B^{-1} \right) = \left( A \otimes B \right)^{-1}$, we have:

\begin{equation} \label{eq:matn-mvn-matn-mvn-s4}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{vec}(X-M)^\mathrm{T} \, \left( V \otimes U \right)^{-1} \mathrm{vec}(X-M) \right] \; .
\end{equation}

Using the vectorization property $\mathrm{vec}(A+B) = \mathrm{vec}(A) + \mathrm{vec}(B)$, we have:

\begin{equation} \label{eq:matn-mvn-matn-mvn-s5}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \left[ \mathrm{vec}(X) - \mathrm{vec}(M) \right]^\mathrm{T} \, \left( V \otimes U \right)^{-1} \left[ \mathrm{vec}(X) - \mathrm{vec}(M) \right] \right] \; .
\end{equation}

Using the Kronecker-determinant relation $\lvert A \otimes B \rvert = \lvert A \rvert^m \lvert B \rvert^n$, we have:

\begin{equation} \label{eq:matn-mvn-matn-mvn-s6}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V \otimes U|}} \cdot \exp\left[-\frac{1}{2} \left[ \mathrm{vec}(X) - \mathrm{vec}(M) \right]^\mathrm{T} \, \left( V \otimes U \right)^{-1} \left[ \mathrm{vec}(X) - \mathrm{vec}(M) \right] \right] \; .
\end{equation}

This is the probability density function of the multivariate normal distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}) with the $np \times 1$ mean vector $\mathrm{vec}(M)$ and the $np \times np$ covariance matrix $V \otimes U$:

\begin{equation} \label{eq:matn-mvn-matn-mvn}
\mathcal{MN}(X; M, U, V) = \mathcal{N}(\mathrm{vec}(X); \mathrm{vec}(M), V \otimes U) \; .
\end{equation}

By showing that the probability density functions ($\rightarrow$ Definition "pdf") are identical, it is proven that the associated probability distributions ($\rightarrow$ Definition "pd") are equivalent.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Matrix normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-20; URL: \url{https://en.wikipedia.org/wiki/Matrix_normal_distribution#Proof}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P26 | shortcut: matn-mvn | author: JoramSoch | date: 2020-01-20, 21:09.




% Chapter 3 %
\chapter{Statistical Models} \label{sec:Statistical Models} \newpage

\pagebreak
\section{Normal data}

\subsection{Multiple linear regression}

\subsubsection[\textbf{Ordinary least squares (1)}]{Ordinary least squares (1)} \label{sec:mlr-ols}

\vspace{1em}
\textbf{Theorem:} Given a linear regression model ($\rightarrow$ Definition "mlr") with independent observations

\begin{equation} \label{eq:mlr-ols-MLR}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; ,
\end{equation}

the parameters minimizing the residual sum of squares ($\rightarrow$ Definition "rss") are given by

\begin{equation} \label{eq:mlr-ols-OLS}
\hat{\beta} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Let $\hat{\beta}$ be the ordinary least squares (OLS) solution and let $\hat{\varepsilon} = y - X\hat{\beta}$ be the resulting vector of residuals. Then, this vector must be orthogonal to the design matrix,

\begin{equation} \label{eq:mlr-ols-X-e-orth}
X^\mathrm{T} \hat{\varepsilon} = 0 \; ,
\end{equation}

because if it wasn't, there would be another solution $\tilde{\beta}$ giving another vector $\tilde{\varepsilon}$ with a smaller residual sum of squares. From \eqref{eq:mlr-ols-X-e-orth}, the OLS formula can be directly derived:

\begin{equation} \label{eq:mlr-ols-OLS-proof}
\begin{split}
X^\mathrm{T} \hat{\varepsilon} &= 0 \\
X^\mathrm{T} \left( y - X\hat{\beta} \right) &= 0 \\
X^\mathrm{T} y - X^\mathrm{T} X\hat{\beta} &= 0 \\
X^\mathrm{T} X\hat{\beta} &= X^\mathrm{T} y \\
\hat{\beta} &= (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \; .
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Stephan, Klaas Enno (2010): "The General Linear Model (GLM)"; in: \textit{Methods and models for fMRI data analysis in neuroeconomics}; URL: \url{http://www.socialbehavior.uzh.ch/teaching/methodsspring10.html}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P2 | shortcut: mlr-ols | author: JoramSoch | date: 2019-09-27, 07:18.


\subsubsection[\textbf{Ordinary least squares (2)}]{Ordinary least squares (2)} \label{sec:mlr-ols2}

\vspace{1em}
\textbf{Theorem:} Given a linear regression model ($\rightarrow$ Definition "mlr") with independent observations

\begin{equation} \label{eq:mlr-ols2-MLR}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; ,
\end{equation}

the parameters minimizing the residual sum of squares ($\rightarrow$ Definition "rss") are given by

\begin{equation} \label{eq:mlr-ols2-OLS}
\hat{\beta} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The residual sum of squares ($\rightarrow$ Definition "rss") is defined as

\begin{equation} \label{eq:mlr-ols2-RSS}
\mathrm{RSS}(\beta) = \sum_{i=1}^n \varepsilon_i = \varepsilon^\mathrm{T} \varepsilon = (y-X\beta)^\mathrm{T} (y-X\beta)
\end{equation}

which can be developed into

\begin{equation} \label{eq:mlr-ols2-RSS-dev}
\begin{split}
\mathrm{RSS}(\beta) &= y^\mathrm{T} y - y^\mathrm{T} X \beta - \beta^\mathrm{T} X^\mathrm{T} y + \beta^\mathrm{T} X^\mathrm{T} X \beta \\
&= y^\mathrm{T} y - 2 \beta^\mathrm{T} X^\mathrm{T} y + \beta^\mathrm{T} X^\mathrm{T} X \beta \; .
\end{split}
\end{equation}

The derivative of $\mathrm{RSS}(\beta)$ with respect to $\beta$ is

\begin{equation} \label{eq:mlr-ols2-RSS-der}
\frac{\mathrm{d}\mathrm{RSS}(\beta)}{\mathrm{d}\beta} = - 2 X^\mathrm{T} y + 2 X^\mathrm{T} X \beta
\end{equation}

and setting this deriative to zero, we obtain:

\begin{equation} \label{eq:mlr-ols2-OLS-qed}
\begin{split}
\frac{\mathrm{d}\mathrm{RSS}(\hat{\beta})}{\mathrm{d}\beta} &= 0 \\
0 &= - 2 X^\mathrm{T} y + 2 X^\mathrm{T} X \hat{\beta} \\
X^\mathrm{T} X \hat{\beta} &= X^\mathrm{T} y \\
\hat{\beta} &= (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \; .
\end{split}
\end{equation}

Since the quadratic form $y^\mathrm{T} y$ in \eqref{eq:mlr-ols2-RSS-dev} is positive, $\hat{\beta}$ minimizes $\mathrm{RSS}(\beta)$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Proofs involving ordinary least squares"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-03; URL: \url{https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares#Least_squares_estimator_for_%CE%B2}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P40 | shortcut: mlr-ols2 | author: JoramSoch | date: 2020-02-03, 18:43.


\subsection{Bayesian linear regression}

\subsubsection[\textbf{Conjugate prior distribution}]{Conjugate prior distribution} \label{sec:blr-prior}

\vspace{1em}
\textbf{Theorem:} Let

\begin{equation} \label{eq:blr-prior-GLM}
y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

be a linear regression model ($\rightarrow$ Proof "mlr") with measured $n \times 1$ data vector $y$, known $n \times p$ design matrix $X$, known $n \times n$ covariance structure $V$ and unknown $p \times 1$ regression coefficients $\beta$ and noise variance $\sigma^2$.

Then, the conjugate prior ($\rightarrow$ Definition "prior-conj") for this model is a normal-gamma distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:ng})

\begin{equation} \label{eq:blr-prior-GLM-NG-prior}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0)
\end{equation}

where $\tau = 1/\sigma^2$ is the inverse noise variance or noise precision.


\vspace{1em}
\textbf{Proof:} By definition, a conjugate prior ($\rightarrow$ Definition "prior-conj") is a prior distribution ($\rightarrow$ Definition "prior") that, when combined with the likelihood function ($\rightarrow$ Definition "lf"), leads to a posterior distribution ($\rightarrow$ Definition "post") that belongs to the same family of probability distributions ($\rightarrow$ Definition "pd"). This is fulfilled when the prior density and the likelihood function are proportional to the model parameters in the same way, i.e. the model parameters appear in the same functional form in both.

Equation \eqref{eq:blr-prior-GLM} implies the following likelihood function ($\rightarrow$ Definition "lf")

\begin{equation} \label{eq:blr-prior-GLM-LF-class}
p(y|\beta,\sigma^2) = \mathcal{N}(y; X \beta, \sigma^2 V) = \sqrt{\frac{1}{(2 \pi)^n |\sigma^2 V|}} \, \exp\left[ -\frac{1}{2 \sigma^2} (y-X\beta)^\mathrm{T} V^{-1} (y-X\beta) \right]
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:blr-prior-GLM-LF-Bayes}
p(y|\beta,\tau) = \mathcal{N}(y; X \beta, (\tau P)^{-1}) = \sqrt{\frac{|\tau P|}{(2 \pi)^n}} \, \exp\left[ -\frac{\tau}{2} (y-X\beta)^\mathrm{T} P (y-X\beta) \right]
\end{equation}

using the noise precision $\tau = 1/\sigma^2$ and the $n \times n$ precision matrix $P = V^{-1}$.

\vspace{1em}
Seperating constant and variable terms, we have:

\begin{equation} \label{eq:blr-prior-GLM-LF-s1}
p(y|\beta,\tau) = \sqrt{\frac{|P|}{(2 \pi)^n}} \cdot \tau^{n/2} \cdot \exp\left[ -\frac{\tau}{2} (y-X\beta)^\mathrm{T} P (y-X\beta) \right] \; .
\end{equation}

Expanding the product in the exponent, we have:

\begin{equation} \label{eq:blr-prior-GLM-LF-s2}
p(y|\beta,\tau) = \sqrt{\frac{|P|}{(2 \pi)^n}} \cdot \tau^{n/2} \cdot \exp\left[ -\frac{\tau}{2} \left( y^\mathrm{T} P y - y^\mathrm{T} P X \beta - \beta^\mathrm{T} X^\mathrm{T} P y + \beta^\mathrm{T} X^\mathrm{T} P X \beta \right) \right] \; .
\end{equation}

Completing the square over $\beta$, finally gives

\begin{equation} \label{eq:blr-prior-GLM-LF-s3}
p(y|\beta,\tau) = \sqrt{\frac{|P|}{(2 \pi)^n}} \cdot \tau^{n/2} \cdot \exp\left[ -\frac{\tau}{2} \left( (\beta - \tilde{X}y)^\mathrm{T} X^\mathrm{T} P X (\beta - \tilde{X}y) - y^\mathrm{T} Q y + y^\mathrm{T} P y \right) \right]
\end{equation}

where $\tilde{X} = \left( X^\mathrm{T} P X \right)^{-1} X^\mathrm{T} P$ and $Q = \tilde{X}^\mathrm{T} \left( X^\mathrm{T} P X \right) \tilde{X}$.

\vspace{1em}
In other words, the likelihood function ($\rightarrow$ Definition "lf") is proportional to a power of $\tau$ times an exponential of $\tau$ and an exponential of a squared form of $\beta$, weighted by $\tau$:

\begin{equation} \label{eq:blr-prior-GLM-LF-s4}
p(y|\beta,\tau) \propto \tau^{n/2} \cdot \exp\left[ -\frac{\tau}{2} \left( y^\mathrm{T} P y - y^\mathrm{T} Q y \right) \right] \cdot \exp\left[ -\frac{\tau}{2} (\beta - \tilde{X}y)^\mathrm{T} X^\mathrm{T} P X (\beta - \tilde{X}y) \right] \; .
\end{equation}

The same is true for a normal gamma distribution over $\beta$ and $\tau$

\begin{equation} \label{eq:blr-prior-BLR-prior-s1}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0)
\end{equation}

the probability density function of which ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:ng-pdf})

\begin{equation} \label{eq:blr-prior-BLR-prior-s2}
p(\beta,\tau) = \sqrt{\frac{|\tau \Lambda_0|}{(2 \pi)^p}} \exp\left[ -\frac{\tau}{2} (\beta-\mu_0)^\mathrm{T} \Lambda_0 (\beta-\mu_0) \right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau]
\end{equation}

exhibits the same proportionality

\begin{equation} \label{eq:blr-prior-BLR-prior-s3}
p(\beta,\tau) \propto \tau^{a_0+p/2-1} \cdot \exp[-\tau b_0] \cdot \exp\left[ -\frac{\tau}{2} (\beta-\mu_0)^\mathrm{T} \Lambda_0 (\beta-\mu_0) \right]
\end{equation}

and is therefore conjugate relative to the likelihood.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop CM (2006): "Bayesian linear regression"; in: \textit{Pattern Recognition for Machine Learning}, pp. 152-161, ex. 3.12, eq. 3.112; URL: \url{https://www.springer.com/gp/book/9780387310732}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P9 | shortcut: blr-prior | author: JoramSoch | date: 2020-01-03, 15:26.


\subsubsection[\textbf{Posterior distribution}]{Posterior distribution} \label{sec:blr-post}

\vspace{1em}
\textbf{Theorem:} Let

\begin{equation} \label{eq:blr-post-GLM}
y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

be a linear regression model ($\rightarrow$ Definition "mlr") with measured $n \times 1$ data vector $y$, known $n \times p$ design matrix $X$, known $n \times n$ covariance structure $V$ and unknown $p \times 1$ regression coefficients $\beta$ and noise variance $\sigma^2$.  Moreover, assume a normal-gamma prior distribution ($\rightarrow$ Proof \ref{sec:Statistical Models}/\ref{sec:blr-prior}) over the model parameters $\beta$ and $\tau = 1/\sigma^2$:

\begin{equation} \label{eq:blr-post-GLM-NG-prior}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0) \; .
\end{equation}

Then, the posterior distribution ($\rightarrow$ Definition "post") is also a normal-gamma distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:ng})

\begin{equation} \label{eq:blr-post-GLM-NG-post}
p(\beta,\tau|y) = \mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1}) \cdot \mathrm{Gam}(\tau; a_n, b_n)
\end{equation}

and the posterior hyperparameters ($\rightarrow$ Definition "post-hyp") are given by

\begin{equation} \label{eq:blr-post-GLM-NG-post-par}
\begin{split}
\mu_n &= \Lambda_n^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \\
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} According to Bayes' theorem ($\rightarrow$ Proof \ref{sec:General Theorems}/\ref{sec:bayes-th}), the posterior distribution ($\rightarrow$ Definition "post") is given by

\begin{equation} \label{eq:blr-post-GLM-NG-BT}
p(\beta,\tau|y) = \frac{p(y|\beta,\tau) \, p(\beta,\tau)}{p(y)} \; .
\end{equation}

Since $p(y)$ is just a normalization factor, the posterior is proportional ($\rightarrow$ Proof "post-jl") to the numerator:

\begin{equation} \label{eq:blr-post-GLM-NG-post-JL}
p(\beta,\tau|y) \propto p(y|\beta,\tau) \, p(\beta,\tau) = p(y,\beta,\tau) \; .
\end{equation}

Equation \eqref{eq:blr-post-GLM} implies the following likelihood function ($\rightarrow$ Definition "lf")

\begin{equation} \label{eq:blr-post-GLM-LF-class}
p(y|\beta,\sigma^2) = \mathcal{N}(y; X \beta, \sigma^2 V) = \sqrt{\frac{1}{(2 \pi)^n |\sigma^2 V|}} \, \exp\left[ -\frac{1}{2 \sigma^2} (y-X\beta)^\mathrm{T} V^{-1} (y-X\beta) \right]
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:blr-post-GLM-LF-Bayes}
p(y|\beta,\tau) = \mathcal{N}(y; X \beta, (\tau P)^{-1}) = \sqrt{\frac{|\tau P|}{(2 \pi)^n}} \, \exp\left[ -\frac{\tau}{2} (y-X\beta)^\mathrm{T} P (y-X\beta) \right]
\end{equation}

using the noise precision $\tau = 1/\sigma^2$ and the $n \times n$ precision matrix $P = V^{-1}$.

\vspace{1em}
Combining the likelihood function \eqref{eq:blr-post-GLM-LF-Bayes} with the prior distribution \eqref{eq:blr-post-GLM-NG-prior}, the sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss of the model is given by

\begin{equation} \label{eq:blr-post-GLM-NG-JL-s1}
\begin{split}
p(y,\beta,\tau) = \; & p(y|\beta,\tau) \, p(\beta,\tau) \\
= \; & \sqrt{\frac{|\tau P|}{(2 \pi)^n}} \, \exp\left[ -\frac{\tau}{2} (y-X\beta)^\mathrm{T} P (y-X\beta) \right] \cdot \\
& \sqrt{\frac{|\tau \Lambda_0|}{(2 \pi)^p}} \, \exp\left[ -\frac{\tau}{2} (\beta-\mu_0)^\mathrm{T} \Lambda_0 (\beta-\mu_0) \right] \cdot \\
& \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \; .
\end{split}
\end{equation}

Collecting identical variables gives:

\begin{equation} \label{eq:blr-post-GLM-NG-JL-s2}
\begin{split}
p(y,\beta,\tau) = \; & \sqrt{\frac{\tau^{n+p}}{(2 \pi)^{n+p}} |P| |\Lambda_0|} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau}{2} \left( (y-X\beta)^\mathrm{T} P (y-X\beta) + (\beta-\mu_0)^\mathrm{T} \Lambda_0 (\beta-\mu_0) \right) \right] \; .
\end{split}
\end{equation}

Expanding the products in the exponent gives:

\begin{equation} \label{eq:blr-post-GLM-NG-JL-s3}
\begin{split}
p(y,\beta,\tau) = \; & \sqrt{\frac{\tau^{n+p}}{(2 \pi)^{n+p}} |P| |\Lambda_0|} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau}{2} \left( y^\mathrm{T} P y - y^\mathrm{T} P X \beta - \beta^\mathrm{T} X^\mathrm{T} P y + \beta^\mathrm{T} X^\mathrm{T} P X \beta + \right. \right. \\
& \hphantom{\exp \left[ -\frac{\tau}{2} \right.} \; \left. \left. \beta^\mathrm{T} \Lambda_0 \beta - \beta^\mathrm{T} \Lambda_0 \mu_0 - \mu_0^\mathrm{T} \Lambda_0 \beta + \mu_0^\mathrm{T} \Lambda_0 \mu_0 \right) \right] \; .
\end{split}
\end{equation}

Completing the square over $\beta$, we finally have

\begin{equation} \label{eq:blr-post-GLM-NG-JL-s4}
\begin{split}
p(y,\beta,\tau) = \; & \sqrt{\frac{\tau^{n+p}}{(2 \pi)^{n+p}} |P| |\Lambda_0|} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau}{2} \left( (\beta-\mu_n)^\mathrm{T} \Lambda_n (\beta-\mu_n) + (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \right) \right]
\end{split}
\end{equation}

with the posterior hyperparameters ($\rightarrow$ Definition "post-hyp")

\begin{equation} \label{eq:blr-post-GLM-NG-post-beta-par}
\begin{split}
\mu_n &= \Lambda_n^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \; .
\end{split}
\end{equation}

Ergo, the joint likelihood is proportional to

\begin{equation} \label{eq:blr-post-GLM-NG-JL-s5}
p(y,\beta,\tau) \propto \tau^{p/2} \cdot \exp\left[ -\frac{\tau}{2} (\beta-\mu_n)^\mathrm{T} \Lambda_n (\beta-\mu_n) \right] \cdot \tau^{a_n-1} \cdot \exp\left[ -b_n \tau \right]
\end{equation}

with the posterior hyperparameters ($\rightarrow$ Definition "post-hyp")

\begin{equation} \label{eq:blr-post-GLM-NG-post-tau-par}
\begin{split}
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \; .
\end{split}
\end{equation}

From the term in \eqref{eq:blr-post-GLM-NG-JL-s5}, we can isolate the posterior distribution over $\beta$ given $\tau$:

\begin{equation} \label{eq:blr-post-GLM-NG-post-beta}
p(\beta|\tau,y) = \mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1}) \; .
\end{equation}

From the remaining term, we can isolate the posterior distribution over $\tau$:

\begin{equation} \label{eq:blr-post-GLM-NG-post-tau}
p(\tau|y) = \mathrm{Gam}(\tau; a_n, b_n) \; .
\end{equation}

Together, \eqref{eq:blr-post-GLM-NG-post-beta} and \eqref{eq:blr-post-GLM-NG-post-tau} constitute the joint posterior distribution ($\rightarrow$ Definition "jp") of $\beta$ and $\tau$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop CM (2006): "Bayesian linear regression"; in: \textit{Pattern Recognition for Machine Learning}, pp. 152-161, ex. 3.12, eq. 3.113; URL: \url{https://www.springer.com/gp/book/9780387310732}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P10 | shortcut: blr-post | author: JoramSoch | date: 2020-01-03, 17:53.


\subsubsection[\textbf{Log model evidence}]{Log model evidence} \label{sec:blr-lme}

\vspace{1em}
\textbf{Theorem:} Let

\begin{equation} \label{eq:blr-lme-GLM}
m: \; y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

be a linear regression model ($\rightarrow$ Definition "mlr") with measured $n \times 1$ data vector $y$, known $n \times p$ design matrix $X$, known $n \times n$ covariance structure $V$ and unknown $p \times 1$ regression coefficients $\beta$ and noise variance $\sigma^2$.  Moreover, assume a normal-gamma prior distribution ($\rightarrow$ Proof \ref{sec:Statistical Models}/\ref{sec:blr-prior}) over the model parameters $\beta$ and $\tau = 1/\sigma^2$:

\begin{equation} \label{eq:blr-lme-GLM-NG-prior}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0) \; .
\end{equation}

Then, the log model evidence ($\rightarrow$ Definition "lme") for this model is

\begin{equation} \label{eq:blr-lme-GLM-NG-LME}
\begin{split}
\log p(y|m) = \frac{1}{2} & \log |P| - \frac{n}{2} \log (2 \pi)  + \frac{1}{2} \log |\Lambda_0| - \frac{1}{2} \log |\Lambda_n| + \\
& \log \Gamma(a_n) - \log \Gamma(a_0) + a_0 \log b_0 - a_n \log b_n
\end{split}
\end{equation}

where the posterior hyperparameters ($\rightarrow$ Definition "post-hyp") are given by

\begin{equation} \label{eq:blr-lme-GLM-NG-post-par}
\begin{split}
\mu_n &= \Lambda_n^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \\
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} According to the law of marginal probability ($\rightarrow$ Proof "lmp"), the model evidence ($\rightarrow$ Definition "ml") for this model is:

\begin{equation} \label{eq:blr-lme-GLM-NG-ME-s1}
p(y|m) = \iint p(y|\beta,\tau) \, p(\beta,\tau) \, \mathrm{d}\beta \, \mathrm{d}\tau \; .
\end{equation}

According to the law of conditional probability ($\rightarrow$ Proof "lcp"), the integrand is equivalent to the joint likelihood ($\rightarrow$ Definition "jl"):

\begin{equation} \label{eq:blr-lme-GLM-NG-ME-s2}
p(y|m) = \iint p(y,\beta,\tau) \, \mathrm{d}\beta \, \mathrm{d}\tau \; .
\end{equation}

Equation \eqref{eq:blr-lme-GLM} implies the following likelihood function ($\rightarrow$ Definition "lf")

\begin{equation} \label{eq:blr-lme-GLM-LF-class}
p(y|\beta,\sigma^2) = \mathcal{N}(y; X \beta, \sigma^2 V) = \sqrt{\frac{1}{(2 \pi)^n |\sigma^2 V|}} \, \exp\left[ -\frac{1}{2 \sigma^2} (y-X\beta)^\mathrm{T} V^{-1} (y-X\beta) \right]
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:blr-lme-GLM-LF-Bayes}
p(y|\beta,\tau) = \mathcal{N}(y; X \beta, (\tau P)^{-1}) = \sqrt{\frac{|\tau P|}{(2 \pi)^n}} \, \exp\left[ -\frac{\tau}{2} (y-X\beta)^\mathrm{T} P (y-X\beta) \right]
\end{equation}

using the noise precision $\tau = 1/\sigma^2$ and the $n \times n$ precision matrix $P = V^{-1}$.

\vspace{1em}
When deriving the posterior distribution ($\rightarrow$ Proof \ref{sec:Statistical Models}/\ref{sec:blr-post}) $p(\beta,\tau|y)$, the joint likelihood $p(y,\beta,\tau)$ is obtained as

\begin{equation} \label{eq:blr-lme-GLM-NG-LME-s1}
\begin{split}
p(y,\beta,\tau) = \; & \sqrt{\frac{\tau^n |P|}{(2 \pi)^n}} \, \sqrt{\frac{\tau^p |\Lambda_0|}{(2 \pi)^p}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau}{2} \left( (\beta-\mu_n)^T \Lambda_n (\beta-\mu_n) + (y^T P y + \mu_0^T \Lambda_0 \mu_0 - \mu_n^T \Lambda_n \mu_n) \right) \right] \; .
\end{split}
\end{equation}

Using the probability density function of the multivariate normal distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}), we can rewrite this as

\begin{equation} \label{eq:blr-lme-GLM-NG-LME-s2}
\begin{split}
p(y,\beta,\tau) = \; & \sqrt{\frac{\tau^n |P|}{(2 \pi)^n}} \, \sqrt{\frac{\tau^p |\Lambda_0|}{(2 \pi)^p}} \, \sqrt{\frac{(2 \pi)^p}{\tau^p |\Lambda_n|}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1}) \, \exp\left[ -\frac{\tau}{2} (y^T P y + \mu_0^T \Lambda_0 \mu_0 - \mu_n^T \Lambda_n \mu_n) \right] \; .
\end{split}
\end{equation}

Now, $\beta$ can be integrated out easily:

\begin{equation} \label{eq:blr-lme-GLM-NG-LME-s3}
\begin{split}
\int p(y,\beta,\tau) \, \mathrm{d}\beta = \; & \sqrt{\frac{\tau^n |P|}{(2 \pi)^n}} \, \sqrt{\frac{|\Lambda_0|}{|\Lambda_n|}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau}{2} (y^T P y + \mu_0^T \Lambda_0 \mu_0 - \mu_n^T \Lambda_n \mu_n) \right] \; .
\end{split}
\end{equation}

Using the probability density function of the gamma distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), we can rewrite this as

\begin{equation}\label{eq:blr-lme-GLM-NG-LME-s4}
\int p(y,\beta,\tau) \, \mathrm{d}\beta = \; \sqrt{\frac{|P|}{(2 \pi)^n}} \, \sqrt{\frac{|\Lambda_0|}{|\Lambda_n|}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \frac{\Gamma(a_n)}{ {b_n}^{a_n}} \, \mathrm{Gam}(\tau; a_n, b_n) \; .
\end{equation}

Finally, $\tau$ can also be integrated out:

\begin{equation} \label{eq:blr-lme-GLM-NG-LME-s5}
\iint p(y,\beta,\tau) \, \mathrm{d}\beta \, \mathrm{d}\tau = \; \sqrt{\frac{|P|}{(2 \pi)^n}} \, \sqrt{\frac{|\Lambda_0|}{|\Lambda_n|}} \, \frac{\Gamma(a_n)}{\Gamma(a_0)} \, \frac{ {b_0}^{a_0}}{ {b_n}^{a_n}} = p(y|m) \; .
\end{equation}

Thus, the log model evidence ($\rightarrow$ Definition "lme") of this model is given by

\begin{equation} \label{eq:blr-lme-GLM-NG-LME-s6}
\begin{split}
\log p(y|m) = \frac{1}{2} & \log |P| - \frac{n}{2} \log (2 \pi)  + \frac{1}{2} \log |\Lambda_0| - \frac{1}{2} \log |\Lambda_n| + \\
& \log \Gamma(a_n) - \log \Gamma(a_0) + a_0 \log b_0 - a_n \log b_n \; .
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop CM (2006): "Bayesian linear regression"; in: \textit{Pattern Recognition for Machine Learning}, pp. 152-161, ex. 3.23, eq. 3.118; URL: \url{https://www.springer.com/gp/book/9780387310732}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P11 | shortcut: blr-lme | author: JoramSoch | date: 2020-01-03, 22:05.


\subsection{General linear model}

\subsubsection[\textbf{Maximum likelihood estimation}]{Maximum likelihood estimation} \label{sec:glm-mle}

\vspace{1em}
\textbf{Theorem:} Given a general linear model ($\rightarrow$ Definition "glm") with matrix-normally distributed ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:matn}) errors

\begin{equation} \label{eq:glm-mle-GLM}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma) \; ,
\end{equation}

maximum likelihood estimates ($\rightarrow$ Definition "mle") for the unknown parameters $B$ and $\Sigma$ are given by

\begin{equation} \label{eq:glm-mle-GLM-MLE}
\begin{split}
\hat{B} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} Y \\
\hat{\Sigma} &= \frac{1}{n} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \; . \\
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} In \eqref{eq:glm-mle-GLM}, $Y$ is an $n \times v$ matrix of measurements ($n$ observations, $v$ dependent variables), $X$ is an $n \times p$ design matrix ($n$ observations, $p$ independent variables) and $V$ is an $n \times n$ covariance matrix across observations. This multivariate GLM implies the following likelihood function ($\rightarrow$ Definition "lf")

\begin{equation} \label{eq:glm-mle-GLM-LF}
\begin{split}
p(Y|B,\Sigma) &= \mathcal{MN}(Y; XB, V, \Sigma) \\
&= \sqrt{\frac{1}{(2\pi)^{nv} |\Sigma|^n |V|^v}} \cdot \exp\left[ -\frac{1}{2} \, \mathrm{tr}\left( \Sigma^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \right)  \right] \\
\end{split}
\end{equation}

and the log-likelihood function ($\rightarrow$ Definition "llf")

\begin{equation} \label{eq:glm-mle-GLM-LL1}
\begin{split}
\mathrm{LL}(B,\Sigma) = &\log p(Y|B,\Sigma) \\
= &- \frac{nv}{2} \log(2\pi) - \frac{n}{2} \log |\Sigma| - \frac{v}{2} \log |V| \\
&- \frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \right] \; .\\
\end{split}
\end{equation}

Substituting $V^{-1}$ by the precision matrix $P$ to ease notation, we have:

\begin{equation} \label{eq:glm-mle-GLM-LL2}
\begin{split}
\mathrm{LL}(B,\Sigma) = &- \frac{nv}{2} \log(2\pi) - \frac{n}{2} \log(|\Sigma|) - \frac{v}{2} \log(|V|) \\
&- \frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} \left( Y^\mathrm{T} P Y - Y^\mathrm{T} P X B - B^\mathrm{T} X^\mathrm{T} P Y + B^\mathrm{T} X^\mathrm{T} P X B \right) \right] \; .\\
\end{split}
\end{equation}

\vspace{1em}
The derivative of the log-likelihood function \eqref{eq:glm-mle-GLM-LL2} with respect to $B$ is

\begin{equation} \label{eq:glm-mle-dLL-dB}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(B,\Sigma)}{\mathrm{d}B} &= \frac{\mathrm{d}}{\mathrm{d}B} \left( - \frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} \left( Y^\mathrm{T} P Y - Y^\mathrm{T} P X B - B^\mathrm{T} X^\mathrm{T} P Y + B^\mathrm{T} X^\mathrm{T} P X B \right) \right] \right) \\
&= \frac{\mathrm{d}}{\mathrm{d}B} \left( -\frac{1}{2} \, \mathrm{tr}\left[ -2 \Sigma^{-1} Y^\mathrm{T} P X B \right] \right) + \frac{\mathrm{d}}{\mathrm{d}B} \left( -\frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} B^\mathrm{T} X^\mathrm{T} P X B \right] \right) \\
&= - \frac{1}{2} \left( -2 X^\mathrm{T} P Y \Sigma^{-1} \right) - \frac{1}{2} \left( X^\mathrm{T} P X B \Sigma^{-1} + (X^\mathrm{T} P X)^\mathrm{T} B (\Sigma^{-1})^\mathrm{T} \right) \\
&= X^\mathrm{T} P Y \Sigma^{-1} - X^\mathrm{T} P X B \Sigma^{-1} \\
\end{split}
\end{equation}

and setting this derivative to zero gives the MLE for $B$:

\begin{equation} \label{eq:glm-mle-B-MLE}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{B},\Sigma)}{\mathrm{d}B} &= 0 \\
0 &= X^\mathrm{T} P Y \Sigma^{-1} - X^\mathrm{T} P X \hat{B} \Sigma^{-1} \\
0 &= X^\mathrm{T} P Y - X^\mathrm{T} P X \hat{B} \\
X^\mathrm{T} P X \hat{B} &= X^\mathrm{T} P Y \\
\hat{B} &= \left( X^\mathrm{T} P X \right)^{-1} X^\mathrm{T} P Y \\
\end{split}
\end{equation}

\vspace{1em}
The derivative of the log-likelihood function \eqref{eq:glm-mle-GLM-LL1} at $\hat{B}$ with respect to $\Sigma$ is

\begin{equation} \label{eq:glm-mle-dLL-dS}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{B},\Sigma)}{\mathrm{d}\Sigma} &= \frac{\mathrm{d}}{\mathrm{d}\Sigma} \left( - \frac{n}{2} \log |\Sigma| - \frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \right] \right) \\
&= - \frac{n}{2} \left( \Sigma^{-1} \right)^\mathrm{T} + \frac{1}{2} \left( \Sigma^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \, \Sigma^{-1} \right)^\mathrm{T} \\
&= - \frac{n}{2} \, \Sigma^{-1} + \frac{1}{2} \, \Sigma^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \, \Sigma^{-1} \\
\end{split}
\end{equation}

and setting this derivative to zero gives the MLE for $\Sigma$:

\begin{equation} \label{eq:glm-mle-S-MLE}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{B},\hat{\Sigma})}{\mathrm{d}\Sigma} &= 0 \\
0 &= - \frac{n}{2} \, \hat{\Sigma}^{-1} + \frac{1}{2} \, \hat{\Sigma}^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \, \hat{\Sigma}^{-1} \\
\frac{n}{2} \, \hat{\Sigma}^{-1} &= \frac{1}{2} \, \hat{\Sigma}^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \, \hat{\Sigma}^{-1} \\
\hat{\Sigma}^{-1} &= \frac{1}{n} \, \hat{\Sigma}^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \, \hat{\Sigma}^{-1} \\
I_v &= \frac{1}{n} \, (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \, \hat{\Sigma}^{-1} \\
\hat{\Sigma} &= \frac{1}{n} \, (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \\
\end{split}
\end{equation}

\vspace{1em}
Together, \eqref{eq:glm-mle-B-MLE} and \eqref{eq:glm-mle-S-MLE} constitute the MLE for the GLM.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P7 | shortcut: glm-mle | author: JoramSoch | date: 2019-12-06, 10:40.


\pagebreak
\section{Poisson data}

\subsection{Poisson-distributed data}

\subsubsection[\textbf{Maximum likelihood estimation}]{Maximum likelihood estimation} \label{sec:poiss-mle}

\vspace{1em}
\textbf{Theorem:} Let $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ be a set of observed counts independent and identically distributed according to a Poisson distribution ($\rightarrow$ Definition "poiss") with rate $\lambda$:

\begin{equation} \label{eq:poiss-mle-Poiss}
y_i \sim \mathrm{Poiss}(\lambda), \quad i = 1, \ldots, n \; .
\end{equation}

Then, the maximum likelihood estimate ($\rightarrow$ Definition "mle") for the rate parameter $\lambda$ is given by

\begin{equation} \label{eq:poiss-mle-Poiss-MLE}
\hat{\lambda} = \bar{y}
\end{equation}

where $\bar{y}$ is the sample mean ($\rightarrow$ Proof "ev-sample")

\begin{equation} \label{eq:poiss-mle-y-mean}
\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The likelihood function ($\rightarrow$ Definition "lf") for each observation is given by the probability mass function of the Poisson distribution ($\rightarrow$ Proof "poiss-pdf")

\begin{equation} \label{eq:poiss-mle-Poiss-yi}
p(y_i|\lambda) = \mathrm{Poiss}(y_i; \lambda) = \frac{\lambda^{y_i} \cdot \exp(-\lambda)}{y_i !}
\end{equation}

and because observations are independent ($\rightarrow$ Definition "ind"), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:poiss-mle-Poiss-LF}
p(y|\lambda) = \prod_{i=1}^n p(y_i|\lambda) = \prod_{i=1}^n \frac{\lambda^{y_i} \cdot \exp(-\lambda)}{y_i !} \; .
\end{equation}

Thus, the log-likelihood function ($\rightarrow$ Definition "llf") is

\begin{equation} \label{eq:poiss-mle-Poiss-LL}
\mathrm{LL}(\lambda) = \log p(y|\lambda) = \log \left[ \prod_{i=1}^n \frac{\lambda^{y_i} \cdot \exp(-\lambda)}{y_i !} \right]
\end{equation}

which can be developed into

\begin{equation} \label{eq:poiss-mle-Poiss-LL-der}
\begin{split}
\mathrm{LL}(\lambda) &= \sum_{i=1}^n \log \left[ \frac{\lambda^{y_i} \cdot \exp(-\lambda)}{y_i !} \right] \\
&= \sum_{i=1}^n \left[ y_i \cdot \log(\lambda) - \lambda - \log(y_i !) \right] \\
&= - \sum_{i=1}^n \lambda + \sum_{i=1}^n y_i \cdot \log(\lambda) - \sum_{i=1}^n \log(y_i !) \\
&= - n \lambda + \log(\lambda) \sum_{i=1}^n y_i - \sum_{i=1}^n \log(y_i !) \\
\end{split}
\end{equation}

The derivatives of the log-likelihood with respect to $\lambda$ are

\begin{equation} \label{eq:poiss-mle-Poiss-dLLdl-d2LLdl2}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\lambda)}{\mathrm{d}\lambda} &= \frac{1}{\lambda} \sum_{i=1}^n y_i - n \\
\frac{\mathrm{d}^2\mathrm{LL}(\lambda)}{\mathrm{d}\lambda^2} &= -\frac{1}{\lambda^2} \sum_{i=1}^n y_i \; . \\
\end{split}
\end{equation}

Setting the first derivative to zero, we obtain:

\begin{equation} \label{eq:poiss-mle-Poiss-dLLdl}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{\lambda})}{\mathrm{d}\lambda} &= 0 \\
0 &= \frac{1}{\hat{\lambda}} \sum_{i=1}^n y_i - n \\
\hat{\lambda} &= \frac{1}{n} \sum_{i=1}^n y_i = \bar{y} \; .
\end{split}
\end{equation}

Plugging this value into the second deriative, we confirm:

\begin{equation} \label{eq:poiss-mle-Poiss-d2LLdl2}
\begin{split}
\frac{\mathrm{d}^2\mathrm{LL}(\hat{\lambda})}{\mathrm{d}\lambda^2} &= -\frac{1}{\bar{y}^2} \sum_{i=1}^n y_i \\
&= -\frac{n \cdot \bar{y}}{\bar{y}^2} \\
&= -\frac{n}{\bar{y}} < 0 \; .
\end{split}
\end{equation}

This demonstrates that the estimate $\hat{\lambda} = \bar{y}$ maximizes the likelihood $p(y \mid \lambda)$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P27 | shortcut: poiss-mle | author: JoramSoch | date: 2020-01-20, 21:53.


\subsection{Poisson distribution with exposure values}

\subsubsection[\textbf{Conjugate prior distribution}]{Conjugate prior distribution} \label{sec:poissexp-prior}

\vspace{1em}
\textbf{Theorem:} Let $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ be a series of observed counts which are independently distributed according to a Poisson distribution ($\rightarrow$ Definition "poiss") with common rate $\lambda$ and concurrent exposures $\left\lbrace x_1, \ldots, x_n \right\rbrace$:

\begin{equation} \label{eq:poissexp-prior-Poiss-exp}
y_i \sim \mathrm{Poiss}(\lambda x_i), \quad i = 1, \ldots, n \; .
\end{equation}

Then, the conjugate prior ($\rightarrow$ Definition "prior-conj") for the model parameter $\lambda$ is a gamma distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-prior}
p(\lambda) = \mathrm{Gam}(\lambda; a_0, b_0) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the Poisson distribution ($\rightarrow$ Proof "poiss-pmf"), the likelihood function ($\rightarrow$ Definition "lf") for each observation implied by \eqref{eq:poissexp-prior-Poiss-exp} is given by

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-LF-s1}
p(y_i|\lambda) = \mathrm{Poiss}(y_i; \lambda x_i) = \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !}
\end{equation}

and because observations are independent ($\rightarrow$ Definition "ind"), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-LF-s2}
p(y|\lambda) = \prod_{i=1}^n p(y_i|\lambda) = \prod_{i=1}^n \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !} \; .
\end{equation}

Resolving the product in the likelihood function, we have

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-LF-s3}
\begin{split}
p(y|\lambda) &= \prod_{i=1}^n \frac{ {x_i}^{y_i}}{y_i !} \cdot \prod_{i=1}^n \lambda^{y_i} \cdot \prod_{i=1}^n \exp\left[-\lambda x_i\right] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \cdot \lambda^{\sum_{i=1}^n y_i} \cdot \exp\left[-\lambda \sum_{i=1}^n x_i\right] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \cdot \lambda^{n \bar{y}} \cdot \exp\left[-n \bar{x} \lambda\right]
\end{split}
\end{equation}

where $\bar{y}$ and $\bar{x}$ are the means ($\rightarrow$ Proof "ev-sample") of $y$ and $x$ respectively:

\begin{equation} \label{eq:poissexp-prior-xy-mean}
\begin{split}
\bar{y} &= \frac{1}{n} \sum_{i=1}^n y_i \\
\bar{x} &= \frac{1}{n} \sum_{i=1}^n x_i \; .
\end{split}
\end{equation}

In other words, the likelihood function is proportional to a power of $\lambda$ times an exponential of $\lambda$:

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-LF-prop}
p(y|\lambda) \propto \lambda^{n \bar{y}} \cdot \exp\left[-n \bar{x} \lambda\right] \; .
\end{equation}

The same is true for a gamma distribution over $\lambda$

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-prior-s1}
p(\lambda) = \mathrm{Gam}(\lambda; a_0, b_0)
\end{equation}

the probability density function of which ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:gam-pdf})

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-prior-s2}
p(\lambda) = \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda]
\end{equation}

exhibits the same proportionality

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-prior-s3}
p(\lambda) \propto \lambda^{a_0-1} \cdot \exp[-b_0 \lambda]
\end{equation}

and is therefore conjugate relative to the likelihood.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Gelman A, Carlin JB, Stern HS, Dunson DB, Vehtari A, Rubin DB (2014): "Other standard single-parameter models"; in: \textit{Bayesian Data Analysis}, 3rd edition, ch. 2.6, p. 45, eq. 2.14ff.; URL: \url{http://www.stat.columbia.edu/~gelman/book/}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P41 | shortcut: poissexp-prior | author: JoramSoch | date: 2020-02-04, 14:11.


\subsubsection[\textbf{Posterior distribution}]{Posterior distribution} \label{sec:poissexp-post}

\vspace{1em}
\textbf{Theorem:} Let $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ be a series of observed counts which are independently distributed according to a Poisson distribution ($\rightarrow$ Definition "poiss") with common rate $\lambda$ and concurrent exposures $\left\lbrace x_1, \ldots, x_n \right\rbrace$:

\begin{equation} \label{eq:poissexp-post-Poiss-exp}
y_i \sim \mathrm{Poiss}(\lambda x_i), \quad i = 1, \ldots, n \; .
\end{equation}

Moreover, assume a gamma prior distribution ($\rightarrow$ Proof \ref{sec:Statistical Models}/\ref{sec:poissexp-prior}) over the model parameter $\lambda$:

\begin{equation} \label{eq:poissexp-post-Poiss-exp-prior}
p(\lambda) = \mathrm{Gam}(\lambda; a_0, b_0) \; .
\end{equation}

Then, the posterior distribution ($\rightarrow$ Definition "post") is also a gamma distribution ($\rightarrow$ Definition \ref{sec:Probability Distributions}/\ref{sec:gam})

\begin{equation} \label{eq:poissexp-post-Poiss-exp-post}
p(\lambda|y) = \mathrm{Gam}(\lambda; a_n, b_n)
\end{equation}

and the posterior hyperparameters ($\rightarrow$ Definition "post-hyp") are given by

\begin{equation} \label{eq:poissexp-post-Poiss-exp-post-par}
\begin{split}
a_n &= a_0 + n \bar{y} \\
a_n &= a_0 + n \bar{x} \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the Poisson distribution ($\rightarrow$ Proof "poiss-pmf"), the likelihood function ($\rightarrow$ Definition "lf") for each observation implied by \eqref{eq:poissexp-post-Poiss-exp} is given by

\begin{equation} \label{eq:poissexp-post-Poiss-exp-LF-s1}
p(y_i|\lambda) = \mathrm{Poiss}(y_i; \lambda x_i) = \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !}
\end{equation}

and because observations are independent ($\rightarrow$ Definition "ind"), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:poissexp-post-Poiss-exp-LF-s2}
p(y|\lambda) = \prod_{i=1}^n p(y_i|\lambda) = \prod_{i=1}^n \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !} \; .
\end{equation}

Combining the likelihood function \eqref{eq:poissexp-post-Poiss-exp-LF-s2} with the prior distribution \eqref{eq:poissexp-post-Poiss-exp-prior}, the joint likelihood ($\rightarrow$ Definition "jl") of the model is given by

\begin{equation} \label{eq:poissexp-post-Poiss-exp-JL-s1}
\begin{split}
p(y,\lambda) &= p(y|\lambda) \, p(\lambda) \\
&= \prod_{i=1}^n \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !} \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \; .
\end{split}
\end{equation}

Resolving the product in the joint likelihood, we have

\begin{equation} \label{eq:poissexp-post-Poiss-JL-s2}
\begin{split}
p(y,\lambda) &= \prod_{i=1}^n \frac{ {x_i}^{y_i}}{y_i !} \prod_{i=1}^n \lambda^{y_i} \prod_{i=1}^n \exp\left[-\lambda x_i\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \lambda^{\sum_{i=1}^n y_i} \exp\left[-\lambda \sum_{i=1}^n x_i\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \lambda^{n \bar{y}} \exp\left[-n \bar{x} \lambda\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \frac{ {b_0}^{a_0}}{\Gamma(a_0)}  \cdot \lambda^{a_0 + n \bar{y} - 1} \cdot \exp\left[-(b_0 + n \bar{x}) \lambda\right] \\
\end{split}
\end{equation}

where $\bar{y}$ and $\bar{x}$ are the means ($\rightarrow$ Proof "ev-sample") of $y$ and $x$ respectively:

\begin{equation} \label{eq:poissexp-post-xy-mean}
\begin{split}
\bar{y} &= \frac{1}{n} \sum_{i=1}^n y_i \\
\bar{x} &= \frac{1}{n} \sum_{i=1}^n x_i \; .
\end{split}
\end{equation}

Note that the posterior distribution is proportional to the joint likelihood ($\rightarrow$ Proof "post-jl"):

\begin{equation} \label{eq:poissexp-post-Poiss-exp-post-s1}
p(\lambda|y) \propto p(y,\lambda) \; .
\end{equation}

Setting $a_n = a_0 + n \bar{y}$ and $b_n = b_0 + n \bar{x}$, the posterior distribution is therefore proportional to

\begin{equation} \label{eq:poissexp-post-Poiss-exp-post-s2}
p(\lambda|y) \propto \lambda^{a_n-1} \cdot \exp\left[-b_n \lambda\right]
\end{equation}

which, when normalized to one, results in the probability density function of the gamma distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}):

\begin{equation} \label{eq:poissexp-post-Poiss-exp-post-s3}
p(\lambda|y) = \frac{ {b_n}^{a_n}}{\Gamma(a_0)} \lambda^{a_n-1} \exp\left[-b_n \lambda\right] = \mathrm{Gam}(\lambda; a_n, b_n) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Gelman A, Carlin JB, Stern HS, Dunson DB, Vehtari A, Rubin DB (2014): "Other standard single-parameter models"; in: \textit{Bayesian Data Analysis}, 3rd edition, ch. 2.6, p. 45, eq. 2.15; URL: \url{http://www.stat.columbia.edu/~gelman/book/}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P42 | shortcut: poissexp-post | author: JoramSoch | date: 2020-02-04, 14:42.


\subsubsection[\textbf{Log model evidence}]{Log model evidence} \label{sec:poissexp-lme}

\vspace{1em}
\textbf{Theorem:} Let $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ be a series of observed counts which are independently distributed according to a Poisson distribution ($\rightarrow$ Definition "poiss") with common rate $\lambda$ and concurrent exposures $\left\lbrace x_1, \ldots, x_n \right\rbrace$:

\begin{equation} \label{eq:poissexp-lme-Poiss-exp}
y_i \sim \mathrm{Poiss}(\lambda x_i), \quad i = 1, \ldots, n \; .
\end{equation}

Moreover, assume a gamma prior distribution ($\rightarrow$ Proof \ref{sec:Statistical Models}/\ref{sec:poissexp-prior}) over the model parameter $\lambda$:

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-prior}
p(\lambda) = \mathrm{Gam}(\lambda; a_0, b_0) \; .
\end{equation}

Then, the log model evidence ($\rightarrow$ Definition "lme") for this model is

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-LME}
\begin{split}
\log p(y|m) = &\sum_{i=1}^n y_i \log(x_i) - \sum_{i=1}^n \log y_i ! + \\ 
&\log \Gamma(a_n) - \log \Gamma(a_0) + a_0 \log b_0 - a_n \log b_n \; .
\end{split}
\end{equation}

where the posterior hyperparameters ($\rightarrow$ Definition "post-hyp") are given by

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-post-par}
\begin{split}
a_n &= a_0 + n \bar{y} \\
a_n &= a_0 + n \bar{x} \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the Poisson distribution ($\rightarrow$ Proof "poiss-pmf"), the likelihood function ($\rightarrow$ Definition "lf") for each observation implied by \eqref{eq:poissexp-lme-Poiss-exp} is given by

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-LF-s1}
p(y_i|\lambda) = \mathrm{Poiss}(y_i; \lambda x_i) = \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !}
\end{equation}

and because observations are independent ($\rightarrow$ Definition "ind"), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-LF-s2}
p(y|\lambda) = \prod_{i=1}^n p(y_i|\lambda) = \prod_{i=1}^n \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !} \; .
\end{equation}

Combining the likelihood function \eqref{eq:poissexp-lme-Poiss-exp-LF-s2} with the prior distribution \eqref{eq:poissexp-lme-Poiss-exp-prior}, the joint likelihood ($\rightarrow$ Definition "jl") of the model is given by

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-JL-s1}
\begin{split}
p(y,\lambda) &= p(y|\lambda) \, p(\lambda) \\
&= \prod_{i=1}^n \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !} \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \; .
\end{split}
\end{equation}

Resolving the product in the joint likelihood, we have

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-JL-s2}
\begin{split}
p(y,\lambda) &= \prod_{i=1}^n \frac{ {x_i}^{y_i}}{y_i !} \prod_{i=1}^n \lambda^{y_i} \prod_{i=1}^n \exp\left[-\lambda x_i\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \lambda^{\sum_{i=1}^n y_i} \exp\left[-\lambda \sum_{i=1}^n x_i\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \lambda^{n \bar{y}} \exp\left[-n \bar{x} \lambda\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \frac{ {b_0}^{a_0}}{\Gamma(a_0)}  \cdot \lambda^{a_0 + n \bar{y} - 1} \cdot \exp\left[-(b_0 + n \bar{x}) \lambda\right] \\
\end{split}
\end{equation}

where $\bar{y}$ and $\bar{x}$ are the means ($\rightarrow$ Proof "ev-sample") of $y$ and $x$ respectively:

\begin{equation} \label{eq:poissexp-lme-xy-mean}
\begin{split}
\bar{y} &= \frac{1}{n} \sum_{i=1}^n y_i \\
\bar{x} &= \frac{1}{n} \sum_{i=1}^n x_i \; .
\end{split}
\end{equation}

Note that the model evidence is the marginal density of the joint likelihood ($\rightarrow$ Definition "ml"):

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-ME}
p(y) = \int p(y,\lambda) \, \mathrm{d}\lambda \; .
\end{equation}

Setting $a_n = a_0 + n \bar{y}$ and $b_n = b_0 + n \bar{x}$, the joint likelihood can also be written as

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-JL-s3}
p(y,\lambda) = \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \frac{\Gamma(a_n)}{ {b_n}^{a_n}} \cdot \frac{ {b_n}^{a_n}}{\Gamma(a_n)} \lambda^{a_n-1} \exp\left[-b_n \lambda\right] \; .
\end{equation}

Using the probability density function of the gamma distribution ($\rightarrow$ Proof \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), $\lambda$ can now be integrated out easily

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-ME-qed}
\begin{split}
\mathrm{p}(y) &= \int \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \frac{\Gamma(a_n)}{ {b_n}^{a_n}} \cdot \frac{ {b_n}^{a_n}}{\Gamma(a_n)} \lambda^{a_n-1} \exp\left[-b_n \lambda\right] \, \mathrm{d}\lambda \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \frac{\Gamma(a_n)}{\Gamma(a_0)} \frac{ {b_0}^{a_0}}{ {b_n}^{a_n}} \int \mathrm{Gam}(\lambda; a_n, b_n) \, \mathrm{d}\lambda \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \frac{\Gamma(a_n)}{\Gamma(a_0)} \frac{ {b_0}^{a_0}}{ {b_n}^{a_n}} \; ,
\end{split}
\end{equation}

such that the log model evidence ($\rightarrow$ Definition "lme") is shown to be

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-LME-qed}
\begin{split}
\log p(y|m) = &\sum_{i=1}^n y_i \log(x_i) - \sum_{i=1}^n \log y_i ! + \\ 
&\log \Gamma(a_n) - \log \Gamma(a_0) + a_0 \log b_0 - a_n \log b_n \; .
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P43 | shortcut: poissexp-lme | author: JoramSoch | date: 2020-02-04, 15:12.


\pagebreak
\section{Probability data}

\subsection{Beta-distributed data}

\subsubsection[\textbf{Method of moments}]{Method of moments} \label{sec:beta-mom}

\vspace{1em}
\textbf{Theorem:} Let $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ be a set of observed counts independent and identically distributed ($\rightarrow$ Definition "iid") according to a beta distribution ($\rightarrow$ Definition "beta") with shapes $\alpha$ and $\beta$:

\begin{equation} \label{eq:beta-mom-Beta}
y_i \sim \mathrm{Bet}(\alpha,\beta), \quad i = 1, \ldots, n \; .
\end{equation}

Then, the method-of-moments estimates ($\rightarrow$ Definition "mom") for the shape parameters $\alpha$ and $\beta$ are given by

\begin{equation} \label{eq:beta-mom-Beta-MoM}
\begin{split}
\hat{\alpha} &= \bar{y} \left( \frac{\bar{y} (1-\bar{y})}{\bar{v}} - 1  \right) \\
\hat{\beta} &= (1-\bar{y}) \left( \frac{\bar{y} (1-\bar{y})}{\bar{v}} - 1  \right)
\end{split}
\end{equation}

where $\bar{y}$ is the sample mean ($\rightarrow$ Proof "ev-sample") and $\bar{v}$ is the unbiased sample variance ($\rightarrow$ Proof "var-unbias"):

\begin{equation} \label{eq:beta-mom-y-mean-var}
\begin{split}
\bar{y} &= \frac{1}{n} \sum_{i=1}^n y_i \\
\bar{v} &= \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2 \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} Mean ($\rightarrow$ Proof "beta-mean") and variance ($\rightarrow$ Proof "beta-var") of the beta distribution ($\rightarrow$ Definition "beta") in terms of the parameters $\alpha$ and $\beta$ are given by

\begin{equation} \label{eq:beta-mom-Beta-E-Var}
\begin{split}
\mathrm{E}(X) &= \frac{\alpha}{\alpha+\beta} \\
\mathrm{Var}(X) &= \frac{\alpha\beta}{(\alpha+\beta)^2 (\alpha+\beta+1)} \; .
\end{split}
\end{equation}

Thus, matching the moments ($\rightarrow$ Definition "mom") requires us to solve the following equation system for $\alpha$ and $\beta$:

\begin{equation} \label{eq:beta-mom-Beta-mean-var}
\begin{split}
\bar{y} &= \frac{\alpha}{\alpha+\beta} \\
\bar{v} &= \frac{\alpha\beta}{(\alpha+\beta)^2 (\alpha+\beta+1)} \; .
\end{split}
\end{equation}

From the first equation, we can deduce:

\begin{equation} \label{eq:beta-mom-beta-as-alpha}
\begin{split}
\bar{y}(\alpha+\beta) &= \alpha \\
\alpha \bar{y} + \beta \bar{y} &= \alpha \\
\beta \bar{y} &= \alpha - \alpha \bar{y} \\
\beta &= \frac{\alpha}{\bar{y}} - \alpha \\
\beta &= \alpha \left( \frac{1}{\bar{y}} - 1 \right) \; .
\end{split}
\end{equation}

If we define $q = 1/\bar{y} - 1$ and plug \eqref{eq:beta-mom-beta-as-alpha} into the second equation, we have:

\begin{equation} \label{eq:beta-mom-alpha-as-q}
\begin{split}
\bar{v} &= \frac{\alpha \cdot \alpha q}{(\alpha + \alpha q)^2 (\alpha + \alpha q + 1)} \\
&= \frac{\alpha^2 q}{(\alpha (1+q))^2 (\alpha (1+q) + 1)} \\
&= \frac{q}{(1+q)^2 (\alpha (1+q) + 1)} \\
&= \frac{q}{\alpha (1+q)^3 + (1+q)^2} \\
q &= \bar{v} \left[ \alpha (1+q)^3 + (1+q)^2 \right] \; .
\end{split}
\end{equation}

Noting that $1+q = 1/\bar{y}$ and $q = (1-\bar{y})/\bar{y}$, one obtains for $\alpha$:

\begin{equation} \label{eq:beta-mom-Beta-MoM-alpha}
\begin{split}
\frac{1-\bar{y}}{\bar{y}} &= \bar{v} \left[ \frac{\alpha}{\bar{y}^3} + \frac{1}{\bar{y}^2} \right] \\
\frac{1-\bar{y}}{\bar{y} \, \bar{v}} &= \frac{\alpha}{\bar{y}^3} + \frac{1}{\bar{y}^2} \\
\frac{\bar{y}^3(1-\bar{y})}{\bar{y} \, \bar{v}} &= \alpha + \bar{y} \\
\alpha &= \frac{\bar{y}^2(1-\bar{y})}{\bar{v}} - \bar{y} \\
&= \bar{y} \left( \frac{\bar{y} (1-\bar{y})}{\bar{v}} - 1 \right) \; .
\end{split}
\end{equation}

Plugging this into equation \eqref{eq:beta-mom-beta-as-alpha}, one obtains for $\beta$:

\begin{equation} \label{eq:beta-mom-Beta-MoM-beta}
\begin{split}
\beta &= \bar{y} \left( \frac{\bar{y} (1-\bar{y})}{\bar{v}} - 1 \right) \cdot \left( \frac{1-\bar{y}}{\bar{y}} \right) \\
&= (1-\bar{y}) \left( \frac{\bar{y} (1-\bar{y})}{\bar{v}} - 1 \right) \; .
\end{split}
\end{equation}

Together, \eqref{eq:beta-mom-Beta-MoM-alpha} and \eqref{eq:beta-mom-Beta-MoM-beta} constitute the method-of-moment estimates of $\alpha$ and $\beta$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Beta distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-20; URL: \url{https://en.wikipedia.org/wiki/Beta_distribution#Method_of_moments}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P28 | shortcut: beta-mom | author: JoramSoch | date: 2020-01-22, 02:53.


\pagebreak
\section{Categorical data}

\subsection{Binomial observations}

\subsubsection[\textbf{Conjugate prior distribution}]{Conjugate prior distribution} \label{sec:bin-prior}

\vspace{1em}
\textbf{Theorem:} Let $y$ be the number of successes resulting from $n$ independent trials with unknown success probability $p$, such that $y$ follows a binomial distribution ($\rightarrow$ Definition "bin"):

\begin{equation} \label{eq:bin-prior-Bin}
y \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Then, the conjugate prior ($\rightarrow$ Definition "prior-conj") for the model parameter $p$ is a beta distribution ($\rightarrow$ Definition "beta"):

\begin{equation} \label{eq:bin-prior-Beta}
\mathrm{p}(p) = \mathrm{Bet}(p; \alpha_0, \beta_0) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the binomial distribution ($\rightarrow$ Proof "bin-pmf"), the likelihood function ($\rightarrow$ Definition "lf") implied by \eqref{eq:bin-prior-Bin} is given by

\begin{equation} \label{eq:bin-prior-Bin-LF}
\mathrm{p}(y|p) = {n \choose y} \, p^y \, (1-p)^{n-y} \; .
\end{equation}

In other words, the likelihood function is proportional to a power of $p$ times a power of $(1-p)$:

\begin{equation} \label{eq:bin-prior-Bin-LF-prop}
\mathrm{p}(y|p) \propto p^y \, (1-p)^{n-y} \; .
\end{equation}

The same is true for a beta distribution over $p$

\begin{equation} \label{eq:bin-prior-Bin-prior-s1}
\mathrm{p}(p) = \mathrm{Bet}(p; \alpha_0, \beta_0)
\end{equation}

the probability density function of which ($\rightarrow$ Proof "beta-pdf")

\begin{equation} \label{eq:bin-prior-Bin-prior-s2}
\mathrm{p}(p) = \frac{1}{B(\alpha_0,\beta_0)} \, p^{\alpha_0-1} \, (1-p)^{\beta_0-1}
\end{equation}

exhibits the same proportionality

\begin{equation} \label{eq:bin-prior-Bin-prior-s3}
\mathrm{p}(p) \propto p^{\alpha_0-1} \, (1-p)^{\beta_0-1}
\end{equation}

and is therefore conjugate relative to the likelihood.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-23; URL: \url{https://en.wikipedia.org/wiki/Binomial_distribution#Estimation_of_parameters}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P29 | shortcut: bin-prior | author: JoramSoch | date: 2020-01-23, 23:38.


\subsubsection[\textbf{Posterior distribution}]{Posterior distribution} \label{sec:bin-post}

\vspace{1em}
\textbf{Theorem:} Let $y$ be the number of successes resulting from $n$ independent trials with unknown success probability $p$, such that $y$ follows a binomial distribution ($\rightarrow$ Definition "bin"):

\begin{equation} \label{eq:bin-post-Bin}
y \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Moreover, assume a beta prior distribution ($\rightarrow$ Proof \ref{sec:Statistical Models}/\ref{sec:bin-prior}) over the model parameter $p$:

\begin{equation} \label{eq:bin-post-Bin-prior}
\mathrm{p}(p) = \mathrm{Bet}(p; \alpha_0, \beta_0) \; .
\end{equation}

Then, the posterior distribution ($\rightarrow$ Definition "post") is also a beta distribution ($\rightarrow$ Definition "beta")

\begin{equation} \label{eq:bin-post-Bin-post}
\mathrm{p}(p|y) = \mathrm{Bet}(p; \alpha_n, \beta_n) \; .
\end{equation}

and the posterior hyperparameters ($\rightarrow$ Definition "post-hyp") are given by

\begin{equation} \label{eq:bin-post-Bin-post-par}
\begin{split}
\alpha_n &= \alpha_0 + y \\
\beta_n &= \beta_0 + (n-y) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the binomial distribution ($\rightarrow$ Proof "bin-pmf"), the likelihood function ($\rightarrow$ Definition "lf") implied by \eqref{eq:bin-post-Bin} is given by

\begin{equation} \label{eq:bin-post-Bin-LF}
\mathrm{p}(y|p) = {n \choose y} \, p^y \, (1-p)^{n-y} \; .
\end{equation}

Combining the likelihood function \eqref{eq:bin-post-Bin-LF} with the prior distribution \eqref{eq:bin-post-Bin-prior}, the joint likelihood ($\rightarrow$ Definition "jl") of the model is given by

\begin{equation} \label{eq:bin-post-Bin-JL}
\begin{split}
\mathrm{p}(y,p) &= \mathrm{p}(y|p) \, \mathrm{p}(p) \\
&= {n \choose y} \, p^y \, (1-p)^{n-y} \cdot frac{1}{B(\alpha_0,\beta_0)} \, p^{\alpha_0-1} \, (1-p)^{\beta_0-1} \\
&= \frac{1}{B(\alpha_0,\beta_0)} {n \choose y} \, p^{\alpha_0+y-1} \, (1-p)^{\beta_0+(n-y)-1} \; .
\end{split}
\end{equation}

Note that the posterior distribution is proportional to the joint likelihood ($\rightarrow$ Proof "post-jl"):

\begin{equation} \label{eq:bin-post-Bin-post-s1}
\mathrm{p}(p|y) \propto \mathrm{p}(y,p) \; .
\end{equation}

Setting $\alpha_n = \alpha_0 + y$ and $\beta_n = \beta_0 + (n-y)$, the posterior distribution is therefore proportional to

\begin{equation} \label{eq:bin-post-Bin-post-s2}
\mathrm{p}(p|y) \propto p^{\alpha_n-1} \, (1-p)^{\beta_n-1}
\end{equation}

which, when normalized to one, results in the probability density function of the beta distribution ($\rightarrow$ Proof "beta-pdf"):

\begin{equation} \label{eq:bin-post-Bin-post-qed}
\mathrm{p}(p|y) = \frac{1}{B(\alpha_n,\beta_n)} \, p^{\alpha_n-1} \, (1-p)^{\beta_n-1} = \mathrm{Bet}(p; \alpha_n, \beta_n) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-23; URL: \url{https://en.wikipedia.org/wiki/Binomial_distribution#Estimation_of_parameters}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P30 | shortcut: bin-post | author: JoramSoch | date: 2020-01-24, 00:20.


\subsubsection[\textbf{Log model evidence}]{Log model evidence} \label{sec:bin-lme}

\vspace{1em}
\textbf{Theorem:} Let $y$ be the number of successes resulting from $n$ independent trials with unknown success probability $p$, such that $y$ follows a binomial distribution ($\rightarrow$ Definition "bin"):

\begin{equation} \label{eq:bin-lme-Bin}
y \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Moreover, assume a beta prior distribution ($\rightarrow$ Proof \ref{sec:Statistical Models}/\ref{sec:bin-prior}) over the model parameter $p$:

\begin{equation} \label{eq:bin-lme-Bin-prior}
\mathrm{p}(p) = \mathrm{Bet}(p; \alpha_0, \beta_0) \; .
\end{equation}

Then, the log model evidence ($\rightarrow$ Definition "lme") for this model is

\begin{equation} \label{eq:bin-lme-Bin-LME}
\log \mathrm{p}(y|m) = \log {n \choose y} + \log B(\alpha_n,\beta_n) - \log B(\alpha_0,\beta_0)
\end{equation}

where the posterior hyperparameters ($\rightarrow$ Definition "post-hyp") are given by

\begin{equation} \label{eq:bin-lme-Bin-post-par}
\begin{split}
\alpha_n &= \alpha_0 + y \\
\beta_n &= \beta_0 + (n-y) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the binomial distribution ($\rightarrow$ Proof "bin-pmf"), the likelihood function ($\rightarrow$ Definition "lf") implied by \eqref{eq:bin-lme-Bin} is given by

\begin{equation} \label{eq:bin-lme-Bin-LF}
\mathrm{p}(y|p) = {n \choose y} \, p^y \, (1-p)^{n-y} \; .
\end{equation}

Combining the likelihood function \eqref{eq:bin-lme-Bin-LF} with the prior distribution \eqref{eq:bin-lme-Bin-prior}, the joint likelihood ($\rightarrow$ Definition "jl") of the model is given by

\begin{equation} \label{eq:bin-lme-Bin-JL-s1}
\begin{split}
\mathrm{p}(y,p) &= \mathrm{p}(y|p) \, \mathrm{p}(p) \\
&= {n \choose y} \, p^y \, (1-p)^{n-y} \cdot frac{1}{B(\alpha_0,\beta_0)} \, p^{\alpha_0-1} \, (1-p)^{\beta_0-1} \\
&= {n \choose y} \, \frac{1}{B(\alpha_0,\beta_0)} \, p^{\alpha_0+y-1} \, (1-p)^{\beta_0+(n-y)-1} \; .
\end{split}
\end{equation}

Note that the model evidence is the marginal density of the joint likelihood ($\rightarrow$ Definition "ml"):

\begin{equation} \label{eq:bin-lme-Bin-ME}
\mathrm{p}(y) = \int \mathrm{p}(y,p) \, \mathrm{d}p \; .
\end{equation}

Setting $\alpha_n = \alpha_0 + y$ and $\beta_n = \beta_0 + (n-y)$, the joint likelihood can also be written as

\begin{equation} \label{eq:bin-lme-Bin-JL-s2}
\mathrm{p}(y,p) = {n \choose y} \, \frac{1}{B(\alpha_0,\beta_0)} \, \frac{B(\alpha_n,\beta_n)}{1} \, \frac{1}{B(\alpha_n,\beta_n)} \, p^{\alpha_n-1} \, (1-p)^{\beta_n-1} \; .
\end{equation}

Using the probability density function of the beta distribution ($\rightarrow$ Proof "beta-pdf"), $p$ can now be integrated out easily

\begin{equation} \label{eq:bin-lme-Bin-ME-qed}
\begin{split}
\mathrm{p}(y) &= \int {n \choose y} \, \frac{1}{B(\alpha_0,\beta_0)} \, \frac{B(\alpha_n,\beta_n)}{1} \, \frac{1}{B(\alpha_n,\beta_n)} \, p^{\alpha_n-1} \, (1-p)^{\beta_n-1} \, \mathrm{d}p \\
&= {n \choose y} \, \frac{B(\alpha_n,\beta_n)}{B(\alpha_0,\beta_0)} \int \mathrm{Bet}(p; \alpha_n, \beta_n) \, \mathrm{d}p \\
&= {n \choose y} \, \frac{B(\alpha_n,\beta_n)}{B(\alpha_0,\beta_0)} \; ,
\end{split}
\end{equation}

such that the log model evidence ($\rightarrow$ Definition "lme") is shown to be

\begin{equation} \label{eq:bin-lme-Bin-LME-qed}
\log \mathrm{p}(y|m) = \log {n \choose y} + \log B(\alpha_n,\beta_n) - \log B(\alpha_0,\beta_0) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Beta-binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-24; URL: \url{https://en.wikipedia.org/wiki/Beta-binomial_distribution#Motivation_and_derivation}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P31 | shortcut: bin-lme | author: JoramSoch | date: 2020-01-24, 00:44.




% Chapter 4 %
\chapter{Model Selection} \label{sec:Model Selection} \newpage

\pagebreak
\section{Goodness-of-fit measures}

\subsection{R-squared}

\subsubsection[\textbf{Derivation of R² and adjusted R²}]{Derivation of R² and adjusted R²} \label{sec:rsq-der}

\vspace{1em}
\textbf{Theorem:} Given a linear regression model ($\rightarrow$ Definition "mlr")

\begin{equation} \label{eq:rsq-der-rsq-mlr}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)
\end{equation}

with $n$ independent observations and $p$ independent variables,

1) the coefficient of determination is given by

\begin{equation} \label{eq:rsq-der-R2}
R^2 = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}}
\end{equation}

2) the adjusted coefficient of determination is

\begin{equation} \label{eq:rsq-der-R2-adj}
R^2_{\mathrm{adj}} = 1 - \frac{\mathrm{RSS}/(n-p)}{\mathrm{TSS}/(n-1)}
\end{equation}

where the residual ($\rightarrow$ Definition "rss") and total sum of squares ($\rightarrow$ Definition "tss") are

\begin{equation} \label{eq:rsq-der-SS}
\begin{split}
\mathrm{RSS} &= \sum_{i=1}^{n} (y_i - \hat{y}_i)^2, \quad \hat{y} = X\hat{\beta} \\
\mathrm{TSS} &= \sum_{i=1}^{n} (y_i - \bar{y})^2\;, \quad \bar{y} = \frac{1}{n} \sum_{i=1}^n y_i \\
\end{split}
\end{equation}

where $X$ is the $n \times p$ design matrix and $\hat{\beta}$ are the ordinary least squares ($\rightarrow$ Definition "mlr-ols") estimates.


\vspace{1em}
\textbf{Proof:} The coefficient of determination $R^2$ is defined as ($\rightarrow$ Definition "rsq") the proportion of the variance explained by the independent variables, relative to the total variance in the data.

\vspace{1em}
1) If we define the explained sum of squares ($\rightarrow$ Definition "ess") as

\begin{equation} \label{eq:rsq-der-ESS}
\mathrm{ESS} = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 \; ,
\end{equation}

then $R^2$ is given by

\begin{equation} \label{eq:rsq-der-R2-s1}
R^2 = \frac{\mathrm{ESS}}{\mathrm{TSS}} \; .
\end{equation}

which is equal to

\begin{equation} \label{eq:rsq-der-R2-s2}
R^2 = \frac{\mathrm{TSS}-\mathrm{RSS}}{\mathrm{TSS}} = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} \; ,
\end{equation}

because ($\rightarrow$ Proof "mlr-pss") $\mathrm{TSS} = \mathrm{ESS} + \mathrm{RSS}$.

\vspace{1em}
2) Using \eqref{eq:rsq-der-SS}, the coefficient of determination can be also written as:

\begin{equation} \label{eq:rsq-der-R2'}
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} = 1 - \frac{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\frac{1}{n} \sum_{i=1}^{n} (y_i - \bar{y})^2} \; .
\end{equation}

If we replace the variance estimates by their unbiased estimators ($\rightarrow$ Proof "resvar-bias"), we obtain

\begin{equation} \label{eq:rsq-der-R2-adj'}
R^2_{\mathrm{adj}} = 1 - \frac{\frac{1}{n-p} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\frac{1}{n-1} \sum_{i=1}^{n} (y_i - \bar{y})^2} = 1 - \frac{\mathrm{RSS}/\mathrm{df}_r}{\mathrm{TSS}/\mathrm{df}_t}
\end{equation}

where $\mathrm{df}_r = n-p$ and $\mathrm{df}_t = n-1$ are the residual and total degrees of freedom ($\rightarrow$ Definition "dof").

\vspace{1em}
This gives the adjusted $R^2$ which adjusts $R^2$ for the number of explanatory variables.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2019): "Coefficient of determination"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2019-12-06; URL: \url{https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2}.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P8 | shortcut: rsq-der | author: JoramSoch | date: 2019-12-06, 11:19.


\subsubsection[\textbf{Relationship to maximum log-likelihood}]{Relationship to maximum log-likelihood} \label{sec:rsq-mll}

\vspace{1em}
\textbf{Theorem:} Given a linear regression model ($\rightarrow$ Definition "mlr") with independent observations

\begin{equation} \label{eq:rsq-mll-MLR}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; ,
\end{equation}

the coefficient of determination ($\rightarrow$ Definition "rsq") can be expressed in terms of the maximum log-likelihood ($\rightarrow$ Definition "mll") as

\begin{equation} \label{eq:rsq-mll-R2-MLL}
R^2 = 1 - \left( \exp[\Delta\mathrm{MLL}] \right)^{-2/n}
\end{equation}

where $n$ is the number of observations and $\Delta\mathrm{MLL}$ is the difference in maximum log-likelihood between the model given by \eqref{eq:rsq-mll-MLR} and a linear regression model with only a constant regressor.


\vspace{1em}
\textbf{Proof:} First, we express the maximum log-likelihood ($\rightarrow$ Definition "mll") (MLL) of a linear regression model in terms of its residual sum of squares ($\rightarrow$ Definition "rss") (RSS). The model in \eqref{eq:rsq-mll-MLR} implies the following log-likelihood function ($\rightarrow$ Definition "llf")

\begin{equation} \label{eq:rsq-mll-MLR-LL}
\mathrm{LL}(\beta,\sigma^2) = \log p(y|\beta,\sigma^2) = - \frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} (y - X\beta)^\mathrm{T} (y - X\beta) \; ,
\end{equation}

such that maximum likelihood estimates are ($\rightarrow$ Proof "mlr-mle")

\begin{equation} \label{eq:rsq-mll-MLR-MLE-beta}
\hat{\beta} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y
\end{equation}

\begin{equation} \label{eq:rsq-mll-MLR-MLE-sigma2}
\hat{\sigma}^2 = \frac{1}{n} (y - X\hat{\beta})^\mathrm{T} (y - X\hat{\beta})
\end{equation}

and the residual sum of squares ($\rightarrow$ Definition "rss") is

\begin{equation} \label{eq:rsq-mll-RSS}
\mathrm{RSS} = \sum_{i=1}^n \hat{\varepsilon}_i = \hat{\varepsilon}^\mathrm{T} \hat{\varepsilon} = (y - X\hat{\beta})^\mathrm{T} (y - X\hat{\beta}) = n \cdot \hat{\sigma}^2 \; .
\end{equation}

Since $\hat{\beta}$ and $\hat{\sigma}^2$ are maximum likelihood estimates ($\rightarrow$ Definition "mle"), plugging them into the log-likelihood function gives the maximum log-likelihood:

\begin{equation} \label{eq:rsq-mll-MLR-MLL}
\mathrm{MLL} = \mathrm{LL}(\hat{\beta},\hat{\sigma}^2) = - \frac{n}{2} \log(2\pi\hat{\sigma}^2) - \frac{1}{2\hat{\sigma}^2} (y - X\hat{\beta})^\mathrm{T} (y - X\hat{\beta}) \; .
\end{equation}

With \eqref{eq:rsq-mll-RSS} for the first $\hat{\sigma}^2$ and \eqref{eq:rsq-mll-MLR-MLE-sigma2} for the second $\hat{\sigma}^2$, the MLL becomes

\begin{equation} \label{eq:rsq-mll-MLR-MLL-RSS}
\mathrm{MLL} = - \frac{n}{2} \log(\mathrm{RSS}) - \frac{n}{2} \log \left( \frac{2\pi}{n} \right) - \frac{n}{2} \; .
\end{equation}

Second, we establish the relationship between maximum log-likelihood (MLL) and coefficient of determination (R²). Consider the two models

\begin{equation} \label{eq:rsq-mll-m0-m1}
\begin{split}
m_0: \; X_0 &= 1_n \\
m_1: \; X_1 &= X
\end{split}
\end{equation}

For $m_1$, the residual sum of squares is given by \eqref{eq:rsq-mll-RSS}; and for $m_0$, the residual sum of squares is equal to the total sum of squares ($\rightarrow$ Definition "tss"):

\begin{equation} \label{eq:rsq-mll-TSS}
\mathrm{TSS} = \sum_{i=1}^n (y_i - \bar{y})^2 \; .
\end{equation}

Using \eqref{eq:rsq-mll-MLR-MLL-RSS}, we can therefore write

\begin{equation} \label{eq:rsq-mll-MLR-DMLL}
\Delta\mathrm{MLL} = \mathrm{MLL}(m_1) - \mathrm{MLL}(m_0) = - \frac{n}{2} \log(\mathrm{RSS}) + \frac{n}{2} \log(\mathrm{TSS}) \; .
\end{equation}

Exponentiating both sides of the equation, we have:

\begin{equation} \label{eq:rsq-mll-MLR-DMLL-RTSS}
\begin{split}
\mathrm{exp}[\Delta\mathrm{MLL}] &= \mathrm{exp} \left[ - \frac{n}{2} \log(\mathrm{RSS}) + \frac{n}{2} \log(\mathrm{TSS}) \right] \\
&= \left( \mathrm{exp} \left[ \log(\mathrm{RSS}) - \log(\mathrm{TSS}) \right] \right)^{-n/2} \\
&= \left( \frac{\mathrm{exp}[\log(\mathrm{RSS})]}{\mathrm{exp}[\log(\mathrm{TSS})]} \right)^{-n/2} \\
&= \left( \frac{\mathrm{RSS}}{\mathrm{TSS}} \right)^{-n/2} \; .
\end{split}
\end{equation}

Taking both sides to the power of $-2/n$ and subtracting from 1, we have

\begin{equation} \label{eq:rsq-mll-MLR-DMLL-R2}
\begin{split}
\left( \mathrm{exp}[\Delta\mathrm{MLL}] \right)^{-2/n} &= \frac{\mathrm{RSS}}{\mathrm{TSS}} \\
1 - \left( \mathrm{exp}[\Delta\mathrm{MLL}] \right)^{-2/n} &= 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} = R^2
\end{split}
\end{equation}

which proves the identity given above.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P14 | shortcut: rsq-mll | author: JoramSoch | date: 2020-01-08, 04:46.


\pagebreak
\section{Classical information criteria}

\subsection{Bayesian information criterion}

\subsubsection[\textbf{Derivation}]{Derivation} \label{sec:bic-der}

\vspace{1em}
\textbf{Theorem:} Let $p(y \mid \theta, m)$ be the likelihood function ($\rightarrow$ Definition "lf") of a generative model ($\rightarrow$ Definition "gm") $m \in \mathcal{M}$ with model parameters $\theta \in \Theta$ describing measured data $y \in \mathbb{R}^n$. Let $p(\theta \mid m)$ be a prior distribution ($\rightarrow$ Definition "prior") on the model parameters. Assume that likelihood function and prior density are twice differentiable.

Then, as the number of data points goes to infinity, an approximation to the log-marginal likelihood ($\rightarrow$ Definition "ml") $\log p(y \mid m)$, up to constant terms not depending on the model, is given by the Bayesian information criterion ($\rightarrow$ Definition "bic") (BIC) as

\begin{equation} \label{eq:bic-der-BIC}
-2 \log p(y \mid m) \approx \mathrm{BIC}(m) = -2 \log p(y \mid \hat{\theta}, m) + p \log n
\end{equation}

where $\hat{\theta}$ is the maximum likelihood estimator ($\rightarrow$ Definition "mle") (MLE) of $\theta$, $n$ is the number of data points and $p$ is the number of model parameters.


\vspace{1em}
\textbf{Proof:} Let $\mathrm{LL}(\theta)$ be the log-likelihood function ($\rightarrow$ Definition "llf")

\begin{equation} \label{eq:bic-der-LL}
\mathrm{LL}(\theta) = \log p(y|\theta,m)
\end{equation}

and define the functions $g$ and $h$ as follows:

\begin{equation} \label{eq:bic-der-gh}
\begin{split}
g(\theta) &= p(\theta|m) \\
h(\theta) &= \frac{1}{n} \, \mathrm{LL}(\theta) \; .
\end{split}
\end{equation}

Then, the marginal likelihood ($\rightarrow$ Definition "ml") can be written as follows:

\begin{equation} \label{eq:bic-der-ML}
\begin{split}
p(y|m) &= \int_{\Theta} p(y|\theta,m) \, p(\theta|m) \, \mathrm{d}\theta \\
&= \int_{\Theta} \mathrm{exp}\left[n \, h(\theta)\right] \, g(\theta) \, \mathrm{d}\theta \; .
\end{split}
\end{equation}

This is an integral suitable for Laplace approximation which states that

\begin{equation} \label{eq:bic-der-LA}
\int_{\Theta} \mathrm{exp}\left[n \, h(\theta)\right] \, g(\theta) \, \mathrm{d}\theta = \left( \sqrt{\frac{2 \pi}{n}} \right)^p \mathrm{exp}\left[n \, h(\theta_0)\right] \left( g(\theta_0) \left| J(\theta_0) \right|^{-1/2} + O(1/n) \right)
\end{equation}

where $\theta_0$ is the value that maximizes $h(\theta)$ and $J(\theta_0)$ is the Hessian matrix evaluated at $\theta_0$. In our case, we have $h(\theta) = 1/n \, \mathrm{LL}(\theta)$ such that $\theta_0$ is the maximum likelihood estimator $\hat{\theta}$:

\begin{equation} \label{eq:bic-der-MLE}
\hat{\theta} = \operatorname*{arg\,max}_\theta \mathrm{LL}(\theta) \; .
\end{equation}

With this, \eqref{eq:bic-der-LA} can be applied to \eqref{eq:bic-der-ML} using \eqref{eq:bic-der-gh} to give:

\begin{equation} \label{eq:bic-der-ML-approx}
p(y|m) \approx \left( \sqrt{\frac{2 \pi}{n}} \right)^p p(y|\hat{\theta},m) \, p(\hat{\theta}|m) \, \left| J(\hat{\theta}) \right|^{-1/2} \; .
\end{equation}

Logarithmizing and multiplying with $-2$, we have:

\begin{equation} \label{eq:bic-der-LME-approx}
-2 \log p(y|m) \approx -2 \, \mathrm{LL}(\hat{\theta}) + p \log n - p \log(2 \pi) - 2 \log p(\hat{\theta}|m) + \log \left| J(\hat{\theta}) \right| \; .
\end{equation}

As $n \to \infty$, the last three terms are $O_p(1)$ and can therefore be ignored when comparing between models $\mathcal{M} = \left\lbrace m_1, \ldots, m_M \right\rbrace$ and using $p(y \mid m_j)$ to compute posterior model probabilies ($\rightarrow$ Definition "led-pmp") $p(m_j \mid y)$. With that, the BIC is given as

\begin{equation} \label{eq:bic-der-BIC-qed}
\mathrm{BIC}(m) = -2 \log p(y|\hat{\theta}, m) + p \log n \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Claeskens G, Hjort NL (2008): "The Bayesian information criterion"; in: \textit{Model Selection and Model Averaging}, ch. 3.2, pp. 78-81; URL: \url{https://www.cambridge.org/core/books/model-selection-and-model-averaging/E6F1EC77279D1223423BB64FC3A12C37}; DOI: 10.1017/CBO9780511790485.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P32 | shortcut: bic-der | author: JoramSoch | date: 2020-01-26, 23:36.


\pagebreak
\section{Bayesian model selection}

\subsection{Log model evidence}

\subsubsection[\textbf{Derivation}]{Derivation} \label{sec:lme-der}

\vspace{1em}
\textbf{Theorem:} Let $p(y \mid \theta,m)$ be a likelihood function ($\rightarrow$ Definition "lf") of a generative model ($\rightarrow$ Definition "gm") $m$ for making inferences on model parameters $\theta$ given measured data $y$. Moreover, let $p(\theta \mid m)$ be a prior distribution ($\rightarrow$ Definition "prior") on model parameters $\theta$. Then, the log model evidence ($\rightarrow$ Definition "lme") (LME), also called marginal log-likelihood,

\begin{equation} \label{eq:lme-der-LME-term}
\mathrm{LME}(m) = \log p(y|m) \; ,
\end{equation}

can be expressed

1) as

\begin{equation} \label{eq:lme-der-LME-marg}
\mathrm{LME}(m) = \log \int p(y|\theta,m) \, p(\theta|m) \, \mathrm{d}\theta
\end{equation}

2) or

\begin{equation} \label{eq:lme-der-LME-bayes}
\mathrm{LME}(m) = \log p(y|\theta,m) + \log p(\theta|m) - \log p(\theta|y,m) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:}

1) The first expression is a simple consequence of the law of marginal probability ($\rightarrow$ Proof "lmp") for continuous variables according to which

\begin{equation} \label{eq:lme-der-ME}
p(y|m) = \int p(y|\theta,m) \, p(\theta|m) \, \mathrm{d}\theta
\end{equation}

which, when logarithmized, gives

\begin{equation} \label{eq:lme-der-LME-marg-qed}
\mathrm{LME}(m) = \log p(y|m) = \log \int p(y|\theta,m) \, p(\theta|m) \, \mathrm{d}\theta \; .
\end{equation}

2) The second expression can be derived from Bayes' theorem ($\rightarrow$ Proof \ref{sec:General Theorems}/\ref{sec:bayes-th}) which makes a statement about the posterior distribution ($\rightarrow$ Definition "post"):

\begin{equation} \label{eq:lme-der-BT}
p(\theta|y,m) = \frac{p(y|\theta,m) \, p(\theta|m)}{p(y|m)} \; .
\end{equation}

Rearranging for $p(y \mid m)$ and logarithmizing, we have:

\begin{equation} \label{eq:lme-der-LME-bayes-qed}
\begin{split}
\mathrm{LME}(m) = \log p(y|m) & = \log \frac{p(y|\theta,m) \, p(\theta|m)}{p(\theta|y,m)} \\
&= \log p(y|\theta,m) + \log p(\theta|m) - \log p(\theta|y,m) \; .
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item original work\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P13 | shortcut: lme-der | author: JoramSoch | date: 2020-01-06, 21:27.


\subsubsection[\textbf{Partition into accuracy and complexity}]{Partition into accuracy and complexity} \label{sec:lme-anc}

\vspace{1em}
\textbf{Theorem:} The log model evidence ($\rightarrow$ Definition "lme") can be partitioned into accuracy and complexity

\begin{equation} \label{eq:lme-anc-LME}
\mathrm{LME}(m) = \mathrm{Acc}(m) - \mathrm{Com}(m)
\end{equation}

where the accuracy term is the posterior expectation of the log-likelihood function ($\rightarrow$ Definition "lf")

\begin{equation} \label{eq:lme-anc-Acc}
\mathrm{Acc}(m) = \left\langle p(y|\theta,m) \right\rangle_{p(\theta|y,m)}
\end{equation}

and the complexity penalty is the Kullback-Leibler divergence ($\rightarrow$ Definition "kl") of posterior ($\rightarrow$ Definition "post") from prior ($\rightarrow$ Definition "prior")

\begin{equation} \label{eq:lme-anc-Com}
\mathrm{Com}(m) = \mathrm{KL} \left[ p(\theta|y,m) \, || \, p(\theta|m) \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} We consider Bayesian inference on data $y$ using model $m$ with parameters $\theta$. Then, Bayes' theorem ($\rightarrow$ Proof \ref{sec:General Theorems}/\ref{sec:bayes-th}) makes a statement about the posterior distribution, i.e. the probability of parameters, given the data and the model:

\begin{equation} \label{eq:lme-anc-AnC-s1}
p(\theta|y,m) = \frac{p(y|\theta,m) \, p(\theta|m)}{p(y|m)} \; .
\end{equation}

Rearranging this for the model evidence ($\rightarrow$ Proof \ref{sec:Model Selection}/\ref{sec:lme-der}), we have:

\begin{equation} \label{eq:lme-anc-AnC-s2}
p(y|m) = \frac{p(y|\theta,m) \, p(\theta|m)}{p(\theta|y,m)} \; .
\end{equation}

Logarthmizing both sides of the equation, we obtain:

\begin{equation} \label{eq:lme-anc-AnC-s3}
\log p(y|m) = \log p(y|\theta,m) - \log \frac{p(\theta|y,m)}{p(\theta|m)} \; .
\end{equation}

Now taking the expectation over the posterior distribution yields:

\begin{equation} \label{eq:lme-anc-AnC-s4}
\log p(y|m) = \int p(\theta|y,m) \log p(y|\theta,m) \, \mathrm{d}\theta - \int p(\theta|y,m) \log \frac{p(\theta|y,m)}{p(\theta|m)} \, \mathrm{d}\theta \; .
\end{equation}

By definition, the left-hand side is the log model evidence and the terms on the right-hand side correspond to the posterior expectation of the log-likelihood function and the Kullback-Leibler divergence of posterior from prior

\begin{equation} \label{eq:lme-anc-LME-AnC}
\mathrm{LME}(m) = \left\langle p(y|\theta,m) \right\rangle_{p(\theta|y,m)} - \mathrm{KL} \left[ p(\theta|y,m) \, || \, p(\theta|m) \right]
\end{equation}

which proofs the partition given by \eqref{eq:lme-anc-LME}.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Penny et al. (2007): "Bayesian Comparison of Spatially Regularised General Linear Models"; in: \textit{Human Brain Mapping}, vol. 28, pp. 275–293; URL: \url{https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.20327}; DOI: 10.1002/hbm.20327.
\item Soch et al. (2016): "How to avoid mismodelling in GLM-based fMRI data analysis: cross-validated Bayesian model selection"; in: \textit{NeuroImage}, vol. 141, pp. 469–489; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811916303615}; DOI: 10.1016/j.neuroimage.2016.07.047.
\end{itemize}


\vspace{1em}
\textbf{Metadata:} ID: P3 | shortcut: lme-anc | author: JoramSoch | date: 2019-09-27, 16:13.


\end{document}