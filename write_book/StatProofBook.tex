\documentclass[a4paper,12pt,twoside]{book}

%%% Packages %%%
\usepackage[cm,headings]{fullpage}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{url}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{longtable}

%%% Settings %%%
\pagestyle{headings}
\setlength{\parindent}{0pt}
\raggedbottom
\frenchspacing
\urlstyle{same}
\MakeOuterQuote{"}
\setlist{nolistsep}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\renewcommand{\arraystretch}{1.5}

%%% Format %%%
\renewcommand\thechapter{\Roman{chapter}}
\renewcommand\thesection{\arabic{section}}
\renewcommand\theequation{\arabic{equation}}
\renewcommand\thefootnote{\arabic{footnote}}
\let\Chaptermark\chaptermark
\def\chaptermark#1{\def\Chaptername{#1}\Chaptermark{#1}}
\let\Sectionmark\sectionmark
\def\sectionmark#1{\def\Sectionname{#1}\Sectionmark{#1}}

%%% Title %%%
\title{\Huge{The Book of Statistical Proofs}}
\author{DOI: 10.5281/zenodo.4305949 \\ \url{https://statproofbook.github.io/} \\ StatProofBook@gmail.com}
\date{2024-07-18, 10:50}

\begin{document}


%%% Title %%%
\maketitle

%%% Contents %%%
\pagebreak
\pagenumbering{roman}
\tableofcontents

%%% Text %%%
\newpage
\pagenumbering{arabic}


% Chapter 1 %
\chapter{General Theorems} \label{sec:General Theorems} \newpage

\pagebreak
\section{Probability theory}

\subsection{Random experiments}

\subsubsection[\textit{Random experiment}]{Random experiment} \label{sec:rexp}
\setcounter{equation}{0}

\textbf{Definition:} A random experiment is any repeatable procedure that results in one ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) out of a well-defined set of possible outcomes.

\begin{itemize}

\item The set of possible outcomes is called sample space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:samp-spc}).

\item A set of zero or more outcomes is called a random event ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:reve}).

\item A function that maps from events to probabilities is called a probability function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}).

\end{itemize}

Together, sample space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:samp-spc}), event space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:eve-spc}) and probability function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-spc}) characterize a random experiment.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Experiment (probability theory)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-19; URL: \url{https://en.wikipedia.org/wiki/Experiment_(probability_theory)}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Sample space}]{Sample space} \label{sec:samp-spc}
\setcounter{equation}{0}

\textbf{Definition:} Given a random experiment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rexp}), the set of all possible outcomes from this experiment is called the sample space of the experiment. A sample space is usually denoted as $\Omega$ and specified using set notation.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Sample space"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-11-26; URL: \url{https://en.wikipedia.org/wiki/Sample_space}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Event space}]{Event space} \label{sec:eve-spc}
\setcounter{equation}{0}

\textbf{Definition:} Given a random experiment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rexp}), an event space $\mathcal{E}$ is any set of events, where an event ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:reve}) is any set of zero or more elements from the sample space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:samp-spc}) $\Omega$ of this experiment.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Event (probability theory)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-11-26; URL: \url{https://en.wikipedia.org/wiki/Event_(probability_theory)}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Probability space}]{Probability space} \label{sec:prob-spc}
\setcounter{equation}{0}

\textbf{Definition:} Given a random experiment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rexp}), a probability space $(\Omega, \mathcal{E}, P)$ is a triple consisting of

\begin{itemize}

\item the sample space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:samp-spc}) $\Omega$, i.e. the set of all possible outcomes from this experiment;

\item an event space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:eve-spc}) $\mathcal{E} \subseteq 2^\Omega$, i.e. a set of subsets from the sample space, called events ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:reve});

\item a probability measure $P: \; \mathcal{E} \rightarrow [0,1]$, i.e. a function mapping from the event space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:eve-spc}) to the real numbers, observing the axioms of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}).

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Probability space"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-11-26; URL: \url{https://en.wikipedia.org/wiki/Probability_space#Definition}.
\end{itemize}
\vspace{1em}



\subsection{Random variables}

\subsubsection[\textit{Random event}]{Random event} \label{sec:reve}
\setcounter{equation}{0}

\textbf{Definition:} A random event $E$ is the outcome of a random experiment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rexp}) which can be described by a statement that is either true or false.

\begin{itemize}

\item If the statement is true, the event is said to take place, denoted as $E$.

\item If the statement is false, the complement of $E$ occurs, denoted as $\overline{E}$.

\end{itemize}

In other words, a random event is a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with two possible values (true and false, or 1 and 0). A random experiment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rexp}) with two possible outcomes is called a Bernoulli trial ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Event (probability theory)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-19; URL: \url{https://en.wikipedia.org/wiki/Event_(probability_theory)}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Random variable}]{Random variable} \label{sec:rvar}
\setcounter{equation}{0}

\textbf{Definition:} A random variable may be understood

\begin{itemize}

\item informally, as a real number $X \in \mathbb{R}$ whose value is the outcome of a random experiment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rexp});

\item formally, as a measurable function $X$ defined on a probability space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-spc}) $(\Omega, \mathcal{E}, P)$ that maps from a sample space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:samp-spc}) $\Omega$ to the real numbers $\mathbb{R}$ using an event space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:eve-spc}) $\mathcal{E}$ and a probability function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) $P$;

\item more broadly, as any random quantity $X$ such as a random event ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:reve}), a random scalar ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}), a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) or a random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}).

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Random variable"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-27; URL: \url{https://en.wikipedia.org/wiki/Random_variable#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Random vector}]{Random vector} \label{sec:rvec}
\setcounter{equation}{0}

\textbf{Definition:} A random vector, also called "multivariate random variable", is an $n$-dimensional column vector $X \in \mathbb{R}^{n \times 1}$ whose entries are random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Multivariate random variable"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-27; URL: \url{https://en.wikipedia.org/wiki/Multivariate_random_variable}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Random matrix}]{Random matrix} \label{sec:rmat}
\setcounter{equation}{0}

\textbf{Definition:} A random matrix, also called "matrix-valued random variable", is an $n \times p$ matrix $X \in \mathbb{R}^{n \times p}$ whose entries are random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Equivalently, a random matrix is an $n \times p$ matrix whose columns are $n$-dimensional random vectors ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Random matrix"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-27; URL: \url{https://en.wikipedia.org/wiki/Random_matrix}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Constant}]{Constant} \label{sec:const}
\setcounter{equation}{0}

\textbf{Definition:} A constant is a quantity which does not change and thus always has the same value. From a statistical perspective, a constant is a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) which is equal to its expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean})

\begin{equation} \label{eq:const-EX}
X = \mathrm{E}(X)
\end{equation}

or equivalently, whose variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is zero

\begin{equation} \label{eq:const-VarX}
\mathrm{Var}(X) = 0 \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item ProofWiki (2020): "Definition: Constant"; in: \textit{ProofWiki}, retrieved on 2020-09-09; URL: \url{https://proofwiki.org/wiki/Definition:Constant#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Discrete vs. continuous}]{Discrete vs. continuous} \label{sec:rvar-disc}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$. Then,

\begin{itemize}

\item $X$ is called a discrete random variable, if $\mathcal{X}$ is either a finite set or a countably infinite set; in this case, $X$ can be described by a probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf});

\item $X$ is called a continuous random variable, if $\mathcal{X}$ is an uncountably infinite set; if it is absolutely continuous, $X$ can be described by a probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}).

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Random variable"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-10-29; URL: \url{https://en.wikipedia.org/wiki/Random_variable#Standard_case}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Univariate vs. multivariate}]{Univariate vs. multivariate} \label{sec:rvar-uni}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$. Then,

\begin{itemize}

\item $X$ is called a two-valued random variable or random event ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:reve}), if $\mathcal{X}$ has exactly two elements, e.g. $\mathcal{X} = \left\lbrace E, \overline{E} \right\rbrace$ or $\mathcal{X} = \left\lbrace \mathrm{true}, \mathrm{false} \right\rbrace$ or $\mathcal{X} = \left\lbrace 1, 0 \right\rbrace$;

\item $X$ is called a univariate random variable or random scalar ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}), if $\mathcal{X}$ is one-dimensional, i.e. (a subset of) the real numbers $\mathbb{R}$;

\item $X$ is called a multivariate random variable or random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}), if $\mathcal{X}$ is multi-dimensional, e.g. (a subset of) the $n$-dimensional Euclidean space $\mathbb{R}^n$;

\item $X$ is called a matrix-valued random variable or random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}), if $\mathcal{X}$ is (a subset of) the set of $n \times p$ real matrices $\mathbb{R}^{n \times p}$.

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Multivariate random variable"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-06; URL: \url{https://en.wikipedia.org/wiki/Multivariate_random_variable}.
\end{itemize}
\vspace{1em}



\subsection{Probability}

\subsubsection[\textit{Probability}]{Probability} \label{sec:prob}
\setcounter{equation}{0}

\textbf{Definition:} Let $E$ be a statement about an arbitrary event such as the outcome of a random experiment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rexp}). Then, $p(E)$ is called the probability of $E$ and may be interpreted as

\begin{itemize}

\item (objectivist interpretation of probability:) some physical state of affairs, e.g. the relative frequency of occurrence of $E$, when repeating the experiment ("Frequentist probability"); or

\item (subjectivist interpretation of probability:) a degree of belief in $E$, e.g. the price at which someone would buy or sell a bet that pays 1 unit of utility if $E$ and 0 if not $E$ ("Bayesian probability").

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Probability"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-10; URL: \url{https://en.wikipedia.org/wiki/Probability#Interpretations}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Joint probability}]{Joint probability} \label{sec:prob-joint}
\setcounter{equation}{0}

\textbf{Definition:} Let $A$ and $B$ be two arbitrary statements about random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}), such as statements about the presence or absence of an event or about the value of a scalar, vector or matrix. Then, $p(A,B)$ is called the joint probability of $A$ and $B$ and is defined as the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) that $A$ and $B$ are both true.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Joint probability distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-10; URL: \url{https://en.wikipedia.org/wiki/Joint_probability_distribution}.
\item Jason Browlee (2019): "A Gentle Introduction to Joint, Marginal, and Conditional Probability"; in: \textit{Machine Learning Mastery}, retrieved on 2021-08-01; URL: \url{https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Marginal probability}]{Marginal probability} \label{sec:prob-marg}
\setcounter{equation}{0}

\textbf{Definition:} (law of marginal probability, also called "sum rule") Let $A$ and $X$ be two arbitrary statements about random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}), such as statements about the presence or absence of an event or about the value of a scalar, vector or matrix. Furthermore, assume a joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) distribution $p(A,X)$. Then, $p(A)$ is called the marginal probability of $A$ and,

1) if $X$ is a discrete ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with domain $\mathcal{X}$, is given by

\begin{equation} \label{eq:prob-marg-prob-marg-disc}
p(A) = \sum_{x \in \mathcal{X}} p(A,x) \; ;
\end{equation}

2) if $X$ is a continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with domain $\mathcal{X}$, is given by

\begin{equation} \label{eq:prob-marg-prob-marg-cont}
p(A) = \int_{\mathcal{X}} p(A,x) \, \mathrm{d}x \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Marginal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-10; URL: \url{https://en.wikipedia.org/wiki/Marginal_distribution#Definition}.
\item Jason Browlee (2019): "A Gentle Introduction to Joint, Marginal, and Conditional Probability"; in: \textit{Machine Learning Mastery}, retrieved on 2021-08-01; URL: \url{https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Conditional probability}]{Conditional probability} \label{sec:prob-cond}
\setcounter{equation}{0}

\textbf{Definition:} (law of conditional probability, also called "product rule") Let $A$ and $B$ be two arbitrary statements about random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}), such as statements about the presence or absence of an event or about the value of a scalar, vector or matrix. Furthermore, assume a joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) distribution $p(A,B)$. Then, $p(A \vert B)$ is called the conditional probability that $A$ is true, given that $B$ is true, and is given by

\begin{equation} \label{eq:prob-cond-prob-cond}
p(A|B) = \frac{p(A,B)}{p(B)}
\end{equation}

where $p(B)$ is the marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}) of $B$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Conditional probability"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-10; URL: \url{https://en.wikipedia.org/wiki/Conditional_probability#Definition}.
\item Jason Browlee (2019): "A Gentle Introduction to Joint, Marginal, and Conditional Probability"; in: \textit{Machine Learning Mastery}, retrieved on 2021-08-01; URL: \url{https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Exceedance probability}]{Exceedance probability} \label{sec:prob-exc}
\setcounter{equation}{0}

\textbf{Definition:} Let $X = \left\lbrace X_1, \ldots, X_n \right\rbrace$ be a set of $n$ random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) which the joint probability distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) $p(X) = p(X_1, \ldots, X_n)$. Then, the exceedance probability for random variable $X_i$ is the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) that $X_i$ is larger than all other random variables $X_j, \; j \neq i$:

\begin{equation} \label{eq:prob-exc-EP}
\begin{split}
\varphi(X_i) &= \mathrm{Pr}\left( \forall j \in \left\lbrace 1, \ldots, n | j \neq i \right\rbrace: \, X_i > X_j \right) \\
&= \mathrm{Pr}\left( \bigwedge_{j \neq i} X_i > X_j \right) \\
&= \mathrm{Pr}\left( X_i = \mathrm{max}(\left\lbrace X_1, \ldots, X_n \right\rbrace) \right) \\
&= \int_{X_i = \mathrm{max}(X)} p(X) \, \mathrm{d}X \; .
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Stephan KE, Penny WD, Daunizeau J, Moran RJ, Friston KJ (2009): "Bayesian model selection for group studies"; in: \textit{NeuroImage}, vol. 46, pp. 1004â€“1017, eq. 16; URL: \url{https://www.sciencedirect.com/science/article/abs/pii/S1053811909002638}; DOI: 10.1016/j.neuroimage.2009.03.025.
\item Soch J, Allefeld C (2016): "Exceedance Probabilities for the Dirichlet Distribution"; in: \textit{arXiv stat.AP}, 1611.01439; URL: \url{https://arxiv.org/abs/1611.01439}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Statistical independence}]{Statistical independence} \label{sec:ind}
\setcounter{equation}{0}

\textbf{Definition:} Generally speaking, random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) are statistically independent, if their joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) can be expressed in terms of their marginal probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}).

\vspace{1em}
1) A set of discrete random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X_1, \ldots, X_n$ with possible values $\mathcal{X}_1, \ldots, \mathcal{X}_n$ is called statistically independent, if

\begin{equation} \label{eq:ind-disc-ind}
p(X_1 = x_1, \ldots, X_n = x_n) = \prod_{i=1}^{n} p(X_i = x_i) \quad \text{for all} \; x_i \in \mathcal{X}_i, \; i = 1, \ldots, n
\end{equation}

where $p(x_1, \ldots, x_n)$ are the joint probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) of $X_1, \ldots, X_n$ and $p(x_i)$ are the marginal probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}) of $X_i$.

\vspace{1em}
2) A set of continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X_1, \ldots, X_n$ defined on the domains $\mathcal{X}_1, \ldots, \mathcal{X}_n$ is called statistically independent, if

\begin{equation} \label{eq:ind-cont-ind-F}
F_{X_1,\ldots,X_n}(x_1,\ldots,x_n) = \prod_{i=1}^{n} F_{X_i}(x_i) \quad \text{for all} \; x_i \in \mathcal{X}_i, \; i = 1, \ldots, n
\end{equation}

or equivalently, if the probability densities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) exist, if

\begin{equation} \label{eq:ind-cont-ind-f}
f_{X_1,\ldots,X_n}(x_1,\ldots,x_n) = \prod_{i=1}^{n} f_{X_i}(x_i) \quad \text{for all} \; x_i \in \mathcal{X}_i, \; i = 1, \ldots, n
\end{equation}

where $F$ are the joint ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) or marginal ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) cumulative distribution functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) and $f$ are the respective probability density functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Independence (probability theory)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-06-06; URL: \url{https://en.wikipedia.org/wiki/Independence_(probability_theory)#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Conditional independence}]{Conditional independence} \label{sec:ind-cond}
\setcounter{equation}{0}

\textbf{Definition:} Generally speaking, random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) are conditionally independent given another random variable, if they are statistically independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) in their conditional probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-cond}) given this random variable.

\vspace{1em}
1) A set of discrete random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) $X_1, \ldots, X_n$ with possible values $\mathcal{X}_1, \ldots, \mathcal{X}_n$ is called conditionally independent given the random variable $Y$ with possible values $\mathcal{Y}$, if

\begin{equation} \label{eq:ind-cond-disc-ind}
p(X_1 = x_1, \ldots, X_n = x_n|Y = y) = \prod_{i=1}^{n} p(X_i = x_i|Y = y) \quad \text{for all} \; x_i \in \mathcal{X}_i \quad \text{and all} \; y \in \mathcal{Y}
\end{equation}

where $p(x_1, \ldots, x_n \vert y)$ are the joint (conditional) probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) of $X_1, \ldots, X_n$ given $Y$ and $p(x_i \vert y)$ are the marginal (conditional) probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}) of $X_i$ given $Y$.

\vspace{1em}
2) A set of continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) $X_1, \ldots, X_n$ with possible values $\mathcal{X}_1, \ldots, \mathcal{X}_n$ is called conditionally independent given the random variable $Y$ with possible values $\mathcal{Y}$, if

\begin{equation} \label{eq:ind-cond-cond-ind-F}
F_{X_1,\ldots,X_n|Y=y}(x_1,\ldots,x_n) = \prod_{i=1}^{n} F_{X_i|Y=y}(x_i) \quad \text{for all} \; x_i \in \mathcal{X}_i \quad \text{and all} \; y \in \mathcal{Y}
\end{equation}

or equivalently, if the probability densities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) exist, if

\begin{equation} \label{eq:ind-cond-cont-ind-f}
f_{X_1,\ldots,X_n|Y=y}(x_1,\ldots,x_n) = \prod_{i=1}^{n} f_{X_i|Y=y}(x_i) \quad \text{for all} \; x_i \in \mathcal{X}_i \quad \text{and all} \; y \in \mathcal{Y}
\end{equation}

where $F$ are the joint (conditional) ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) or marginal (conditional) ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) cumulative distribution functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) and $f$ are the respective probability density functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Conditional independence"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-19; URL: \url{https://en.wikipedia.org/wiki/Conditional_independence#Conditional_independence_of_random_variables}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability under independence}]{Probability under independence} \label{sec:prob-ind}
\setcounter{equation}{0}

\textbf{Theorem:} Let $A$ and $B$ be two statements about random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, if $A$ and $B$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), marginal ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}) and conditional ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}) probabilities are equal:

\begin{equation} \label{eq:prob-ind-prob-ind}
\begin{split}
p(A) &= p(A|B) \\
p(B) &= p(B|A) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} If $A$ and $B$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), then the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) is equal to the product of the marginal probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}):

\begin{equation} \label{eq:prob-ind-ind}
p(A,B) = p(A) \cdot p(B) \; .
\end{equation}

The law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}) states that

\begin{equation} \label{eq:prob-ind-prob-cond}
p(A|B) = \frac{p(A,B)}{p(B)} \; .
\end{equation}

Combining \eqref{eq:prob-ind-ind} and \eqref{eq:prob-ind-prob-cond}, we have:

\begin{equation} \label{eq:prob-ind-prob-ind-qed-A}
p(A|B) = \frac{p(A) \cdot p(B)}{p(B)} = p(A) \; .
\end{equation}

Equivalently, we can write:

\begin{equation} \label{eq:prob-ind-prob-ind-qed-B}
p(B|A) \overset{\eqref{eq:prob-ind-prob-cond}}{=} \frac{p(A,B)}{p(A)} \overset{\eqref{eq:prob-ind-ind}}{=} \frac{p(A) \cdot p(B)}{p(A)} = p(B) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Independence (probability theory)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-07-23; URL: \url{https://en.wikipedia.org/wiki/Independence_(probability_theory)#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Mutual exclusivity}]{Mutual exclusivity} \label{sec:exc}
\setcounter{equation}{0}

\textbf{Definition:} Generally speaking, random events ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:reve}) are mutually exclusive, if they cannot occur together, such that their intersection is equal to the empty set ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-emp}).

\vspace{1em}
More precisely, a set of statements $A_1, \ldots, A_n$ is called mutually exclusive, if

\begin{equation} \label{eq:exc-exc}
p(A_1, \ldots, A_n) = 0
\end{equation}

where $p(A_1, \ldots, A_n)$ is the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) of the statements $A_1, \ldots, A_n$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Mutual exclusivity"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-07-23; URL: \url{https://en.wikipedia.org/wiki/Mutual_exclusivity#Probability}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability under exclusivity}]{Probability under exclusivity} \label{sec:prob-exc}
\setcounter{equation}{0}

\textbf{Theorem:} Let $A$ and $B$ be two statements about random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, if $A$ and $B$ are mutually exclusive ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:exc}), the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of their disjunction is equal to the sum of the marginal probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}):

\begin{equation} \label{eq:prob-exc-prob-exc}
p(A \vee B) = p(A) + p(B) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} If $A$ and $B$ are mutually exclusive ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:exc}), then their joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) is zero:

\begin{equation} \label{eq:prob-exc-exc}
p(A,B) = 0 \; .
\end{equation}

The addition law of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}) states that

\begin{equation} \label{eq:prob-exc-prob-add-set}
p(A \cup B) = p(A) + p(B) - p(A \cap B)
\end{equation}

which, in logical rather than set-theoretic expression, becomes

\begin{equation} \label{eq:prob-exc-prob-add-log}
p(A \vee B) = p(A) + p(B) - p(A,B) \; .
\end{equation}

Because the union of mutually exclusive events is the empty set ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:exc}) and the probability of the empty set is zero ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-emp}), the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) term cancels out:

\begin{equation} \label{eq:prob-exc-prob-exc-qed}
p(A \vee B) = p(A) + p(B) - p(A,B) \overset{\eqref{eq:prob-exc-exc}}{=} p(A) + p(B) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Mutual exclusivity"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-07-23; URL: \url{https://en.wikipedia.org/wiki/Mutual_exclusivity#Probability}.
\end{itemize}
\vspace{1em}



\subsection{Probability axioms}

\subsubsection[\textit{Axioms of probability}]{Axioms of probability} \label{sec:prob-ax}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a sample space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:samp-spc}) $\Omega$, an event space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:eve-spc}) $\mathcal{E}$ and a probability measure $P$, such that $P(E)$ is the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of some event ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:reve}) $E \in \mathcal{E}$. Then, we introduce three axioms of probability:

\begin{itemize}

\item First axiom: The probability of an event is a non-negative real number:

\end{itemize}

\begin{equation} \label{eq:prob-ax-prob-ax1}
P(E) \in \mathbb{R}, \; P(E) \geq 0, \; \text{for all } E \in \mathcal{E} \; .
\end{equation}

\begin{itemize}

\item Second axiom: The probability that at least one elementary event in the sample space will occur is one:

\end{itemize}

\begin{equation} \label{eq:prob-ax-prob-ax2}
P(\Omega) = 1 \; .
\end{equation}

\begin{itemize}

\item Third axiom: The probability of any countable sequence of disjoint (i.e. mutually exclusive ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:exc})) events $E_1, E_2, E_3, \ldots$ is equal to the sum of the probabilities of the individual events:

\end{itemize}

\begin{equation} \label{eq:prob-ax-prob-ax3}
P\left(\bigcup_{i=1}^\infty E_i \right) = \sum_{i=1}^\infty P(E_i) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item A.N. Kolmogorov (1950): "Elementary Theory of Probability"; in: \textit{Foundations of the Theory of Probability}, p. 2; URL: \url{https://archive.org/details/foundationsofthe00kolm/page/2/mode/2up}.
\item Alan Stuart \& J. Keith Ord (1994): "Probability and Statistical Inference"; in: \textit{Kendall's Advanced Theory of Statistics, Vol. 1: Distribution Theory}, ch. 8.6, p. 288, eqs. 8.2-8.4; URL: \url{https://www.wiley.com/en-us/Kendall%27s+Advanced+Theory+of+Statistics%2C+3+Volumes%2C+Set%2C+6th+Edition-p-9780470669549}.
\item Wikipedia (2021): "Probability axioms"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-07-30; URL: \url{https://en.wikipedia.org/wiki/Probability_axioms#Axioms}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Monotonicity of probability}]{Monotonicity of probability} \label{sec:prob-mon}
\setcounter{equation}{0}

\textbf{Theorem:} Probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) is monotonic, i.e. if $A$ is a subset of or equal to $B$, then the probability of $A$ is smaller than or equal to $B$:

\begin{equation} \label{eq:prob-mon-prob-mon}
A \subseteq B \quad \Rightarrow \quad P(A) \leq P(B) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Set $E_1 = A$, $E_2 = B \setminus A$ and $E_i = \emptyset$ for $i \geq 3$. Then, the sets $E_i$ are pairwise disjoint and $E_1 \cup E_2 \cup \ldots = B$, because $A \subseteq B$. Thus, from the third axiom of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}), we have:

\begin{equation} \label{eq:prob-mon-pB}
P(B) = P(A) + P(B \setminus A) + \sum_{i=3}^\infty P(E_i) \; .
\end{equation}

Since, by the first axiom of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}), the right-hand side is a series of non-negative numbers converging to $P(B)$ on the left-hand side, it follows that

\begin{equation} \label{eq:prob-mon-prob-mon-qed}
P(A) \leq P(B) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item A.N. Kolmogorov (1950): "Elementary Theory of Probability"; in: \textit{Foundations of the Theory of Probability}, p. 6; URL: \url{https://archive.org/details/foundationsofthe00kolm/page/6/mode/2up}.
\item Alan Stuart \& J. Keith Ord (1994): "Probability and Statistical Inference"; in: \textit{Kendall's Advanced Theory of Statistics, Vol. 1: Distribution Theory}, pp. 288-289; URL: \url{https://www.wiley.com/en-us/Kendall%27s+Advanced+Theory+of+Statistics%2C+3+Volumes%2C+Set%2C+6th+Edition-p-9780470669549}.
\item Wikipedia (2021): "Probability axioms"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-07-30; URL: \url{https://en.wikipedia.org/wiki/Probability_axioms#Monotonicity}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability of the empty set}]{Probability of the empty set} \label{sec:prob-emp}
\setcounter{equation}{0}

\textbf{Theorem:} The probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of the empty set is zero:

\begin{equation} \label{eq:prob-emp-prob-emp}
P(\emptyset) = 0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Let $A$ and $B$ be two events fulfilling $A \subseteq B$. Set $E_1 = A$, $E_2 = B \setminus A$ and $E_i = \emptyset$ for $i \geq 3$. Then, the sets $E_i$ are pairwise disjoint and $E_1 \cup E_2 \cup \ldots = B$. Thus, from the third axiom of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}), we have:

\begin{equation} \label{eq:prob-emp-pB}
P(B) = P(A) + P(B \setminus A) + \sum_{i=3}^\infty P(E_i) \; .
\end{equation}

Assume that the probability of the empty set is not zero, i.e. $P(\emptyset) > 0$. Then, the right-hand side of \eqref{eq:prob-emp-pB} would be infinite. However, by the first axiom of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}), the left-hand side must be finite. This is a contradiction. Therefore, $P(\emptyset) = 0$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item A.N. Kolmogorov (1950): "Elementary Theory of Probability"; in: \textit{Foundations of the Theory of Probability}, p. 6, eq. 3; URL: \url{https://archive.org/details/foundationsofthe00kolm/page/6/mode/2up}.
\item Alan Stuart \& J. Keith Ord (1994): "Probability and Statistical Inference"; in: \textit{Kendall's Advanced Theory of Statistics, Vol. 1: Distribution Theory}, ch. 8.6, p. 288, eq. (b); URL: \url{https://www.wiley.com/en-us/Kendall%27s+Advanced+Theory+of+Statistics%2C+3+Volumes%2C+Set%2C+6th+Edition-p-9780470669549}.
\item Wikipedia (2021): "Probability axioms"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-07-30; URL: \url{https://en.wikipedia.org/wiki/Probability_axioms#The_probability_of_the_empty_set}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability of the complement}]{Probability of the complement} \label{sec:prob-comp}
\setcounter{equation}{0}

\textbf{Theorem:} The probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of a complement of a set is one minus the probability of this set:

\begin{equation} \label{eq:prob-comp-prob-comp}
P(A^\mathrm{c}) = 1 - P(A)
\end{equation}

where $A^\mathrm{c} = \Omega \setminus A$ and $\Omega$ is the sample space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:samp-spc}).


\vspace{1em}
\textbf{Proof:} Since $A$ and $A^\mathrm{c}$ are mutually exclusive ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:exc}) and $A \cup A^\mathrm{c} = \Omega$, the third axiom of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}) implies:

\begin{equation} \label{eq:prob-comp-pAAc}
\begin{split}
P(A \cup A^\mathrm{c}) &= P(A) + P(A^\mathrm{c}) \\
P(\Omega) &= P(A) + P(A^\mathrm{c}) \\
P(A^\mathrm{c}) &= P(\Omega) - P(A) \; .
\end{split}
\end{equation}

The second axiom of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}) states that $P(\Omega) =1$, such that we obtain:

\begin{equation} \label{eq:prob-comp-prob-comp-qed}
P(A^\mathrm{c}) = 1 - P(A) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item A.N. Kolmogorov (1950): "Elementary Theory of Probability"; in: \textit{Foundations of the Theory of Probability}, p. 6, eq. 2; URL: \url{https://archive.org/details/foundationsofthe00kolm/page/6/mode/2up}.
\item Alan Stuart \& J. Keith Ord (1994): "Probability and Statistical Inference"; in: \textit{Kendall's Advanced Theory of Statistics, Vol. 1: Distribution Theory}, ch. 8.6, p. 288, eq. (c); URL: \url{https://www.wiley.com/en-us/Kendall%27s+Advanced+Theory+of+Statistics%2C+3+Volumes%2C+Set%2C+6th+Edition-p-9780470669549}.
\item Wikipedia (2021): "Probability axioms"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-07-30; URL: \url{https://en.wikipedia.org/wiki/Probability_axioms#The_complement_rule}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Range of probability}]{Range of probability} \label{sec:prob-range}
\setcounter{equation}{0}

\textbf{Theorem:} The probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of an event is bounded between 0 and 1:

\begin{equation} \label{eq:prob-range-prob-range}
0 \leq P(E) \leq 1 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} From the first axiom of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}), we have:

\begin{equation} \label{eq:prob-range-pEg0}
P(E) \geq 0 \; .
\end{equation}

By combining the first axiom of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}) and the probability of the complement ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-comp}), we obtain:

\begin{equation} \label{eq:prob-range-pEl1}
\begin{split}
1- P(E) = P(E^\mathrm{c}) &\geq 0 \\
1- P(E) &\geq 0 \\
P(E) &\leq 1 \; .
\end{split}
\end{equation}

Together, \eqref{eq:prob-range-pEg0} and \eqref{eq:prob-range-pEl1} imply that

\begin{equation} \label{eq:prob-range-prob-range-qed}
0 \leq P(E) \leq 1 \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item A.N. Kolmogorov (1950): "Elementary Theory of Probability"; in: \textit{Foundations of the Theory of Probability}, p. 6; URL: \url{https://archive.org/details/foundationsofthe00kolm/page/6/mode/2up}.
\item Alan Stuart \& J. Keith Ord (1994): "Probability and Statistical Inference"; in: \textit{Kendall's Advanced Theory of Statistics, Vol. 1: Distribution Theory}, pp. 288-289; URL: \url{https://www.wiley.com/en-us/Kendall%27s+Advanced+Theory+of+Statistics%2C+3+Volumes%2C+Set%2C+6th+Edition-p-9780470669549}.
\item Wikipedia (2021): "Probability axioms"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-07-30; URL: \url{https://en.wikipedia.org/wiki/Probability_axioms#The_numeric_bound}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Addition law of probability}]{Addition law of probability} \label{sec:prob-add}
\setcounter{equation}{0}

\textbf{Theorem:} The probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of the union of $A$ and $B$ is the sum of the probabilities of $A$ and $B$ minus the probability of the intersection of $A$ and $B$:

\begin{equation} \label{eq:prob-add-prob-add}
P(A \cup B) = P(A) + P(B) - P(A \cap B) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Let $E_1 = A$ and $E_2 = B \setminus A$, such that $E_1 \cup E_2 = A \cup B$. Then, by the third axiom of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}), we have:

\begin{equation} \label{eq:prob-add-pAoB}
\begin{split}
P(A \cup B) &= P(A) + P(B \setminus A) \\
P(A \cup B) &= P(A) + P(B \setminus [A \cap B]) \; .
\end{split}
\end{equation}

Then, let $E_1 = B \setminus [A \cap B]$ and $E_2 = A \cap B$, such that $E_1 \cup E_2 = B$. Again, from the third axiom of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}), we obtain:

\begin{equation} \label{eq:prob-add-pB}
\begin{split}
P(B) &= P(B \setminus [A \cap B]) + P(A \cap B) \\
P(B \setminus [A \cap B]) &= P(B) - P(A \cap B) \; .
\end{split}
\end{equation}

Plugging \eqref{eq:prob-add-pB} into \eqref{eq:prob-add-pAoB}, we finally get:

\begin{equation} \label{eq:prob-add-prob-add-qed}
P(A \cup B) = P(A) + P(B) - P(A \cap B) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item A.N. Kolmogorov (1950): "Elementary Theory of Probability"; in: \textit{Foundations of the Theory of Probability}, p. 2; URL: \url{https://archive.org/details/foundationsofthe00kolm/page/2/mode/2up}.
\item Alan Stuart \& J. Keith Ord (1994): "Probability and Statistical Inference"; in: \textit{Kendall's Advanced Theory of Statistics, Vol. 1: Distribution Theory}, ch. 8.6, p. 288, eq. (a); URL: \url{https://www.wiley.com/en-us/Kendall%27s+Advanced+Theory+of+Statistics%2C+3+Volumes%2C+Set%2C+6th+Edition-p-9780470669549}.
\item Wikipedia (2021): "Probability axioms"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-07-30; URL: \url{https://en.wikipedia.org/wiki/Probability_axioms#Further_consequences}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Law of total probability}]{Law of total probability} \label{sec:prob-tot}
\setcounter{equation}{0}

\textbf{Theorem:} Let $A$ be a subset of sample space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:samp-spc}) $\Omega$ and let $B_1, \ldots, B_n$ be finite or countably infinite partition of $\Omega$, such that $B_i \cap B_j = \emptyset$ for all $i \neq j$ and $\cup_i \, B_i = \Omega$. Then, the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of the event $A$ is

\begin{equation} \label{eq:prob-tot-prob-tot}
P(A) = \sum_i P(A \cap B_i) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Because all $B_i$ are disjoint, sets $(A \cap B_i)$ are also disjoint:

\begin{equation} \label{eq:prob-tot-B-disjoint}
B_i \cap B_j = \emptyset \quad \Rightarrow \quad (A \cap B_i) \cap (A \cap B_j) = A \cap (B_i \cap B_j) = A \cap \emptyset = \emptyset \; .
\end{equation}

Because the $B_i$ are exhaustive, the sets $(A \cap B_i)$ are also exhaustive:

\begin{equation} \label{eq:prob-tot-B-exhaustive}
\cup_i \, B_i = \Omega \quad \Rightarrow \quad \cup_i \, (A \cap B_i) = A \cap \left( \cup_i \, B_i \right) = A \cap \Omega = A \; .
\end{equation}

Thus, the third axiom of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}) implies that

\begin{equation} \label{eq:prob-tot-prob-tot-qed}
P(A) = \sum_i P(A \cap B_i) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Alan Stuart \& J. Keith Ord (1994): "Probability and Statistical Inference"; in: \textit{Kendall's Advanced Theory of Statistics, Vol. 1: Distribution Theory}, p. 288, eq. (d); p. 289, eq. 8.7; URL: \url{https://www.wiley.com/en-us/Kendall%27s+Advanced+Theory+of+Statistics%2C+3+Volumes%2C+Set%2C+6th+Edition-p-9780470669549}.
\item Wikipedia (2021): "Law of total probability"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-08-08; URL: \url{https://en.wikipedia.org/wiki/Law_of_total_probability#Statement}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability of exhaustive events}]{Probability of exhaustive events} \label{sec:prob-exh}
\setcounter{equation}{0}

\textbf{Theorem:} Let $B_1, \ldots, B_n$ be mutually exclusive ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:exc}) and collectively exhaustive subsets of a sample space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:samp-spc}) $\Omega$. Then, their total probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-tot}) is one:

\begin{equation} \label{eq:prob-exh-prob-exh}
\sum_i P(B_i) = 1 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Because all $B_i$ are mutually exclusive, we have:

\begin{equation} \label{eq:prob-exh-B-exclusive}
B_i \cap B_j = \emptyset \quad \text{for all} \quad i \neq j \; .
\end{equation}

Because the $B_i$ are collectively exhaustive, we have:

\begin{equation} \label{eq:prob-exh-B-exhaustive}
\cup_i \, B_i = \Omega \; .
\end{equation}

Thus, the third axiom of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}) implies that

\begin{equation} \label{eq:prob-exh-prob-exh-s1}
\sum_i P(B_i) = P(\Omega) \; .
\end{equation}

and the second axiom of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}) implies that

\begin{equation} \label{eq:prob-exh-prob-exh-s2}
\sum_i P(B_i) = 1 \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Alan Stuart \& J. Keith Ord (1994): "Probability and Statistical Inference"; in: \textit{Kendall's Advanced Theory of Statistics, Vol. 1: Distribution Theory}, pp. 288-289; URL: \url{https://www.wiley.com/en-us/Kendall%27s+Advanced+Theory+of+Statistics%2C+3+Volumes%2C+Set%2C+6th+Edition-p-9780470669549}.
\item Wikipedia (2021): "Probability axioms"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-08-08; URL: \url{https://en.wikipedia.org/wiki/Probability_axioms#Axioms}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability of exhaustive events}]{Probability of exhaustive events} \label{sec:prob-exh2}
\setcounter{equation}{0}

\textbf{Theorem:} Let $B_1, \ldots, B_n$ be mutually exclusive ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:exc}) and collectively exhaustive subsets of a sample space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:samp-spc}) $\Omega$. Then, their total probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-tot}) is one:

\begin{equation} \label{eq:prob-exh2-prob-exh}
\sum_i P(B_i) = 1 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The addition law of probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-add}) states that for two events ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:reve}) $A$ and $B$, the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of at least one of them occurring is:

\begin{equation} \label{eq:prob-exh2-prob-add}
P(A \cup B) = P(A) + P(B) - P(A \cap B) \; .
\end{equation}

Recursively applying this law to the events $B_1, \ldots, B_n$, we have:

\begin{equation} \label{eq:prob-exh2-prob-all-s1}
\begin{split}
P(B_1 \cup \ldots \cup B_n) &= P(B_1) + P(B_2 \cup \ldots \cup B_n) - P(B_1 \cap [B_2 \cup \ldots \cup B_n]) \\
&= P(B_1) + P(B_2) + P(B_3 \cup \ldots \cup B_n) - P(B_2 \cap [B_3 \cup \ldots \cup B_n])- P(B_1 \cap [B_2 \cup \ldots \cup B_n]) \\
&\;\; \vdots \\
&= P(B_1) + \ldots + P(B_n) - P(B_1 \cap [B_2 \cup \ldots \cup B_n]) - \ldots - P(B_{n-1} \cap B_n) \\
P(\cup_i^n \, B_i) &= \sum_i^n P(B_i) - \sum_i^{n-1} P(B_i \cap [\cup_{j=i+1}^n B_j]) \\
&= \sum_i^n P(B_i) - \sum_i^{n-1} P(\cup_{j=i+1}^n [B_i \cap B_j]) \; .
\end{split}
\end{equation}

Because all $B_i$ are mutually exclusive, we have:

\begin{equation} \label{eq:prob-exh2-B-exclusive}
B_i \cap B_j = \emptyset \quad \text{for all} \quad i \neq j \; .
\end{equation}

Since the probability of the empty set is zero ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-emp}), this means that the second sum on the right-hand side of \eqref{eq:prob-exh2-prob-all-s1} disappears:

\begin{equation} \label{eq:prob-exh2-prob-all-s2}
P(\cup_i^n \, B_i) = \sum_i^n P(B_i) \; .
\end{equation}

Because the $B_i$ are collectively exhaustive, we have:

\begin{equation} \label{eq:prob-exh2-B-exhaustive}
\cup_i \, B_i = \Omega \; .
\end{equation}

Since the probability of the sample space is one ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ax}), this means that the left-hand side of \eqref{eq:prob-exh2-prob-all-s2} becomes equal to one:

\begin{equation} \label{eq:prob-exh2-prob-all-s3}
1 = \sum_i^n P(B_i) \; .
\end{equation}

This proofs the statement in \eqref{eq:prob-exh2-prob-exh}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Alan Stuart \& J. Keith Ord (1994): "Probability and Statistical Inference"; in: \textit{Kendall's Advanced Theory of Statistics, Vol. 1: Distribution Theory}, pp. 288-289; URL: \url{https://www.wiley.com/en-us/Kendall%27s+Advanced+Theory+of+Statistics%2C+3+Volumes%2C+Set%2C+6th+Edition-p-9780470669549}.
\item Wikipedia (2022): "Probability axioms"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-03-27; URL: \url{https://en.wikipedia.org/wiki/Probability_axioms#Consequences}.
\end{itemize}
\vspace{1em}



\subsection{Probability distributions}

\subsubsection[\textit{Probability distribution}]{Probability distribution} \label{sec:dist}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with the set of possible outcomes $\mathcal{X}$. Then, a probability distribution of $X$ is a mathematical function that gives the probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of occurrence of all possible outcomes $x \in \mathcal{X}$ of this random variable.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Probability distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-17; URL: \url{https://en.wikipedia.org/wiki/Probability_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Joint distribution}]{Joint distribution} \label{sec:dist-joint}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ and $Y$ be random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with sets of possible outcomes $\mathcal{X}$ and $\mathcal{Y}$. Then, a joint distribution of $X$ and $Y$ is a probability distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) that specifies the probability of the event that $X = x$ and $Y = y$ for each possible combination of $x \in \mathcal{X}$ and $y \in \mathcal{Y}$.

\begin{itemize}

\item The joint distribution of two scalar random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) is called a bivariate distribution.

\item The joint distribution of a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) is called a multivariate distribution.

\item The joint distribution of a random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}) is called a matrix-variate distribution.

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Joint probability distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-17; URL: \url{https://en.wikipedia.org/wiki/Joint_probability_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Marginal distribution}]{Marginal distribution} \label{sec:dist-marg}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ and $Y$ be random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with sets of possible outcomes $\mathcal{X}$ and $\mathcal{Y}$. Then, the marginal distribution of $X$ is a probability distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) that specifies the probability of the event that $X = x$ irrespective of the value of $Y$ for each possible value $x \in \mathcal{X}$. The marginal distribution can be obtained from the joint distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) of $X$ and $Y$ using the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Marginal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-17; URL: \url{https://en.wikipedia.org/wiki/Marginal_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Conditional distribution}]{Conditional distribution} \label{sec:dist-cond}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ and $Y$ be random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with sets of possible outcomes $\mathcal{X}$ and $\mathcal{Y}$. Then, the conditional distribution of $X$ given that $Y$ is a probability distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) that specifies the probability of the event that $X = x$ given that $Y = y$ for each possible combination of $x \in \mathcal{X}$ and $y \in \mathcal{Y}$. The conditional distribution of $X$ can be obtained from the joint distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) of $X$ and $Y$ and the marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of $Y$ using the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Conditional probability distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-17; URL: \url{https://en.wikipedia.org/wiki/Conditional_probability_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Sampling distribution}]{Sampling distribution} \label{sec:dist-samp}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a random sample with finite sample size. Then, the probability distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) of a given statistic computed from this sample, e.g. a test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat}), is called a sampling distribution.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Sampling distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-31; URL: \url{https://en.wikipedia.org/wiki/Sampling_distribution}.
\end{itemize}
\vspace{1em}



\subsection{Probability mass function}

\subsubsection[\textit{Definition}]{Definition} \label{sec:pmf}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a discrete ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$. Then, $f_X(x): \mathbb{R} \to [0,1]$ is the probability mass function (PMF) of $X$, if

\begin{equation} \label{eq:pmf-pmf-def-s0}
f_X(x) = 0
\end{equation}

for all $x \notin \mathcal{X}$,

\begin{equation} \label{eq:pmf-pmf-def-s1}
\mathrm{Pr}(X = x) = f_X(x)
\end{equation}

for all $x \in \mathcal{X}$ and

\begin{equation} \label{eq:pmf-pmf-def-s2}
\sum_{x \in \mathcal{X}} f_X(x) = 1 \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Probability mass function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-13; URL: \url{https://en.wikipedia.org/wiki/Probability_mass_function}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability mass function of sum of independents}]{Probability mass function of sum of independents} \label{sec:pmf-sumind}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be two independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) discrete ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible values $\mathcal{X}$ and $\mathcal{Y}$ and let $Z = X + Y$. Then, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $Z$ is given by

\begin{equation} \label{eq:pmf-sumind-pmf-sumind}
\begin{split}
f_Z(z) &= \sum_{y \in \mathcal{Y}} f_X(z-y) f_Y(y)  \\
\text{or} \quad f_Z(z) &= \sum_{x \in \mathcal{X}} f_Y(z-x) f_X(x)
\end{split}
\end{equation}

where $f_X(x)$, $f_Y(y)$ and $f_Z(z)$ are the probability mass functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$, $Y$ and $Z$.


\vspace{1em}
\textbf{Proof:} Using the definition of the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) and the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), the first equation can be derived as follows:

\begin{equation} \label{eq:pmf-sumind-pmf-sumind-s1}
\begin{split}
f_Z(z) &= \mathrm{Pr}(Z = z) \\
&= \mathrm{Pr}(X + Y = z) \\
&= \mathrm{Pr}(X = z - Y) \\
&= \mathrm{E} \left[ \mathrm{Pr}(X = z - Y \vert Y = y) \right] \; .
\end{split}
\end{equation}

By construction, $X$ and $Y$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), such that conditional probabilities are equal to marginal probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ind}), i.e. $\mathrm{Pr}(X = z - Y \vert Y = y) = \mathrm{Pr}(X = z - Y)$ and we have:

\begin{equation} \label{eq:pmf-sumind-pmf-sumind-s2}
\begin{split}
f_Z(z) &= \mathrm{E} \left[ \mathrm{Pr}(X = z - Y) \right] \\
&= \mathrm{E} \left[ f_X(z-Y) \right] \\
&= \sum_{y \in \mathcal{Y}} f_X(z-y) f_Y(y) \; .
\end{split}
\end{equation}

The second equation can be derived by switching $X$ and $Y$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Sums of independent random variables"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2021-08-30; URL: \url{https://www.statlect.com/fundamentals-of-probability/sums-of-independent-random-variables}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability mass function of strictly increasing function}]{Probability mass function of strictly increasing function} \label{sec:pmf-sifct}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a discrete ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and let $g(x)$ be a strictly increasing function on the support of $X$. Then, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $Y = g(X)$ is given by

\begin{equation} \label{eq:pmf-sifct-pmf-sifct}
f_Y(y) = \left\{
\begin{array}{rl}
f_X(g^{-1}(y)) \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y \notin \mathcal{Y}
\end{array}
\right.
\end{equation}

where $g^{-1}(y)$ is the inverse function of $g(x)$ and $\mathcal{Y}$ is the set of possible outcomes of $Y$:

\begin{equation} \label{eq:pmf-sifct-Y-range}
\mathcal{Y} = \left\lbrace y = g(x): x \in \mathcal{X} \right\rbrace \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Because a strictly increasing function is invertible, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $Y$ can be derived as follows:

\begin{equation} \label{eq:pmf-sifct-pmf-sifct-qed}
\begin{split}
f_Y(y) &= \mathrm{Pr}(Y = y) \\
&= \mathrm{Pr}(g(X) = y) \\
&= \mathrm{Pr}(X = g^{-1}(y)) \\
&= f_X(g^{-1}(y)) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Functions of random variables and their distribution"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2020-10-29; URL: \url{https://www.statlect.com/fundamentals-of-probability/functions-of-random-variables-and-their-distribution#hid3}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability mass function of strictly decreasing function}]{Probability mass function of strictly decreasing function} \label{sec:pmf-sdfct}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a discrete ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and let $g(x)$ be a strictly decreasing function on the support of $X$. Then, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $Y = g(X)$ is given by

\begin{equation} \label{eq:pmf-sdfct-pmf-sdfct}
f_Y(y) = \left\{
\begin{array}{rl}
f_X(g^{-1}(y)) \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y \notin \mathcal{Y}
\end{array}
\right.
\end{equation}

where $g^{-1}(y)$ is the inverse function of $g(x)$ and $\mathcal{Y}$ is the set of possible outcomes of $Y$:

\begin{equation} \label{eq:pmf-sdfct-Y-range}
\mathcal{Y} = \left\lbrace y = g(x): x \in \mathcal{X} \right\rbrace \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Because a strictly decreasing function is invertible, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $Y$ can be derived as follows:

\begin{equation} \label{eq:pmf-sdfct-pmf-sdfct-qed}
\begin{split}
f_Y(y) &= \mathrm{Pr}(Y = y) \\
&= \mathrm{Pr}(g(X) = y) \\
&= \mathrm{Pr}(X = g^{-1}(y)) \\
&= f_X(g^{-1}(y)) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Functions of random variables and their distribution"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2020-11-06; URL: \url{https://www.statlect.com/fundamentals-of-probability/functions-of-random-variables-and-their-distribution#hid6}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability mass function of invertible function}]{Probability mass function of invertible function} \label{sec:pmf-invfct}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) of discrete random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) with possible outcomes $\mathcal{X}$ and let $g: \; \mathbb{R}^n \rightarrow \mathbb{R}^n$ be an invertible function on the support of $X$. Then, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $Y = g(X)$ is given by

\begin{equation} \label{eq:pmf-invfct-pmf-invfct}
f_Y(y) = \left\{
\begin{array}{rl}
f_X(g^{-1}(y)) \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y \notin \mathcal{Y}
\end{array}
\right.
\end{equation}

where $g^{-1}(y)$ is the inverse function of $g(x)$ and $\mathcal{Y}$ is the set of possible outcomes of $Y$:

\begin{equation} \label{eq:pmf-invfct-Y-range}
\mathcal{Y} = \left\lbrace y = g(x): x \in \mathcal{X} \right\rbrace \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Because an invertible function is a one-to-one mapping, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $Y$ can be derived as follows:

\begin{equation} \label{eq:pmf-invfct-pmf-invfct-qed}
\begin{split}
f_Y(y) &= \mathrm{Pr}(Y = y) \\
&= \mathrm{Pr}(g(X) = y) \\
&= \mathrm{Pr}(X = g^{-1}(y)) \\
&= f_X(g^{-1}(y)) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Functions of random vectors and their distribution"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2021-08-30; URL: \url{https://www.statlect.com/fundamentals-of-probability/functions-of-random-vectors}.
\end{itemize}
\vspace{1em}



\subsection{Probability density function}

\subsubsection[\textit{Definition}]{Definition} \label{sec:pdf}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$. Then, $f_X(x): \mathbb{R} \to \mathbb{R}$ is the probability density function (PDF) of $X$, if

\begin{equation} \label{eq:pdf-pdf-def-s0}
f_X(x) \geq 0
\end{equation}

for all $x \in \mathbb{R}$,

\begin{equation} \label{eq:pdf-pdf-def-s1}
\mathrm{Pr}(X \in A) = \int_{A} f_X(x) \, \mathrm{d}x
\end{equation}

for any $A \subset \mathcal{X}$ and

\begin{equation} \label{eq:pdf-pdf-def-s2}
\int_{\mathcal{X}} f_X(x) \, \mathrm{d}x = 1 \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Probability density function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-13; URL: \url{https://en.wikipedia.org/wiki/Probability_density_function}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function of sum of independents}]{Probability density function of sum of independents} \label{sec:pdf-sumind}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be two independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible values $\mathcal{X}$ and $\mathcal{Y}$ and let $Z = X + Y$. Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $Z$ is given by

\begin{equation} \label{eq:pdf-sumind-pdf-sumind}
\begin{split}
f_Z(z) &= \int_{-\infty}^{+\infty} f_X(z-y) f_Y(y) \, \mathrm{d}y \\
\text{or} \quad f_Z(z) &= \int_{-\infty}^{+\infty} f_Y(z-x) f_X(x) \, \mathrm{d}x
\end{split}
\end{equation}

where $f_X(x)$, $f_Y(y)$ and $f_Z(z)$ are the probability density functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$, $Y$ and $Z$.


\vspace{1em}
\textbf{Proof:} The cumulative distribution function of a sum of independent random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf-sumind}) is

\begin{equation} \label{eq:pdf-sumind-cdf-sumind}
F_Z(z) = \mathrm{E}\left[ F_X(z-Y) \right] \; .
\end{equation}

The probability density function is the first derivative of the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-cdf}), such that

\begin{equation} \label{eq:pdf-sumind-pdf-sumind-qed}
\begin{split}
f_Z(z) &= \frac{\mathrm{d}}{\mathrm{d}z} F_Z(z) \\
&= \frac{\mathrm{d}}{\mathrm{d}z} \mathrm{E}\left[ F_X(z-Y) \right] \\
&= \mathrm{E}\left[ \frac{\mathrm{d}}{\mathrm{d}z} F_X(z-Y) \right] \\
&= \mathrm{E}\left[ f_X(z-Y) \right] \\
&= \int_{-\infty}^{+\infty} f_X(z-y) f_Y(y) \, \mathrm{d}y \; .
\end{split}
\end{equation}

The second equation can be derived by switching $X$ and $Y$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Sums of independent random variables"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2021-08-30; URL: \url{https://www.statlect.com/fundamentals-of-probability/sums-of-independent-random-variables}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function of strictly increasing function}]{Probability density function of strictly increasing function} \label{sec:pdf-sifct}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and let $g(x)$ be a strictly increasing function on the support of $X$. Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $Y = g(X)$ is given by

\begin{equation} \label{eq:pdf-sifct-pdf-sifct}
f_Y(y) = \left\{
\begin{array}{rl}
f_X(g^{-1}(y)) \, \frac{\mathrm{d}g^{-1}(y)}{\mathrm{d}y} \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y \notin \mathcal{Y}
\end{array}
\right.
\end{equation}

where $g^{-1}(y)$ is the inverse function of $g(x)$ and $\mathcal{Y}$ is the set of possible outcomes of $Y$:

\begin{equation} \label{eq:pdf-sifct-Y-range}
\mathcal{Y} = \left\lbrace y = g(x): x \in \mathcal{X} \right\rbrace \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The cumulative distribution function of a strictly increasing function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf-sifct}) is

\begin{equation} \label{eq:pdf-sifct-cdf-sifct}
F_Y(y) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; y < \mathrm{min}(\mathcal{Y}) \\
F_X(g^{-1}(y)) \; , & \text{if} \; y \in \mathcal{Y} \\
1 \; , & \text{if} \; y > \mathrm{max}(\mathcal{Y})
\end{array}
\right.
\end{equation}

Because the probability density function is the first derivative of the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-cdf})

\begin{equation} \label{eq:pdf-sifct-pdf-cdf}
f_X(x) = \frac{\mathrm{d}F_X(x)}{\mathrm{d}x} \; ,
\end{equation}

the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $Y$ can be derived as follows:

1) If $y$ does not belong to the support of $Y$, $F_Y(y)$ is constant, such that

\begin{equation} \label{eq:pdf-sifct-pdf-sifct-p1}
f_Y(y) = 0, \quad \text{if} \quad y \notin \mathcal{Y} \; .
\end{equation}

2) If $y$ belongs to the support of $Y$, then $f_Y(y)$ can be derived using the chain rule:

\begin{equation} \label{eq:pdf-sifct-pdf-sifct-p2}
\begin{split}
f_Y(y) &\overset{\eqref{eq:pdf-sifct-pdf-cdf}}{=} \frac{\mathrm{d}}{\mathrm{d}y} F_Y(y) \\
&\overset{\eqref{eq:pdf-sifct-cdf-sifct}}{=} \frac{\mathrm{d}}{\mathrm{d}y} F_X(g^{-1}(y)) \\
&= f_X(g^{-1}(y)) \, \frac{\mathrm{d}g^{-1}(y)}{\mathrm{d}y} \; .
\end{split}
\end{equation}

Taking together \eqref{eq:pdf-sifct-pdf-sifct-p1} and \eqref{eq:pdf-sifct-pdf-sifct-p2}, eventually proves \eqref{eq:pdf-sifct-pdf-sifct}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Functions of random variables and their distribution"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2020-10-29; URL: \url{https://www.statlect.com/fundamentals-of-probability/functions-of-random-variables-and-their-distribution#hid4}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function of strictly decreasing function}]{Probability density function of strictly decreasing function} \label{sec:pdf-sdfct}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and let $g(x)$ be a strictly decreasing function on the support of $X$. Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $Y = g(X)$ is given by

\begin{equation} \label{eq:pdf-sdfct-pdf-sdfct}
f_Y(y) = \left\{
\begin{array}{rl}
-f_X(g^{-1}(y)) \, \frac{\mathrm{d}g^{-1}(y)}{\mathrm{d}y} \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y \notin \mathcal{Y}
\end{array}
\right.
\end{equation}

where $g^{-1}(y)$ is the inverse function of $g(x)$ and $\mathcal{Y}$ is the set of possible outcomes of $Y$:

\begin{equation} \label{eq:pdf-sdfct-Y-range}
\mathcal{Y} = \left\lbrace y = g(x): x \in \mathcal{X} \right\rbrace \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The cumulative distribution function of a strictly decreasing function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf-sdfct}) is

\begin{equation} \label{eq:pdf-sdfct-cdf-sdfct}
F_Y(y) = \left\{
\begin{array}{rl}
1 \; , & \text{if} \; y > \mathrm{max}(\mathcal{Y}) \\
1 - F_X(g^{-1}(y)) + \mathrm{Pr}(X = g^{-1}(y)) \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y < \mathrm{min}(\mathcal{Y})
\end{array}
\right.
\end{equation}

Note that for continuous random variables, the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of point events is

\begin{equation} \label{eq:pdf-sdfct-pdf-cont}
\mathrm{Pr}(X = a) = \int_a^a f_X(x) \, \mathrm{d}x = 0 \; .
\end{equation}

Because the probability density function is the first derivative of the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-cdf})

\begin{equation} \label{eq:pdf-sdfct-pdf-cdf}
f_X(x) = \frac{\mathrm{d}F_X(x)}{\mathrm{d}x} \; ,
\end{equation}

the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $Y$ can be derived as follows:

1) If $y$ does not belong to the support of $Y$, $F_Y(y)$ is constant, such that

\begin{equation} \label{eq:pdf-sdfct-pdf-sdfct-p1}
f_Y(y) = 0, \quad \text{if} \quad y \notin \mathcal{Y} \; .
\end{equation}

2) If $y$ belongs to the support of $Y$, then $f_Y(y)$ can be derived using the chain rule:

\begin{equation} \label{eq:pdf-sdfct-pdf-sdfct-p2}
\begin{split}
f_Y(y) &\overset{\eqref{eq:pdf-sdfct-pdf-cdf}}{=} \frac{\mathrm{d}}{\mathrm{d}y} F_Y(y) \\
&\overset{\eqref{eq:pdf-sdfct-cdf-sdfct}}{=} \frac{\mathrm{d}}{\mathrm{d}y} \left[ 1 - F_X(g^{-1}(y)) + \mathrm{Pr}(X = g^{-1}(y)) \right] \\
&\overset{\eqref{eq:pdf-sdfct-pdf-cont}}{=} \frac{\mathrm{d}}{\mathrm{d}y} \left[ 1 - F_X(g^{-1}(y)) \right] \\
&= -\frac{\mathrm{d}}{\mathrm{d}y} F_X(g^{-1}(y)) \\
&= - f_X(g^{-1}(y)) \, \frac{\mathrm{d}g^{-1}(y)}{\mathrm{d}y} \; .
\end{split}
\end{equation}

Taking together \eqref{eq:pdf-sdfct-pdf-sdfct-p1} and \eqref{eq:pdf-sdfct-pdf-sdfct-p2}, eventually proves \eqref{eq:pdf-sdfct-pdf-sdfct}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Functions of random variables and their distribution"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2020-11-06; URL: \url{https://www.statlect.com/fundamentals-of-probability/functions-of-random-variables-and-their-distribution#hid7}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function of invertible function}]{Probability density function of invertible function} \label{sec:pdf-invfct}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) of continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) with possible outcomes $\mathcal{X} \subseteq \mathbb{R}^n$ and let $g: \; \mathbb{R}^n \rightarrow \mathbb{R}^n$ be an invertible and differentiable function on the support of $X$. Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $Y = g(X)$ is given by

\begin{equation} \label{eq:pdf-invfct-pdf-invfct}
f_Y(y) = \left\{
\begin{array}{rl}
f_X(g^{-1}(y)) \, \left| J_{g^{-1}}(y) \right| \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y \notin \mathcal{Y}
\end{array}
\right. \; ,
\end{equation}

if the Jacobian determinant satisfies

\begin{equation} \label{eq:pdf-invfct-jac-det}
\left| J_{g^{-1}}(y) \right| \neq 0 \quad \text{for all} \quad y \in \mathcal{Y}
\end{equation}

where $g^{-1}(y)$ is the inverse function of $g(x)$, $J_{g^{-1}}(y)$ is the Jacobian matrix of $g^{-1}(y)$

\begin{equation} \label{eq:pdf-invfct-jac}
J_{g^{-1}}(y) = \left[ \begin{matrix}
\frac{\mathrm{d}x_1}{\mathrm{d}y_1} & \ldots & \frac{\mathrm{d}x_1}{\mathrm{d}y_n} \\
\vdots & \ddots & \vdots \\
\frac{\mathrm{d}x_n}{\mathrm{d}y_1} & \ldots & \frac{\mathrm{d}x_n}{\mathrm{d}y_n}
\end{matrix} \right] \; ,
\end{equation}

$\lvert J \rvert$ is the determinant of $J$ and $\mathcal{Y}$ is the set of possible outcomes of $Y$:

\begin{equation} \label{eq:pdf-invfct-Y-range}
\mathcal{Y} = \left\lbrace y = g(x): x \in \mathcal{X} \right\rbrace \; .
\end{equation}


\vspace{1em}
\textbf{Proof:}

1) First, we obtain the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $Y = g(X)$. The joint CDF ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf-joint}) is given by

\begin{equation} \label{eq:pdf-invfct-Y-cdf-s1}
\begin{split}
F_Y(y) &= \mathrm{Pr}(Y_1 \leq y_1, \ldots, Y_n \leq y_n) \\
&= \mathrm{Pr}(g_1(X) \leq y_1, \ldots, g_n(X) \leq y_n) \\
&= \int_{A(y)} f_X(x) \, \mathrm{d}x
\end{split}
\end{equation}

where $A(y)$ is the following subset of the $n$-dimensional Euclidean space:

\begin{equation} \label{eq:pdf-invfct-A-y}
A(y) = \left\lbrace x \in \mathbb{R}^n: g_j(x) \leq y_j \; \text{for all} \; j = 1, \ldots, n \right\rbrace
\end{equation}

and $g_j(X)$ is the function which returns the $j$-th element of $Y$, given a vector $X$.

\vspace{1em}
2) Next, we substitute $x = g^{-1}(y)$ into the integral which gives us

\begin{equation} \label{eq:pdf-invfct-Y-cdf-s2}
\begin{split}
F_Y(z) &= \int_{B(z)} f_X(g^{-1}(y)) \, \mathrm{d}g^{-1}(y) \\
&= \int_{-\infty}^{z_n} \ldots \int_{-\infty}^{z_1} f_X(g^{-1}(y)) \, \mathrm{d}g^{-1}(y) \; .
\end{split}
\end{equation}

where we have the modified the integration regime $B(z)$ which reads

\begin{equation} \label{eq:pdf-invfct-B-z}
B(z) = \left\lbrace y \in \mathbb{R}^n: y \leq z_j \; \text{for all} \; j = 1, \ldots, n \right\rbrace \; .
\end{equation}

\vspace{1em}
3) The formula for change of variables in multivariable calculus states that

\begin{equation} \label{eq:pdf-invfct-cov-multi}
y = f(x) \quad \Rightarrow \quad \mathrm{d}y = \left| J_f(x) \right| \, \mathrm{d}x \; .
\end{equation}

Applied to equation \eqref{eq:pdf-invfct-Y-cdf-s2}, this yields

\begin{equation} \label{eq:pdf-invfct-Y-cdf-s3}
\begin{split}
F_Y(z) &= \int_{-\infty}^{z_n} \ldots \int_{-\infty}^{z_1} f_X(g^{-1}(y)) \, \left| J_{g^{-1}}(y) \right| \, \mathrm{d}y \\
&= \int_{-\infty}^{z_n} \ldots \int_{-\infty}^{z_1} f_X(g^{-1}(y)) \, \left| J_{g^{-1}}(y) \right| \, \mathrm{d}y_1 \ldots \mathrm{d}y_n \; .
\end{split}
\end{equation}

\vspace{1em}
4) Finally, we obtain the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $Y = g(X)$. Because the PDF is the derivative of the CDF ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-cdf}), we can differentiate the joint CDF to get

\begin{equation} \label{eq:pdf-invfct-Y-cdf-s4}
\begin{split}
f_Y(z) &= \frac{\mathrm{d}^n}{\mathrm{d}z_1 \ldots \mathrm{d}z_n} \, F_Y(z) \\
&= \frac{\mathrm{d}^n}{\mathrm{d}z_1 \ldots \mathrm{d}z_n} \int_{-\infty}^{z_n} \ldots \int_{-\infty}^{z_1} f_X(g^{-1}(y)) \, \left| J_{g^{-1}}(y) \right| \, \mathrm{d}y_1 \ldots \mathrm{d}y_n \\
&= f_X(g^{-1}(z)) \, \left| J_{g^{-1}}(z) \right|
\end{split}
\end{equation}

which can also be written as

\begin{equation} \label{eq:pdf-invfct-pdf-invfct-qed}
f_Y(y) = f_X(g^{-1}(y)) \, \left| J_{g^{-1}}(y) \right| \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Functions of random vectors and their distribution"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2021-08-30; URL: \url{https://www.statlect.com/fundamentals-of-probability/functions-of-random-vectors}.
\item Lebanon, Guy (2017): "Functions of a Random Vector"; in: \textit{Probability: The Analysis of Data, Vol. 1}, retrieved on 2021-08-30; URL: \url{http://theanalysisofdata.com/probability/4_4.html}.
\item Poirier, Dale J. (1995): "Distributions of Functions of Random Variables"; in: \textit{Intermediate Statistics and Econometrics: A Comparative Approach}, ch. 4, pp. 149ff.; URL: \url{https://books.google.de/books?id=K52_YvD1YNwC&hl=de&source=gbs_navlinks_s}.
\item Devore, Jay L.; Berk, Kennth N. (2011): "Conditional Distributions"; in: \textit{Modern Mathematical Statistics with Applications}, ch. 5.2, pp. 253ff.; URL: \url{https://books.google.de/books?id=5PRLUho-YYgC&hl=de&source=gbs_navlinks_s}.
\item peek-a-boo (2019): "How to come up with the Jacobian in the change of variables formula"; in: \textit{StackExchange Mathematics}, retrieved on 2021-08-30; URL: \url{https://math.stackexchange.com/a/3239222}.
\item Bazett, Trefor (2019): "Change of Variables \& The Jacobian | Multi-variable Integration"; in: \textit{YouTube}, retrieved on 2021-08-30; URL: \url{https://www.youtube.com/watch?v=wUF-lyyWpUc}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function of linear transformation}]{Probability density function of linear transformation} \label{sec:pdf-linfct}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) of continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) with possible outcomes $\mathcal{X} \subseteq \mathbb{R}^n$ and let $Y = \Sigma X + \mu$ be a linear transformation of this random variable with constant ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:const}) $n \times 1$ vector $\mu$ and constant ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:const}) $n \times n$ matrix $\Sigma$. Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $Y$ is

\begin{equation} \label{eq:pdf-linfct-pdf-linfct}
f_Y(y) = \left\{
\begin{array}{rl}
\frac{1}{\left| \Sigma \right|} f_X(\Sigma^{-1}(y-\mu)) \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y \notin \mathcal{Y}
\end{array}
\right.
\end{equation}

where $\lvert \Sigma \rvert$ is the determinant of $\Sigma$ and $\mathcal{Y}$ is the set of possible outcomes of $Y$:

\begin{equation} \label{eq:pdf-linfct-Y-range}
\mathcal{Y} = \left\lbrace y = \Sigma x + \mu: x \in \mathcal{X} \right\rbrace \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Because the linear function $g(X) = \Sigma X + \mu$ is invertible and differentiable, we can determine the probability density function of an invertible function of a continuous random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-invfct}) using the relation

\begin{equation} \label{eq:pdf-linfct-pdf-invfct}
f_Y(y) = \left\{
\begin{array}{rl}
f_X(g^{-1}(y)) \, \left| J_{g^{-1}}(y) \right| \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y \notin \mathcal{Y}
\end{array}
\right. \; .
\end{equation}

The inverse function is

\begin{equation} \label{eq:pdf-linfct-g-inv}
X = g^{-1}(Y) = \Sigma^{-1}(Y-\mu) = \Sigma^{-1} Y - \Sigma^{-1} \mu
\end{equation}

and the Jacobian matrix is

\begin{equation} \label{eq:pdf-linfct-J-g-inv}
J_{g^{-1}}(y) = \left[ \begin{matrix}
\frac{\mathrm{d}x_1}{\mathrm{d}y_1} & \ldots & \frac{\mathrm{d}x_1}{\mathrm{d}y_n} \\
\vdots & \ddots & \vdots \\
\frac{\mathrm{d}x_n}{\mathrm{d}y_1} & \ldots & \frac{\mathrm{d}x_n}{\mathrm{d}y_n}
\end{matrix} \right] = \Sigma^{-1} \; .
\end{equation}

Plugging \eqref{eq:pdf-linfct-g-inv} and \eqref{eq:pdf-linfct-J-g-inv} into \eqref{eq:pdf-linfct-pdf-invfct} and applying the determinant property $\lvert A^{-1} \rvert = \lvert A \rvert^{-1}$, we obtain

\begin{equation} \label{eq:pdf-linfct-pdf-linfct-qed}
f_Y(y) = \frac{1}{\left| \Sigma \right|} f_X(\Sigma^{-1}(y-\mu)) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Functions of random vectors and their distribution"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2021-08-30; URL: \url{https://www.statlect.com/fundamentals-of-probability/functions-of-random-vectors}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function in terms of cumulative distribution function}]{Probability density function in terms of cumulative distribution function} \label{sec:pdf-cdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, the probability distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ is the first derivative of the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$:

\begin{equation} \label{eq:pdf-cdf-pdf-cdf}
f_X(x) = \frac{\mathrm{d}F_X(x)}{\mathrm{d}x} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The cumulative distribution function in terms of the probability density function of a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf-pdf}) is given by:

\begin{equation} \label{eq:pdf-cdf-cdf-pdf}
F_X(x) = \int_{-\infty}^{x} f_X(t) \, \mathrm{d}t, \; x \in \mathbb{R} \; .
\end{equation}

Taking the derivative with respect to $x$, we have:

\begin{equation} \label{eq:pdf-cdf-ddx-cdf}
\frac{\mathrm{d}F_X(x)}{\mathrm{d}x} = \frac{\mathrm{d}}{\mathrm{d}x} \int_{-\infty}^{x} f_X(t) \, \mathrm{d}t \; .
\end{equation}

The fundamental theorem of calculus states that, if $f(x)$ is a continuous real-valued function defined on the interval $[a,b]$, then it holds that

\begin{equation} \label{eq:pdf-cdf-FToC-1st}
F(x) = \int_{a}^{x} f(t) \, \mathrm{d}t \quad \Rightarrow \quad F'(x) = f(x) \quad \text{for all} \quad x \in (a,b) \; .
\end{equation}

Applying \eqref{eq:pdf-cdf-FToC-1st} to \eqref{eq:pdf-cdf-cdf-pdf}, it follows that

\begin{equation} \label{eq:pdf-cdf-pdf-cdf-qed}
F_X(x) = \int_{-\infty}^{x} f_X(t) \, \mathrm{d}t \quad \Rightarrow \quad \frac{\mathrm{d}F_X(x)}{\mathrm{d}x} = f_X(x) \quad \text{for all} \quad x \in \mathbb{R} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Fundamental theorem of calculus"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-12; URL: \url{https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus#Formal_statements}.
\end{itemize}
\vspace{1em}



\subsection{Cumulative distribution function}

\subsubsection[\textit{Definition}]{Definition} \label{sec:cdf}
\setcounter{equation}{0}

\textbf{Definition:} The cumulative distribution function (CDF) of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ at a given value $x$ is defined as the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) that $X$ is smaller than $x$:

\begin{equation} \label{eq:cdf-cdf}
F_X(x) = \mathrm{Pr}(X \leq x) \; .
\end{equation}

\vspace{1em}
1) If $X$ is a discrete ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) $f_X(x)$, then the cumulative distribution function is the function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf-pmf}) $F_X(x): \mathbb{R} \to [0,1]$ with

\begin{equation} \label{eq:cdf-cdf-disc}
F_X(x) = \sum_{\overset{t \in \mathcal{X}}{t \leq x}} f_X(t) \; .
\end{equation}

\vspace{1em}
2) If $X$ is a continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) $f_X(x)$, then the cumulative distribution function is the function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf-pdf}) $F_X(x): \mathbb{R} \to [0,1]$ with

\begin{equation} \label{eq:cdf-cdf-cont}
F_X(x) = \int_{-\infty}^{x} f_X(t) \, \mathrm{d}t \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Cumulative distribution function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-17; URL: \url{https://en.wikipedia.org/wiki/Cumulative_distribution_function#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Cumulative distribution function of sum of independents}]{Cumulative distribution function of sum of independents} \label{sec:cdf-sumind}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be two independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) and let $Z = X + Y$. Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $Z$ is given by

\begin{equation} \label{eq:cdf-sumind-cdf-sumind}
\begin{split}
F_Z(z) &= \mathrm{E}\left[ F_X(z-Y) \right] \\
\text{or} \quad F_Z(z) &= \mathrm{E}\left[ F_Y(z-X) \right]
\end{split}
\end{equation}

where $F_X(x)$, $F_Y(y)$ and $F_Z(z)$ are the cumulative distribution functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$, $Y$ and $Z$ and $\mathrm{E}\left[ \cdot \right]$ denotes the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}).


\vspace{1em}
\textbf{Proof:} Using the definition of the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}), the first equation can be derived as follows:

\begin{equation} \label{eq:cdf-sumind-cdf-sumind-qed}
\begin{split}
F_Z(z) &= \mathrm{Pr}(Z \leq z) \\
&= \mathrm{Pr}(X + Y \leq z) \\
&= \mathrm{Pr}(X \leq z - Y) \\
&= \mathrm{E} \left[ \mathrm{Pr}(X \leq z - Y \vert Y = y) \right] \\
&= \mathrm{E} \left[ \mathrm{Pr}(X \leq z - Y) \right] \\
&= \mathrm{E} \left[ F_X(z-Y) \right] \; .
\end{split}
\end{equation}

Note that the second-last transition is justified by the fact that $X$ and $Y$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), such that conditional probabilities are equal to marginal probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ind}). The second equation can be derived by switching $X$ and $Y$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Sums of independent random variables"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2021-08-30; URL: \url{https://www.statlect.com/fundamentals-of-probability/sums-of-independent-random-variables}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Cumulative distribution function of strictly increasing function}]{Cumulative distribution function of strictly increasing function} \label{sec:cdf-sifct}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and let $g(x)$ be a strictly increasing function on the support of $X$. Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $Y = g(X)$ is given by

\begin{equation} \label{eq:cdf-sifct-cdf-sifct}
F_Y(y) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; y < \mathrm{min}(\mathcal{Y}) \\
F_X(g^{-1}(y)) \; , & \text{if} \; y \in \mathcal{Y} \\
1 \; , & \text{if} \; y > \mathrm{max}(\mathcal{Y})
\end{array}
\right.
\end{equation}

where $g^{-1}(y)$ is the inverse function of $g(x)$ and $\mathcal{Y}$ is the set of possible outcomes of $Y$:

\begin{equation} \label{eq:cdf-sifct-Y-range}
\mathcal{Y} = \left\lbrace y = g(x): x \in \mathcal{X} \right\rbrace \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The support of $Y$ is determined by $g(x)$ and by the set of possible outcomes of $X$. Moreover, if $g(x)$ is strictly increasing, then $g^{-1}(y)$ is also strictly increasing. Therefore, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $Y$ can be derived as follows:

1) If $y$ is lower than the lowest value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:min}) $Y$ can take, then $\mathrm{Pr}(Y \leq y) = 0$, so

\begin{equation} \label{eq:cdf-sifct-cdf-sifct-p1}
F_Y(y) = 0, \quad \text{if} \quad y < \mathrm{min}(\mathcal{Y}) \; .
\end{equation}

2) If $y$ belongs to the support of $Y$, then $F_Y(y)$ can be derived as follows:

\begin{equation} \label{eq:cdf-sifct-cdf-sifct-p2}
\begin{split}
F_Y(y) &= \mathrm{Pr}(Y \leq y) \\
&= \mathrm{Pr}(g(X) \leq y) \\
&= \mathrm{Pr}(X \leq g^{-1}(y)) \\
&= F_X(g^{-1}(y)) \; .
\end{split}
\end{equation}

3) If $y$ is higher than the highest value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:max}) $Y$ can take, then $\mathrm{Pr}(Y \leq y) = 1$, so

\begin{equation} \label{eq:cdf-sifct-cdf-sifct-p3}
F_Y(y) = 1, \quad \text{if} \quad y > \mathrm{max}(\mathcal{Y}) \; .
\end{equation}

Taking together \eqref{eq:cdf-sifct-cdf-sifct-p1}, \eqref{eq:cdf-sifct-cdf-sifct-p2}, \eqref{eq:cdf-sifct-cdf-sifct-p3}, eventually proves \eqref{eq:cdf-sifct-cdf-sifct}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Functions of random variables and their distribution"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2020-10-29; URL: \url{https://www.statlect.com/fundamentals-of-probability/functions-of-random-variables-and-their-distribution#hid2}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Cumulative distribution function of strictly decreasing function}]{Cumulative distribution function of strictly decreasing function} \label{sec:cdf-sdfct}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and let $g(x)$ be a strictly decreasing function on the support of $X$. Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $Y = g(X)$ is given by

\begin{equation} \label{eq:cdf-sdfct-cdf-sdfct}
F_Y(y) = \left\{
\begin{array}{rl}
1 \; , & \text{if} \; y > \mathrm{max}(\mathcal{Y}) \\
1 - F_X(g^{-1}(y)) + \mathrm{Pr}(X = g^{-1}(y)) \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y < \mathrm{min}(\mathcal{Y})
\end{array}
\right.
\end{equation}

where $g^{-1}(y)$ is the inverse function of $g(x)$ and $\mathcal{Y}$ is the set of possible outcomes of $Y$:

\begin{equation} \label{eq:cdf-sdfct-Y-range}
\mathcal{Y} = \left\lbrace y = g(x): x \in \mathcal{X} \right\rbrace \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The support of $Y$ is determined by $g(x)$ and by the set of possible outcomes of $X$. Moreover, if $g(x)$ is strictly decreasing, then $g^{-1}(y)$ is also strictly decreasing. Therefore, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $Y$ can be derived as follows:

1) If $y$ is higher than the highest value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:max}) $Y$ can take, then $\mathrm{Pr}(Y \leq y) = 1$, so

\begin{equation} \label{eq:cdf-sdfct-cdf-sdfct-p1}
F_Y(y) = 1, \quad \text{if} \quad y > \mathrm{max}(\mathcal{Y}) \; .
\end{equation}

2) If $y$ belongs to the support of $Y$, then $F_Y(y)$ can be derived as follows:

\begin{equation} \label{eq:cdf-sdfct-cdf-sdfct-p2}
\begin{split}
F_Y(y) &= \mathrm{Pr}(Y \leq y) \\
&= 1 - \mathrm{Pr}(Y > y) \\
&= 1 - \mathrm{Pr}(g(X) > y) \\
&= 1 - \mathrm{Pr}(X < g^{-1}(y)) \\
&= 1 - \mathrm{Pr}(X < g^{-1}(y)) - \mathrm{Pr}(X = g^{-1}(y)) + \mathrm{Pr}(X = g^{-1}(y)) \\
&= 1 - \left[ \mathrm{Pr}(X < g^{-1}(y)) + \mathrm{Pr}(X = g^{-1}(y)) \right] + \mathrm{Pr}(X = g^{-1}(y)) \\
&= 1 - \mathrm{Pr}(X \leq g^{-1}(y)) + \mathrm{Pr}(X = g^{-1}(y)) \\
&= 1 - F_X(g^{-1}(y)) + \mathrm{Pr}(X = g^{-1}(y)) \; .
\end{split}
\end{equation}

3) If $y$ is lower than the lowest value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:min}) $Y$ can take, then $\mathrm{Pr}(Y \leq y) = 0$, so

\begin{equation} \label{eq:cdf-sdfct-cdf-sdfct-p3}
F_Y(y) = 0, \quad \text{if} \quad y < \mathrm{min}(\mathcal{Y}) \; .
\end{equation}

Taking together \eqref{eq:cdf-sdfct-cdf-sdfct-p1}, \eqref{eq:cdf-sdfct-cdf-sdfct-p2}, \eqref{eq:cdf-sdfct-cdf-sdfct-p3}, eventually proves \eqref{eq:cdf-sdfct-cdf-sdfct}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Functions of random variables and their distribution"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2020-11-06; URL: \url{https://www.statlect.com/fundamentals-of-probability/functions-of-random-variables-and-their-distribution#hid5}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Cumulative distribution function of discrete random variable}]{Cumulative distribution function of discrete random variable} \label{sec:cdf-pmf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a discrete ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible values $\mathcal{X}$ and probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) $f_X(x)$. Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$ is

\begin{equation} \label{eq:cdf-pmf-cdf-pmf}
F_X(x) = \sum_{\overset{t \in \mathcal{X}}{t \leq x}} f_X(t) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ is defined as the probability that $X$ is smaller than $x$:

\begin{equation} \label{eq:cdf-pmf-cdf}
F_X(x) = \mathrm{Pr}(X \leq x) \; .
\end{equation}

The probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of a discrete ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ returns the probability that $X$ takes a particular value $x$:

\begin{equation} \label{eq:cdf-pmf-pmf}
f_X(x) = \mathrm{Pr}(X = x) \; .
\end{equation}

Taking these two definitions together, we have:

\begin{equation} \label{eq:cdf-pmf-cdf-pmf-qed}
\begin{split}
F_X(x) &\overset{\eqref{eq:cdf-pmf-cdf}}{=} \sum_{\overset{t \in \mathcal{X}}{t \leq x}} \mathrm{Pr}(X = t) \\
&\overset{\eqref{eq:cdf-pmf-pmf}}{=} \sum_{\overset{t \in \mathcal{X}}{t \leq x}} f_X(t) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Cumulative distribution function of continuous random variable}]{Cumulative distribution function of continuous random variable} \label{sec:cdf-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible values $\mathcal{X}$ and probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) $f_X(x)$. Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$ is

\begin{equation} \label{eq:cdf-pdf-cdf-pdf}
F_X(x) = \int_{-\infty}^{x} f_X(t) \, \mathrm{d}t \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ is defined as the probability that $X$ is smaller than $x$:

\begin{equation} \label{eq:cdf-pdf-cdf}
F_X(x) = \mathrm{Pr}(X \leq x) \; .
\end{equation}

The probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of a continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ can be used to calculate the probability that $X$ falls into a particular interval $A$:

\begin{equation} \label{eq:cdf-pdf-pdf}
\mathrm{Pr}(X \in A) = \int_{A} f_X(x) \, \mathrm{d}x \; .
\end{equation}

Taking these two definitions together, we have:

\begin{equation} \label{eq:cdf-pdf-cdf-pdf-qed}
\begin{split}
F_X(x) &\overset{\eqref{eq:cdf-pdf-cdf}}{=} \mathrm{Pr}(X \in (-\infty, x]) \\
&\overset{\eqref{eq:cdf-pdf-pdf}}{=} \int_{-\infty}^{x} f_X(t) \, \mathrm{d}t \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Probability integral transform}]{Probability integral transform} \label{sec:cdf-pit}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with invertible ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) $F_X(x)$. Then, the random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar})

\begin{equation} \label{eq:cdf-pit-cdf-pit}
Y = F_X(X)
\end{equation}

has a standard uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:suni}).


\vspace{1em}
\textbf{Proof:} The cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $Y = F_X(X)$ can be derived as

\begin{equation} \label{eq:cdf-pit-cdf-pit-qed}
\begin{split}
F_Y(y) &= \mathrm{Pr}(Y \leq y) \\
&= \mathrm{Pr}(F_X(X) \leq y) \\
&= \mathrm{Pr}(X \leq F_X^{-1}(y)) \\
&= F_X(F_X^{-1}(y)) \\
&= y \\
\end{split}
\end{equation}

which is the cumulative distribution function of a continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-cdf}) with $a = 0$ and $b = 1$, i.e. the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of the standard uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:suni}) $\mathcal{U}(0,1)$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Probability integral transform"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-04-07; URL: \url{https://en.wikipedia.org/wiki/Probability_integral_transform#Proof}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Inverse transformation method}]{Inverse transformation method} \label{sec:cdf-itm}
\setcounter{equation}{0}

\textbf{Theorem:} Let $U$ be a continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) having a standard uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:suni}). Then, the random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar})

\begin{equation} \label{eq:cdf-itm-cdf-itm}
X = F_X^{-1}(U)
\end{equation}

has a probability distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) characterized by the invertible ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) $F_X(x)$.


\vspace{1em}
\textbf{Proof:} The cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of the transformation $X = F_X^{-1}(U)$ can be derived as

\begin{equation} \label{eq:cdf-itm-cdf-itm-qed}
\begin{split}
&\hphantom{=} \;\; \mathrm{Pr}(X \leq x) \\
&= \mathrm{Pr}(F_X^{-1}(U) \leq x) \\
&= \mathrm{Pr}(U \leq F_X(x)) \\
&= F_X(x) \; ,
\end{split}
\end{equation}

because the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of the standard uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:suni}) $\mathcal{U}(0,1)$ is

\begin{equation} \label{eq:cdf-itm-suni-cdf}
U \sim \mathcal{U}(0,1) \quad \Rightarrow \quad F_U(u) = \mathrm{Pr}(U \leq u) = u \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Inverse transform sampling"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-04-07; URL: \url{https://en.wikipedia.org/wiki/Inverse_transform_sampling#Proof_of_correctness}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Distributional transformation}]{Distributional transformation} \label{sec:cdf-dt}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be two continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) $F_X(x)$ and invertible cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) $F_Y(y)$. Then, the random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar})

\begin{equation} \label{eq:cdf-dt-cdf-dt}
\tilde{X} = F_Y^{-1}(F_X(X))
\end{equation}

has the same probability distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) as $Y$.


\vspace{1em}
\textbf{Proof:} The cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of the transformation $\tilde{X} = F_Y^{-1}(F_X(X))$ can be derived as

\begin{equation} \label{eq:cdf-dt-cdf-dt-qed}
\begin{split}
F_{\tilde{X}}(y) &= \mathrm{Pr}\left( \tilde{X} \leq y \right) \\
&= \mathrm{Pr}\left( F_Y^{-1}(F_X(X)) \leq y \right) \\
&= \mathrm{Pr}\left( F_X(X) \leq F_Y(y) \right) \\
&= \mathrm{Pr}\left( X \leq F_X^{-1}(F_Y(y)) \right) \\
&= F_X\left( F_X^{-1}(F_Y(y)) \right) \\
&= F_Y(y) \\
\end{split}
\end{equation}

which shows that $\tilde{X}$ and $Y$ have the same cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) and are thus identically distributed ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}).
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Soch, Joram (2020): "Distributional Transformation Improves Decoding Accuracy When Predicting Chronological Age From Structural MRI"; in: \textit{Frontiers in Psychiatry}, vol. 11, art. 604268; URL: \url{https://www.frontiersin.org/articles/10.3389/fpsyt.2020.604268/full}; DOI: 10.3389/fpsyt.2020.604268.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Joint cumulative distribution function}]{Joint cumulative distribution function} \label{sec:cdf-joint}
\setcounter{equation}{0}

\textbf{Definition:} Let $X \in \mathbb{R}^{n \times 1}$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, the joint ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$ is defined as the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) that each entry $X_i$ is smaller than a specific value $x_i$ for $i = 1, \ldots, n$:

\begin{equation} \label{eq:cdf-joint-cdf-joint}
F_X(x) = \mathrm{Pr}(X_1 \leq x_1, \ldots, X_n \leq x_n) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Cumulative distribution function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-04-07; URL: \url{https://en.wikipedia.org/wiki/Cumulative_distribution_function#Definition_for_more_than_two_random_variables}.
\end{itemize}
\vspace{1em}



\subsection{Other probability functions}

\subsubsection[\textit{Quantile function}]{Quantile function} \label{sec:qf}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) (CDF) $F_X(x)$. Then, the function $Q_X(p): [0,1] \to \mathbb{R}$ which is the inverse CDF is the quantile function (QF) of $X$. More precisely, the QF is the function that, for a given quantile $p \in [0,1]$, returns the smallest $x$ for which $F_X(x) = p$:

\begin{equation} \label{eq:qf-qf}
Q_X(p) = \min \left\lbrace x \in \mathbb{R} \, \vert \, F_X(x) = p \right\rbrace \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Probability density function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-17; URL: \url{https://en.wikipedia.org/wiki/Quantile_function#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Quantile function in terms of cumulative distribution function}]{Quantile function in terms of cumulative distribution function} \label{sec:qf-cdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) $F_X(x)$. If the cumulative distribution function is strictly monotonically increasing, then the quantile function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf})  is identical to the inverse of $F_X(x)$:

\begin{equation} \label{eq:qf-cdf-qf-cdf}
Q_X(p) = F_X^{-1}(x) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The quantile function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) $Q_X(p)$ is defined as the function that, for a given quantile $p \in [0,1]$, returns the smallest $x$ for which $F_X(x) = p$:

\begin{equation} \label{eq:qf-cdf-qf}
Q_X(p) = \min \left\lbrace x \in \mathbb{R} \, \vert \, F_X(x) = p \right\rbrace \; .
\end{equation}

If $F_X(x)$ is continuous and strictly monotonically increasing, then there is exactly one $x$ for which $F_X(x) = p$ and $F_X(x)$ is an invertible function, such that

\begin{equation} \label{eq:qf-cdf-qf-cdf-qed}
Q_X(p) = F_X^{-1}(x) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Quantile function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-12; URL: \url{https://en.wikipedia.org/wiki/Quantile_function#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Characteristic function}]{Characteristic function} \label{sec:cf}
\setcounter{equation}{0}

\textbf{Definition:}

1) The characteristic function of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X \in \mathbb{R}$ is

\begin{equation} \label{eq:cf-cf-var}
\varphi_X(t) = \mathrm{E} \left[ e^{itX} \right], \quad t \in \mathbb{R} \; .
\end{equation}

2) The characteristic function of a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $X \in \mathbb{R}^n$ is

\begin{equation} \label{eq:cf-cf-vec}
\varphi_X(t) = \mathrm{E} \left[ e^{i t^\mathrm{T}X} \right], \quad t \in \mathbb{R}^n \; .
\end{equation}

3) The characteristic function of a random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}) $X \in \mathbb{R}^{n \times p}$ is

\begin{equation} \label{eq:cf-cf-mat}
\varphi_X(t) = \mathrm{E} \left[ e^{i \, \mathrm{tr} \left( t^\mathrm{T}X \right)} \right], \quad t \in \mathbb{R}^{n \times p} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Characteristic function (probability theory)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-09-22; URL: \url{https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)#Definition}.
\item Taboga, Marco (2017): "Joint characteristic function"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2021-10-07; URL: \url{https://www.statlect.com/fundamentals-of-probability/joint-characteristic-function}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Characteristic function of arbitrary function}]{Characteristic function of arbitrary function} \label{sec:cf-fct}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) function $\mathrm{E}_X[\cdot]$. Then, the characteristic function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cf}) of $Y = g(X)$ is equal to

\begin{equation} \label{eq:cf-fct-cf-fct}
\varphi_Y(t) = \mathrm{E}_X \left[ \mathrm{exp}(it \, g(X)) \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The characteristic function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cf}) is defined as

\begin{equation} \label{eq:cf-fct-cf}
\varphi_Y(t) = \mathrm{E} \left[ \mathrm{exp}(it \, Y) \right] \; .
\end{equation}

Due of the law of the unconscious statistician ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lotus})

\begin{equation} \label{eq:cf-fct-mean-lotus}
\begin{split}
\mathrm{E}[g(X)] &= \sum_{x \in \mathcal{X}} g(x) f_X(x) \\
\mathrm{E}[g(X)] &= \int_{\mathcal{X}} g(x) f_X(x) \, \mathrm{d}x \; ,
\end{split}
\end{equation}

$Y = g(X)$ can simply be substituted into \eqref{eq:cf-fct-cf} to give

\begin{equation} \label{eq:cf-fct-cf-fct-qed}
\varphi_Y(t) = \mathrm{E}_X \left[ \mathrm{exp}(it \, g(X)) \right] \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Functions of random vectors and their distribution"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2021-09-22; URL: \url{https://www.statlect.com/fundamentals-of-probability/functions-of-random-vectors}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Moment-generating function}]{Moment-generating function} \label{sec:mgf}
\setcounter{equation}{0}

\textbf{Definition:}

1) The moment-generating function of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X \in \mathbb{R}$ is

\begin{equation} \label{eq:mgf-mgf-var}
M_X(t) = \mathrm{E} \left[ e^{tX} \right], \quad t \in \mathbb{R} \; .
\end{equation}

2) The moment-generating function of a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $X \in \mathbb{R}^n$ is

\begin{equation} \label{eq:mgf-mgf-vec}
M_X(t) = \mathrm{E} \left[ e^{t^\mathrm{T}X} \right], \quad t \in \mathbb{R}^n \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Moment-generating function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-22; URL: \url{https://en.wikipedia.org/wiki/Moment-generating_function#Definition}.
\item Taboga, Marco (2017): "Joint moment generating function"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2021-10-07; URL: \url{https://www.statlect.com/fundamentals-of-probability/joint-moment-generating-function}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Moment-generating function of arbitrary function}]{Moment-generating function of arbitrary function} \label{sec:mgf-fct}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) function $\mathrm{E}_X[\cdot]$. Then, the moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) of $Y = g(X)$ is equal to

\begin{equation} \label{eq:mgf-fct-mgf-fct}
M_Y(t) = \mathrm{E}_X \left[ \mathrm{exp}(t \, g(X)) \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) is defined as

\begin{equation} \label{eq:mgf-fct-mgf}
M_Y(t) = \mathrm{E} \left[ \mathrm{exp}(t \, Y) \right] \; .
\end{equation}

Due of the law of the unconscious statistician ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lotus})

\begin{equation} \label{eq:mgf-fct-mean-lotus}
\begin{split}
\mathrm{E}[g(X)] &= \sum_{x \in \mathcal{X}} g(x) f_X(x) \\
\mathrm{E}[g(X)] &= \int_{\mathcal{X}} g(x) f_X(x) \, \mathrm{d}x \; ,
\end{split}
\end{equation}

$Y = g(X)$ can simply be substituted into \eqref{eq:mgf-fct-mgf} to give

\begin{equation} \label{eq:mgf-fct-mgf-fct-qed}
M_Y(t) = \mathrm{E}_X \left[ \mathrm{exp}(t \, g(X)) \right] \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Functions of random vectors and their distribution"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2021-09-22; URL: \url{https://www.statlect.com/fundamentals-of-probability/functions-of-random-vectors}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Moment-generating function of linear transformation}]{Moment-generating function of linear transformation} \label{sec:mgf-ltt}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) with the moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) $M_X(t)$. Then, the moment-generating function of the linear transformation $Y = A X + b$ is given by

\begin{equation} \label{eq:mgf-ltt-mgf-ltt}
M_Y(t) = \exp \left[ t^\mathrm{T} b \right] \cdot M_X(At)
\end{equation}

where $A$ is an $m \times n$ matrix and $b$ is an $m \times 1$ vector.


\vspace{1em}
\textbf{Proof:} The moment-generating function of a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) $X$ is

\begin{equation} \label{eq:mgf-ltt-mfg-vect}
M_X(t) = \mathrm{E} \left( \exp \left[ t^\mathrm{T} X \right] \right)
\end{equation}

and therefore the moment-generating function of the random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $Y$ is given by

\begin{equation} \label{eq:mgf-ltt-mgf-ltt-qed}
\begin{split}
M_Y(t) &= \mathrm{E} \left( \exp \left[ t^\mathrm{T} (AX + b) \right] \right) \\
&= \mathrm{E} \left( \exp \left[ t^\mathrm{T} A X \right] \cdot \exp \left[ t^\mathrm{T} b \right] \right) \\
&= \exp \left[ t^\mathrm{T} b \right] \cdot \mathrm{E} \left( \exp \left[ (A t)^\mathrm{T} X \right] \right) \\
&= \exp \left[ t^\mathrm{T} b \right] \cdot M_X(At) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item ProofWiki (2020): "Moment Generating Function of Linear Transformation of Random Variable"; in: \textit{ProofWiki}, retrieved on 2020-08-19; URL: \url{https://proofwiki.org/wiki/Moment_Generating_Function_of_Linear_Transformation_of_Random_Variable}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Moment-generating function of linear combination}]{Moment-generating function of linear combination} \label{sec:mgf-lincomb}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X_1, \ldots, X_n$ be $n$ independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with moment-generating functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) $M_{X_i}(t)$. Then, the moment-generating function of the linear combination $X = \sum_{i=1}^{n} a_i X_i$ is given by

\begin{equation} \label{eq:mgf-lincomb-mgf-lincomb}
M_X(t) = \prod_{i=1}^{n} M_{X_i}(a_i t)
\end{equation}

where $a_1, \ldots, a_n$ are $n$ real numbers.


\vspace{1em}
\textbf{Proof:} The moment-generating function of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) $X_i$ is

\begin{equation} \label{eq:mgf-lincomb-mfg-vect}
M_{X_i}(t) = \mathrm{E} \left( \exp \left[ t X_i \right] \right)
\end{equation}

and therefore the moment-generating function of the linear combination $X$ is given by

\begin{equation} \label{eq:mgf-lincomb-mgf-lincomb-s1}
\begin{split}
M_X(t) &= \mathrm{E} \left( \exp \left[ t X \right] \right) \\
&= \mathrm{E} \left( \exp \left[ t \sum_{i=1}^{n} a_i X_i \right] \right) \\
&= \mathrm{E} \left( \prod_{i=1}^{n} \exp \left[ t \, a_i X_i \right] \right) \; .
\end{split}
\end{equation}

Because the expected value is multiplicative for independent random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-mult}), we have

\begin{equation} \label{eq:mgf-lincomb-mgf-lincomb-s2}
\begin{split}
M_X(t) &= \prod_{i=1}^{n} \mathrm{E} \left( \exp \left[ (a_i t) X_i \right] \right) \\
&= \prod_{i=1}^{n} M_{X_i}(a_i t) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item ProofWiki (2020): "Moment Generating Function of Linear Combination of Independent Random Variables"; in: \textit{ProofWiki}, retrieved on 2020-08-19; URL: \url{https://proofwiki.org/wiki/Moment_Generating_Function_of_Linear_Combination_of_Independent_Random_Variables}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Probability-generating function}]{Probability-generating function} \label{sec:pgf}
\setcounter{equation}{0}

\textbf{Definition:}

1) If $X$ is a discrete ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) taking values in the non-negative integers $\left\lbrace 0, 1, \ldots \right\rbrace$, then the probability-generating function of $X$ is defined as

\begin{equation} \label{eq:pgf-pgf-var}
G_X(z) = \sum_{x=0}^{\infty} p(x) \, z^x
\end{equation}

where $z \in \mathbb{C}$ and $p(x)$ is the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$.

2) If $X$ is a discrete ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) taking values in the $n$-dimensional integer lattice $x \in \left\lbrace 0, 1, \ldots \right\rbrace^n$, then the probability-generating function of $X$ is defined as

\begin{equation} \label{eq:pgf-cgf-vec}
G_X(z) = \sum_{x_1=0}^{\infty} \cdots \sum_{x_n=0}^{\infty} p(x_1,\ldots,x_n) \, {z_1}^{x_1} \cdot \ldots \cdot {z_n}^{x_n}
\end{equation}

where $z \in \mathbb{C}^n$ and $p(x)$ is the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Probability-generating function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-31; URL: \url{https://en.wikipedia.org/wiki/Probability-generating_function#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability-generating function in terms of expected value}]{Probability-generating function in terms of expected value} \label{sec:pgf-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a discrete ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) whose set of possible values $\mathcal{X}$ is a subset of the natural numbers $\mathbb{N}$. Then, the probability-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pgf}) of $X$ can be expressed in terms of an expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of a function of $X$

\begin{equation} \label{eq:pgf-mean-pgf-mean}
G_X(z) = \mathrm{E}\left[ z^X \right]
\end{equation}

where $z \in \mathbb{C}$.


\vspace{1em}
\textbf{Proof:} The law of the unconscious statistician ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lotus}) states that

\begin{equation} \label{eq:pgf-mean-mean-lotus}
\mathrm{E}[g(X)] = \sum_{x \in \mathcal{X}} g(x) f_X(x)
\end{equation}

where $f_X(x)$ is the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$. Here, we have $g(X) = z^X$, such that

\begin{equation} \label{eq:pgf-mean-E-zX-s1}
\mathrm{E}\left[ z^X \right] = \sum_{x \in \mathcal{X}} z^x f_X(x) \; .
\end{equation}

By the definition of $X$, this is equal to

\begin{equation} \label{eq:pgf-mean-E-zX-s2}
\mathrm{E}\left[ z^X \right] = \sum_{x=0}^{\infty} f_X(x) \, z^x \; .
\end{equation}

The right-hand side is equal to the probability-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pgf}) of $X$:

\begin{equation} \label{eq:pgf-mean-pgf-mean-qed}
\mathrm{E}\left[ z^X \right] = G_X(z) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item ProofWiki (2022): "Probability Generating Function as Expectation"; in: \textit{ProofWiki}, retrieved on 2022-10-11; URL: \url{https://proofwiki.org/wiki/Probability_Generating_Function_as_Expectation}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability-generating function of zero}]{Probability-generating function of zero} \label{sec:pgf-zero}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with probability-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pgf}) $G_X(z)$ and probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) $f_X(x)$. Then, the value of the probability-generating function at zero is equal to the value of the probability mass function at zero:

\begin{equation} \label{eq:pgf-zero-pgf-zero}
G_X(0) = f_X(0) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pgf}) of $X$ is defined as

\begin{equation} \label{eq:pgf-zero-pgf}
G_X(z) = \sum_{x=0}^{\infty} f_X(x) \, z^x
\end{equation}

where $f_X(x)$ is the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$. Setting $z = 0$, we obtain:

\begin{equation} \label{eq:pgf-zero-pgf-zero-qed}
\begin{split}
G_X(0) &= \sum_{x=0}^{\infty} f_X(x) \cdot 0^x \\
&= f_X(0) + 0^1 \cdot f_X(1)  + 0^2 \cdot f_X(2) + \ldots \\
&= f_X(0) + 0 + 0 + \ldots \\
&= f_X(0) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item ProofWiki (2022): "Probability Generating Function of Zero"; in: \textit{ProofWiki}, retrieved on 2022-10-11; URL: \url{https://proofwiki.org/wiki/Probability_Generating_Function_of_Zero}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability-generating function of one}]{Probability-generating function of one} \label{sec:pgf-one}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with probability-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pgf}) $G_X(z)$ and set of possible values $\mathcal{X}$. Then, the value of the probability-generating function at one is equal to one:

\begin{equation} \label{eq:pgf-one-pgf-one}
G_X(1) = 1 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pgf}) of $X$ is defined as

\begin{equation} \label{eq:pgf-one-pgf}
G_X(z) = \sum_{x=0}^{\infty} f_X(x) \, z^x
\end{equation}

where $f_X(x)$ is the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$. Setting $z = 1$, we obtain:

\begin{equation} \label{eq:pgf-one-pgf-zero-s1}
\begin{split}
G_X(1) &= \sum_{x=0}^{\infty} f_X(x) \cdot 1^x \\
&= \sum_{x=0}^{\infty} f_X(x) \cdot 1 \\
&= \sum_{x=0}^{\infty} f_X(x) \; .
\end{split}
\end{equation}

Because the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) sums up to one, this becomes:

\begin{equation} \label{eq:pgf-one-pgf-zero-s2}
\begin{split}
G_X(1) &= \sum_{x \in \mathcal{X}} f_X(x) \\
&= 1 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item ProofWiki (2022): "Probability Generating Function of One"; in: \textit{ProofWiki}, retrieved on 2022-10-11; URL: \url{https://proofwiki.org/wiki/Probability_Generating_Function_of_One}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Cumulant-generating function}]{Cumulant-generating function} \label{sec:cgf}
\setcounter{equation}{0}

\textbf{Definition:}

1) The cumulant-generating function of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X \in \mathbb{R}$ is

\begin{equation} \label{eq:cgf-cgf-var}
K_X(t) = \log \mathrm{E} \left[ e^{tX} \right], \quad t \in \mathbb{R} \; .
\end{equation}

2) The cumulant-generating function of a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $X \in \mathbb{R}^n$ is

\begin{equation} \label{eq:cgf-cgf-vec}
K_X(t) = \log \mathrm{E} \left[ e^{t^\mathrm{T}X} \right], \quad t \in \mathbb{R}^n \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Cumulant"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-31; URL: \url{https://en.wikipedia.org/wiki/Cumulant#Definition}.
\end{itemize}
\vspace{1em}



\subsection{Expected value}

\subsubsection[\textit{Definition}]{Definition} \label{sec:mean}
\setcounter{equation}{0}

\textbf{Definition:}

1) The expected value (or, mean) of a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ with domain $\mathcal{X}$ is

\begin{equation} \label{eq:mean-mean-disc}
\mathrm{E}(X) = \sum_{x \in \mathcal{X}} x \cdot f_X(x)
\end{equation}

where $f_X(x)$ is the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$.

\vspace{1em}
2) The expected value (or, mean) of a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ with domain $\mathcal{X}$ is

\begin{equation} \label{eq:mean-mean-cont}
\mathrm{E}(X) = \int_{\mathcal{X}} x \cdot f_X(x) \, \mathrm{d}x
\end{equation}

where $f_X(x)$ is the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Expected value"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-13; URL: \url{https://en.wikipedia.org/wiki/Expected_value#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Sample mean}]{Sample mean} \label{sec:mean-samp}
\setcounter{equation}{0}

\textbf{Definition:} Let $x = \left\lbrace x_1, \ldots, x_n \right\rbrace$ be a sample from a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$. Then, the sample mean of $x$ is denoted as $\bar{x}$ and is given by

\begin{equation} \label{eq:mean-samp-mean-samp}
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Sample mean and covariance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-04-16; URL: \url{https://en.wikipedia.org/wiki/Sample_mean_and_covariance#Definition_of_the_sample_mean}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Non-negative random variable}]{Non-negative random variable} \label{sec:mean-nnrvar}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a non-negative random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation} \label{eq:mean-nnrvar-mean-cdf}
\mathrm{E}(X) = \int_{0}^{\infty} (1 - F_X(x)) \, \mathrm{d}x
\end{equation}

where $F_X(x)$ is the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$.


\vspace{1em}
\textbf{Proof:} Because the cumulative distribution function gives the probability of a random variable being smaller than a given value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}),

\begin{equation} \label{eq:mean-nnrvar-cdf-Pr-leq}
F_X(x) = \mathrm{Pr}(X \leq x) \; ,
\end{equation}

we have

\begin{equation} \label{eq:mean-nnrvar-cdf-Pr-geq}
1 - F_X(x) = \mathrm{Pr}(X > x) \; ,
\end{equation}

such that

\begin{equation} \label{eq:mean-nnrvar-mean-cdf-s1}
\int_{0}^{\infty} (1 - F_X(x)) \, \mathrm{d}x = \int_{0}^{\infty} \mathrm{Pr}(X > x) \, \mathrm{d}x
\end{equation}

which, using the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$, can be rewritten as

\begin{equation} \label{eq:mean-nnrvar-mean-cdf-s2}
\begin{split}
\int_{0}^{\infty} (1 - F_X(x)) \, \mathrm{d}x &= \int_{0}^{\infty} \int_{x}^{\infty} f_X(z) \, \mathrm{d}z \, \mathrm{d}x \\
&= \int_{0}^{\infty} \int_{0}^{z} f_X(z) \, \mathrm{d}x \, \mathrm{d}z \\
&= \int_{0}^{\infty} f_X(z) \int_{0}^{z} 1 \, \mathrm{d}x \, \mathrm{d}z \\
&= \int_{0}^{\infty} [x]_{0}^{z} \cdot f_X(z) \, \mathrm{d}z \\
&= \int_{0}^{\infty} z \cdot f_X(z) \, \mathrm{d}z \\
\end{split}
\end{equation}

and by applying the definition of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), we see that

\begin{equation} \label{eq:mean-nnrvar-mean-cdf-s3}
\int_{0}^{\infty} (1 - F_X(x)) \, \mathrm{d}x = \int_{0}^{\infty} z \cdot f_X(z) \, \mathrm{d}z = \mathrm{E}(X)
\end{equation}

which proves the identity given above.

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Kemp, Graham (2014): "Expected value of a non-negative random variable"; in: \textit{StackExchange Mathematics}, retrieved on 2020-05-18; URL: \url{https://math.stackexchange.com/questions/958472/expected-value-of-a-non-negative-random-variable}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Non-negativity}]{Non-negativity} \label{sec:mean-nonneg}
\setcounter{equation}{0}

\textbf{Theorem:} If a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) is strictly non-negative, its expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is also non-negative, i.e.

\begin{equation} \label{eq:mean-nonneg-mean-nonneg}
\mathrm{E}(X) \geq 0, \quad \text{if} \quad X \geq 0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:}

1) If $X \geq 0$ is a discrete random variable, then, because the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) is always non-negative, all the addends in

\begin{equation} \label{eq:mean-nonneg-mean-disc}
\mathrm{E}(X) = \sum_{x \in \mathcal{X}} x \cdot f_X(x)
\end{equation}

are non-negative, thus the entire sum must be non-negative.

\vspace{1em}
2) If $X \geq 0$ is a continuous random variable, then, because the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) is always non-negative, the integrand in

\begin{equation} \label{eq:mean-nonneg-mean-cont}
\mathrm{E}(X) = \int_{\mathcal{X}} x \cdot f_X(x) \, \mathrm{d}x
\end{equation}

is strictly non-negative, thus the term on the right-hand side is a Lebesgue integral, so that the result on the left-hand side must be non-negative.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Expected value"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-13; URL: \url{https://en.wikipedia.org/wiki/Expected_value#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Linearity}]{Linearity} \label{sec:mean-lin}
\setcounter{equation}{0}

\textbf{Theorem:} The expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is a linear operator, i.e.

\begin{equation} \label{eq:mean-lin-mean-lin}
\begin{split}
\mathrm{E}(X + Y) &= \mathrm{E}(X) + \mathrm{E}(Y) \\
\mathrm{E}(a\,X) &= a\,\mathrm{E}(X)
\end{split}
\end{equation}

for random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ and $Y$ and a constant ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:const}) $a$.


\vspace{1em}
\textbf{Proof:}

1) If $X$ and $Y$ are discrete random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}), the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is

\begin{equation} \label{eq:mean-lin-mean-disc}
\mathrm{E}(X) = \sum_{x \in \mathcal{X}} x \cdot f_X(x)
\end{equation}

and the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}) states that

\begin{equation} \label{eq:mean-lin-lmp-disc}
p(x) = \sum_{y \in \mathcal{Y}} p(x,y) \; .
\end{equation}

Applying this, we have

\begin{equation} \label{eq:mean-lin-mean-lin-s1-disc}
\begin{split}
\mathrm{E}(X + Y) &= \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} (x+y) \cdot f_{X,Y}(x,y) \\
&= \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} x \cdot f_{X,Y}(x,y) + \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} y \cdot f_{X,Y}(x,y) \\
&= \sum_{x \in \mathcal{X}} x \sum_{y \in \mathcal{Y}} f_{X,Y}(x,y) + \sum_{y \in \mathcal{Y}} y \sum_{x \in \mathcal{X}} f_{X,Y}(x,y) \\
&\overset{\eqref{eq:mean-lin-lmp-disc}}{=} \sum_{x \in \mathcal{X}} x \cdot f_X(x) + \sum_{y \in \mathcal{Y}} y \cdot f_{Y}(y) \\
&\overset{\eqref{eq:mean-lin-mean-disc}}{=} \mathrm{E}(X) + \mathrm{E}(Y)
\end{split}
\end{equation}

as well as

\begin{equation} \label{eq:mean-lin-mean-lin-s2-disc}
\begin{split}
\mathrm{E}(a\,X) &= \sum_{x \in \mathcal{X}} a \, x \cdot f_X(x) \\
&= a \, \sum_{x \in \mathcal{X}} x \cdot f_X(x) \\
&\overset{\eqref{eq:mean-lin-mean-disc}}{=} a \, \mathrm{E}(X) \; .
\end{split}
\end{equation}

\vspace{1em}
2) If $X$ and $Y$ are continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}), the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is

\begin{equation} \label{eq:mean-lin-mean-cont}
\mathrm{E}(X) = \int_{\mathcal{X}} x \cdot f_X(x) \, \mathrm{d}x
\end{equation}

and the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}) states that

\begin{equation} \label{eq:mean-lin-lmp-cont}
p(x) = \int_{\mathcal{Y}} p(x,y) \, \mathrm{d}y \; .
\end{equation}

Applying this, we have

\begin{equation} \label{eq:mean-lin-mean-lin-s1-cont}
\begin{split}
\mathrm{E}(X + Y) &= \int_{\mathcal{X}} \int_{\mathcal{Y}} (x+y) \cdot f_{X,Y}(x,y) \, \mathrm{d}y \, \mathrm{d}x \\
&= \int_{\mathcal{X}} \int_{\mathcal{Y}} x \cdot f_{X,Y}(x,y) \, \mathrm{d}y \, \mathrm{d}x + \int_{\mathcal{X}} \int_{\mathcal{Y}} y \cdot f_{X,Y}(x,y) \, \mathrm{d}y \, \mathrm{d}x \\
&= \int_{\mathcal{X}} x \int_{\mathcal{Y}} f_{X,Y}(x,y) \, \mathrm{d}y \, \mathrm{d}x + \int_{\mathcal{Y}} y \int_{\mathcal{X}} f_{X,Y}(x,y) \, \mathrm{d}x \, \mathrm{d}y \\
&\overset{\eqref{eq:mean-lin-lmp-cont}}{=} \int_{\mathcal{X}} x \cdot f_X(x) \, \mathrm{d}x + \int_{\mathcal{Y}} y \cdot f_Y(y) \, \mathrm{d}y \\
&\overset{\eqref{eq:mean-lin-mean-cont}}{=} \mathrm{E}(X) + \mathrm{E}(Y)
\end{split}
\end{equation}

as well as

\begin{equation} \label{eq:mean-lin-mean-lin-s2-cont}
\begin{split}
\mathrm{E}(a\,X) &= \int_{\mathcal{X}} a \, x \cdot f_X(x) \, \mathrm{d}x \\
&= a \int_{\mathcal{X}} x \cdot f_X(x) \, \mathrm{d}x \\
&\overset{\eqref{eq:mean-lin-mean-cont}}{=} a \, \mathrm{E}(X) \; .
\end{split}
\end{equation}

\vspace{1em}
Collectively, this shows that both requirements for linearity are fulfilled for the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), for discrete ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) as well as for continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variables.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Expected value"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-13; URL: \url{https://en.wikipedia.org/wiki/Expected_value#Basic_properties}.
\item Michael B, Kuldeep Guha Mazumder, Geoff Pilling et al. (2020): "Linearity of Expectation"; in: \textit{brilliant.org}, retrieved on 2020-02-13; URL: \url{https://brilliant.org/wiki/linearity-of-expectation/}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Monotonicity}]{Monotonicity} \label{sec:mean-mono}
\setcounter{equation}{0}

\textbf{Theorem:} The expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is monotonic, i.e.

\begin{equation} \label{eq:mean-mono-mean-mono}
\mathrm{E}(X) \leq \mathrm{E}(Y), \quad \text{if} \quad X \leq Y \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Let $Z = Y - X$. Due to the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), we have

\begin{equation} \label{eq:mean-mono-mean-XYZ}
\mathrm{E}(Z) = \mathrm{E}(Y-X) = \mathrm{E}(Y) - \mathrm{E}(X) \; .
\end{equation}

With the non-negativity property of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-nonneg}), it also holds that

\begin{equation} \label{eq:mean-mono-mean-Z}
Z \geq 0 \quad \Rightarrow \quad \mathrm{E}(Z) \geq 0 \; .
\end{equation}

Together with \eqref{eq:mean-mono-mean-XYZ}, this yields

\begin{equation} \label{eq:mean-mono-mean-mono-qed}
\mathrm{E}(Y) - \mathrm{E}(X) \geq 0 \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Expected value"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-17; URL: \url{https://en.wikipedia.org/wiki/Expected_value#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{(Non-)Multiplicativity}]{(Non-)Multiplicativity} \label{sec:mean-mult}
\setcounter{equation}{0}

\textbf{Theorem:}

1) If two random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ and $Y$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is multiplicative, i.e.

\begin{equation} \label{eq:mean-mult-mean-mult}
\mathrm{E}(X\,Y) = \mathrm{E}(X) \, \mathrm{E}(Y) \; .
\end{equation}

2) If two random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ and $Y$ are dependent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is not necessarily multiplicative, i.e. there exist $X$ and $Y$ such that

\begin{equation} \label{eq:mean-mult-mean-nonmult}
\mathrm{E}(X\,Y) \neq \mathrm{E}(X) \, \mathrm{E}(Y) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:}

1) If $X$ and $Y$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), it holds that

\begin{equation} \label{eq:mean-mult-ind}
p(x,y) = p(x) \, p(y) \quad \text{for all} \quad x \in \mathcal{X}, y \in \mathcal{Y} \; .
\end{equation}

Applying this to the expected value for discrete random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), we have

\begin{equation} \label{eq:mean-mult-mean-mult-disc}
\begin{split}
\mathrm{E}(X\,Y) &= \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} (x \cdot y) \cdot f_{X,Y}(x,y) \\
&\overset{\eqref{eq:mean-mult-ind}}{=} \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} (x \cdot y) \cdot \left( f_X(x) \cdot f_Y(y) \right) \\
&= \sum_{x \in \mathcal{X}} x \cdot f_X(x) \, \sum_{y \in \mathcal{Y}} y \cdot f_Y(y) \\
&= \sum_{x \in \mathcal{X}} x \cdot f_X(x) \cdot \mathrm{E}(Y) \\
&= \mathrm{E}(X) \, \mathrm{E}(Y) \; . \\
\end{split}
\end{equation}

And applying it to the expected value for continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), we have

\begin{equation} \label{eq:mean-mult-mean-mult-cont}
\begin{split}
\mathrm{E}(X\,Y) &= \int_{\mathcal{X}} \int_{\mathcal{Y}} (x \cdot y) \cdot f_{X,Y}(x,y) \, \mathrm{d}y \, \mathrm{d}x \\
&\overset{\eqref{eq:mean-mult-ind}}{=} \int_{\mathcal{X}} \int_{\mathcal{Y}} (x \cdot y) \cdot \left( f_X(x) \cdot f_Y(y) \right) \, \mathrm{d}y \, \mathrm{d}x \\
&= \int_{\mathcal{X}} x \cdot f_X(x) \, \int_{\mathcal{Y}} y \cdot f_Y(y)  \, \mathrm{d}y \, \mathrm{d}x \\
&= \int_{\mathcal{X}} x \cdot f_X(x) \cdot \mathrm{E}(Y) \, \mathrm{d}x \\
&= \mathrm{E}(X) \, \mathrm{E}(Y) \; . \\
\end{split}
\end{equation}

\vspace{1em}
2) Let $X$ and $Y$ be Bernoulli random variables ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}) with the following joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf})

\begin{equation} \label{eq:mean-mult-joint}
\begin{split}
p(X=0, Y=0) &= 1/2 \\
p(X=0, Y=1) &= 0 \\
p(X=1, Y=0) &= 0 \\
p(X=1, Y=1) &= 1/2
\end{split}
\end{equation}

and thus, the following marginal probabilities:

\begin{equation} \label{eq:mean-mult-marg}
\begin{split}
p(X=0) = p(X=1) &= 1/2 \\
p(Y=0) = p(Y=1) &= 1/2 \; .
\end{split}
\end{equation}

Then, $X$ and $Y$ are dependent, because

\begin{equation} \label{eq:mean-mult-dep}
p(X=0, Y=1) \overset{\eqref{eq:mean-mult-joint}}{=} 0 \neq \frac{1}{2} \cdot \frac{1}{2} \overset{\eqref{eq:mean-mult-marg}}{=} p(X=0) \, p(Y=1) \; ,
\end{equation}

and the expected value of their product is

\begin{equation} \label{eq:mean-mult-mean-prod}
\begin{split}
\mathrm{E}(X\,Y) &= \sum_{x \in \left\lbrace 0,1 \right\rbrace} \sum_{y \in \left\lbrace 0,1 \right\rbrace} (x \cdot y) \cdot p(x,y) \\
&= (1 \cdot 1) \cdot p(X=1, Y=1) \\
&\overset{\eqref{eq:mean-mult-joint}}{=} \frac{1}{2}
\end{split}
\end{equation}

while the product of their expected values is

\begin{equation} \label{eq:mean-mult-prod-mean}
\begin{split}
\mathrm{E}(X) \, \mathrm{E}(Y) &= \left( \sum_{x \in \left\lbrace 0,1 \right\rbrace} x \cdot p(x) \right) \cdot \left( \sum_{y \in \left\lbrace 0,1 \right\rbrace} y \cdot p(y) \right) \\
&= \left( 1 \cdot p(X=1) \right) \cdot \left( 1 \cdot p(Y=1) \right) \\
&\overset{\eqref{eq:mean-mult-marg}}{=} \frac{1}{4}
\end{split}
\end{equation}

and thus,

\begin{equation} \label{eq:mean-mult-mean-nonmult-qed}
\mathrm{E}(X\,Y) \neq \mathrm{E}(X) \, \mathrm{E}(Y) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Expected value"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-17; URL: \url{https://en.wikipedia.org/wiki/Expected_value#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Expectation of a trace}]{Expectation of a trace} \label{sec:mean-tr}
\setcounter{equation}{0}

\textbf{Theorem:} Let $A$ be an $n \times n$ random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}). Then, the expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the trace of $A$ is equal to the trace of the expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $A$:

\begin{equation} \label{eq:mean-tr-mean-tr}
\mathrm{E}\left[ \mathrm{tr}(A) \right] = \mathrm{tr}\left( \mathrm{E}[A] \right) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The trace of an $n \times n$ matrix $A$ is defined as:

\begin{equation} \label{eq:mean-tr-tr}
\mathrm{tr}(A) = \sum_{i=1}^{n} a_{ii} \; .
\end{equation}

Using this definition of the trace, the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}) and the expected value of a random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-rmat}), we have:

\begin{equation} \label{eq:mean-tr-mean-tr-qed}
\begin{split}
\mathrm{E}\left[ \mathrm{tr}(A) \right] &= \mathrm{E}\left[ \sum_{i=1}^{n} a_{ii} \right] \\
&= \sum_{i=1}^{n} \mathrm{E}\left[ a_{ii} \right] \\
&= \mathrm{tr}\left( \left[ \begin{matrix} \mathrm{E}[a_{11}] & \ldots & \mathrm{E}[a_{1n}] \\ \vdots & \ddots & \vdots \\ \mathrm{E}[a_{n1}] & \ldots & \mathrm{E}[a_{nn}] \end{matrix} \right] \right) \\
&= \mathrm{tr}\left( \mathrm{E}[A] \right) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item drerD (2018): "'Trace trick' for expectations of quadratic forms"; in: \textit{StackExchange Mathematics}, retrieved on 2021-12-07; URL: \url{https://math.stackexchange.com/a/3004034/480910}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Expectation of a quadratic form}]{Expectation of a quadratic form} \label{sec:mean-qf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) with mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu$ and covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) $\Sigma$ and let $A$ be a symmetric $n \times n$ matrix. Then, the expectation of the quadratic form $X^\mathrm{T} A X$ is

\begin{equation} \label{eq:mean-qf-mean-qf}
\mathrm{E}\left[ X^\mathrm{T} A X \right] = \mu^\mathrm{T} A \mu + \mathrm{tr}(A \Sigma) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Note that $X^\mathrm{T} A X$ is a $1 \times 1$ matrix. We can therefore write

\begin{equation} \label{eq:mean-qf-mean-qf-s1}
\mathrm{E}\left[ X^\mathrm{T} A X \right] =  \mathrm{E}\left[ \mathrm{tr} \left( X^\mathrm{T} A X \right) \right] \; .
\end{equation}

Using the trace property $\mathrm{tr}(ABC) = \mathrm{tr}(BCA)$, this becomes

\begin{equation} \label{eq:mean-qf-mean-qf-s2}
\mathrm{E}\left[ X^\mathrm{T} A X \right] =  \mathrm{E}\left[ \mathrm{tr} \left( A X X^\mathrm{T} \right) \right] \; .
\end{equation}

Because mean and trace are linear operators ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-tr}), we have

\begin{equation} \label{eq:mean-qf-mean-qf-s3}
\mathrm{E}\left[ X^\mathrm{T} A X \right] =  \mathrm{tr} \left( A \; \mathrm{E}\left[ X X^\mathrm{T} \right] \right) \; .
\end{equation}

Note that the covariance matrix can be partitioned into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat-mean})

\begin{equation} \label{eq:mean-qf-covmat-mean}
\mathrm{Cov}(X,X) = \mathrm{E}(X X^\mathrm{T}) - \mathrm{E}(X) \mathrm{E}(X)^\mathrm{T} \; ,
\end{equation}

such that the expected value of the quadratic form becomes

\begin{equation} \label{eq:mean-qf-mean-qf-s4}
\mathrm{E}\left[ X^\mathrm{T} A X \right] =  \mathrm{tr} \left( A \left[ \mathrm{Cov}(X,X) + \mathrm{E}(X) \mathrm{E}(X)^\mathrm{T} \right] \right) \; .
\end{equation}

Finally, applying mean and covariance of $X$, we have

\begin{equation} \label{eq:mean-qf-mean-qf-s5}
\begin{split}
\mathrm{E}\left[ X^\mathrm{T} A X \right] &= \mathrm{tr} \left( A \left[ \Sigma + \mu \mu^\mathrm{T} \right] \right) \\
&= \mathrm{tr} \left( A \Sigma + A \mu \mu^\mathrm{T} \right) \\
&= \mathrm{tr}(A \Sigma) + \mathrm{tr}(A \mu \mu^\mathrm{T}) \\
&= \mathrm{tr}(A \Sigma) + \mathrm{tr}(\mu^\mathrm{T} A \mu) \\
&= \mu^\mathrm{T} A \mu + \mathrm{tr}(A \Sigma) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Kendrick, David (1981): "Expectation of a quadratic form"; in: \textit{Stochastic Control for Economic Models}, pp. 170-171.
\item Wikipedia (2020): "Multivariate random variable"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-07-13; URL: \url{https://en.wikipedia.org/wiki/Multivariate_random_variable#Expectation_of_a_quadratic_form}.
\item Halvorsen, Kjetil B. (2012): "Expected value and variance of trace function"; in: \textit{StackExchange CrossValidated}, retrieved on 2020-07-13; URL: \url{https://stats.stackexchange.com/questions/34477/expected-value-and-variance-of-trace-function}.
\item Sarwate, Dilip (2013): "Expected Value of Quadratic Form"; in: \textit{StackExchange CrossValidated}, retrieved on 2020-07-13; URL: \url{https://stats.stackexchange.com/questions/48066/expected-value-of-quadratic-form}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Squared expectation of a product}]{Squared expectation of a product} \label{sec:mean-prodsqr}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be two random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mathrm{E}(X)$ and $\mathrm{E}(Y)$ and let $\mathrm{E}(XY)$ exist and be finite. Then, the square of the expectation of the product of $X$ and $Y$ is less than or equal to the product of the expectation of the squares of $X$ and $Y$:

\begin{equation} \label{eq:mean-prodsqr-EXY2-EX2-EY2}
\left[ \mathrm{E}(XY) \right]^2 \leq \mathrm{E}\left( X^2 \right) \mathrm{E}\left( Y^2 \right) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Note that $Y^2$ is a non-negative random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) whose expected value is also non-negative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-nonneg}):

\begin{equation} \label{eq:mean-prodsqr-mean-Y2-nonneg}
\mathrm{E}\left( Y^2 \right) \geq 0 \; .
\end{equation}

1) First, consider the case that $\mathrm{E}\left( Y^2 \right) > 0$. Define a new random variable Z as

\begin{equation} \label{eq:mean-prodsqr-Z}
Z = X - Y \, \frac{\mathrm{E}(XY)}{\mathrm{E}\left( Y^2 \right)} \; .
\end{equation}

Once again, because $Z^2$ is always non-negative, we have the expected value:

\begin{equation} \label{eq:mean-prodsqr-mean-Z2-nonneg}
\mathrm{E}\left( Z^2 \right) \geq 0 \; .
\end{equation}

Thus, using the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), we have

\begin{equation} \label{eq:mean-prodsqr-mean-prodsqr-v1}
\begin{split}
0 &\leq \mathrm{E}\left( Z^2 \right) \\
&\leq \mathrm{E}\left( \left( X - Y \, \frac{\mathrm{E}(XY)}{\mathrm{E}\left( Y^2 \right)} \right)^2 \right) \\
&\leq \mathrm{E}\left( X^2 - 2 X Y \, \frac{\mathrm{E}(XY)}{\mathrm{E}\left( Y^2 \right)} + Y^2 \, \frac{\left[ \mathrm{E}(XY) \right]^2}{\left[ \mathrm{E}\left( Y^2 \right) \right]^2} \right) \\
&\leq \mathrm{E}\left( X^2 \right) - 2 \, \mathrm{E}(XY) \, \frac{\mathrm{E}(XY)}{\mathrm{E}\left( Y^2 \right)} + \mathrm{E}\left( Y^2 \right) \, \frac{\left[ \mathrm{E}(XY) \right]^2}{\left[ \mathrm{E}\left( Y^2 \right) \right]^2} \\
&\leq \mathrm{E}\left( X^2 \right) - 2 \, \frac{\left[ \mathrm{E}(XY) \right]^2}{\mathrm{E}\left( Y^2 \right)} + \frac{\left[ \mathrm{E}(XY) \right]^2}{\mathrm{E}\left( Y^2 \right)} \\
&\leq \mathrm{E}\left( X^2 \right) - \frac{\left[ \mathrm{E}(XY) \right]^2}{\mathrm{E}\left( Y^2 \right)} \; , \\
\end{split}
\end{equation}

giving

\begin{equation} \label{eq:mean-prodsqr-EXY2-EX2-EY2-qed-v1}
\left[ \mathrm{E}(XY) \right]^2 \leq \mathrm{E}\left( X^2 \right) \mathrm{E}\left( Y^2 \right)
\end{equation}

as required.

2) Next, consider the case that $\mathrm{E}\left( Y^2 \right) = 0$. In this case, $Y$ must be a constant ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:const}) with mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mathrm{E}(Y) = 0$ and variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) $\mathrm{Var}(Y) = 0$, thus we have

\begin{equation} \label{eq:mean-prodsqr-Pr-Y-0}
\mathrm{Pr}(Y = 0) = 1 \; .
\end{equation}

This implies

\begin{equation} \label{eq:mean-prodsqr-Pr-XY-0}
\mathrm{Pr}(XY = 0) = 1 \; ,
\end{equation}

such that

\begin{equation} \label{eq:mean-prodsqr-EXY}
\mathrm{E}(XY) = 0 \; .
\end{equation}

Thus, we can write

\begin{equation} \label{eq:mean-prodsqr-mean-prodsqr-v2}
0 = \left[ \mathrm{E}(XY) \right]^2 = \mathrm{E}\left( X^2 \right) \mathrm{E}\left( Y^2 \right) = 0 \; ,
\end{equation}

giving

\begin{equation} \label{eq:mean-prodsqr-EXY2-EX2-EY2-qed-v2}
\left[ \mathrm{E}(XY) \right]^2 \leq \mathrm{E}\left( X^2 \right) \mathrm{E}\left( Y^2 \right)
\end{equation}

as required.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item ProofWiki (2022): "Square of Expectation of Product is Less Than or Equal to Product of Expectation of Squares"; in: \textit{ProofWiki}, retrieved on 2022-10-11; URL: \url{https://proofwiki.org/wiki/Square_of_Expectation_of_Product_is_Less_Than_or_Equal_to_Product_of_Expectation_of_Squares}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Law of total expectation}]{Law of total expectation} \label{sec:mean-tot}
\setcounter{equation}{0}

\textbf{Theorem:} (law of total expectation, also called "law of iterated expectations") Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mathrm{E}(X)$ and let $Y$ be any random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) defined on the same probability space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-spc}). Then, the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the conditional expectation of $X$ given $Y$ is the same as the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$:

\begin{equation} \label{eq:mean-tot-mean-tot}
\mathrm{E}(X) = \mathrm{E}[\mathrm{E}(X \vert Y)] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Let $X$ and $Y$ be discrete random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) with sets of possible outcomes $\mathcal{X}$ and $\mathcal{Y}$. Then, the expectation of the conditional expetectation can be rewritten as:

\begin{equation} \label{eq:mean-tot-mean-tot-s1}
\begin{split}
\mathrm{E}[\mathrm{E}(X \vert Y)] &= \mathrm{E}\left[ \sum_{x \in \mathcal{X}} x \cdot \mathrm{Pr}(X = x \vert Y) \right] \\
&= \sum_{y \in \mathcal{Y}} \left[ \sum_{x \in \mathcal{X}} x \cdot \mathrm{Pr}(X = x \vert Y = y) \right] \cdot \mathrm{Pr}(Y = y) \\
&= \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} x \cdot \mathrm{Pr}(X = x \vert Y = y) \cdot \mathrm{Pr}(Y = y) \; .
\end{split}
\end{equation}

Using the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), this becomes:

\begin{equation} \label{eq:mean-tot-mean-tot-s2}
\begin{split}
\mathrm{E}[\mathrm{E}(X \vert Y)] &= \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} x \cdot \mathrm{Pr}(X = x, Y = y) \\
&= \sum_{x \in \mathcal{X}} x \sum_{y \in \mathcal{Y}} \mathrm{Pr}(X = x, Y = y) \; .
\end{split}
\end{equation}

Using the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), this becomes:

\begin{equation} \label{eq:mean-tot-mean-tot-s3}
\begin{split}
\mathrm{E}[\mathrm{E}(X \vert Y)] &= \sum_{x \in \mathcal{X}} x \cdot \mathrm{Pr}(X = x) \\
&= \mathrm{E}(X) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Law of total expectation"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-11-26; URL: \url{https://en.wikipedia.org/wiki/Law_of_total_expectation#Proof_in_the_finite_and_countable_cases}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Law of the unconscious statistician}]{Law of the unconscious statistician} \label{sec:mean-lotus}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) and let $Y = g(X)$ be a function of this random variable.

1) If $X$ is a discrete random variable with possible outcomes $\mathcal{X}$ and probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) $f_X(x)$, the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $g(X)$ is

\begin{equation} \label{eq:mean-lotus-mean-lotus-disc}
\mathrm{E}[g(X)] = \sum_{x \in \mathcal{X}} g(x) f_X(x) \; .
\end{equation}

2) If $X$ is a continuous random variable with possible outcomes $\mathcal{X}$ and probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) $f_X(x)$, the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $g(X)$ is

\begin{equation} \label{eq:mean-lotus-mean-lotus-cont}
\mathrm{E}[g(X)] = \int_{\mathcal{X}} g(x) f_X(x) \, \mathrm{d}x \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Suppose that $g$ is differentiable and that its inverse $g^{-1}$ is monotonic.

1) The expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $Y = g(X)$ is defined as

\begin{equation} \label{eq:mean-lotus-mean-lotus-disc-s1}
\mathrm{E}[Y] = \sum_{y \in \mathcal{Y}} y \, f_Y(y) \; .
\end{equation}

Writing the probability mass function $f_Y(y)$ in terms of $y = g(x)$, we have:

\begin{equation} \label{eq:mean-lotus-mean-lotus-disc-s2}
\begin{split}
\mathrm{E}[g(X)] &= \sum_{y \in \mathcal{Y}} y \, \mathrm{Pr}(g(x) = y) \\
&= \sum_{y \in \mathcal{Y}} y \, \mathrm{Pr}(x = g^{-1}(y)) \\
&= \sum_{y \in \mathcal{Y}} y \sum_{x = g^{-1}(y)} f_X(x) \\
&= \sum_{y \in \mathcal{Y}} \sum_{x = g^{-1}(y)} y f_X(x) \\
&= \sum_{y \in \mathcal{Y}} \sum_{x = g^{-1}(y)} g(x) f_X(x) \; .
\end{split}
\end{equation}

Finally, noting that "for all $y$, then for all $x = g^{-1}(y)$" is equivalent to "for all $x$" if $g^{-1}$ is a monotonic function, we can conclude that

\begin{equation} \label{eq:mean-lotus-mean-lotus-disc-s3}
\mathrm{E}[g(X)] = \sum_{x \in \mathcal{X}} g(x) f_X(x) \; .
\end{equation}

\vspace{1em}
2) Let $y = g(x)$. The derivative of an inverse function is

\begin{equation} \label{eq:mean-lotus-der-inv}
\frac{\mathrm{d}}{\mathrm{d}y} (g^{-1}(y)) = \frac{1}{g'(g^{-1}(y))}
\end{equation}

Because $x = g^{-1}(y)$, this can be rearranged into

\begin{equation} \label{eq:mean-lotus-dx-dy}
\mathrm{d}x = \frac{1}{g'(g^{-1}(y))} \, \mathrm{d}y
\end{equation}

and subsitituing \eqref{eq:mean-lotus-dx-dy} into \eqref{eq:mean-lotus-mean-lotus-cont}, we get

\begin{equation} \label{eq:mean-lotus-mean-lotus-cont-s1}
\int_{\mathcal{X}} g(x) f_X(x) \, \mathrm{d}x = \int_{\mathcal{Y}} y \, f_X(g^{-1}(y)) \, \frac{1}{g'(g^{-1}(y))} \, \mathrm{d}y \; .
\end{equation}

Considering the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $Y$, one can deduce:

\begin{equation} \label{eq:mean-lotus-Y-cdf}
\begin{split}
F_Y(y) &= \mathrm{Pr}(Y \leq y) \\
&= \mathrm{Pr}(g(X) \leq y) \\
&= \mathrm{Pr}(X \leq g^{-1}(y)) \\
&= F_X(g^{-1}(y)) \; .
\end{split}
\end{equation}

Differentiating to get the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $Y$, the result is:

\begin{equation} \label{eq:mean-lotus-Y-pdf}
\begin{split}
f_Y(y) &= \frac{\mathrm{d}}{\mathrm{d}y} F_Y(y) \\
&\overset{\eqref{eq:mean-lotus-Y-cdf}}{=} \frac{\mathrm{d}}{\mathrm{d}y} F_X(g^{-1}(y)) \\
&= f_X(g^{-1}(y)) \, \frac{\mathrm{d}}{\mathrm{d}y} (g^{-1}(y)) \\
&\overset{\eqref{eq:mean-lotus-der-inv}}{=} f_X(g^{-1}(y)) \, \frac{1}{g'(g^{-1}(y))} \; .
\end{split}
\end{equation}

Finally, substituing \eqref{eq:mean-lotus-Y-pdf} into \eqref{eq:mean-lotus-mean-lotus-cont-s1}, we have:

\begin{equation} \label{eq:mean-lotus-mean-lotus-cont-s2}
\int_{\mathcal{X}} g(x) f_X(x) \, \mathrm{d}x = \int_{\mathcal{Y}} y \, f_Y(y) \, \mathrm{d}y = \mathrm{E}[Y] = \mathrm{E}[g(X)] \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Law of the unconscious statistician"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-07-22; URL: \url{https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician#Proof}.
\item Taboga, Marco (2017): "Transformation theorem"; in: \textit{Lectures on probability and mathematical statistics}, retrieved on 2021-09-22; URL: \url{https://www.statlect.com/glossary/transformation-theorem}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Expected value of a random vector}]{Expected value of a random vector} \label{sec:mean-rvec}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is an $n \times 1$ vector whose entries correspond to the expected values of the entries of the random vector:

\begin{equation} \label{eq:mean-rvec-mean-rvec}
\mathrm{E}(X) = \mathrm{E}\left( \left[ \begin{array}{c} X_1 \\ \vdots \\ X_n \end{array} \right] \right) = \left[ \begin{array}{c} \mathrm{E}(X_1) \\ \vdots \\ \mathrm{E}(X_n) \end{array} \right] \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Expected value"; in: \textit{Lectures on probability theory and mathematical statistics}, retrieved on 2021-07-08; URL: \url{https://www.statlect.com/fundamentals-of-probability/expected-value#hid12}.
\item Wikipedia (2021): "Multivariate random variable"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-07-08; URL: \url{https://en.wikipedia.org/wiki/Multivariate_random_variable#Expected_value}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Expected value of a random matrix}]{Expected value of a random matrix} \label{sec:mean-rmat}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be an $n \times p$ random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}). Then, the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is an $n \times p$ matrix whose entries correspond to the expected values of the entries of the random matrix:

\begin{equation} \label{eq:mean-rmat-mean-rmat}
\mathrm{E}(X) = \mathrm{E}\left( \left[ \begin{array}{ccc} X_{11} & \ldots & X_{1p} \\ \vdots & \ddots & \vdots \\ X_{n1} & \ldots & X_{np} \end{array} \right] \right) = \left[ \begin{array}{ccc} \mathrm{E}(X_{11}) & \ldots & \mathrm{E}(X_{1p}) \\ \vdots & \ddots & \vdots \\ \mathrm{E}(X_{n1}) & \ldots & \mathrm{E}(X_{np}) \end{array} \right] \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2017): "Expected value"; in: \textit{Lectures on probability theory and mathematical statistics}, retrieved on 2021-07-08; URL: \url{https://www.statlect.com/fundamentals-of-probability/expected-value#hid13}.
\end{itemize}
\vspace{1em}



\subsection{Variance}

\subsubsection[\textit{Definition}]{Definition} \label{sec:var}
\setcounter{equation}{0}

\textbf{Definition:} The variance of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ is defined as the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the squared deviation from its expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}):

\begin{equation} \label{eq:var-var}
\mathrm{Var}(X) = \mathrm{E}\left[ (X-\mathrm{E}(X))^2 \right] \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-13; URL: \url{https://en.wikipedia.org/wiki/Variance#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Sample variance}]{Sample variance} \label{sec:var-samp}
\setcounter{equation}{0}

\textbf{Definition:} Let $x = \left\lbrace x_1, \ldots, x_n \right\rbrace$ be a sample from a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$. Then, the sample variance of $x$ is given by

\begin{equation} \label{eq:var-samp-var-samp}
\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
\end{equation}

and the unbiased sample variance of $x$ is given by

\begin{equation} \label{eq:var-samp-var-samp-unb}
s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2
\end{equation}

where $\bar{x}$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-04-16; URL: \url{https://en.wikipedia.org/wiki/Variance#Sample_variance}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Partition into expected values}]{Partition into expected values} \label{sec:var-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is equal to the mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the square of $X$ minus the square of the mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$:

\begin{equation} \label{eq:var-mean-var-mean}
\mathrm{Var}(X) = \mathrm{E}(X^2) - \mathrm{E}(X)^2 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is defined as

\begin{equation} \label{eq:var-mean-var}
\mathrm{Var}(X) = \mathrm{E}\left[ \left( X - \mathrm{E}[X] \right)^2 \right]
\end{equation}

which, due to the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), can be rewritten as

\begin{equation} \label{eq:var-mean-var-mean-qed}
\begin{split}
\mathrm{Var}(X) &= \mathrm{E}\left[ \left( X - \mathrm{E}[X] \right)^2 \right] \\
&= \mathrm{E}\left[ X^2 - 2 \, X \, \mathrm{E}(X) + \mathrm{E}(X)^2 \right] \\
&= \mathrm{E}(X^2) - 2 \, \mathrm{E}(X) \, \mathrm{E}(X) + \mathrm{E}(X)^2 \\
&= \mathrm{E}(X^2) - \mathrm{E}(X)^2 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-19; URL: \url{https://en.wikipedia.org/wiki/Variance#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Non-negativity}]{Non-negativity} \label{sec:var-nonneg}
\setcounter{equation}{0}

\textbf{Theorem:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is always non-negative, i.e.

\begin{equation} \label{eq:var-nonneg-var-nonneg}
\mathrm{Var}(X) \geq 0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) is defined as

\begin{equation} \label{eq:var-nonneg-var}
\mathrm{Var}(X) = \mathrm{E}\left[ (X-\mathrm{E}(X))^2 \right] \; .
\end{equation}

\vspace{1em}
1) If $X$ is a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}), then, because squares and probabilities are strictly non-negative, all the addends in

\begin{equation} \label{eq:var-nonneg-var-disc}
\mathrm{Var}(X) = \sum_{x \in \mathcal{X}} (x-\mathrm{E}(X))^2 \cdot f_X(x)
\end{equation}

are also non-negative, thus the entire sum must be non-negative.

\vspace{1em}
2) If $X$ is a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}), then, because squares and probability densities are strictly non-negative, the integrand in

\begin{equation} \label{eq:var-nonneg-var-cont}
\mathrm{Var}(X) = \int_{\mathcal{X}} (x-\mathrm{E}(X))^2 \cdot f_X(x) \, \mathrm{d}x
\end{equation}

is always non-negative, thus the term on the right-hand side is a Lebesgue integral, so that the result on the left-hand side must be non-negative.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-06-06; URL: \url{https://en.wikipedia.org/wiki/Variance#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Variance of a constant}]{Variance of a constant} \label{sec:var-const}
\setcounter{equation}{0}

\textbf{Theorem:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of a constant ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:const}) is zero

\begin{equation} \label{eq:var-const-var-const}
a = \text{const.} \quad \Rightarrow \quad \mathrm{Var}(a) = 0
\end{equation}

and if the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is zero, then $X$ is a constant ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:const})

\begin{equation} \label{eq:var-const-var-zero}
\mathrm{Var}(X) = 0 \quad \Rightarrow \quad X = \text{const.}
\end{equation}


\vspace{1em}
\textbf{Proof:}

1) A constant ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:const}) is defined as a quantity that always has the same value. Thus, if understood as a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}), the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of a constant is equal to itself:

\begin{equation} \label{eq:var-const-mean-const}
\mathrm{E}(a) = a \; .
\end{equation}

Plugged into the formula of the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}), we have

\begin{equation} \label{eq:var-const-var-const-s1}
\begin{split}
\mathrm{Var}(a) &= \mathrm{E}\left[ (a-\mathrm{E}(a))^2 \right] \\
&= \mathrm{E}\left[ (a-a)^2 \right] \\
&= \mathrm{E}(0) \; .
\end{split}
\end{equation}

Applied to the formula of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), this gives

\begin{equation} \label{eq:var-const-var-const-s2}
\mathrm{E}(0) = \sum_{x=0} x \cdot f_X(x) = 0 \cdot 1 = 0 \; .
\end{equation}

Together, \eqref{eq:var-const-var-const-s1} and \eqref{eq:var-const-var-const-s2} imply \eqref{eq:var-const-var-const}.

\vspace{1em}

2) The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is defined as

\begin{equation} \label{eq:var-const-var}
\mathrm{Var}(X) = \mathrm{E}\left[ (X-\mathrm{E}(X))^2 \right] \; .
\end{equation}

Because $(X-\mathrm{E}(X))^2$ is strictly non-negative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-nonneg}), the only way for the variance to become zero is, if the squared deviation is always zero:

\begin{equation} \label{eq:var-const-sqr-dev-zero}
(X-\mathrm{E}(X))^2 = 0 \; .
\end{equation}

This, in turn, requires that $X$ is equal to its expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean})

\begin{equation} \label{eq:var-const-X-eq-E-X}
X = \mathrm{E}(X)
\end{equation}

which can only be the case, if $X$ always has the same value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:const}):

\begin{equation} \label{eq:var-const-X-const}
X = \text{const.}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-06-27; URL: \url{https://en.wikipedia.org/wiki/Variance#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Invariance under addition}]{Invariance under addition} \label{sec:var-inv}
\setcounter{equation}{0}

\textbf{Theorem:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is invariant under addition of a constant ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:const}):

\begin{equation} \label{eq:var-inv-var-inv}
\mathrm{Var}(X+a) = \mathrm{Var}(X)
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is defined in terms of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) as

\begin{equation} \label{eq:var-inv-var}
\mathrm{Var}(X) = \mathrm{E}\left[ (X-\mathrm{E}(X))^2 \right] \; .
\end{equation}

Using this and the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), we can derive \eqref{eq:var-inv-var-inv} as follows:

\begin{equation} \label{eq:var-inv-var-inv-qed}
\begin{split}
\mathrm{Var}(X+a) &\overset{\eqref{eq:var-inv-var}}{=} \mathrm{E}\left[ ((X+a)-\mathrm{E}(X+a))^2 \right] \\
&= \mathrm{E}\left[ (X + a - \mathrm{E}(X) - a)^2 \right] \\
&= \mathrm{E}\left[ (X-\mathrm{E}(X))^2 \right] \\
&\overset{\eqref{eq:var-inv-var}}{=} \mathrm{Var}(X) \; . \\
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-07-07; URL: \url{https://en.wikipedia.org/wiki/Variance#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Scaling upon multiplication}]{Scaling upon multiplication} \label{sec:var-scal}
\setcounter{equation}{0}

\textbf{Theorem:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) scales upon multiplication with a constant ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:const}):

\begin{equation} \label{eq:var-scal-var-scal}
\mathrm{Var}(aX) = a^2 \, \mathrm{Var}(X)
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is defined in terms of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) as

\begin{equation} \label{eq:var-scal-var}
\mathrm{Var}(X) = \mathrm{E}\left[ (X-\mathrm{E}(X))^2 \right] \; .
\end{equation}

Using this and the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), we can derive \eqref{eq:var-scal-var-scal} as follows:

\begin{equation} \label{eq:var-scal-var-scal-qed}
\begin{split}
\mathrm{Var}(aX) &\overset{\eqref{eq:var-scal-var}}{=} \mathrm{E}\left[ ((aX)-\mathrm{E}(aX))^2 \right] \\
&= \mathrm{E}\left[ (aX - a\mathrm{E}(X))^2 \right] \\
&= \mathrm{E}\left[ (a [X - \mathrm{E}(X)])^2 \right] \\
&= \mathrm{E}\left[ a^2 (X - \mathrm{E}(X))^2 \right] \\
&= a^2 \, \mathrm{E}\left[ (X - \mathrm{E}(X))^2 \right] \\
&\overset{\eqref{eq:var-scal-var}}{=} a^2 \, \mathrm{Var}(X) \; . \\
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-07-07; URL: \url{https://en.wikipedia.org/wiki/Variance#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Variance of a sum}]{Variance of a sum} \label{sec:var-sum}
\setcounter{equation}{0}

\textbf{Theorem:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of the sum of two random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) equals the sum of the variances of those random variables, plus two times their covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}):

\begin{equation} \label{eq:var-sum-var-sum}
\mathrm{Var}(X+Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) + 2 \, \mathrm{Cov}(X,Y) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is defined in terms of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) as

\begin{equation} \label{eq:var-sum-var}
\mathrm{Var}(X) = \mathrm{E}\left[ (X-\mathrm{E}(X))^2 \right] \; .
\end{equation}

Using this and the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), we can derive \eqref{eq:var-sum-var-sum} as follows:

\begin{equation} \label{eq:var-sum-var-sum-qed}
\begin{split}
\mathrm{Var}(X+Y) &\overset{\eqref{eq:var-sum-var}}{=} \mathrm{E}\left[ ((X+Y)-\mathrm{E}(X+Y))^2 \right] \\
&= \mathrm{E}\left[ ([X-\mathrm{E}(X)] + [Y-\mathrm{E}(Y)])^2 \right] \\
&= \mathrm{E}\left[ (X-\mathrm{E}(X))^2 + (Y-\mathrm{E}(Y))^2 + 2 \, (X-\mathrm{E}(X)) (Y-\mathrm{E}(Y)) \right] \\
&= \mathrm{E}\left[ (X-\mathrm{E}(X))^2 \right] + \mathrm{E}\left[ (Y-\mathrm{E}(Y))^2 \right] + \mathrm{E}\left[ 2 \, (X-\mathrm{E}(X)) (Y-\mathrm{E}(Y)) \right] \\
&\overset{\eqref{eq:var-sum-var}}{=} \mathrm{Var}(X) + \mathrm{Var}(Y) + 2 \, \mathrm{Cov}(X,Y) \; . \\
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-07-07; URL: \url{https://en.wikipedia.org/wiki/Variance#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Variance of linear combination}]{Variance of linear combination} \label{sec:var-lincomb}
\setcounter{equation}{0}

\textbf{Theorem:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of the linear combination of two random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) is a function of the variances as well as the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of those random variables:

\begin{equation} \label{eq:var-lincomb-var-lincomb}
\mathrm{Var}(aX+bY) = a^2 \, \mathrm{Var}(X) + b^2 \, \mathrm{Var}(Y) + 2ab \, \mathrm{Cov}(X,Y) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is defined in terms of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) as

\begin{equation} \label{eq:var-lincomb-var}
\mathrm{Var}(X) = \mathrm{E}\left[ (X-\mathrm{E}(X))^2 \right] \; .
\end{equation}

Using this and the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), we can derive \eqref{eq:var-lincomb-var-lincomb} as follows:

\begin{equation} \label{eq:var-lincomb-var-lincomb-qed}
\begin{split}
\mathrm{Var}(aX+bY) &\overset{\eqref{eq:var-lincomb-var}}{=} \mathrm{E}\left[ ((aX+bY)-\mathrm{E}(aX+bY))^2 \right] \\
&= \mathrm{E}\left[ (a [X-\mathrm{E}(X)] + b [Y-\mathrm{E}(Y)])^2 \right] \\
&= \mathrm{E}\left[ a^2 \, (X-\mathrm{E}(X))^2 + b^2 \, (Y-\mathrm{E}(Y))^2 + 2ab \, (X-\mathrm{E}(X)) (Y-\mathrm{E}(Y)) \right] \\
&= \mathrm{E}\left[ a^2 \, (X-\mathrm{E}(X))^2 \right] + \mathrm{E}\left[ b^2 \, (Y-\mathrm{E}(Y))^2 \right] + \mathrm{E}\left[ 2ab \, (X-\mathrm{E}(X)) (Y-\mathrm{E}(Y)) \right] \\
&\overset{\eqref{eq:var-lincomb-var}}{=} a^2 \, \mathrm{Var}(X) + b^2 \, \mathrm{Var}(Y) + 2ab \, \mathrm{Cov}(X,Y) \; . \\
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-07-07; URL: \url{https://en.wikipedia.org/wiki/Variance#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Additivity under independence}]{Additivity under independence} \label{sec:var-add}
\setcounter{equation}{0}

\textbf{Theorem:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is additive for independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}):

\begin{equation} \label{eq:var-add-var-add}
p(X,Y) = p(X) \, p(Y) \quad \Rightarrow \quad \mathrm{Var}(X+Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance of the sum of two random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-sum}) is given by

\begin{equation} \label{eq:var-add-var-sum}
\mathrm{Var}(X+Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) + 2 \, \mathrm{Cov}(X,Y) \; .
\end{equation}

The covariance of independent random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-ind}) is zero:

\begin{equation} \label{eq:var-add-cov-ind}
p(X,Y) = p(X) \, p(Y) \quad \Rightarrow \quad \mathrm{Cov}(X,Y) = 0 \; .
\end{equation}

Combining \eqref{eq:var-add-var-sum} and \eqref{eq:var-add-cov-ind}, we have:

\begin{equation} \label{eq:var-add-var-add-qed}
\mathrm{Var}(X+Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-07-07; URL: \url{https://en.wikipedia.org/wiki/Variance#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Law of total variance}]{Law of total variance} \label{sec:var-tot}
\setcounter{equation}{0}

\textbf{Theorem:} (law of total variance, also called "conditional variance formula") Let $X$ and $Y$ be random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) defined on the same probability space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-spc}) and assume that the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $Y$ is finite. Then, the sum of the expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the conditional variance and the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of the conditional expectation of $Y$ given $X$ is equal to the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $Y$:

\begin{equation} \label{eq:var-tot-var-tot}
\mathrm{Var}(Y) = \mathrm{E}[\mathrm{Var}(Y \vert X)] + \mathrm{Var}[\mathrm{E}(Y \vert X)] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance can be decomposed into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-mean}) as follows:

\begin{equation} \label{eq:var-tot-var-tot-s1}
\mathrm{Var}(Y) = \mathrm{E}(Y^2) - \mathrm{E}(Y)^2 \; .
\end{equation}

This can be rearranged into:

\begin{equation} \label{eq:var-tot-var-tot-s2}
\mathrm{E}(Y^2) = \mathrm{Var}(Y) + \mathrm{E}(Y)^2 \; .
\end{equation}

Applying the law of total expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-tot}), we have:

\begin{equation} \label{eq:var-tot-var-tot-s3}
\mathrm{E}(Y^2) = \mathrm{E}\left[ \mathrm{Var}(Y \vert X) + \mathrm{E}(Y \vert X)^2 \right] \; .
\end{equation}

Now subtract the second term from \eqref{eq:var-tot-var-tot-s1}:

\begin{equation} \label{eq:var-tot-var-tot-s4}
\mathrm{E}(Y^2) - \mathrm{E}(Y)^2 = \mathrm{E}\left[ \mathrm{Var}(Y \vert X) + \mathrm{E}(Y \vert X)^2 \right] - \mathrm{E}(Y)^2 \; .
\end{equation}

Again applying the law of total expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-tot}), we have:

\begin{equation} \label{eq:var-tot-var-tot-s5}
\mathrm{E}(Y^2) - \mathrm{E}(Y)^2 = \mathrm{E}\left[ \mathrm{Var}(Y \vert X) + \mathrm{E}(Y \vert X)^2 \right] - \mathrm{E}\left[ \mathrm{E}(Y \vert X) \right]^2 \; .
\end{equation}

With the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), the terms can be regrouped to give:

\begin{equation} \label{eq:var-tot-var-tot-s6}
\mathrm{E}(Y^2) - \mathrm{E}(Y)^2 = \mathrm{E}\left[ \mathrm{Var}(Y \vert X) \right] + \left( \mathrm{E}\left[ \mathrm{E}(Y \vert X)^2 \right] - \mathrm{E}\left[ \mathrm{E}(Y \vert X) \right]^2 \right) \; .
\end{equation}

Using the decomposition of variance into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-mean}), we finally have:

\begin{equation} \label{eq:var-tot-var-tot-s7}
\mathrm{Var}(Y) = \mathrm{E}[\mathrm{Var}(Y \vert X)] + \mathrm{Var}[\mathrm{E}(Y \vert X)] \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Law of total variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-11-26; URL: \url{https://en.wikipedia.org/wiki/Law_of_total_variance#Proof}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Precision}]{Precision} \label{sec:prec}
\setcounter{equation}{0}

\textbf{Definition:} The precision of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ is defined as the inverse of the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}), i.e. one divided by the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the squared deviation from its expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}):

\begin{equation} \label{eq:prec-prec}
\mathrm{Prec}(X) = \mathrm{Var}(X)^{-1} = \frac{1}{\mathrm{E}\left[ (X-\mathrm{E}(X))^2 \right]} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Precision (statistics)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-04-21; URL: \url{https://en.wikipedia.org/wiki/Precision_(statistics)}.
\end{itemize}
\vspace{1em}



\subsection{Skewness}

\subsubsection[\textit{Definition}]{Definition} \label{sec:skew}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu$ and standard deviation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:std}) $\sigma$. Then, the skewness of $X$ is defined as the third standardized moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-stand}) of $X$:

\begin{equation} \label{eq:skew-skew}
\mathrm{Skew}(X) = \frac{\mathrm{E}[(X-\mu)^3]}{\sigma^3} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2023): "Skewness"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2023-04-20; URL: \url{https://en.wikipedia.org/wiki/Skewness}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Sample skewness}]{Sample skewness} \label{sec:skew-samp}
\setcounter{equation}{0}

\textbf{Definition:} Let $x = \left\lbrace x_1, \ldots, x_n \right\rbrace$ be a sample from a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$. Then, the sample skewness of $x$ is given by

\begin{equation} \label{eq:skew-samp-skew-samp}
\hat{s} = \frac{\frac{1}{n}\sum_{i=1}^n (x_i-\bar{x})^3}{\left[\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2\right]^{3/2}} \; ,
\end{equation}

where $\bar{x}$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}).


\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Joanes, D. N. and Gill, C. A. (1998): "Comparing measures of sample skewness and kurtosis"; in: \textit{The Statistician}, vol. 47, part 1, pp. 183-189; URL: \url{https://www.jstor.org/stable/2988433}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Partition into expected values}]{Partition into expected values} \label{sec:skew-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu$ and standard deviation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:std}) $\sigma$. Then, the skewness ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:skew}) of $X$ can be computed as:

\begin{equation} \label{eq:skew-mean-skew-partition}
\mathrm{Skew}(X) = \frac{\mathrm{E}(X^3)-3\mu\sigma^2-\mu^3}{\sigma^3} \; .
\end{equation}

\vspace{1em}
\textbf{Proof:} The skewness ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:skew}) of $X$ is defined as 

\begin{equation} \label{eq:skew-mean-skew}
\mathrm{Skew}(X) = \frac{\mathrm{E}[(X-\mu)^3]}{\sigma^3} \; .
\end{equation}

Because the expected value is a linear operator ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), we can rewrite \eqref{eq:skew-mean-skew} as

\begin{equation} \label{eq:skew-mean-partition-s1}
\begin{split}
\mathrm{Skew}(X) &= \frac{\mathrm{E}[(X-\mu)^3]}{\sigma^3}\\
&= \frac{\mathrm{E}[X^3-3X^2\mu + 3X\mu^2 - \mu^3]}{\sigma^3}\\
&= \frac{\mathrm{E}(X^3) -3\mathrm{E}(X^2)\mu + 3\mathrm{E}(X)\mu^2 - \mu^3}{\sigma^3}\\
&= \frac{\mathrm{E}(X^3) -3\mu\left[\mathrm{E}(X^2)-\mathrm{E}(X)\mu\right]-\mu^3}{\sigma^3}\\
&= \frac{\mathrm{E}(X^3) -3\mu\left[\mathrm{E}(X^2)-\mathrm{E}(X)^2\right]-\mu^3}{\sigma^3} \; .
\end{split}
\end{equation}

Because the variance can be written in terms of expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-mean}) as

\begin{equation} \label{eq:skew-mean-var-partition}
\sigma^2 = \mathrm{E}(X^2)-\mathrm{E}(X)^2 \; ,
\end{equation}

we can rewrite \eqref{eq:skew-mean-partition-s1} as

\begin{equation} \label{eq:skew-mean-partition-s2}
\mathrm{Skew}(X) = \frac{\mathrm{E}(X^3) -3\mu\sigma^2-\mu^3}{\sigma^3} \; .
\end{equation}

This finishes the proof of \eqref{eq:skew-mean-skew-partition}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2023): "Skewness"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2023-04-20; URL: \url{https://en.wikipedia.org/wiki/Skewness}.
\end{itemize}
\vspace{1em}



\subsection{Covariance}

\subsubsection[\textit{Definition}]{Definition} \label{sec:cov}
\setcounter{equation}{0}

\textbf{Definition:} The covariance of two random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ and $Y$ is defined as the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the product of their deviations from their individual expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}):

\begin{equation} \label{eq:cov-cov}
\mathrm{Cov}(X,Y) = \mathrm{E}\left[ (X-\mathrm{E}[X]) (Y-\mathrm{E}[Y]) \right] \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Covariance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-06; URL: \url{https://en.wikipedia.org/wiki/Covariance#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Sample covariance}]{Sample covariance} \label{sec:cov-samp}
\setcounter{equation}{0}

\textbf{Definition:} Let $x = \left\lbrace x_1, \ldots, x_n \right\rbrace$ and $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ be samples from random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ and $Y$. Then, the sample covariance of $x$ and $y$ is given by

\begin{equation} \label{eq:cov-samp-cov-samp}
\hat{\sigma}_{xy} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x}) (y_i - \bar{y})
\end{equation}

and the unbiased sample covariance of $x$ and $y$ is given by

\begin{equation} \label{eq:cov-samp-cov-samp-unb}
s_{xy} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x}) (y_i - \bar{y})
\end{equation}

where $\bar{x}$ and $\bar{y}$ are the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Covariance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-20; URL: \url{https://en.wikipedia.org/wiki/Covariance#Calculating_the_sample_covariance}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Partition into expected values}]{Partition into expected values} \label{sec:cov-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of $X$ and $Y$ is equal to the mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the product of $X$ and $Y$ minus the product of the means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ and $Y$:

\begin{equation} \label{eq:cov-mean-cov-mean}
\mathrm{Cov}(X,Y) = \mathrm{E}(X Y) - \mathrm{E}(X) \mathrm{E}(Y) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of $X$ and $Y$ is defined as

\begin{equation} \label{eq:cov-mean-cov}
\mathrm{Cov}(X,Y) = \mathrm{E}\left[ (X-\mathrm{E}[X]) (Y-\mathrm{E}[Y]) \right] \; .
\end{equation}

which, due to the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), can be rewritten as

\begin{equation} \label{eq:cov-mean-cov-mean-qed}
\begin{split}
\mathrm{Cov}(X,Y) &= \mathrm{E}\left[ (X-\mathrm{E}[X]) (Y-\mathrm{E}[Y]) \right] \\
&= \mathrm{E}\left[ X Y - X \, \mathrm{E}(Y) - \mathrm{E}(X) \, Y + \mathrm{E}(X) \mathrm{E}(Y) \right] \\
&= \mathrm{E}(X Y) - \mathrm{E}(X) \mathrm{E}(Y) - \mathrm{E}(X) \mathrm{E}(Y) + \mathrm{E}(X) \mathrm{E}(Y) \\
&= \mathrm{E}(X Y) - \mathrm{E}(X) \mathrm{E}(Y) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Covariance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-06-02; URL: \url{https://en.wikipedia.org/wiki/Covariance#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Symmetry}]{Symmetry} \label{sec:cov-symm}
\setcounter{equation}{0}

\textbf{Theorem:} The covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of two random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) is a symmetric function:

\begin{equation} \label{eq:cov-symm-cov-symm}
\mathrm{Cov}(X,Y) = \mathrm{Cov}(Y,X) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ and $Y$ is defined as:

\begin{equation} \label{eq:cov-symm-cov}
\mathrm{Cov}(X,Y) = \mathrm{E}\left[ (X-\mathrm{E}[X]) (Y-\mathrm{E}[Y]) \right] \; .
\end{equation}

Switching $X$ and $Y$ in \eqref{eq:cov-symm-cov}, we can easily see:

\begin{equation} \label{eq:cov-symm-cov-symm-qed}
\begin{split}
\mathrm{Cov}(Y,X) &\overset{\eqref{eq:cov-symm-cov}}{=} \mathrm{E}\left[ (Y-\mathrm{E}[Y]) (X-\mathrm{E}[X]) \right] \\
&= \mathrm{E}\left[ (X-\mathrm{E}[X]) (Y-\mathrm{E}[Y]) \right] \\
&= \mathrm{Cov}(X,Y) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Covariance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-09-26; URL: \url{https://en.wikipedia.org/wiki/Covariance#Covariance_of_linear_combinations}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Self-covariance}]{Self-covariance} \label{sec:cov-var}
\setcounter{equation}{0}

\textbf{Theorem:} The covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with itself is equal to the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}):

\begin{equation} \label{eq:cov-var-cov-var}
\mathrm{Cov}(X,X) = \mathrm{Var}(X) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ and $Y$ is defined as:

\begin{equation} \label{eq:cov-var-cov}
\mathrm{Cov}(X,Y) = \mathrm{E}\left[ (X-\mathrm{E}[X]) (Y-\mathrm{E}[Y]) \right] \; .
\end{equation}

Inserting $X$ for $Y$ in \eqref{eq:cov-var-cov}, the result is the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$:

\begin{equation} \label{eq:cov-var-cov-var-qed}
\begin{split}
\mathrm{Cov}(X,X) &\overset{\eqref{eq:cov-var-cov}}{=} \mathrm{E}\left[ (X-\mathrm{E}[X]) (X-\mathrm{E}[X]) \right] \\
&= \mathrm{E}\left[ (X-\mathrm{E}[X])^2 \right] \\
&= \mathrm{Var}(X) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Covariance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-09-26; URL: \url{https://en.wikipedia.org/wiki/Covariance#Covariance_with_itself}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Covariance under independence}]{Covariance under independence} \label{sec:cov-ind}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of $X$ and $Y$ is zero:

\begin{equation} \label{eq:cov-ind-cov-ind}
X, Y \; \text{independent} \quad \Rightarrow \quad \mathrm{Cov}(X,Y) = 0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The covariance can be expressed in terms of expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-mean}) as

\begin{equation} \label{eq:cov-ind-cov-mean}
\mathrm{Cov}(X,Y) = \mathrm{E}(X\,Y) - \mathrm{E}(X) \, \mathrm{E}(Y) \; .
\end{equation}

For independent random variables, the expected value of the product is equal to the product of the expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-mult}):

\begin{equation} \label{eq:cov-ind-mean-mult}
\mathrm{E}(X\,Y) = \mathrm{E}(X) \, \mathrm{E}(Y) \; .
\end{equation}

Taking \eqref{eq:cov-ind-cov-mean} and \eqref{eq:cov-ind-mean-mult} together, we have

\begin{equation} \label{eq:cov-ind-cov-ind-qed}
\begin{split}
\mathrm{Cov}(X,Y) &\overset{\eqref{eq:cov-ind-cov-mean}}{=} \mathrm{E}(X\,Y) - \mathrm{E}(X) \, \mathrm{E}(Y) \\
&\overset{\eqref{eq:cov-ind-mean-mult}}{=} \mathrm{E}(X) \, \mathrm{E}(Y) - \mathrm{E}(X) \, \mathrm{E}(Y) \\
&= 0 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Covariance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-09-03; URL: \url{https://en.wikipedia.org/wiki/Covariance#Uncorrelatedness_and_independence}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relationship to correlation}]{Relationship to correlation} \label{sec:cov-corr}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of $X$ and $Y$ is equal to the product of their correlation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr}) and the standard deviations ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:std}) of $X$ and $Y$:

\begin{equation} \label{eq:cov-corr-cov-corr}
\mathrm{Cov}(X,Y) = \sigma_X \, \mathrm{Corr}(X,Y) \, \sigma_Y \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The correlation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr}) of $X$ and $Y$ is defined as

\begin{equation} \label{eq:cov-corr-corr}
\mathrm{Corr}(X,Y) = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y} \; .
\end{equation}

which can be rearranged for the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) to give

\begin{equation} \label{eq:cov-corr-cov-corr-qed}
\mathrm{Cov}(X,Y) = \sigma_X \, \mathrm{Corr}(X,Y) \, \sigma_Y
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Law of total covariance}]{Law of total covariance} \label{sec:cov-tot}
\setcounter{equation}{0}

\textbf{Theorem:} (law of total covariance, also called "conditional covariance formula") Let $X$, $Y$ and $Z$ be random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) defined on the same probability space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-spc}) and assume that the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of $X$ and $Y$ is finite. Then, the sum of the expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the conditional covariance and the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of the conditional expectations of $X$ and $Y$ given $Z$ is equal to the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of $X$ and $Y$:

\begin{equation} \label{eq:cov-tot-cov-tot}
\mathrm{Cov}(X,Y) = \mathrm{E}[\mathrm{Cov}(X,Y \vert Z)] + \mathrm{Cov}[\mathrm{E}(X \vert Z),\mathrm{E}(Y \vert Z)] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The covariance can be decomposed into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-mean}) as follows:

\begin{equation} \label{eq:cov-tot-cov-tot-s1}
\mathrm{Cov}(X,Y) = \mathrm{E}(XY) - \mathrm{E}(X) \mathrm{E}(Y) \; .
\end{equation}

Then, conditioning on $Z$ and applying the law of total expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-tot}), we have:

\begin{equation} \label{eq:cov-tot-cov-tot-s2}
\mathrm{Cov}(X,Y) = \mathrm{E}\left[ \mathrm{E}(XY \vert Z) \right] - \mathrm{E}\left[ \mathrm{E}(X \vert Z ) \right] \mathrm{E}\left[ \mathrm{E}(Y \vert Z) \right] \; .
\end{equation}

Applying the decomposition of covariance into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-mean}) to the first term gives:

\begin{equation} \label{eq:cov-tot-cov-tot-s3}
\mathrm{Cov}(X,Y) = \mathrm{E}\left[ \mathrm{Cov}(X,Y \vert Z) + \mathrm{E}(X \vert Z) \mathrm{E}(Y \vert Z) \right] - \mathrm{E}\left[ \mathrm{E}(X \vert Z ) \right] \mathrm{E}\left[ \mathrm{E}(Y \vert Z) \right] \; .
\end{equation}

With the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), the terms can be regrouped to give:

\begin{equation} \label{eq:cov-tot-cov-tot-s4}
\mathrm{Cov}(X,Y) = \mathrm{E}\left[ \mathrm{Cov}(X,Y \vert Z) \right] + \left( \mathrm{E}\left[ \mathrm{E}(X \vert Z) \mathrm{E}(Y \vert Z) \right] - \mathrm{E}\left[ \mathrm{E}(X \vert Z ) \right] \mathrm{E}\left[ \mathrm{E}(Y \vert Z) \right] \right) \; .
\end{equation}

Once more using the decomposition of covariance into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-mean}), we finally have:

\begin{equation} \label{eq:cov-tot-var-tot-s5}
\mathrm{Cov}(X,Y) = \mathrm{E}[\mathrm{Cov}(X,Y \vert Z)] + \mathrm{Cov}[\mathrm{E}(X \vert Z),\mathrm{E}(Y \vert Z)] \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Law of total covariance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-11-26; URL: \url{https://en.wikipedia.org/wiki/Law_of_total_covariance#Proof}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Covariance matrix}]{Covariance matrix} \label{sec:covmat}
\setcounter{equation}{0}

\textbf{Definition:} Let $X = [X_1, \ldots, X_n]^\mathrm{T}$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, the covariance matrix of $X$ is defined as the $n \times n$ matrix in which the entry $(i,j)$ is the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of $X_i$ and $X_j$:

\begin{equation} \label{eq:covmat-covmat}
\Sigma_{XX} =
\begin{bmatrix}
\mathrm{Cov}(X_1,X_1) & \ldots & \mathrm{Cov}(X_1,X_n) \\
\vdots & \ddots & \vdots \\
\mathrm{Cov}(X_n,X_1) & \ldots & \mathrm{Cov}(X_n,X_n)
\end{bmatrix} =
\begin{bmatrix}
\mathrm{E}\left[ (X_1-\mathrm{E}[X_1]) (X_1-\mathrm{E}[X_1]) \right] & \ldots & \mathrm{E}\left[ (X_1-\mathrm{E}[X_1]) (X_n-\mathrm{E}[X_n]) \right] \\
\vdots & \ddots & \vdots \\
\mathrm{E}\left[ (X_n-\mathrm{E}[X_n]) (X_1-\mathrm{E}[X_1]) \right] & \ldots & \mathrm{E}\left[ (X_n-\mathrm{E}[X_n]) (X_n-\mathrm{E}[X_n]) \right]
\end{bmatrix} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Covariance matrix"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-06-06; URL: \url{https://en.wikipedia.org/wiki/Covariance_matrix#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Sample covariance matrix}]{Sample covariance matrix} \label{sec:covmat-samp}
\setcounter{equation}{0}

\textbf{Definition:} Let $x = \left\lbrace x_1, \ldots, x_n \right\rbrace$ be a sample from a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $X \in \mathbb{R}^{p \times 1}$. Then, the sample covariance matrix of $x$ is given by

\begin{equation} \label{eq:covmat-samp-covmat-samp}
\hat{\Sigma} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x}) (x_i - \bar{x})^\mathrm{T}
\end{equation}

and the unbiased sample covariance matrix of $x$ is given by

\begin{equation} \label{eq:covmat-samp-covmat-samp-unb}
S = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x}) (x_i - \bar{x})^\mathrm{T}
\end{equation}

where $\bar{x}$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Sample mean and covariance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-20; URL: \url{https://en.wikipedia.org/wiki/Sample_mean_and_covariance#Definition_of_sample_covariance}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Covariance matrix and expected values}]{Covariance matrix and expected values} \label{sec:covmat-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, the covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of $X$ is equal to the mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the outer product of $X$ with itself minus the outer product of the mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ with itself:

\begin{equation} \label{eq:covmat-mean-covmat-mean}
\Sigma_{XX} = \mathrm{E}(X X^\mathrm{T}) - \mathrm{E}(X) \mathrm{E}(X)^\mathrm{T} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of $X$ is defined as

\begin{equation} \label{eq:covmat-mean-covmat1}
\Sigma_{XX} =
\begin{bmatrix}
\mathrm{E}\left[ (X_1-\mathrm{E}[X_1]) (X_1-\mathrm{E}[X_1]) \right] & \ldots & \mathrm{E}\left[ (X_1-\mathrm{E}[X_1]) (X_n-\mathrm{E}[X_n]) \right] \\
\vdots & \ddots & \vdots \\
\mathrm{E}\left[ (X_n-\mathrm{E}[X_n]) (X_1-\mathrm{E}[X_1]) \right] & \ldots & \mathrm{E}\left[ (X_n-\mathrm{E}[X_n]) (X_n-\mathrm{E}[X_n]) \right]
\end{bmatrix}
\end{equation}

which can also be expressed using matrix multiplication as

\begin{equation} \label{eq:covmat-mean-covmat2}
\Sigma_{XX} = \mathrm{E}\left[ (X-\mathrm{E}[X]) (X-\mathrm{E}[X])^\mathrm{T} \right]
\end{equation}

Due to the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), this can be rewritten as

\begin{equation} \label{eq:covmat-mean-covmat-mean-qed}
\begin{split}
\Sigma_{XX} &= \mathrm{E}\left[ (X-\mathrm{E}[X]) (X-\mathrm{E}[X])^\mathrm{T} \right] \\
&= \mathrm{E}\left[ X X^\mathrm{T} - X \, \mathrm{E}(X)^\mathrm{T} - \mathrm{E}(X) \, X^\mathrm{T} + \mathrm{E}(X) \mathrm{E}(X)^\mathrm{T} \right] \\
&= \mathrm{E}(X X^\mathrm{T}) - \mathrm{E}(X) \mathrm{E}(X)^\mathrm{T} - \mathrm{E}(X) \mathrm{E}(X)^\mathrm{T} + \mathrm{E}(X) \mathrm{E}(X)^\mathrm{T} \\
&= \mathrm{E}(X X^\mathrm{T}) - \mathrm{E}(X) \mathrm{E}(X)^\mathrm{T} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2010): "Covariance matrix"; in: \textit{Lectures on probability and statistics}, retrieved on 2020-06-06; URL: \url{https://www.statlect.com/fundamentals-of-probability/covariance-matrix}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Symmetry}]{Symmetry} \label{sec:covmat-symm}
\setcounter{equation}{0}

\textbf{Theorem:} Each covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) is symmetric:

\begin{equation} \label{eq:covmat-symm-covmat-symm}
\Sigma_{XX}^\mathrm{T} = \Sigma_{XX} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $X$ is defined as

\begin{equation} \label{eq:covmat-symm-covmat}
\Sigma_{XX} =
\begin{bmatrix}
\mathrm{Cov}(X_1,X_1) & \ldots & \mathrm{Cov}(X_1,X_n) \\
\vdots & \ddots & \vdots \\
\mathrm{Cov}(X_n,X_1) & \ldots & \mathrm{Cov}(X_n,X_n)
\end{bmatrix} \; .
\end{equation}

A symmetric matrix is a matrix whose transpose is equal to itself. The transpose of $\Sigma_{XX}$ is 

\begin{equation} \label{eq:covmat-symm-covmat-trans}
\Sigma_{XX}^\mathrm{T} =
\begin{bmatrix}
\mathrm{Cov}(X_1,X_1) & \ldots & \mathrm{Cov}(X_n,X_1) \\
\vdots & \ddots & \vdots \\
\mathrm{Cov}(X_1,X_n) & \ldots & \mathrm{Cov}(X_n,X_n)
\end{bmatrix} \; .
\end{equation}

Because the covariance is a symmetric function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-symm}), i.e. $\mathrm{Cov}(X,Y) = \mathrm{Cov}(Y,X)$, this matrix is equal to

\begin{equation} \label{eq:covmat-symm-covmat-symm-qed}
\Sigma_{XX}^\mathrm{T} =
\begin{bmatrix}
\mathrm{Cov}(X_1,X_1) & \ldots & \mathrm{Cov}(X_1,X_n) \\
\vdots & \ddots & \vdots \\
\mathrm{Cov}(X_n,X_1) & \ldots & \mathrm{Cov}(X_n,X_n)
\end{bmatrix}
\end{equation}

which is equivalent to our original definition in \eqref{eq:covmat-symm-covmat}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Covariance matrix"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-09-26; URL: \url{https://en.wikipedia.org/wiki/Covariance_matrix#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Positive semi-definiteness}]{Positive semi-definiteness} \label{sec:covmat-psd}
\setcounter{equation}{0}

\textbf{Theorem:} Each covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) is positive semi-definite:

\begin{equation} \label{eq:covmat-psd-covmat-symm}
a^\mathrm{T} \Sigma_{XX} a \geq 0 \quad \text{for all} \quad a \in \mathbb{R}^n \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of $X$ can be expressed ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat-mean}) in terms of expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) as follows

\begin{equation} \label{eq:covmat-psd-covmat}
\Sigma_{XX} = \Sigma(X) = \mathrm{E}\left[ (X-\mathrm{E}[X]) (X-\mathrm{E}[X])^\mathrm{T} \right]
\end{equation}

A positive semi-definite matrix is a matrix whose eigenvalues are all non-negative or, equivalently,

\begin{equation} \label{eq:covmat-psd-psd}
M \; \text{pos. semi-def.} \quad \Leftrightarrow \quad x^\mathrm{T} M x \geq 0 \quad \text{for all} \quad x \in \mathbb{R}^n \; .
\end{equation}

Here, for an arbitrary real column vector $a \in \mathbb{R}^n$, we have:

\begin{equation} \label{eq:covmat-psd-covmat-symm-s1}
a^\mathrm{T} \Sigma_{XX} a \overset{\eqref{eq:covmat-psd-covmat}}{=} a^\mathrm{T} \mathrm{E}\left[ (X-\mathrm{E}[X]) (X-\mathrm{E}[X])^\mathrm{T} \right] a \; .
\end{equation}

Because the expected value is a linear operator ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), we can write:

\begin{equation} \label{eq:covmat-psd-covmat-symm-s2}
a^\mathrm{T} \Sigma_{XX} a = \mathrm{E}\left[ a^\mathrm{T} (X-\mathrm{E}[X]) (X-\mathrm{E}[X])^\mathrm{T} a \right] \; .
\end{equation}

Now define the scalar random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar})

\begin{equation} \label{eq:covmat-psd-Y-X}
Y = a^\mathrm{T} (X-\mu_X) \; .
\end{equation}

where $\mu_X = \mathrm{E}[X]$ and note that

\begin{equation} \label{eq:covmat-psd-YT-Y}
a^\mathrm{T} (X-\mu_X) = (X-\mu_X)^\mathrm{T} a \; .
\end{equation}

Thus, combing \eqref{eq:covmat-psd-covmat-symm-s2} with \eqref{eq:covmat-psd-Y-X}, we have:

\begin{equation} \label{eq:covmat-psd-covmat-symm-s3}
a^\mathrm{T} \Sigma_{XX} a = \mathrm{E}\left[ Y^2 \right] \; .
\end{equation}

Because $Y^2$ is a random variable that cannot become negative and the expected value of a strictly non-negative random variable is also non-negative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-nonneg}), we finally have

\begin{equation} \label{eq:covmat-psd-covmat-symm-s4}
a^\mathrm{T} \Sigma_{XX} a \geq 0
\end{equation}

for any $a \in \mathbb{R}^n$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item hkBattousai (2013): "What is the proof that covariance matrices are always semi-definite?"; in: \textit{StackExchange Mathematics}, retrieved on 2022-09-26; URL: \url{https://math.stackexchange.com/a/327872}.
\item Wikipedia (2022): "Covariance matrix"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-09-26; URL: \url{https://en.wikipedia.org/wiki/Covariance_matrix#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Invariance under addition of vector}]{Invariance under addition of vector} \label{sec:covmat-inv}
\setcounter{equation}{0}

\textbf{Theorem:} The covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) $\Sigma_{XX}$ of a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $X$ is invariant under addition of a constant vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:const}) $a$:

\begin{equation} \label{eq:covmat-inv-covmat-inv}
\Sigma(X+a) = \Sigma(X) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of $X$ can be expressed ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat-mean}) in terms of expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) as follows:

\begin{equation} \label{eq:covmat-inv-covmat}
\Sigma_{XX} = \Sigma(X) = \mathrm{E}\left[ (X-\mathrm{E}[X]) (X-\mathrm{E}[X])^\mathrm{T} \right] \; .
\end{equation}

Using this and the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), we can derive \eqref{eq:covmat-inv-covmat-inv} as follows:

\begin{equation} \label{eq:covmat-inv-covmat-inv-qed}
\begin{split}
\Sigma(X+a) &\overset{\eqref{eq:covmat-inv-covmat}}{=} \mathrm{E}\left[ ([X+a]-\mathrm{E}[X+a]) ([X+a]-\mathrm{E}[X+a])^\mathrm{T} \right] \\
&= \mathrm{E}\left[ (X + a - \mathrm{E}[X] - a) (X + a - \mathrm{E}[X] - a)^\mathrm{T} \right] \\
&= \mathrm{E}\left[ (X - \mathrm{E}[X]) (X - \mathrm{E}[X])^\mathrm{T} \right] \\
&\overset{\eqref{eq:covmat-inv-covmat}}{=} \Sigma(X) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Covariance matrix"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-09-22; URL: \url{https://en.wikipedia.org/wiki/Covariance_matrix#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Scaling upon multiplication with matrix}]{Scaling upon multiplication with matrix} \label{sec:covmat-scal}
\setcounter{equation}{0}

\textbf{Theorem:} The covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) $\Sigma_{XX}$ of a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $X$ scales upon multiplication with a constant matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:const}) $A$:

\begin{equation} \label{eq:covmat-scal-covmat-scal}
\Sigma(AX) = A \, \Sigma(X) A^\mathrm{T} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of $X$ can be expressed ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat-mean}) in terms of expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) as follows:

\begin{equation} \label{eq:covmat-scal-covmat}
\Sigma_{XX} = \Sigma(X) = \mathrm{E}\left[ (X-\mathrm{E}[X]) (X-\mathrm{E}[X])^\mathrm{T} \right] \; .
\end{equation}

Using this and the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), we can derive \eqref{eq:covmat-scal-covmat-scal} as follows:

\begin{equation} \label{eq:covmat-scal-covmat-scal-qed}
\begin{split}
\Sigma(AX) &\overset{\eqref{eq:covmat-scal-covmat}}{=} \mathrm{E}\left[ ([AX]-\mathrm{E}[AX]) ([AX]-\mathrm{E}[AX])^\mathrm{T} \right] \\
&= \mathrm{E}\left[ (A[X-\mathrm{E}[X]]) (A[X-\mathrm{E}[X]])^\mathrm{T} \right] \\
&= \mathrm{E}\left[ A (X-\mathrm{E}[X]) (X-\mathrm{E}[X])^\mathrm{T} A^\mathrm{T} \right] \\
&= A \, \mathrm{E}\left[ (X-\mathrm{E}[X]) (X-\mathrm{E}[X])^\mathrm{T} \right] A^\mathrm{T} \\
&\overset{\eqref{eq:covmat-scal-covmat}}{=} A \, \Sigma(X) A^\mathrm{T} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Covariance matrix"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-09-22; URL: \url{https://en.wikipedia.org/wiki/Covariance_matrix#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Cross-covariance matrix}]{Cross-covariance matrix} \label{sec:covmat-cross}
\setcounter{equation}{0}

\textbf{Definition:} Let $X = [X_1, \ldots, X_n]^\mathrm{T}$ and $Y = [Y_1, \ldots, Y_m]^\mathrm{T}$ be two random vectors ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) that can or cannot be of equal size. Then, the cross-covariance matrix of $X$ and $Y$ is defined as the $n \times m$ matrix in which the entry $(i,j)$ is the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of $X_i$ and $Y_j$:

\begin{equation} \label{eq:covmat-cross-covmat-cross}
\Sigma_{XY} =
\begin{bmatrix}
\mathrm{Cov}(X_1,Y_1) & \ldots & \mathrm{Cov}(X_1,Y_m) \\
\vdots & \ddots & \vdots \\
\mathrm{Cov}(X_n,Y_1) & \ldots & \mathrm{Cov}(X_n,Y_m)
\end{bmatrix} =
\begin{bmatrix}
\mathrm{E}\left[ (X_1-\mathrm{E}[X_1]) (Y_1-\mathrm{E}[Y_1]) \right] & \ldots & \mathrm{E}\left[ (X_1-\mathrm{E}[X_1]) (Y_m-\mathrm{E}[Y_m]) \right] \\
\vdots & \ddots & \vdots \\
\mathrm{E}\left[ (X_n-\mathrm{E}[X_n]) (Y_1-\mathrm{E}[Y_1]) \right] & \ldots & \mathrm{E}\left[ (X_n-\mathrm{E}[X_n]) (Y_m-\mathrm{E}[Y_m]) \right]
\end{bmatrix} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Cross-covariance matrix"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-09-26; URL: \url{https://en.wikipedia.org/wiki/Cross-covariance_matrix#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Covariance matrix of a sum}]{Covariance matrix of a sum} \label{sec:covmat-sum}
\setcounter{equation}{0}

\textbf{Theorem:} The covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of the sum of two random vectors ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) of the same dimension equals the sum of the covariances of those random vectors, plus the sum of their cross-covariances ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat-cross}):

\begin{equation} \label{eq:covmat-sum-covmat-sum}
\Sigma(X+Y) = \Sigma_{XX} + \Sigma_{YY} + \Sigma_{XY} + \Sigma_{YX} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of $X$ can be expressed ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat-mean}) in terms of expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) as follows

\begin{equation} \label{eq:covmat-sum-covmat}
\Sigma_{XX} = \Sigma(X) = \mathrm{E}\left[ (X-\mathrm{E}[X]) (X-\mathrm{E}[X])^\mathrm{T} \right]
\end{equation}

and the cross-covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat-cross}) of $X$ and $Y$ can similarly be written as

\begin{equation} \label{eq:covmat-sum-covmat-cross}
\Sigma_{XY} = \Sigma(X,Y) = \mathrm{E}\left[ (X-\mathrm{E}[X]) (Y-\mathrm{E}[Y])^\mathrm{T} \right]
\end{equation}

Using this and the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}) as well as the definitions of covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) and cross-covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat-cross}), we can derive \eqref{eq:covmat-sum-covmat-sum} as follows:

\begin{equation} \label{eq:covmat-sum-covmat-sum-qed}
\begin{split}
\Sigma(X+Y) &\overset{\eqref{eq:covmat-sum-covmat}}{=} \mathrm{E}\left[ ([X+Y]-\mathrm{E}[X+Y]) ([X+Y]-\mathrm{E}[X+Y])^\mathrm{T} \right] \\
&= \mathrm{E}\left[ ([X-\mathrm{E}(X)] + [Y-\mathrm{E}(Y)]) ([X-\mathrm{E}(X)] + [Y-\mathrm{E}(Y)])^\mathrm{T} \right] \\
&= \mathrm{E}\left[ (X-\mathrm{E}[X]) (X-\mathrm{E}[X])^\mathrm{T} + (X-\mathrm{E}[X]) (Y-\mathrm{E}[Y])^\mathrm{T} + (Y-\mathrm{E}[Y]) (X-\mathrm{E}[X])^\mathrm{T} + (Y-\mathrm{E}[Y]) (Y-\mathrm{E}[Y])^\mathrm{T} \right] \\
&= \mathrm{E}\left[ (X-\mathrm{E}[X]) (X-\mathrm{E}[X])^\mathrm{T} \right] + \mathrm{E}\left[ (X-\mathrm{E}[X]) (Y-\mathrm{E}[Y])^\mathrm{T} \right] + \mathrm{E}\left[ (Y-\mathrm{E}[Y]) (X-\mathrm{E}[X])^\mathrm{T} \right] + \mathrm{E}\left[ (Y-\mathrm{E}[Y]) (Y-\mathrm{E}[Y])^\mathrm{T} \right] \\
&\overset{\eqref{eq:covmat-sum-covmat}}{=} \Sigma_{XX} + \Sigma_{YY} + \mathrm{E}\left[ (X-\mathrm{E}[X]) (Y-\mathrm{E}[Y])^\mathrm{T} \right] + \mathrm{E}\left[ (Y-\mathrm{E}[Y]) (X-\mathrm{E}[X])^\mathrm{T} \right] \\
&\overset{\eqref{eq:covmat-sum-covmat-cross}}{=} \Sigma_{XX} + \Sigma_{YY} + \Sigma_{XY} + \Sigma_{YX} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Covariance matrix"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-09-26; URL: \url{https://en.wikipedia.org/wiki/Covariance_matrix#Basic_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Covariance matrix and correlation matrix}]{Covariance matrix and correlation matrix} \label{sec:covmat-corrmat}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, the covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) $\Sigma_{XX}$ of $X$ can be expressed in terms of its correlation matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corrmat}) $\mathrm{C}_{XX}$ as follows

\begin{equation} \label{eq:covmat-corrmat-covmat-corrmat}
\Sigma_{XX} = \mathrm{D}_X \cdot \mathrm{C}_{XX} \cdot \mathrm{D}_X \; ,
\end{equation}

where $\mathrm{D}_X$ is a diagonal matrix with the standard deviations ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:std}) of $X_1, \ldots, X_n$ as entries on the diagonal:

\begin{equation} \label{eq:covmat-corrmat-diagmat}
\mathrm{D}_X = \mathrm{diag}(\sigma_{X_1},\ldots,\sigma_{X_n}) =
\begin{bmatrix}
\sigma_{X_1} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & \sigma_{X_n}
\end{bmatrix} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Reiterating \eqref{eq:covmat-corrmat-covmat-corrmat} and applying \eqref{eq:covmat-corrmat-diagmat}, we have:

\begin{equation} \label{eq:covmat-corrmat-covmat-corrmat-s1}
\Sigma_{XX} =
\begin{bmatrix}
\sigma_{X_1} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & \sigma_{X_n}
\end{bmatrix} \cdot
\mathrm{C}_{XX} \cdot
\begin{bmatrix}
\sigma_{X_1} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & \sigma_{X_n}
\end{bmatrix} \; .
\end{equation}

Together with the definition of the correlation matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corrmat}), this gives

\begin{equation} \label{eq:covmat-corrmat-covmat-corrmat-s2}
\begin{split}
\Sigma_{XX} &=
\begin{bmatrix}
\sigma_{X_1} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & \sigma_{X_n}
\end{bmatrix} \cdot
\begin{bmatrix}
\frac{\mathrm{E}\left[(X_1-\mathrm{E}[X_1]) (X_1-\mathrm{E}[X_1])\right]}{\sigma_{X_1} \, \sigma_{X_1}} & \ldots & \frac{\mathrm{E}\left[(X_1-\mathrm{E}[X_1]) (X_n-\mathrm{E}[X_n])\right]}{\sigma_{X_1} \, \sigma_{X_n}} \\
\vdots & \ddots & \vdots \\
\frac{\mathrm{E}\left[(X_n-\mathrm{E}[X_n]) (X_1-\mathrm{E}[X_1])\right]}{\sigma_{X_n} \, \sigma_{X_1}} & \ldots & \frac{\mathrm{E}\left[(X_n-\mathrm{E}[X_n]) (X_n-\mathrm{E}[X_n])\right]}{\sigma_{X_n} \, \sigma_{X_n}}
\end{bmatrix} \cdot
\begin{bmatrix}
\sigma_{X_1} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & \sigma_{X_n}
\end{bmatrix} \\
&=
\begin{bmatrix}
\frac{\sigma_{X_1} \cdot \mathrm{E}\left[(X_1-\mathrm{E}[X_1]) (X_1-\mathrm{E}[X_1])\right]}{\sigma_{X_1} \, \sigma_{X_1}} & \ldots & \frac{\sigma_{X_1} \cdot \mathrm{E}\left[(X_1-\mathrm{E}[X_1]) (X_n-\mathrm{E}[X_n])\right]}{\sigma_{X_1} \, \sigma_{X_n}} \\
\vdots & \ddots & \vdots \\
\frac{\sigma_{X_n} \cdot \mathrm{E}\left[(X_n-\mathrm{E}[X_n]) (X_1-\mathrm{E}[X_1])\right]}{\sigma_{X_n} \, \sigma_{X_1}} & \ldots & \frac{\sigma_{X_n} \cdot \mathrm{E}\left[(X_n-\mathrm{E}[X_n]) (X_n-\mathrm{E}[X_n])\right]}{\sigma_{X_n} \, \sigma_{X_n}}
\end{bmatrix} \cdot
\begin{bmatrix}
\sigma_{X_1} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & \sigma_{X_n}
\end{bmatrix} \\
&=
\begin{bmatrix}
\frac{\sigma_{X_1} \cdot \mathrm{E}\left[(X_1-\mathrm{E}[X_1]) (X_1-\mathrm{E}[X_1])\right] \cdot \sigma_{X_1}}{\sigma_{X_1} \, \sigma_{X_1}} & \ldots & \frac{\sigma_{X_1} \cdot \mathrm{E}\left[(X_1-\mathrm{E}[X_1]) (X_n-\mathrm{E}[X_n])\right] \cdot \sigma_{X_n}}{\sigma_{X_1} \, \sigma_{X_n}} \\
\vdots & \ddots & \vdots \\
\frac{\sigma_{X_n} \cdot \mathrm{E}\left[(X_n-\mathrm{E}[X_n]) (X_1-\mathrm{E}[X_1])\right] \cdot \sigma_{X_1}}{\sigma_{X_n} \, \sigma_{X_1}} & \ldots & \frac{\sigma_{X_n} \cdot \mathrm{E}\left[(X_n-\mathrm{E}[X_n]) (X_n-\mathrm{E}[X_n])\right] \cdot \sigma_{X_n}}{\sigma_{X_n} \, \sigma_{X_n}}
\end{bmatrix} \\
&=
\begin{bmatrix}
\mathrm{E}\left[ (X_1-\mathrm{E}[X_1]) (X_1-\mathrm{E}[X_1]) \right] & \ldots & \mathrm{E}\left[ (X_1-\mathrm{E}[X_1]) (X_n-\mathrm{E}[X_n]) \right] \\
\vdots & \ddots & \vdots \\
\mathrm{E}\left[ (X_n-\mathrm{E}[X_n]) (X_1-\mathrm{E}[X_1]) \right] & \ldots & \mathrm{E}\left[ (X_n-\mathrm{E}[X_n]) (X_n-\mathrm{E}[X_n]) \right]
\end{bmatrix}
\end{split}
\end{equation}

which is nothing else than the definition of the covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}).
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Penny, William (2006): "The correlation matrix"; in: \textit{Mathematics for Brain Imaging}, ch. 1.4.5, p. 28, eq. 1.60; URL: \url{https://ueapsylabs.co.uk/sites/wpenny/mbi/mbi_course.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Precision matrix}]{Precision matrix} \label{sec:precmat}
\setcounter{equation}{0}

\textbf{Definition:} Let $X = [X_1, \ldots, X_n]^\mathrm{T}$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, the precision matrix of $X$ is defined as the inverse of the covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of $X$:

\begin{equation} \label{eq:precmat-corrmat}
\Lambda_{XX} = \Sigma_{XX}^{-1} =
\begin{bmatrix}
\mathrm{Cov}(X_1,X_1) & \ldots & \mathrm{Cov}(X_1,X_n) \\
\vdots & \ddots & \vdots \\
\mathrm{Cov}(X_n,X_1) & \ldots & \mathrm{Cov}(X_n,X_n)
\end{bmatrix}^{-1} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Precision (statistics)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-06-06; URL: \url{https://en.wikipedia.org/wiki/Precision_(statistics)}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Precision matrix and correlation matrix}]{Precision matrix and correlation matrix} \label{sec:precmat-corrmat}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, the precision matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:precmat}) $\Lambda_{XX}$ of $X$ can be expressed in terms of its correlation matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corrmat}) $\mathrm{C}_{XX}$ as follows

\begin{equation} \label{eq:precmat-corrmat-precmat-corrmat}
\Lambda_{XX} = \mathrm{D}_X^{-1} \cdot \mathrm{C}_{XX}^{-1} \cdot \mathrm{D}_X^{-1} \; ,
\end{equation}

where $\mathrm{D}_X^{-1}$ is a diagonal matrix with the inverse standard deviations ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:std}) of $X_1, \ldots, X_n$ as entries on the diagonal:

\begin{equation} \label{eq:precmat-corrmat-invdiagmat}
\mathrm{D}_X^{-1} = \mathrm{diag}(1/\sigma_{X_1},\ldots,1/\sigma_{X_n}) =
\begin{bmatrix}
\frac{1}{\sigma_{X_1}} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & \frac{1}{\sigma_{X_n}}
\end{bmatrix} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The precision matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:precmat}) is defined as the inverse of the covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat})

\begin{equation} \label{eq:precmat-corrmat-precmat-covmat}
\Lambda_{XX} = \Sigma_{XX}^{-1}
\end{equation}

and the relation between covariance matrix and correlation matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat-corrmat}) is given by

\begin{equation} \label{eq:precmat-corrmat-covmat-corrmat}
\Sigma_{XX} = \mathrm{D}_X \cdot \mathrm{C}_{XX} \cdot \mathrm{D}_X
\end{equation}

where

\begin{equation} \label{eq:precmat-corrmat-diagmat}
\mathrm{D}_X = \mathrm{diag}(\sigma_{X_1},\ldots,\sigma_{X_n}) =
\begin{bmatrix}
\sigma_{X_1} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & \sigma_{X_n}
\end{bmatrix} \; .
\end{equation}

Using the matrix product property

\begin{equation} \label{eq:precmat-corrmat-matprod-inv}
\left(A \cdot B \cdot C\right)^{-1} = C^{-1} \cdot B^{-1} \cdot A^{-1}
\end{equation}

and the diagonal matrix property

\begin{equation} \label{eq:precmat-corrmat-diagmat-inv}
\mathrm{diag}(a_1,\ldots,a_n)^{-1} =
\begin{bmatrix}
a_1 & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & a_n
\end{bmatrix}^{-1} =
\begin{bmatrix}
\frac{1}{a_1} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & \frac{1}{a_n}
\end{bmatrix} =
\mathrm{diag}(1/a_1,\ldots,1/a_n) \; ,
\end{equation}

we obtain

\begin{equation} \label{eq:precmat-corrmat-precmat-corrmat-qed}
\begin{split}
\Lambda_{XX} &\overset{\eqref{eq:precmat-corrmat-precmat-covmat}}{=} \Sigma_{XX}^{-1} \\
&\overset{\eqref{eq:precmat-corrmat-covmat-corrmat}}{=} \left( \mathrm{D}_X \cdot \mathrm{C}_{XX} \cdot \mathrm{D}_X \right)^{-1} \\
&\overset{\eqref{eq:precmat-corrmat-matprod-inv}}{=} \mathrm{D}_X^{-1} \cdot \mathrm{C}_{XX}^{-1} \cdot \mathrm{D}_X^{-1} \\
&\overset{\eqref{eq:precmat-corrmat-diagmat-inv}}{=}
\begin{bmatrix}
\frac{1}{\sigma_{X_1}} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & \frac{1}{\sigma_{X_n}}
\end{bmatrix} \cdot
\mathrm{C}_{XX}^{-1} \cdot
\begin{bmatrix}
\frac{1}{\sigma_{X_1}} & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & \frac{1}{\sigma_{X_n}}
\end{bmatrix}
\end{split}
\end{equation}

which conforms to equation \eqref{eq:precmat-corrmat-precmat-corrmat}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{Correlation}

\subsubsection[\textit{Definition}]{Definition} \label{sec:corr}
\setcounter{equation}{0}

\textbf{Definition:} The correlation of two random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ and $Y$, also called Pearson product-moment correlation coefficient (PPMCC), is defined as the ratio of the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of $X$ and $Y$ relative to the product of their standard deviations ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:std}):

\begin{equation} \label{eq:corr-corr}
\mathrm{Corr}(X,Y) = \frac{\sigma_{XY}}{\sigma_X \sigma_Y} = \frac{\mathrm{Cov}(X,Y)}{\sqrt{\mathrm{Var}(X)} \sqrt{\mathrm{Var}(Y)}} = \frac{\mathrm{E}\left[ (X-\mathrm{E}[X]) (Y-\mathrm{E}[Y]) \right]}{\sqrt{\mathrm{E}\left[ (X-\mathrm{E}[X])^2 \right]} \sqrt{\mathrm{E}\left[ (Y-\mathrm{E}[Y])^2 \right]}} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Correlation and dependence"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-06; URL: \url{https://en.wikipedia.org/wiki/Correlation_and_dependence#Pearson's_product-moment_coefficient}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Range}]{Range} \label{sec:corr-range}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be two random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, the correlation of $X$ and $Y$ is between and including $-1$ and $+1$:

\begin{equation} \label{eq:corr-range-corr-range}
-1 \leq \mathrm{Corr}(X,Y) \leq +1 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Consider the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ plus or minus $Y$, each divided by their standard deviations ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:std}):

\begin{equation} \label{eq:corr-range-var-XY}
\mathrm{Var}\left( \frac{X}{\sigma_X} \pm \frac{Y}{\sigma_Y} \right) \; .
\end{equation}

Because the variance is non-negative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-nonneg}), this term is larger than or equal to zero:

\begin{equation} \label{eq:corr-range-var-XY-0}
0 \leq \mathrm{Var}\left( \frac{X}{\sigma_X} \pm \frac{Y}{\sigma_Y} \right) \; .
\end{equation}

Using the variance of a linear combination ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-lincomb}), it can also be written as:

\begin{equation} \label{eq:corr-range-var-XY-s1}
\begin{split}
\mathrm{Var}\left( \frac{X}{\sigma_X} \pm \frac{Y}{\sigma_Y} \right) &= \mathrm{Var}\left( \frac{X}{\sigma_X} \right) + \mathrm{Var}\left( \frac{Y}{\sigma_Y} \right) \pm 2 \, \mathrm{Cov}\left( \frac{X}{\sigma_X}, \frac{Y}{\sigma_Y} \right) \\
&= \frac{1}{\sigma_X^2} \mathrm{Var}(X) + \frac{1}{\sigma_Y^2} \mathrm{Var}(Y) \pm 2 \, \frac{1}{\sigma_X \sigma_Y} \, \mathrm{Cov}(X,Y) \\
&= \frac{1}{\sigma_X^2} \sigma_X^2 + \frac{1}{\sigma_Y^2} \sigma_Y^2 \pm 2 \, \frac{1}{\sigma_X \sigma_Y} \, \sigma_{XY} \; .
\end{split}
\end{equation}

Using the relationship between covariance and correlation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-corr}), we have:

\begin{equation} \label{eq:corr-range-var-XY-s2}
\mathrm{Var}\left( \frac{X}{\sigma_X} \pm \frac{Y}{\sigma_Y} \right) = 1 + 1 + \pm 2 \, \mathrm{Corr}(X,Y) \; .
\end{equation}

Thus, the combination of \eqref{eq:corr-range-var-XY-0} with \eqref{eq:corr-range-var-XY-s2} yields

\begin{equation} \label{eq:corr-range-var-XY-ineq}
0 \leq 2 \pm 2 \, \mathrm{Corr}(X,Y)
\end{equation}

which is equivalent to

\begin{equation} \label{eq:corr-range-corr-range-qed}
-1 \leq \mathrm{Corr}(X,Y) \leq +1 \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Dor Leventer (2021): "How can I simply prove that the pearson correlation coefficient is between -1 and 1?"; in: \textit{StackExchange Mathematics}, retrieved on 2021-12-14; URL: \url{https://math.stackexchange.com/a/4260655/480910}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Sample correlation coefficient}]{Sample correlation coefficient} \label{sec:corr-samp}
\setcounter{equation}{0}

\textbf{Definition:} Let $x = \left\lbrace x_1, \ldots, x_n \right\rbrace$ and $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ be samples from random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ and $Y$. Then, the sample correlation coefficient of $x$ and $y$ is given by

\begin{equation} \label{eq:corr-samp-corr-samp}
r_{xy} = \frac{\sum_{i=1}^n (x_i-\bar{x}) (y_i-\bar{y})}{\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2} \sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}}
\end{equation}

where $\bar{x}$ and $\bar{y}$ are the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Pearson correlation coefficient"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-12-14; URL: \url{https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#For_a_sample}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relationship to standard scores}]{Relationship to standard scores} \label{sec:corr-z}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x = \left\lbrace x_1, \ldots, x_n \right\rbrace$ and $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ be samples from random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ and $Y$. Then, the sample correlation coefficient ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr-samp}) $r_{xy}$ can be expressed in terms of the standard scores of $x$ and $y$:

\begin{equation} \label{eq:corr-z-corr-z}
r_{xy} = \frac{1}{n-1} \sum_{i=1}^n z_i^{(x)} \cdot z_i^{(y)} = \frac{1}{n-1} \sum_{i=1}^n \left( \frac{x_i-\bar{x}}{s_x} \right) \left( \frac{y_i-\bar{y}}{s_y} \right)
\end{equation}

where $\bar{x}$ and $\bar{y}$ are the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) and $s_x$ and $s_y$ are the sample variances ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}).


\vspace{1em}
\textbf{Proof:} The sample correlation coefficient ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr-samp}) is defined as

\begin{equation} \label{eq:corr-z-corr-samp}
r_{xy} = \frac{\sum_{i=1}^n (x_i-\bar{x}) (y_i-\bar{y})}{\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2} \sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}} \; .
\end{equation}

Using the sample variances ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) of $x$ and $y$, we can write:

\begin{equation} \label{eq:corr-z-corr-z-s1}
r_{xy} = \frac{\sum_{i=1}^n (x_i-\bar{x}) (y_i-\bar{y})}{\sqrt{(n-1) s_x^2} \sqrt{(n-1) s_y^2}} \; .
\end{equation}

Rearranging the terms, we arrive at:

\begin{equation} \label{eq:corr-z-corr-z-s2}
r_{xy} = \frac{1}{(n-1) \, s_x \, s_y} \sum_{i=1}^n (x_i-\bar{x}) (y_i-\bar{y}) \; .
\end{equation}

Further simplifying, the result is:

\begin{equation} \label{eq:corr-z-corr-z-s3}
r_{xy} = \frac{1}{n-1} \sum_{i=1}^n \left( \frac{x_i-\bar{x}}{s_x} \right) \left( \frac{y_i-\bar{y}}{s_y} \right) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Peason correlation coefficient"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-12-14; URL: \url{https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#For_a_sample}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Correlation matrix}]{Correlation matrix} \label{sec:corrmat}
\setcounter{equation}{0}

\textbf{Definition:} Let $X = [X_1, \ldots, X_n]^\mathrm{T}$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, the correlation matrix of $X$ is defined as the $n \times n$ matrix in which the entry $(i,j)$ is the correlation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr}) of $X_i$ and $X_j$:

\begin{equation} \label{eq:corrmat-corrmat}
\mathrm{C}_{XX} =
\begin{bmatrix}
\mathrm{Corr}(X_1,X_1) & \ldots & \mathrm{Corr}(X_1,X_n) \\
\vdots & \ddots & \vdots \\
\mathrm{Corr}(X_n,X_1) & \ldots & \mathrm{Corr}(X_n,X_n)
\end{bmatrix} =
\begin{bmatrix}
\frac{\mathrm{E}\left[(X_1-\mathrm{E}[X_1]) (X_1-\mathrm{E}[X_1])\right]}{\sigma_{X_1} \, \sigma_{X_1}} & \ldots & \frac{\mathrm{E}\left[(X_1-\mathrm{E}[X_1]) (X_n-\mathrm{E}[X_n])\right]}{\sigma_{X_1} \, \sigma_{X_n}} \\
\vdots & \ddots & \vdots \\
\frac{\mathrm{E}\left[(X_n-\mathrm{E}[X_n]) (X_1-\mathrm{E}[X_1])\right]}{\sigma_{X_n} \, \sigma_{X_1}} & \ldots & \frac{\mathrm{E}\left[(X_n-\mathrm{E}[X_n]) (X_n-\mathrm{E}[X_n])\right]}{\sigma_{X_n} \, \sigma_{X_n}}
\end{bmatrix} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Correlation and dependence"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-06-06; URL: \url{https://en.wikipedia.org/wiki/Correlation_and_dependence#Correlation_matrices}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Sample correlation matrix}]{Sample correlation matrix} \label{sec:corrmat-samp}
\setcounter{equation}{0}

\textbf{Definition:} Let $x = \left\lbrace x_1, \ldots, x_n \right\rbrace$ be a sample from a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $X \in \mathbb{R}^{p \times 1}$. Then, the sample correlation matrix of $x$ is the matrix whose entries are the sample correlation coefficients ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr-samp}) between pairs of entries of $x_1, \ldots, x_n$:

\begin{equation} \label{eq:corrmat-samp-corrmat-samp-v1}
\mathrm{R}_{xx} =
\begin{bmatrix}
r_{x^{(1)},x^{(1)}} & \ldots & r_{x^{(1)},x^{(n)}} \\
\vdots & \ddots & \vdots \\
r_{x^{(n)},x^{(1)}} & \ldots & r_{x^{(n)},x^{(n)}}
\end{bmatrix}
\end{equation}

where the $r_{x^{(j)},x^{(k)}}$ is the sample correlation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr-samp}) between the $j$-th and the $k$-th entry of $X$ given by

\begin{equation} \label{eq:corrmat-samp-corrmat-samp-v2}
r_{x^{(j)},x^{(k)}} = \frac{\sum_{i=1}^n (x_{ij}-\bar{x}^{(j)}) (x_{ik}-\bar{x}^{(k)})}{\sqrt{\sum_{i=1}^n (x_{ij}-\bar{x}^{(j)})^2} \sqrt{\sum_{i=1}^n (x_{ik}-\bar{x}^{(k)})^2}}
\end{equation}

in which $\bar{x}^{(j)}$ and $\bar{x}^{(k)}$ are the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp})

\begin{equation} \label{eq:corrmat-samp-mean-samp}
\begin{split}
\bar{x}^{(j)} &= \frac{1}{n} \sum_{i=1}^n x_{ij} \\
\bar{x}^{(k)} &= \frac{1}{n} \sum_{i=1}^n x_{ik} \; .
\end{split}
\end{equation}


\subsection{Measures of central tendency}

\subsubsection[\textit{Median}]{Median} \label{sec:med}
\setcounter{equation}{0}

\textbf{Definition:} The median of a sample or random variable is the value separating the higher half from the lower half of its values.

\vspace{1em}
1) Let $x = \left\lbrace x_1, \ldots, x_n \right\rbrace$ be a sample from a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$. Then, the median of $x$ is

\begin{equation} \label{eq:med-med-samp}
\mathrm{median}(x) = \left\{
\begin{array}{cl}
x_{(n+1)/2} \; , & \text{if} \; n \; \text{is odd} \\
\frac{1}{2}(x_{n/2} + x_{n/2+1}) \; , & \text{if} \; n \; \text{is even} \; ,
\end{array}
\right.
\end{equation}

i.e. the median is the "middle" number when all numbers are sorted from smallest to largest.

\vspace{1em}
2) Let $X$ be a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) $F_X(x)$. Then, the median of $X$ is

\begin{equation} \label{eq:med-med-rvar}
\mathrm{median}(X) = x, \quad \mathrm{s.t.} \quad F_X(x) = \frac{1}{2} \; ,
\end{equation}

i.e. the median is the value at which the CDF is $1/2$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Median"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-10-15; URL: \url{https://en.wikipedia.org/wiki/Median}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Mode}]{Mode} \label{sec:mode}
\setcounter{equation}{0}

\textbf{Definition:} The mode of a sample or random variable is the value which occurs most often or with largest probability among all its values.

\vspace{1em}
1) Let $x = \left\lbrace x_1, \ldots, x_n \right\rbrace$ be a sample from a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$. Then, the mode of $x$ is the value which occurs most often in the list $x_1, \ldots, x_n$.

\vspace{1em}
2) Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) or probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) $f_X(x)$. Then, the mode of $X$ is the the value which maximizes the PMF or PDF:

\begin{equation} \label{eq:mode-mode-rvar}
\mathrm{mode}(X) = \operatorname*{arg\,max}_x f_X(x) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Mode (statistics)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-10-15; URL: \url{https://en.wikipedia.org/wiki/Mode_(statistics)}.
\end{itemize}
\vspace{1em}



\subsection{Measures of statistical dispersion}

\subsubsection[\textit{Standard deviation}]{Standard deviation} \label{sec:std}
\setcounter{equation}{0}

\textbf{Definition:} The standard deviation $\sigma$ of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ with expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu$ is defined as the square root of the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}), i.e.

\begin{equation} \label{eq:std-std}
\sigma(X) = \sqrt{\mathrm{E}\left[ (X-\mu)^2 \right]} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Standard deviation"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-09-03; URL: \url{https://en.wikipedia.org/wiki/Standard_deviation#Definition_of_population_values}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Full width at half maximum}]{Full width at half maximum} \label{sec:fwhm}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with a unimodal probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) $f_X(x)$ and mode ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mode}) $x_M$. Then, the full width at half maximum of $X$ is defined as

\begin{equation} \label{eq:fwhm-FWHM}
\mathrm{FHWM}(X) = \Delta x = x_2 - x_1
\end{equation}

where $x_1$ and $x_2$ are specified, such that

\begin{equation} \label{eq:fwhm-x12}
f_X(x_1) = f_X(x_2) = \frac{1}{2} f_X(x_M) \quad \text{and} \quad x_1 < x_M < x_2 \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Full width at half maximum"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-08-19; URL: \url{https://en.wikipedia.org/wiki/Full_width_at_half_maximum}.
\end{itemize}
\vspace{1em}



\subsection{Further summary statistics}

\subsubsection[\textit{Minimum}]{Minimum} \label{sec:min}
\setcounter{equation}{0}

\textbf{Definition:} The minimum of a sample or random variable is its lowest observed or possible value.

\vspace{1em}
1) Let $x = \left\lbrace x_1, \ldots, x_n \right\rbrace$ be a sample from a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$. Then, the minimum of $x$ is

\begin{equation} \label{eq:min-min-samp}
\mathrm{min}(x) = x_j, \quad \text{such that} \quad x_j \leq x_i \quad \text{for all} \quad i = 1, \ldots, n, \; i \neq j \; ,
\end{equation}

i.e. the minimum is the value which is smaller than or equal to all other observed values.

\vspace{1em}
2) Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible values $\mathcal{X}$. Then, the minimum of $X$ is

\begin{equation} \label{eq:min-min-rvar}
\mathrm{min}(X) = \tilde{x}, \quad \text{such that} \quad \tilde{x} < x \quad \text{for all} \quad x \in \mathcal{X}\setminus\left\lbrace \tilde{x} \right\rbrace \; ,
\end{equation}

i.e. the minimum is the value which is smaller than all other possible values.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Sample maximum and minimum"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-12; URL: \url{https://en.wikipedia.org/wiki/Sample_maximum_and_minimum}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Maximum}]{Maximum} \label{sec:max}
\setcounter{equation}{0}

\textbf{Definition:} The maximum of a sample or random variable is its highest observed or possible value.

\vspace{1em}
1) Let $x = \left\lbrace x_1, \ldots, x_n \right\rbrace$ be a sample from a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$. Then, the maximum of $x$ is

\begin{equation} \label{eq:max-max-samp}
\mathrm{max}(x) = x_j, \quad \text{such that} \quad x_j \geq x_i \quad \text{for all} \quad i = 1, \ldots, n, \; i \neq j \; ,
\end{equation}

i.e. the maximum is the value which is larger than or equal to all other observed values.

\vspace{1em}
2) Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible values $\mathcal{X}$. Then, the maximum of $X$ is

\begin{equation} \label{eq:max-max-rvar}
\mathrm{max}(X) = \tilde{x}, \quad \text{such that} \quad \tilde{x} > x \quad \text{for all} \quad x \in \mathcal{X}\setminus\left\lbrace \tilde{x} \right\rbrace \; ,
\end{equation}

i.e. the maximum is the value which is larger than all other possible values.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Sample maximum and minimum"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-12; URL: \url{https://en.wikipedia.org/wiki/Sample_maximum_and_minimum}.
\end{itemize}
\vspace{1em}



\subsection{Further moments}

\subsubsection[\textit{Moment}]{Moment} \label{sec:mom}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}), let $c$ be a constant ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:const}) and let $n$ be a positive integer. Then, the $n$-th moment of $X$ about $c$ is defined as the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the $n$-th power of $X$ minus $c$:

\begin{equation} \label{eq:mom-mom}
\mu_n(c) = \mathrm{E}[(X-c)^n] \; .
\end{equation}

The "$n$-th moment of $X$" may also refer to:

\begin{itemize}

\item the $n$-th raw moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-raw}) $\mu_n' = \mu_n(0)$;

\item the $n$-th central moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-cent}) $\mu_n = \mu_n(\mu)$;

\item the $n$-th standardized moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-stand}) $\mu_n^{*} = \mu_n/\sigma^n$.

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Moment (mathematics)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-08-19; URL: \url{https://en.wikipedia.org/wiki/Moment_(mathematics)#Significance_of_the_moments}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Moment in terms of moment-generating function}]{Moment in terms of moment-generating function} \label{sec:mom-mgf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a scalar random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with the moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) $M_X(t)$. Then, the $n$-th raw moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-raw}) of $X$ can be calculated from the moment-generating function via

\begin{equation} \label{eq:mom-mgf-mom-mgf}
\mathrm{E}(X^n) = M_X^{(n)}(0)
\end{equation}

where $n$ is a positive integer and $M_X^{(n)}(t)$ is the $n$-th derivative of $M_X(t)$.


\vspace{1em}
\textbf{Proof:} Using the definition of the moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}), we can write:

\begin{equation} \label{eq:mom-mgf-mom-mgf-s1}
M_X^{(n)}(t) = \frac{\mathrm{d}^n}{\mathrm{d}t^n} \mathrm{E}(e^{tX}) \; .
\end{equation}

Using the power series expansion of the exponential function

\begin{equation} \label{eq:mom-mgf-exp-ps}
e^x = \sum_{n=0}^\infty \frac{x^n}{n!} \; ,
\end{equation}

equation \eqref{eq:mom-mgf-mom-mgf-s1} becomes

\begin{equation} \label{eq:mom-mgf-mom-mgf-s2}
M_X^{(n)}(t) = \frac{\mathrm{d}^n}{\mathrm{d}t^n} \mathrm{E}\left( \sum_{m=0}^\infty \frac{t^m X^m}{m!} \right) \; .
\end{equation}

Because the expected value is a linear operator ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), we have:

\begin{equation} \label{eq:mom-mgf-mom-mgf-s3}
\begin{split}
M_X^{(n)}(t) &= \frac{\mathrm{d}^n}{\mathrm{d}t^n} \sum_{m=0}^\infty \mathrm{E}\left( \frac{t^m X^m}{m!} \right) \\
&= \sum_{m=0}^\infty \frac{\mathrm{d}^n}{\mathrm{d}t^n} \frac{t^m}{m!} \mathrm{E}\left( X^m \right) \; .
\end{split}
\end{equation}

Using the $n$-th derivative of the $m$-th power

\begin{equation} \label{eq:mom-mgf-dndx-xm}
\frac{\mathrm{d}^n}{\mathrm{d}x^n} x^m = \left\{
\begin{array}{rl}
m^{\underline{n}} \, x^{m-n} \; , & \text{if} \; n \leq m \\
0 \; , & \text{if} \; n > m \; .
\end{array}
\right.
\end{equation}

with the falling factorial

\begin{equation} \label{eq:mom-mgf-fact-fall}
m^{\underline{n}} = \prod_{i=0}^{n-1} (m-i) = \frac{m!}{(m-n)!} \; ,
\end{equation}

equation \eqref{eq:mom-mgf-mom-mgf-s3} becomes

\begin{equation} \label{eq:mom-mgf-mom-mgf-s4}
\begin{split}
M_X^{(n)}(t) &= \sum_{m=n}^\infty \frac{m^{\underline{n}} \, t^{m-n}}{m!} \mathrm{E}\left( X^m \right) \\
&\overset{\eqref{eq:mom-mgf-fact-fall}}{=} \sum_{m=n}^\infty \frac{m! \, t^{m-n}}{(m-n)! \, m!} \mathrm{E}\left( X^m \right) \\
&= \sum_{m=n}^\infty \frac{t^{m-n}}{(m-n)!} \mathrm{E}\left( X^m \right) \\
&= \frac{t^{n-n}}{(n-n)!} \mathrm{E}\left( X^n \right) + \sum_{m=n+1}^\infty \frac{t^{m-n}}{(m-n)!} \mathrm{E}\left( X^m \right) \\
&= \frac{t^0}{0!} \, \mathrm{E}\left( X^n \right) + \sum_{m=n+1}^\infty \frac{t^{m-n}}{(m-n)!} \mathrm{E}\left( X^m \right) \\
&= \mathrm{E}\left( X^n \right) + \sum_{m=n+1}^\infty \frac{t^{m-n}}{(m-n)!} \mathrm{E}\left( X^m \right) \; .
\end{split}
\end{equation}

Setting $t = 0$ in \eqref{eq:mom-mgf-mom-mgf-s4} yields

\begin{equation} \label{eq:mom-mgf-mom-mgf-s5}
\begin{split}
M_X^{(n)}(0) &= \mathrm{E}\left( X^n \right) + \sum_{m=n+1}^\infty \frac{0^{m-n}}{(m-n)!} \mathrm{E}\left( X^m \right) \\
&= \mathrm{E}\left( X^n \right)
\end{split}
\end{equation}

which conforms to equation \eqref{eq:mom-mgf-mom-mgf}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item ProofWiki (2020): "Moment in terms of Moment Generating Function"; in: \textit{ProofWiki}, retrieved on 2020-08-19; URL: \url{https://proofwiki.org/wiki/Moment_in_terms_of_Moment_Generating_Function}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Raw moment}]{Raw moment} \label{sec:mom-raw}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) and let $n$ be a positive integer. Then, the $n$-th raw moment of $X$, also called ($n$-th) "crude moment", is defined as the $n$-th moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom}) of $X$ about the value 0:

\begin{equation} \label{eq:mom-raw-mom-raw}
\mu_n' = \mu_n(0) = \mathrm{E}[(X-0)^n] = \mathrm{E}[X^n] \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Moment (mathematics)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-10-08; URL: \url{https://en.wikipedia.org/wiki/Moment_(mathematics)#Significance_of_the_moments}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{First raw moment is mean}]{First raw moment is mean} \label{sec:momraw-1st}
\setcounter{equation}{0}

\textbf{Theorem:} The first raw moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-raw}) equals the mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), i.e.

\begin{equation} \label{eq:momraw-1st-momraw-1st}
\mu_1' = \mu \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The first raw moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-raw}) of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ is defined as

\begin{equation} \label{eq:momraw-1st-momraw-1st-def}
\mu_1' = \mathrm{E}\left[ (X-0)^1 \right]
\end{equation}

which is equal to the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$:

\begin{equation} \label{eq:momraw-1st-momraw-1st-qed}
\mu_1' = \mathrm{E}\left[ X \right] = \mu \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Second raw moment and variance}]{Second raw moment and variance} \label{sec:momraw-2nd}
\setcounter{equation}{0}

\textbf{Theorem:} The second raw moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-raw}) can be expressed as

\begin{equation} \label{eq:momraw-2nd-momraw-2nd}
\mu_2' = \mathrm{Var}(X) + \mathrm{E}(X)^2
\end{equation}

where $\mathrm{Var}(X)$ is the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ and $\mathrm{E}(X)$ is the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$.


\vspace{1em}
\textbf{Proof:} The second raw moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-raw}) of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ is defined as

\begin{equation} \label{eq:momraw-2nd-momraw-2nd-def}
\mu_2' = \mathrm{E}\left[ (X-0)^2 \right] \; .
\end{equation}

Using the partition of variance into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-mean})

\begin{equation} \label{eq:momraw-2nd-var-mean}
\mathrm{Var}(X) = \mathrm{E}(X^2) - \mathrm{E}(X)^2 \; ,
\end{equation}

the second raw moment can be rearranged into:

\begin{equation} \label{eq:momraw-2nd-momraw-2nd-qed}
\mu_2' \overset{\eqref{eq:momraw-2nd-momraw-2nd-def}}{=} \mathrm{E}(X^2) \overset{\eqref{eq:momraw-2nd-var-mean}}{=} \mathrm{Var}(X) + \mathrm{E}(X)^2 \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textit{Central moment}]{Central moment} \label{sec:mom-cent}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu$ and let $n$ be a positive integer. Then, the $n$-th central moment of $X$ is defined as the $n$-th moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom}) of $X$ about the value $\mu$:

\begin{equation} \label{eq:mom-cent-mom-cent}
\mu_n = \mathrm{E}[(X-\mu)^n] \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Moment (mathematics)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-10-08; URL: \url{https://en.wikipedia.org/wiki/Moment_(mathematics)#Significance_of_the_moments}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{First central moment is zero}]{First central moment is zero} \label{sec:momcent-1st}
\setcounter{equation}{0}

\textbf{Theorem:} The first central moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-cent}) is zero, i.e.

\begin{equation} \label{eq:momcent-1st-momcent-1st}
\mu_1 = 0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The first central moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-cent}) of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ with mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu$ is defined as

\begin{equation} \label{eq:momcent-1st-momcent-1st-def}
\mu_1 = \mathrm{E}\left[ (X-\mu)^1 \right] \; .
\end{equation}

Due to the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}) and by plugging in $\mu = \mathrm{E}(X)$, we have

\begin{equation} \label{eq:momcent-1st-momcent-1st-qed}
\begin{split}
\mu_1 &= \mathrm{E}\left[ X-\mu \right] \\
&= \mathrm{E}(X) - \mu \\
&= \mathrm{E}(X) - \mathrm{E}(X) \\
&= 0 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item ProofWiki (2020): "First Central Moment is Zero"; in: \textit{ProofWiki}, retrieved on 2020-09-09; URL: \url{https://proofwiki.org/wiki/First_Central_Moment_is_Zero}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Second central moment is variance}]{Second central moment is variance} \label{sec:momcent-2nd}
\setcounter{equation}{0}

\textbf{Theorem:} The second central moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-cent}) equals the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}), i.e.

\begin{equation} \label{eq:momcent-2nd-momcent-2nd}
\mu_2 = \mathrm{Var}(X) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The second central moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-cent}) of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ with mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu$ is defined as

\begin{equation} \label{eq:momcent-2nd-momcent-2nd-def}
\mu_2 = \mathrm{E}\left[ (X-\mu)^2 \right]
\end{equation}

which is equivalent to the definition of the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}):

\begin{equation} \label{eq:momcent-2nd-momraw-1st-qed}
\mu_2 = \mathrm{E}\left[ (X - \mathrm{E}(X))^2 \right] = \mathrm{Var}(X) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Moment (mathematics)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-10-08; URL: \url{https://en.wikipedia.org/wiki/Moment_(mathematics)#Significance_of_the_moments}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Standardized moment}]{Standardized moment} \label{sec:mom-stand}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu$ and standard deviation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:std}) $\sigma$ and let $n$ be a positive integer. Then, the $n$-th standardized moment of $X$ is defined as the $n$-th moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom}) of $X$ about the value $\mu$, divided by the $n$-th power of $\sigma$:

\begin{equation} \label{eq:mom-stand-mom-stand}
\mu_n^{*} = \frac{\mu_n}{\sigma^n} =  \frac{\mathrm{E}[(X-\mu)^n]}{\sigma^n} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Moment (mathematics)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-10-08; URL: \url{https://en.wikipedia.org/wiki/Moment_(mathematics)#Standardized_moments}.
\end{itemize}
\vspace{1em}



\pagebreak
\section{Information theory}

\subsection{Shannon entropy}

\subsubsection[\textit{Definition}]{Definition} \label{sec:ent}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and the (observed or assumed) probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) $p(x) = f_X(x)$. Then, the entropy (also referred to as "Shannon entropy") of $X$ is defined as

\begin{equation} \label{eq:ent-ent}
\mathrm{H}(X) = - \sum_{x \in \mathcal{X}} p(x) \cdot \log_b p(x)
\end{equation}

where $b$ is the base of the logarithm specifying in which unit the entropy is determined. By convention, $0 \cdot \log 0$ is taken to be zero when calculating the entropy of $X$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Shannon CE (1948): "A Mathematical Theory of Communication"; in: \textit{Bell System Technical Journal}, vol. 27, iss. 3, pp. 379-423; URL: \url{https://ieeexplore.ieee.org/document/6773024}; DOI: 10.1002/j.1538-7305.1948.tb01338.x.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Non-negativity}]{Non-negativity} \label{sec:ent-nonneg}
\setcounter{equation}{0}

\textbf{Theorem:} The entropy of a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) is a non-negative number:

\begin{equation} \label{eq:ent-nonneg-ent-nonneg}
\mathrm{H}(X) \geq 0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The entropy of a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) is defined as

\begin{equation} \label{eq:ent-nonneg-ent}
\mathrm{H}(X) = - \sum_{x \in \mathcal{X}} p(x) \cdot \log_b p(x)
\end{equation}

The minus sign can be moved into the sum:

\begin{equation} \label{eq:ent-nonneg-ent-dev}
\mathrm{H}(X) = \sum_{x \in \mathcal{X}} \left[ p(x) \cdot \left( - \log_b p(x) \right) \right]
\end{equation}

Because the co-domain of probability mass functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) is $[0,1]$, we can deduce:

\begin{equation} \label{eq:ent-nonneg-nonneg}
\begin{array}{rcccl}
0 &\leq &p(x) &\leq &1 \\
-\infty &\leq &\log_b p(x) &\leq &0 \\
0 &\leq &-\log_b p(x) &\leq &+\infty \\
0 &\leq &p(x) \cdot \left(-\log_b p(x)\right) &\leq &+\infty \; .
\end{array}
\end{equation}

By convention, $0 \cdot \log_b(0)$ is taken to be $0$ when calculating entropy, consistent with

\begin{equation} \label{eq:ent-nonneg-lim-0log0}
\lim_{p \to 0} \left[ p \log_b(p) \right] = 0 \; .
\end{equation}

Taking this together, each addend in \eqref{eq:ent-nonneg-ent-dev} is positive or zero and thus, the entire sum must also be non-negative.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Cover TM, Thomas JA (1991): "Elements of Information Theory", p. 15; URL: \url{https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Concavity}]{Concavity} \label{sec:ent-conc}
\setcounter{equation}{0}

\textbf{Theorem:} The entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) is concave in the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) $p$, i.e.

\begin{equation} \label{eq:ent-conc-ent-conc}
\mathrm{H}[\lambda p_1 + (1-\lambda) p_2] \geq \lambda \mathrm{H}[p_1] + (1-\lambda) \mathrm{H}[p_2]
\end{equation}

where $p_1$ and $p_2$ are probability mass functions and $0 \leq \lambda \leq 1$.


\vspace{1em}
\textbf{Proof:} Let $X$ be a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and let $u(x)$ be the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of a discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni}) on $X \in \mathcal{X}$. Then, the entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) of an arbitrary probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) $p(x)$ can be rewritten as

\begin{equation} \label{eq:ent-conc-ent-kl}
\begin{split}
\mathrm{H}[p] &= - \sum_{x \in \mathcal{X}} p(x) \cdot \log p(x) \\
&= - \sum_{x \in \mathcal{X}} p(x) \cdot \log \frac{p(x)}{u(x)} u(x) \\
&= - \sum_{x \in \mathcal{X}} p(x) \cdot \log \frac{p(x)}{u(x)} - \sum_{x \in \mathcal{X}} p(x) \cdot \log u(x) \\
&= - \mathrm{KL}[p||u] - \log \frac{1}{|\mathcal{X}|} \sum_{x \in \mathcal{X}} p(x) \\
&= \log |\mathcal{X}| - \mathrm{KL}[p||u] \\
\log |\mathcal{X}| - \mathrm{H}[p] &= \mathrm{KL}[p||u]
\end{split}
\end{equation}

where we have applied the definition of the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}), the probability mass function of the discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni-pmf}) and the total sum over the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}).

Note that the KL divergence is convex ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl-conv}) in the pair of probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) $(p,q)$:

\begin{equation} \label{eq:ent-conc-kl-conv}
\mathrm{KL}[\lambda p_1 + (1-\lambda) p_2||\lambda q_1 + (1-\lambda) q_2] \leq \lambda \mathrm{KL}[p_1||q_1] + (1-\lambda) \mathrm{KL}[p_2||q_2]
\end{equation}

A special case of this is given by

\begin{equation} \label{eq:ent-conc-kl-conv-u}
\begin{split}
\mathrm{KL}[\lambda p_1 + (1-\lambda) p_2||\lambda u + (1-\lambda) u] &\leq \lambda \mathrm{KL}[p_1||u] + (1-\lambda) \mathrm{KL}[p_2||u] \\
\mathrm{KL}[\lambda p_1 + (1-\lambda) p_2||u] &\leq \lambda \mathrm{KL}[p_1||u] + (1-\lambda) \mathrm{KL}[p_2||u]
\end{split}
\end{equation}

and applying equation \eqref{eq:ent-conc-ent-kl}, we have

\begin{equation} \label{eq:ent-conc-ent-conc-qed}
\begin{split}
\log |\mathcal{X}| - \mathrm{H}[\lambda p_1 + (1-\lambda) p_2] &\leq \lambda \left( \log |\mathcal{X}| - \mathrm{H}[p_1] \right) + (1-\lambda) \left( \log |\mathcal{X}| - \mathrm{H}[p_2] \right) \\
\log |\mathcal{X}| - \mathrm{H}[\lambda p_1 + (1-\lambda) p_2] &\leq \log |\mathcal{X}| - \lambda \mathrm{H}[p_1] - (1-\lambda) \mathrm{H}[p_2] \\
- \mathrm{H}[\lambda p_1 + (1-\lambda) p_2] &\leq - \lambda \mathrm{H}[p_1] - (1-\lambda) \mathrm{H}[p_2] \\
\mathrm{H}[\lambda p_1 + (1-\lambda) p_2] &\geq \lambda \mathrm{H}[p_1] + (1-\lambda) \mathrm{H}[p_2]
\end{split}
\end{equation}

which is equivalent to \eqref{eq:ent-conc-ent-conc}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Entropy (information theory)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-08-11; URL: \url{https://en.wikipedia.org/wiki/Entropy_(information_theory)#Further_properties}.
\item Cover TM, Thomas JA (1991): "Elements of Information Theory", p. 30; URL: \url{https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959}.
\item Xie, Yao (2012): "Chain Rules and Inequalities"; in: \textit{ECE587: Information Theory}, Lecture 3, Slide 25; URL: \url{https://www2.isye.gatech.edu/~yxie77/ece587/Lecture3.pdf}.
\item Goh, Siong Thye (2016): "Understanding the proof of the concavity of entropy"; in: \textit{StackExchange Mathematics}, retrieved on 2020-11-08; URL: \url{https://math.stackexchange.com/questions/2000194/understanding-the-proof-of-the-concavity-of-entropy}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Conditional entropy}]{Conditional entropy} \label{sec:ent-cond}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ and $Y$ be discrete random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and $\mathcal{Y}$ and probability mass functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) $p(x)$ and $p(y)$. Then, the conditional entropy of $Y$ given $X$ or, entropy of $Y$ conditioned on $X$, is defined as

\begin{equation} \label{eq:ent-cond-ent-cond}
\mathrm{H}(Y|X) = \sum_{x \in \mathcal{X}} p(x) \cdot \mathrm{H}(Y|X=x)
\end{equation}

where $\mathrm{H}(Y \vert X=x)$ is the (marginal) entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) of $Y$, evaluated at $x$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Cover TM, Thomas JA (1991): "Joint Entropy and Conditional Entropy"; in: \textit{Elements of Information Theory}, ch. 2.2, p. 15; URL: \url{https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Joint entropy}]{Joint entropy} \label{sec:ent-joint}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ and $Y$ be discrete random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and $\mathcal{Y}$ and joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) $p(x,y)$. Then, the joint entropy of $X$ and $Y$ is defined as

\begin{equation} \label{eq:ent-joint-ent-joint}
\mathrm{H}(X,Y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \cdot \log_b p(x,y)
\end{equation}

where $b$ is the base of the logarithm specifying in which unit the entropy is determined.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Cover TM, Thomas JA (1991): "Joint Entropy and Conditional Entropy"; in: \textit{Elements of Information Theory}, ch. 2.2, p. 16; URL: \url{https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Cross-entropy}]{Cross-entropy} \label{sec:ent-cross}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and let $P$ and $Q$ be two probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) on $X$ with the probability mass functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) $p(x)$ and $q(x)$. Then, the cross-entropy of $Q$ relative to $P$ is defined as

\begin{equation} \label{eq:ent-cross-ent-cross}
\mathrm{H}(P,Q) = - \sum_{x \in \mathcal{X}} p(x) \cdot \log_b q(x)
\end{equation}

where $b$ is the base of the logarithm specifying in which unit the cross-entropy is determined.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Cross entropy"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-07-28; URL: \url{https://en.wikipedia.org/wiki/Cross_entropy#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Convexity of cross-entropy}]{Convexity of cross-entropy} \label{sec:entcross-conv}
\setcounter{equation}{0}

\textbf{Theorem:} The cross-entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent-cross}) is convex in the probability distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) $q$, i.e.

\begin{equation} \label{eq:entcross-conv-ent-cross-conv}
\mathrm{H}[p,\lambda q_1 + (1-\lambda) q_2] \leq \lambda \mathrm{H}[p,q_1] + (1-\lambda) \mathrm{H}[p,q_2]
\end{equation}

where $p$ is a fixed and $q_1$ and $q_2$ are any two probability distributions and $0 \leq \lambda \leq 1$.


\vspace{1em}
\textbf{Proof:} The relationship between Kullback-Leibler divergence, entropy and cross-entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl-ent}) is:

\begin{equation} \label{eq:entcross-conv-kl-ent}
\mathrm{KL}[P||Q] = \mathrm{H}(P,Q) - \mathrm{H}(P) \; .
\end{equation}

Note that the KL divergence is convex ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl-conv}) in the pair of probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) $(p,q)$:

\begin{equation} \label{eq:entcross-conv-kl-conv}
\mathrm{KL}[\lambda p_1 + (1-\lambda) p_2||\lambda q_1 + (1-\lambda) q_2] \leq \lambda \mathrm{KL}[p_1||q_1] + (1-\lambda) \mathrm{KL}[p_2||q_2]
\end{equation}

A special case of this is given by

\begin{equation} \label{eq:entcross-conv-kl-conv-p}
\begin{split}
\mathrm{KL}[\lambda p + (1-\lambda) p||\lambda q_1 + (1-\lambda) q_2] &\leq \lambda \mathrm{KL}[p||q_1] + (1-\lambda) \mathrm{KL}[p||q_2] \\
\mathrm{KL}[p||\lambda q_1 + (1-\lambda) q_2] &\leq \lambda \mathrm{KL}[p||q_1] + (1-\lambda) \mathrm{KL}[p||q_2]
\end{split}
\end{equation}

and applying equation \eqref{eq:entcross-conv-kl-ent}, we have

\begin{equation} \label{eq:entcross-conv-ent-cross-conv-qed}
\begin{split}
\mathrm{H}[p,\lambda q_1 + (1-\lambda) q_2] - \mathrm{H}[p] &\leq \lambda \left( \mathrm{H}[p,q_1] - \mathrm{H}[p] \right) + (1-\lambda) \left( \mathrm{H}[p,q_2] - \mathrm{H}[p] \right) \\
\mathrm{H}[p,\lambda q_1 + (1-\lambda) q_2] - \mathrm{H}[p] &\leq \lambda \mathrm{H}[p,q_1] + (1-\lambda) \mathrm{H}[p,q_2] - \mathrm{H}[p] \\
\mathrm{H}[p,\lambda q_1 + (1-\lambda) q_2] &\leq \lambda \mathrm{H}[p,q_1] + (1-\lambda) \mathrm{H}[p,q_2]
\end{split}
\end{equation}

which is equivalent to \eqref{eq:entcross-conv-ent-cross-conv}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Cross entropy"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-08-11; URL: \url{https://en.wikipedia.org/wiki/Cross_entropy#Definition}.
\item gunes (2019): "Convexity of cross entropy"; in: \textit{StackExchange CrossValidated}, retrieved on 2020-11-08; URL: \url{https://stats.stackexchange.com/questions/394463/convexity-of-cross-entropy}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Gibbs' inequality}]{Gibbs' inequality} \label{sec:gibbs-ineq}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) and consider two probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) with probability mass functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) $p(x)$ and $q(x)$. Then, Gibbs' inequality states that the entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) of $X$ according to $P$ is smaller than or equal to the cross-entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent-cross}) of $P$ and $Q$:

\begin{equation} \label{eq:gibbs-ineq-Gibbs-ineq}
- \sum_{x \in \mathcal{X}} p(x) \, \log_b p(x) \leq - \sum_{x \in \mathcal{X}} p(x) \, \log_b q(x) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Without loss of generality, we will use the natural logarithm, because a change in the base of the logarithm only implies multiplication by a constant:

\begin{equation} \label{eq:gibbs-ineq-log-ln}
\log_b a = \frac{\ln a}{\ln b} \; .
\end{equation}

Let $I$ be the set of all $x$ for which $p(x)$ is non-zero. Then, proving \eqref{eq:gibbs-ineq-Gibbs-ineq} requires to show that

\begin{equation} \label{eq:gibbs-ineq-Gibbs-ineq-s1}
\sum_{x \in I} p(x) \, \ln \frac{p(x)}{q(x)} \geq 0 \; .
\end{equation}

For all $x > 0$, it holds that $\ln x \leq x - 1$, with equality only if $x = 1$. Multiplying this with $-1$, we have $\ln \frac{1}{x} \geq 1 - x$. Applying this to \eqref{eq:gibbs-ineq-Gibbs-ineq-s1}, we can say about the left-hand side that

\begin{equation} \label{eq:gibbs-ineq-Gibbs-ineq-s2}
\begin{split}
\sum_{x \in I} p(x) \, \ln \frac{p(x)}{q(x)} &\geq \sum_{x \in I} p(x) \left( 1 - \frac{q(x)}{p(x)} \right) \\
&= \sum_{x \in I} p(x) - \sum_{x \in I} q(x) \; .
\end{split}
\end{equation}

Finally, since $p(x)$ and $q(x)$ are probability mass functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}), we have

\begin{equation} \label{eq:gibbs-ineq-p-q-pmf}
\begin{split}
0 \leq p(x) \leq 1, \quad \sum_{x \in I} p(x) &= 1 \quad \text{and} \\
0 \leq q(x) \leq 1, \quad \sum_{x \in I} q(x) &\leq 1 \; ,
\end{split}
\end{equation}

such that it follows from \eqref{eq:gibbs-ineq-Gibbs-ineq-s2} that

\begin{equation} \label{eq:gibbs-ineq-Gibbs-ineq-s3}
\begin{split}
\sum_{x \in I} p(x) \, \ln \frac{p(x)}{q(x)} &\geq \sum_{x \in I} p(x) - \sum_{x \in I} q(x) \\
&= 1 - \sum_{x \in I} q(x) \geq 0 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Gibbs' inequality"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-09-09; URL: \url{https://en.wikipedia.org/wiki/Gibbs%27_inequality#Proof}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Log sum inequality}]{Log sum inequality} \label{sec:logsum-ineq}
\setcounter{equation}{0}

\textbf{Theorem:} Let $a_1, \ldots, a_n$ and $b_1, \ldots, b_n$ be non-negative real numbers and define $a = \sum_{i=1}^{n} a_i$ and $b = \sum_{i=1}^{n} b_i$. Then, the log sum inequality states that

\begin{equation} \label{eq:logsum-ineq-logsum-ineq}
\sum_{i=1}^n a_i \, \log_c \frac{a_i}{b_i} \geq a \, \log_c \frac{a}{b} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Without loss of generality, we will use the natural logarithm, because a change in the base of the logarithm only implies multiplication by a constant:

\begin{equation} \label{eq:logsum-ineq-log-ln}
\log_c a = \frac{\ln a}{\ln c} \; .
\end{equation}

Let $f(x) = x \ln x$. Then, the left-hand side of \eqref{eq:logsum-ineq-logsum-ineq} can be rewritten as

\begin{equation} \label{eq:logsum-ineq-logsum-ineq-s2}
\begin{split}
\sum_{i=1}^n a_i \, \ln \frac{a_i}{b_i} &= \sum_{i=1}^n b_i \, f\left( \frac{a_i}{b_i} \right) \\
&= b \sum_{i=1}^n \frac{b_i}{b} \, f\left( \frac{a_i}{b_i} \right) \; .
\end{split}
\end{equation}

Because $f(x)$ is a convex function and

\begin{equation} \label{eq:logsum-ineq-sum-bi-b}
\begin{split}
\frac{b_i}{b} &\geq 0 \\
\sum_{i=1}^n \frac{b_i}{b} &= 1 \; ,
\end{split}
\end{equation}

applying Jensen's inequality yields

\begin{equation} \label{eq:logsum-ineq-logsum-ineq-s3}
\begin{split}
b \sum_{i=1}^n \frac{b_i}{b} \, f\left( \frac{a_i}{b_i} \right) &\geq b \, f\left( \sum_{i=1}^n \frac{b_i}{b} \frac{a_i}{b_i} \right) \\
&= b \, f\left( \frac{1}{b} \sum_{i=1}^n a_i \right) \\
&= b \, f\left( \frac{a}{b} \right) \\
&= a \, \ln \frac{a}{b} \; .
\end{split}
\end{equation}

Finally, combining \eqref{eq:logsum-ineq-logsum-ineq-s2} and \eqref{eq:logsum-ineq-logsum-ineq-s3}, this demonstrates \eqref{eq:logsum-ineq-logsum-ineq}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Log sum inequality"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-09-09; URL: \url{https://en.wikipedia.org/wiki/Log_sum_inequality#Proof}.
\item Wikipedia (2020): "Jensen's inequality"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-09-09; URL: \url{https://en.wikipedia.org/wiki/Jensen%27s_inequality#Statements}.
\end{itemize}
\vspace{1em}



\subsection{Differential entropy}

\subsubsection[\textit{Definition}]{Definition} \label{sec:dent}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and the (estimated or assumed) probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) $p(x) = f_X(x)$. Then, the differential entropy (also referred to as "continuous entropy") of $X$ is defined as

\begin{equation} \label{eq:dent-dent}
\mathrm{h}(X) = - \int_{\mathcal{X}} p(x) \log_b p(x) \, \mathrm{d}x
\end{equation}

where $b$ is the base of the logarithm specifying in which unit the entropy is determined.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Cover TM, Thomas JA (1991): "Differential Entropy"; in: \textit{Elements of Information Theory}, ch. 8.1, p. 243; URL: \url{https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Negativity}]{Negativity} \label{sec:dent-neg}
\setcounter{equation}{0}

\textbf{Theorem:} Unlike its discrete analogue ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent-nonneg}), the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) can become negative.


\vspace{1em}
\textbf{Proof:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}) with minimum $0$ and maximum $1/2$:

\begin{equation} \label{eq:dent-neg-X}
X \sim \mathcal{U}(0, 1/2) \; .
\end{equation}

Then, its probability density function ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-pdf}) is:

\begin{equation} \label{eq:dent-neg-X-pdf}
f_X(x) = 2 \quad \text{for} \quad 0 \leq x \leq \frac{1}{2} \; .
\end{equation}

Thus, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) follows as

\begin{equation} \label{eq:dent-neg-X-dent}
\begin{split}
\mathrm{h}(X) &= - \int_{\mathcal{X}} f_X(x) \log_b f_X(x) \, \mathrm{d}x \\
&= - \int_{0}^{\frac{1}{2}} 2 \, \log_b(2) \, \mathrm{d}x \\
&= -\log_b(2) \int_{0}^{\frac{1}{2}} 2 \, \mathrm{d}x \\
&= -\log_b(2) \left[ 2x \right]_{0}^{\frac{1}{2}} \\
&= -\log_b(2)
\end{split}
\end{equation}

which is negative for any base $b > 1$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Differential entropy"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-02; URL: \url{https://en.wikipedia.org/wiki/Differential_entropy#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Invariance under addition}]{Invariance under addition} \label{sec:dent-inv}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $X$ remains constant under addition of a constant:

\begin{equation} \label{eq:dent-inv-dent-inv}
\mathrm{h}(X + c) = \mathrm{h}(X) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} By definition, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $X$ is

\begin{equation} \label{eq:dent-inv-X-dent}
\mathrm{h}(X) = - \int_{\mathcal{X}} p(x) \log p(x) \, \mathrm{d}x
\end{equation}

where $p(x) = f_X(x)$ is the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$.

Define the mappings between $X$ and $Y = X + c$ as

\begin{equation} \label{eq:dent-inv-X-Y}
Y = g(X) = X + c \quad \Leftrightarrow \quad X = g^{-1}(Y) = Y - c \; .
\end{equation}

Note that $g(X)$ is a strictly increasing function, such that the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-sifct}) of $Y$ is

\begin{equation} \label{eq:dent-inv-Y-pdf}
f_Y(y) = f_X(g^{-1}(y)) \, \frac{\mathrm{d}g^{-1}(y)}{\mathrm{d}y} \overset{\eqref{eq:dent-inv-X-Y}}{=} f_X(y-c) \; .
\end{equation}

Writing down the differential entropy for $Y$, we have:

\begin{equation} \label{eq:dent-inv-Y-dent-s1}
\begin{split}
\mathrm{h}(Y) &= - \int_{\mathcal{Y}} f_Y(y) \log f_Y(y) \, \mathrm{d}y \\
&\overset{\eqref{eq:dent-inv-Y-pdf}}{=} - \int_{\mathcal{Y}} f_X(y-c) \log f_X(y-c) \, \mathrm{d}y
\end{split}
\end{equation}

Substituting $x = y - c$, such that $y = x + c$, this yields:

\begin{equation} \label{eq:dent-inv-Y-dent-s2}
\begin{split}
\mathrm{h}(Y) &= - \int_{\left\lbrace y-c \,|\, y \in {\mathcal{Y}} \right\rbrace} f_X(x+c-c) \log f_X(x+c-c) \, \mathrm{d}(x+c) \\
&= - \int_{\mathcal{X}} f_X(x) \log f_X(x) \, \mathrm{d}x \\
&\overset{\eqref{eq:dent-inv-X-dent}}{=} \mathrm{h}(X) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Differential entropy"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-12; URL: \url{https://en.wikipedia.org/wiki/Differential_entropy#Properties_of_differential_entropy}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Addition upon multiplication}]{Addition upon multiplication} \label{sec:dent-add}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $X$ increases additively upon multiplication with a constant:

\begin{equation} \label{eq:dent-add-dent-add}
\mathrm{h}(aX) = \mathrm{h}(X) + \log |a| \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} By definition, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $X$ is

\begin{equation} \label{eq:dent-add-X-dent}
\mathrm{h}(X) = - \int_{\mathcal{X}} p(x) \log p(x) \, \mathrm{d}x
\end{equation}

where $p(x) = f_X(x)$ is the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$.

Define the mappings between $X$ and $Y = aX$ as

\begin{equation} \label{eq:dent-add-X-Y}
Y = g(X) = aX \quad \Leftrightarrow \quad X = g^{-1}(Y) = \frac{Y}{a} \; .
\end{equation}

If $a > 0$, then $g(X)$ is a strictly increasing function, such that the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-sifct}) of $Y$ is

\begin{equation} \label{eq:dent-add-Y-pdf-c1}
f_Y(y) = f_X(g^{-1}(y)) \, \frac{\mathrm{d}g^{-1}(y)}{\mathrm{d}y} \overset{\eqref{eq:dent-add-X-Y}}{=} \frac{1}{a} \, f_X\left(\frac{y}{a}\right) \; ;
\end{equation}

if $a < 0$, then $g(X)$ is a strictly decreasing function, such that the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-sdfct}) of $Y$ is

\begin{equation} \label{eq:dent-add-Y-pdf-c2}
f_Y(y) = - f_X(g^{-1}(y)) \, \frac{\mathrm{d}g^{-1}(y)}{\mathrm{d}y} \overset{\eqref{eq:dent-add-X-Y}}{=} -\frac{1}{a} \, f_X\left(\frac{y}{a}\right) \; ;
\end{equation}

thus, we can write

\begin{equation} \label{eq:dent-add-Y-pdf}
f_Y(y) = \frac{1}{|a|} \, f_X\left(\frac{y}{a}\right) \; .
\end{equation}

Writing down the differential entropy for $Y$, we have:

\begin{equation} \label{eq:dent-add-Y-dent-s1}
\begin{split}
\mathrm{h}(Y) &= - \int_{\mathcal{Y}} f_Y(y) \log f_Y(y) \, \mathrm{d}y \\
&\overset{\eqref{eq:dent-add-Y-pdf}}{=} - \int_{\mathcal{Y}} \frac{1}{|a|} \, f_X\left(\frac{y}{a}\right) \log \left[ \frac{1}{|a|} \, f_X\left(\frac{y}{a}\right) \right] \, \mathrm{d}y
\end{split}
\end{equation}

Substituting $x = y/a$, such that $y = ax$, this yields:

\begin{equation} \label{eq:dent-add-Y-dent-s2}
\begin{split}
\mathrm{h}(Y) &= - \int_{\left\lbrace y/a \,|\, y \in {\mathcal{Y}} \right\rbrace} \frac{1}{|a|} \, f_X\left(\frac{ax}{a}\right) \log \left[ \frac{1}{|a|} \, f_X\left(\frac{ax}{a}\right) \right] \, \mathrm{d}(ax) \\
&= - \int_{\mathcal{X}} f_X(x) \log \left[ \frac{1}{|a|} \, f_X(x) \right] \, \mathrm{d}x \\
&= - \int_{\mathcal{X}} f_X(x) \left[ \log f_X(x) - \log |a| \right] \, \mathrm{d}x \\
&= - \int_{\mathcal{X}} f_X(x) \log f_X(x) \, \mathrm{d}x + \log |a| \int_{\mathcal{X}} f_X(x) \, \mathrm{d}x \\
&\overset{\eqref{eq:dent-add-X-dent}}{=} \mathrm{h}(X) + \log |a| \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Differential entropy"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-12; URL: \url{https://en.wikipedia.org/wiki/Differential_entropy#Properties_of_differential_entropy}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Addition upon matrix multiplication}]{Addition upon matrix multiplication} \label{sec:dent-addvec}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a continuous ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $X$ increases additively when multiplied with an invertible matrix $A$:

\begin{equation} \label{eq:dent-addvec-dent-addvec}
\mathrm{h}(AX) = \mathrm{h}(X) + \log |A| \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} By definition, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $X$ is

\begin{equation} \label{eq:dent-addvec-X-dent}
\mathrm{h}(X) = - \int_{\mathcal{X}} f_X(x) \log f_X(x) \, \mathrm{d}x
\end{equation}

where $f_X(x)$ is the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ and $\mathcal{X}$ is the set of possible values of $X$.

The probability density function of a linear function of a continuous random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-linfct}) $Y = g(X) = \Sigma X + \mu$ is

\begin{equation} \label{eq:dent-addvec-pdf-linfct}
f_Y(y) = \left\{
\begin{array}{rl}
\frac{1}{\left| \Sigma \right|} f_X(\Sigma^{-1}(y-\mu)) \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y \notin \mathcal{Y}
\end{array}
\right.
\end{equation}

where $\mathcal{Y} = \left\lbrace y = \Sigma x + \mu: x \in \mathcal{X} \right\rbrace$ is the set of possible outcomes of $Y$.

Therefore, with $Y = g(X) = AX$, i.e. $\Sigma = A$ and $\mu = 0_n$, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $Y$ is given by

\begin{equation} \label{eq:dent-addvec-Y-pdf}
f_Y(y) = \left\{
\begin{array}{rl}
\frac{1}{\left| A \right|} f_X(A^{-1}y) \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y \notin \mathcal{Y}
\end{array}
\right.
\end{equation}

where $\mathcal{Y} = \left\lbrace y = A x: x \in \mathcal{X} \right\rbrace$.

Thus, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $Y$ is

\begin{equation} \label{eq:dent-addvec-Y-dent-s1}
\begin{split}
\mathrm{h}(Y) &\overset{\eqref{eq:dent-addvec-X-dent}}{=} - \int_{\mathcal{Y}} f_Y(y) \log f_Y(y) \, \mathrm{d}y \\
&\overset{\eqref{eq:dent-addvec-Y-pdf}}{=} - \int_{\mathcal{Y}} \left[ \frac{1}{\left| A \right|} f_X(A^{-1}y) \right] \log \left[ \frac{1}{\left| A \right|} f_X(A^{-1}y) \right] \, \mathrm{d}y \; .
\end{split}
\end{equation}

Substituting $y = Ax$ into the integral, we obtain

\begin{equation} \label{eq:dent-addvec-Y-dent-s2}
\begin{split}
\mathrm{h}(Y) &= - \int_{\mathcal{X}} \left[ \frac{1}{\left| A \right|} f_X(A^{-1}Ax) \right] \log \left[ \frac{1}{\left| A \right|} f_X(A^{-1}Ax) \right] \, \mathrm{d}(Ax) \\
&= - \frac{1}{\left| A \right|} \int_{\mathcal{X}} f_X(x) \log \left[ \frac{1}{\left| A \right|} f_X(x) \right] \, \mathrm{d}(Ax) \; .
\end{split}
\end{equation}

Using the differential $\mathrm{d}(Ax) = \lvert A \rvert \mathrm{d}x$, this becomes

\begin{equation} \label{eq:dent-addvec-Y-dent-s3}
\begin{split}
\mathrm{h}(Y) &= - \frac{\left| A \right|}{\left| A \right|} \int_{\mathcal{X}} f_X(x) \log \left[ \frac{1}{\left| A \right|} f_X(x) \right] \, \mathrm{d}x \\
&= - \int_{\mathcal{X}} f_X(x) \log f_X(x) \, \mathrm{d}x - \int_{\mathcal{X}} f_X(x) \log \frac{1}{\left| A \right|} \, \mathrm{d}x \; .
\end{split}
\end{equation}

Finally, employing the fact ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) that $\int_{\mathcal{X}} f_X(x) \, \mathrm{d}x = 1$, we can derive the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $Y$ as

\begin{equation} \label{eq:dent-addvec-Y-dent-s4}
\begin{split}
\mathrm{h}(Y) &= - \int_{\mathcal{X}} f_X(x) \log f_X(x) \, \mathrm{d}x + \log \left| A \right| \int_{\mathcal{X}} f_X(x) \, \mathrm{d}x \\
&\overset{\eqref{eq:dent-addvec-X-dent}}{=} \mathrm{h}(X) + \log |A| \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Cover, Thomas M. \& Thomas, Joy A. (1991): "Properties of Differential Entropy, Relative Entropy, and Mutual Information"; in: \textit{Elements of Information Theory}, sect. 8.6, p. 253; URL: \url{https://www.google.de/books/edition/Elements_of_Information_Theory/j0DBDwAAQBAJ}.
\item Wikipedia (2021): "Differential entropy"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-07; URL: \url{https://en.wikipedia.org/wiki/Differential_entropy#Properties_of_differential_entropy}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Non-invariance and transformation}]{Non-invariance and transformation} \label{sec:dent-noninv}
\setcounter{equation}{0}

\textbf{Theorem:} The differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) is not invariant under change of variables, i.e. there exist random variables $X$ and $Y = g(X)$, such that

\begin{equation} \label{eq:dent-noninv-dent-noninv}
\mathrm{h}(Y) \neq \mathrm{h}(X) \; .
\end{equation}

In particular, for an invertible transformation $g: X \rightarrow Y$ from a random vector $X$ to another random vector of the same dimension $Y$, it holds that

\begin{equation} \label{eq:dent-noninv-dent-trans}
\mathrm{h}(Y) = \mathrm{h}(X) + \int_{\mathcal{X}} f_X(x) \log \left| J_g(x) \right| \, \mathrm{d}x \; .
\end{equation}

where $J_g(x)$ is the Jacobian matrix of the vector-valued function $g$ and $\mathcal{X}$ is the set of possible values of $X$.


\vspace{1em}
\textbf{Proof:} By definition, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $X$ is

\begin{equation} \label{eq:dent-noninv-X-dent}
\mathrm{h}(X) = - \int_{\mathcal{X}} f_X(x) \log f_X(x) \, \mathrm{d}x
\end{equation}

where $f_X(x)$ is the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$.

The probability density function of an invertible function of a continuous random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-invfct}) $Y = g(X)$ is

\begin{equation} \label{eq:dent-noninv-pdf-invfct}
f_Y(y) = \left\{
\begin{array}{rl}
f_X(g^{-1}(y)) \, \left| J_{g^{-1}}(y) \right| \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y \notin \mathcal{Y}
\end{array}
\right.
\end{equation}

where $\mathcal{Y} = \left\lbrace y = g(x): x \in \mathcal{X} \right\rbrace$ is the set of possible outcomes of $Y$ and $J_{g^{-1}}(y)$ is the Jacobian matrix of $g^{-1}(y)$

\begin{equation} \label{eq:dent-noninv-jac}
J_{g^{-1}}(y) = \left[ \begin{matrix}
\frac{\mathrm{d}x_1}{\mathrm{d}y_1} & \ldots & \frac{\mathrm{d}x_1}{\mathrm{d}y_n} \\
\vdots & \ddots & \vdots \\
\frac{\mathrm{d}x_n}{\mathrm{d}y_1} & \ldots & \frac{\mathrm{d}x_n}{\mathrm{d}y_n}
\end{matrix} \right] \; .
\end{equation}

Thus, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $Y$ is

\begin{equation} \label{eq:dent-noninv-Y-dent-s1}
\begin{split}
\mathrm{h}(Y) &\overset{\eqref{eq:dent-noninv-X-dent}}{=} - \int_{\mathcal{Y}} f_Y(y) \log f_Y(y) \, \mathrm{d}y \\
&\overset{\eqref{eq:dent-noninv-pdf-invfct}}{=} - \int_{\mathcal{Y}} \left[ f_X(g^{-1}(y)) \, \left| J_{g^{-1}}(y) \right| \right] \log \left[ f_X(g^{-1}(y)) \, \left| J_{g^{-1}}(y) \right| \right] \, \mathrm{d}y \; .
\end{split}
\end{equation}

Substituting $y = g(x)$ into the integral and applying $J_{f^{-1}}(y) = J_f^{-1}(x)$, we obtain

\begin{equation} \label{eq:dent-noninv-Y-dent-s2}
\begin{split}
\mathrm{h}(Y) &= - \int_{\mathcal{X}} \left[ f_X(g^{-1}(g(x))) \, \left| J_{g^{-1}}(y) \right| \right] \log \left[ f_X(g^{-1}(g(x))) \, \left| J_{g^{-1}}(y) \right| \right] \, \mathrm{d}[g(x)] \\
&= - \int_{\mathcal{X}} \left[ f_X(x) \, \left| J_g^{-1}(x) \right| \right] \log \left[ f_X(x) \, \left| J_g^{-1}(x) \right| \right] \, \mathrm{d}[g(x)] \; .
\end{split}
\end{equation}

Using the relations $y = f(x) \Rightarrow \mathrm{d}y = \lvert J_f(x) \rvert \, \mathrm{d}x$ and $\lvert A \rvert \lvert B \rvert = \lvert AB \rvert$, this becomes

\begin{equation} \label{eq:dent-noninv-Y-dent-s3}
\begin{split}
\mathrm{h}(Y) &= - \int_{\mathcal{X}} \left[ f_X(x) \, \left| J_g^{-1}(x) \right| \left| J_g(x) \right| \right] \log \left[ f_X(x) \, \left| J_g^{-1}(x) \right| \right] \, \mathrm{d}x \\
&= - \int_{\mathcal{X}} f_X(x) \log f_X(x) \, \mathrm{d}x - \int_{\mathcal{X}} f_X(x) \log \left| J_g^{-1}(x) \right| \, \mathrm{d}x \; .
\end{split}
\end{equation}

Finally, employing the fact ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) that $\int_{\mathcal{X}} f_X(x) \, \mathrm{d}x = 1$ and the determinant property $\lvert A^{-1} \rvert = 1/\lvert A \rvert$, we can derive the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $Y$ as

\begin{equation} \label{eq:dent-noninv-Y-dent-s4}
\begin{split}
\mathrm{h}(Y) &= - \int_{\mathcal{X}} f_X(x) \log f_X(x) \, \mathrm{d}x - \int_{\mathcal{X}} f_X(x) \log \frac{1}{\left| J_g(x) \right|} \, \mathrm{d}x \\
&\overset{\eqref{eq:dent-noninv-X-dent}}{=} \mathrm{h}(X) + \int_{\mathcal{X}} f_X(x) \log \left| J_g(x) \right| \, \mathrm{d}x \; .
\end{split}
\end{equation}

Because there exist $X$ and $Y$, such that the integral term in \eqref{eq:dent-noninv-Y-dent-s4} is non-zero, this also demonstrates that there exist $X$ and $Y$, such that \eqref{eq:dent-noninv-dent-noninv} is fulfilled.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Differential entropy"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-07; URL: \url{https://en.wikipedia.org/wiki/Differential_entropy#Properties_of_differential_entropy}.
\item Bernhard (2016): "proof of upper bound on differential entropy of f(X)"; in: \textit{StackExchange Mathematics}, retrieved on 2021-10-07; URL: \url{https://math.stackexchange.com/a/1759531}.
\item peek-a-boo (2019): "How to come up with the Jacobian in the change of variables formula"; in: \textit{StackExchange Mathematics}, retrieved on 2021-08-30; URL: \url{https://math.stackexchange.com/a/3239222}.
\item Wikipedia (2021): "Jacobian matrix and determinant"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-07; URL: \url{https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant#Inverse}.
\item Wikipedia (2021): "Inverse function theorem"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-07; URL: \url{https://en.wikipedia.org/wiki/Inverse_function_theorem#Statement}.
\item Wikipedia (2021): "Determinant"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-07; URL: \url{https://en.wikipedia.org/wiki/Determinant#Properties_of_the_determinant}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Conditional differential entropy}]{Conditional differential entropy} \label{sec:dent-cond}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ and $Y$ be continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and $\mathcal{Y}$ and probability density functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) $p(x)$ and $p(y)$. Then, the conditional differential entropy of $Y$ given $X$ or, differential entropy of $Y$ conditioned on $X$, is defined as

\begin{equation} \label{eq:dent-cond-dent-cond}
\mathrm{h}(Y|X) = \int_{x \in \mathcal{X}} p(x) \cdot \mathrm{h}(Y|X=x) \, \mathrm{d}x
\end{equation}

where $\mathrm{h}(Y \vert X=x)$ is the (marginal) differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $Y$, evaluated at $x$.


\subsubsection[\textit{Joint differential entropy}]{Joint differential entropy} \label{sec:dent-joint}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ and $Y$ be continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and $\mathcal{Y}$ and joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) $p(x,y)$. Then, the joint differential entropy of $X$ and $Y$ is defined as

\begin{equation} \label{eq:dent-joint-dent-joint}
\mathrm{h}(X,Y) = - \int_{x \in \mathcal{X}} \int_{y \in \mathcal{Y}} p(x,y) \cdot \log_b p(x,y) \, \mathrm{d}y \, \mathrm{d}x
\end{equation}

where $b$ is the base of the logarithm specifying in which unit the differential entropy is determined.


\subsubsection[\textit{Differential cross-entropy}]{Differential cross-entropy} \label{sec:dent-cross}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and let $P$ and $Q$ be two probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) on $X$ with the probability density functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) $p(x)$ and $q(x)$. Then, the differential cross-entropy of $Q$ relative to $P$ is defined as

\begin{equation} \label{eq:dent-cross-dent-cross}
\mathrm{h}(P,Q) = - \int_{\mathcal{X}} p(x) \log_b q(x) \, \mathrm{d}x
\end{equation}

where $b$ is the base of the logarithm specifying in which unit the differential cross-entropy is determined.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Cross entropy"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-07-28; URL: \url{https://en.wikipedia.org/wiki/Cross_entropy#Definition}.
\end{itemize}
\vspace{1em}



\subsection{Discrete mutual information}

\subsubsection[\textit{Definition}]{Definition} \label{sec:mi}
\setcounter{equation}{0}

\textbf{Definition:}

1) The mutual information of two discrete random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ and $Y$ is defined as

\begin{equation} \label{eq:mi-mi-disc}
\mathrm{I}(X,Y) = - \sum_{x \in \mathcal{X}} \sum_{x \in \mathcal{Y}} p(x,y) \cdot \log \frac{p(x,y)}{p(x) \cdot p(y)}
\end{equation}

where $p(x)$ and $p(y)$ are the probability mass functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$ and $Y$ and $p(x,y)$ is the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) mass function of $X$ and $Y$.

2) The mutual information of two continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ and $Y$ is defined as

\begin{equation} \label{eq:mi-mi-cont}
\mathrm{I}(X,Y) = - \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \cdot \log \frac{p(x,y)}{p(x) \cdot p(y)} \, \mathrm{d}y \, \mathrm{d}x
\end{equation}

where $p(x)$ and $p(y)$ are the probability density functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$ and $Y$ and $p(x,y)$ is the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) density function of $X$ and $Y$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Cover TM, Thomas JA (1991): "Relative Entropy and Mutual Information"; in: \textit{Elements of Information Theory}, ch. 2.3/8.5, p. 20/251; URL: \url{https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relation to marginal and conditional entropy}]{Relation to marginal and conditional entropy} \label{sec:dmi-mce}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be discrete random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) $p(x,y)$ for $x \in \mathcal{X}$ and $y \in \mathcal{Y}$. Then, the mutual information ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mi}) of $X$ and $Y$ can be expressed as

\begin{equation} \label{eq:dmi-mce-dmi-mce}
\begin{split}
\mathrm{I}(X,Y) &= \mathrm{H}(X) - \mathrm{H}(X|Y) \\
&= \mathrm{H}(Y) - \mathrm{H}(Y|X)
\end{split}
\end{equation}

where $\mathrm{H}(X)$ and $\mathrm{H}(Y)$ are the marginal entropies ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) of $X$ and $Y$ and $\mathrm{H}(X \vert Y)$ and $\mathrm{H}(Y \vert X)$ are the conditional entropies ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent-cond}).


\vspace{1em}
\textbf{Proof:} The mutual information ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mi}) of $X$ and $Y$ is defined as

\begin{equation} \label{eq:dmi-mce-MI}
\mathrm{I}(X,Y) = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)\,p(y)} \; .
\end{equation}

Separating the logarithm, we have:

\begin{equation} \label{eq:dmi-mce-MI-s1}
\mathrm{I}(X,Y) = \sum_x \sum_y p(x,y) \log \frac{p(x,y)}{p(y)} - \sum_x \sum_y p(x,y) \log p(x) \; .
\end{equation}

Applying the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), i.e. $p(x,y) = p(x \vert y) \, p(y)$, we get:

\begin{equation} \label{eq:dmi-mce-MI-s2}
\mathrm{I}(X,Y) = \sum_x \sum_y p(x|y) \, p(y) \log p(x|y) - \sum_x \sum_y p(x,y) \log p(x) \; .
\end{equation}

Regrouping the variables, we have:

\begin{equation} \label{eq:dmi-mce-MI-s3}
\mathrm{I}(X,Y) = \sum_y p(y) \sum_x p(x|y) \log p(x|y) - \sum_x \left( \sum_y p(x,y) \right) \log p(x) \; .
\end{equation}

Applying the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), i.e. $p(x) = \sum_y p(x,y)$, we get:

\begin{equation} \label{eq:dmi-mce-MI-s4}
\mathrm{I}(X,Y) = \sum_y p(y) \sum_x p(x|y) \log p(x|y) - \sum_x p(x) \log p(x) \; .
\end{equation}

Now considering the definitions of marginal ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) and conditional ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent-cond}) entropy

\begin{equation} \label{eq:dmi-mce-ME-CE}
\begin{split}
\mathrm{H}(X) &= - \sum_{x \in \mathcal{X}} p(x) \log p(x) \\
\mathrm{H}(X|Y) &= \sum_{y \in \mathcal{Y}} p(y) \, \mathrm{H}(X|Y=y) \; ,
\end{split}
\end{equation}

we can finally show:

\begin{equation} \label{eq:dmi-mce-MI-qed}
\begin{split}
\mathrm{I}(X,Y) &= - \mathrm{H}(X|Y) + \mathrm{H}(X) \\
&= \mathrm{H}(X) - \mathrm{H}(X|Y) \; .
\end{split}
\end{equation}

The conditioning of $X$ on $Y$ in this proof is without loss of generality. Thus, the proof for the expression using the reverse conditional entropy of $Y$ given $X$ is obtained by simply switching $x$ and $y$ in the derivation.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Mutual information"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-13; URL: \url{https://en.wikipedia.org/wiki/Mutual_information#Relation_to_conditional_and_joint_entropy}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relation to marginal and joint entropy}]{Relation to marginal and joint entropy} \label{sec:dmi-mje}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be discrete random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) $p(x,y)$ for $x \in \mathcal{X}$ and $y \in \mathcal{Y}$. Then, the mutual information ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mi}) of $X$ and $Y$ can be expressed as

\begin{equation} \label{eq:dmi-mje-dmi-mje}
\mathrm{I}(X,Y) = \mathrm{H}(X) + \mathrm{H}(Y) - \mathrm{H}(X,Y)
\end{equation}

where $\mathrm{H}(X)$ and $\mathrm{H}(Y)$ are the marginal entropies ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) of $X$ and $Y$ and $\mathrm{H}(X,Y)$ is the joint entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent-joint}).


\vspace{1em}
\textbf{Proof:} The mutual information ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mi}) of $X$ and $Y$ is defined as

\begin{equation} \label{eq:dmi-mje-MI}
\mathrm{I}(X,Y) = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)\,p(y)} \; .
\end{equation}

Separating the logarithm, we have:

\begin{equation} \label{eq:dmi-mje-MI-s1}
\mathrm{I}(X,Y) = \sum_x \sum_y p(x,y) \log p(x,y) - \sum_x \sum_y p(x,y) \log p(x) - \sum_x \sum_y p(x,y) \log p(y) \; .
\end{equation}

Regrouping the variables, this reads:

\begin{equation} \label{eq:dmi-mje-MI-s2}
\mathrm{I}(X,Y) = \sum_x \sum_y p(x,y) \log p(x,y) - \sum_x \left( \sum_y p(x,y) \right) \log p(x) - \sum_y \left( \sum_x p(x,y) \right) \log p(y) \; .
\end{equation}

Applying the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), i.e. $p(x) = \sum_y p(x,y)$, we get:

\begin{equation} \label{eq:dmi-mje-MI-s3}
\mathrm{I}(X,Y) = \sum_x \sum_y p(x,y) \log p(x,y) - \sum_x p(x) \log p(x) - \sum_y p(y) \log p(y) \; .
\end{equation}

Now considering the definitions of marginal ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) and joint ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent-joint}) entropy

\begin{equation} \label{eq:dmi-mje-ME-JE}
\begin{split}
\mathrm{H}(X) &= - \sum_{x \in \mathcal{X}} p(x) \log p(x) \\
\mathrm{H}(X,Y) &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) \; ,
\end{split}
\end{equation}

we can finally show:

\begin{equation} \label{eq:dmi-mje-MI-qed}
\begin{split}
\mathrm{I}(X,Y) &= - \mathrm{H}(X,Y) + \mathrm{H}(X) + \mathrm{H}(Y) \\
&= \mathrm{H}(X) + \mathrm{H}(Y) - \mathrm{H}(X,Y) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Mutual information"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-13; URL: \url{https://en.wikipedia.org/wiki/Mutual_information#Relation_to_conditional_and_joint_entropy}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relation to joint and conditional entropy}]{Relation to joint and conditional entropy} \label{sec:dmi-jce}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be discrete random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) $p(x,y)$ for $x \in \mathcal{X}$ and $y \in \mathcal{Y}$. Then, the mutual information ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mi}) of $X$ and $Y$ can be expressed as

\begin{equation} \label{eq:dmi-jce-dmi-jce}
\mathrm{I}(X,Y) = \mathrm{H}(X,Y) - \mathrm{H}(X|Y) - \mathrm{H}(Y|X)
\end{equation}

where $\mathrm{H}(X,Y)$ is the joint entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent-joint}) of $X$ and $Y$ and $\mathrm{H}(X \vert Y)$ and $\mathrm{H}(Y \vert X)$ are the conditional entropies ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent-cond}).


\vspace{1em}
\textbf{Proof:} The existence of the joint probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) ensures that the mutual information ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mi}) is defined:

\begin{equation} \label{eq:dmi-jce-MI}
\mathrm{I}(X,Y) = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)\,p(y)} \; .
\end{equation}

The relation of mutual information to conditional entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dmi-mce}) is:

\begin{equation} \label{eq:dmi-jce-dmi-mce1}
\mathrm{I}(X,Y) = \mathrm{H}(X) - \mathrm{H}(X|Y)
\end{equation}

\begin{equation} \label{eq:dmi-jce-dmi-mce2}
\mathrm{I}(X,Y) = \mathrm{H}(Y) - \mathrm{H}(Y|X)
\end{equation}

The relation of mutual information to joint entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dmi-mje}) is:

\begin{equation} \label{eq:dmi-jce-dmi-mje}
\mathrm{I}(X,Y) = \mathrm{H}(X) + \mathrm{H}(Y) - \mathrm{H}(X,Y) \; .
\end{equation}

It is true that

\begin{equation} \label{eq:dmi-jce-MI-s1}
\mathrm{I}(X,Y) = \mathrm{I}(X,Y) + \mathrm{I}(X,Y) - \mathrm{I}(X,Y) \; .
\end{equation}

Plugging in \eqref{eq:dmi-jce-dmi-mce1}, \eqref{eq:dmi-jce-dmi-mce2} and \eqref{eq:dmi-jce-dmi-mje} on the right-hand side, we have

\begin{equation} \label{eq:dmi-jce-MI-s2}
\begin{split}
\mathrm{I}(X,Y) &= \mathrm{H}(X) - \mathrm{H}(X|Y) + \mathrm{H}(Y) - \mathrm{H}(Y|X) - \mathrm{H}(X) - \mathrm{H}(Y) + \mathrm{H}(X,Y) \\
&= \mathrm{H}(X,Y) - \mathrm{H}(X|Y) - \mathrm{H}(Y|X)
\end{split}
\end{equation}

which proves the identity given above.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Mutual information"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-13; URL: \url{https://en.wikipedia.org/wiki/Mutual_information#Relation_to_conditional_and_joint_entropy}.
\end{itemize}
\vspace{1em}



\subsection{Continuous mutual information}

\subsubsection[\textit{Definition}]{Definition} \label{sec:mi}
\setcounter{equation}{0}

\textbf{Definition:}

1) The mutual information of two discrete random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ and $Y$ is defined as

\begin{equation} \label{eq:mi-mi-disc}
\mathrm{I}(X,Y) = - \sum_{x \in \mathcal{X}} \sum_{x \in \mathcal{Y}} p(x,y) \cdot \log \frac{p(x,y)}{p(x) \cdot p(y)}
\end{equation}

where $p(x)$ and $p(y)$ are the probability mass functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$ and $Y$ and $p(x,y)$ is the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) mass function of $X$ and $Y$.

2) The mutual information of two continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ and $Y$ is defined as

\begin{equation} \label{eq:mi-mi-cont}
\mathrm{I}(X,Y) = - \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \cdot \log \frac{p(x,y)}{p(x) \cdot p(y)} \, \mathrm{d}y \, \mathrm{d}x
\end{equation}

where $p(x)$ and $p(y)$ are the probability density functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$ and $Y$ and $p(x,y)$ is the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) density function of $X$ and $Y$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Cover TM, Thomas JA (1991): "Relative Entropy and Mutual Information"; in: \textit{Elements of Information Theory}, ch. 2.3/8.5, p. 20/251; URL: \url{https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relation to marginal and conditional differential entropy}]{Relation to marginal and conditional differential entropy} \label{sec:cmi-mcde}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) $p(x,y)$ for $x \in \mathcal{X}$ and $y \in \mathcal{Y}$. Then, the mutual information ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mi}) of $X$ and $Y$ can be expressed as

\begin{equation} \label{eq:cmi-mcde-cmi-mcde}
\begin{split}
\mathrm{I}(X,Y) &= \mathrm{h}(X) - \mathrm{h}(X|Y) \\
&= \mathrm{h}(Y) - \mathrm{h}(Y|X)
\end{split}
\end{equation}

where $\mathrm{h}(X)$ and $\mathrm{h}(Y)$ are the marginal differential entropies ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $X$ and $Y$ and $\mathrm{h}(X \vert Y)$ and $\mathrm{h}(Y \vert X)$ are the conditional differential entropies ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent-cond}).


\vspace{1em}
\textbf{Proof:} The mutual information ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mi}) of $X$ and $Y$ is defined as

\begin{equation} \label{eq:cmi-mcde-MI}
\mathrm{I}(X,Y) = \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)\,p(y)} \, \mathrm{d}y \, \mathrm{d}x \; .
\end{equation}

Separating the logarithm, we have:

\begin{equation} \label{eq:cmi-mcde-MI-s1}
\mathrm{I}(X,Y) = \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(y)} \, \mathrm{d}y \, \mathrm{d}x - \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \log p(x) \, \mathrm{d}x \, \mathrm{d}y \; .
\end{equation}

Applying the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), i.e. $p(x,y) = p(x \vert y) \, p(y)$, we get:

\begin{equation} \label{eq:cmi-mcde-MI-s2}
\mathrm{I}(X,Y) = \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x|y) \, p(y) \log p(x|y) \, \mathrm{d}y \, \mathrm{d}x - \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \log p(x) \, \mathrm{d}y \, \mathrm{d}x \; .
\end{equation}

Regrouping the variables, we have:

\begin{equation} \label{eq:cmi-mcde-MI-s3}
\mathrm{I}(X,Y) = \int_{\mathcal{Y}} p(y) \int_{\mathcal{X}} p(x|y) \log p(x|y) \, \mathrm{d}x \, \mathrm{d}y - \int_{\mathcal{X}} \left( \int_{\mathcal{Y}} p(x,y) \, \mathrm{d}y \right) \log p(x)\, \mathrm{d}x \; .
\end{equation}

Applying the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), i.e. $p(x) = \int_{\mathcal{Y}} p(x,y) \, \mathrm{d}y$, we get:

\begin{equation} \label{eq:cmi-mcde-MI-s4}
\mathrm{I}(X,Y) = \int_{\mathcal{Y}} p(y) \int_{\mathcal{X}} p(x|y) \log p(x|y) \, \mathrm{d}x \, \mathrm{d}y - \int_{\mathcal{X}} p(x) \log p(x) \, \mathrm{d}x \; .
\end{equation}

Now considering the definitions of marginal ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) and conditional ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent-cond}) differential entropy

\begin{equation} \label{eq:cmi-mcde-MDE-CDE}
\begin{split}
\mathrm{h}(X) &= - \int_{\mathcal{X}} p(x) \log p(x) \, \mathrm{d}x \\
\mathrm{h}(X|Y) &= \int_{\mathcal{Y}} p(y) \, \mathrm{h}(X|Y=y) \, \mathrm{d}y \; ,
\end{split}
\end{equation}

we can finally show:

\begin{equation} \label{eq:cmi-mcde-MI-qed}
\mathrm{I}(X,Y) = - \mathrm{h}(X|Y) + \mathrm{h}(X) = \mathrm{h}(X) - \mathrm{h}(X|Y) \; .
\end{equation}

The conditioning of $X$ on $Y$ in this proof is without loss of generality. Thus, the proof for the expression using the reverse conditional differential entropy of $Y$ given $X$ is obtained by simply switching $x$ and $y$ in the derivation.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Mutual information"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-21; URL: \url{https://en.wikipedia.org/wiki/Mutual_information#Relation_to_conditional_and_joint_entropy}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relation to marginal and joint differential entropy}]{Relation to marginal and joint differential entropy} \label{sec:cmi-mjde}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) $p(x,y)$ for $x \in \mathcal{X}$ and $y \in \mathcal{Y}$. Then, the mutual information ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mi}) of $X$ and $Y$ can be expressed as

\begin{equation} \label{eq:cmi-mjde-cmi-mjde}
\mathrm{I}(X,Y) = \mathrm{h}(X) + \mathrm{h}(Y) - \mathrm{h}(X,Y)
\end{equation}

where $\mathrm{h}(X)$ and $\mathrm{h}(Y)$ are the marginal differential entropies ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $X$ and $Y$ and $\mathrm{h}(X,Y)$ is the joint differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent-joint}).


\vspace{1em}
\textbf{Proof:} The mutual information ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mi}) of $X$ and $Y$ is defined as

\begin{equation} \label{eq:cmi-mjde-MI}
\mathrm{I}(X,Y) = \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)\,p(y)} \, \mathrm{d}y \, \mathrm{d}x \; .
\end{equation}

Separating the logarithm, we have:

\begin{equation} \label{eq:cmi-mjde-MI-s1}
\mathrm{I}(X,Y) = \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \log p(x,y) \, \mathrm{d}y \, \mathrm{d}x - \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \log p(x) \, \mathrm{d}y \, \mathrm{d}x - \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \log p(y) \, \mathrm{d}y \, \mathrm{d}x \; .
\end{equation}

Regrouping the variables, this reads:

\begin{equation} \label{eq:cmi-mjde-MI-s2}
\mathrm{I}(X,Y) = \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \log p(x,y) \, \mathrm{d}y \, \mathrm{d}x - \int_{\mathcal{X}} \left( \int_{\mathcal{Y}} p(x,y) \, \mathrm{d}y \right) \log p(x) \, \mathrm{d}x - \int_{\mathcal{Y}} \left( \int_{\mathcal{X}} p(x,y) \, \mathrm{d}x \right) \log p(y) \, \mathrm{d}y \; .
\end{equation}

Applying the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), i.e. $p(x) = \int_{\mathcal{Y}} p(x,y)$, we get:

\begin{equation} \label{eq:cmi-mjde-MI-s3}
\mathrm{I}(X,Y) = \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \log p(x,y) \, \mathrm{d}y \, \mathrm{d}x - \int_{\mathcal{X}} p(x) \log p(x) \, \mathrm{d}x - \int_{\mathcal{Y}} p(y) \log p(y) \, \mathrm{d}y \; .
\end{equation}

Now considering the definitions of marginal ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) and joint ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent-joint}) differential entropy

\begin{equation} \label{eq:cmi-mjde-MDE-JDE}
\begin{split}
\mathrm{h}(X) &= - \int_{\mathcal{X}} p(x) \log p(x) \, \mathrm{d}x \\
\mathrm{h}(X,Y) &= - \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \log p(x,y) \, \mathrm{d}y \, \mathrm{d}x \; ,
\end{split}
\end{equation}

we can finally show:

\begin{equation} \label{eq:cmi-mjde-MI-qed}
\begin{split}
\mathrm{I}(X,Y) &= - \mathrm{h}(X,Y) + \mathrm{h}(X) + \mathrm{h}(Y) \\
&= \mathrm{h}(X) + \mathrm{h}(Y) - \mathrm{h}(X,Y) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Mutual information"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-21; URL: \url{https://en.wikipedia.org/wiki/Mutual_information#Relation_to_conditional_and_joint_entropy}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relation to joint and conditional differential entropy}]{Relation to joint and conditional differential entropy} \label{sec:cmi-jcde}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) $p(x,y)$ for $x \in \mathcal{X}$ and $y \in \mathcal{Y}$. Then, the mutual information ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mi}) of $X$ and $Y$ can be expressed as

\begin{equation} \label{eq:cmi-jcde-dmi-jce}
\mathrm{I}(X,Y) = \mathrm{h}(X,Y) - \mathrm{h}(X|Y) - \mathrm{h}(Y|X)
\end{equation}

where $\mathrm{h}(X,Y)$ is the joint differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent-joint}) of $X$ and $Y$ and $\mathrm{h}(X \vert Y)$ and $\mathrm{h}(Y \vert X)$ are the conditional differential entropies ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent-cond}).


\vspace{1em}
\textbf{Proof:} The existence of the joint probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) ensures that the mutual information ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mi}) is defined:

\begin{equation} \label{eq:cmi-jcde-MI}
\mathrm{I}(X,Y) = \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)\,p(y)} \, \mathrm{d}y \, \mathrm{d}x \; .
\end{equation}

The relation of mutual information to conditional differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cmi-mcde}) is:

\begin{equation} \label{eq:cmi-jcde-cmi-mcde1}
\mathrm{I}(X,Y) = \mathrm{h}(X) - \mathrm{h}(X|Y)
\end{equation}

\begin{equation} \label{eq:cmi-jcde-cmi-mcde2}
\mathrm{I}(X,Y) = \mathrm{h}(Y) - \mathrm{h}(Y|X)
\end{equation}

The relation of mutual information to joint differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cmi-mjde}) is:

\begin{equation} \label{eq:cmi-jcde-cmi-mjde}
\mathrm{I}(X,Y) = \mathrm{h}(X) + \mathrm{h}(Y) - \mathrm{h}(X,Y) \; .
\end{equation}

It is true that

\begin{equation} \label{eq:cmi-jcde-MI-s1}
\mathrm{I}(X,Y) = \mathrm{I}(X,Y) + \mathrm{I}(X,Y) - \mathrm{I}(X,Y) \; .
\end{equation}

Plugging in \eqref{eq:cmi-jcde-cmi-mcde1}, \eqref{eq:cmi-jcde-cmi-mcde2} and \eqref{eq:cmi-jcde-cmi-mjde} on the right-hand side, we have

\begin{equation} \label{eq:cmi-jcde-MI-s2}
\begin{split}
\mathrm{I}(X,Y) &= \mathrm{h}(X) - \mathrm{h}(X|Y) + \mathrm{h}(Y) - \mathrm{h}(Y|X) - \mathrm{h}(X) - \mathrm{h}(Y) + \mathrm{h}(X,Y) \\
&= \mathrm{h}(X,Y) - \mathrm{h}(X|Y) - \mathrm{h}(Y|X)
\end{split}
\end{equation}

which proves the identity given above.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Mutual information"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-21; URL: \url{https://en.wikipedia.org/wiki/Mutual_information#Relation_to_conditional_and_joint_entropy}.
\end{itemize}
\vspace{1em}



\subsection{Kullback-Leibler divergence}

\subsubsection[\textit{Definition}]{Definition} \label{sec:kl}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and let $P$ and $Q$ be two probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) on $X$.

1) The Kullback-Leibler divergence of $P$ from $Q$ for a discrete random variable $X$ is defined as

\begin{equation} \label{eq:kl-KL-disc}
\mathrm{KL}[P||Q] = \sum_{x \in \mathcal{X}} p(x) \cdot \log \frac{p(x)}{q(x)}
\end{equation}

where $p(x)$ and $q(x)$ are the probability mass functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $P$ and $Q$.

2) The Kullback-Leibler divergence of $P$ from $Q$ for a continuous random variable $X$ is defined as

\begin{equation} \label{eq:kl-KL-cont}
\mathrm{KL}[P||Q] = \int_{\mathcal{X}} p(x) \cdot \log \frac{p(x)}{q(x)} \, \mathrm{d}x
\end{equation}

where $p(x)$ and $q(x)$ are the probability density functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $P$ and $Q$.

By convention ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}), $0 \cdot \log 0$ is taken to be zero when calculating the divergence between $P$ and $Q$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item MacKay, David J.C. (2003): "Probability, Entropy, and Inference"; in: \textit{Information Theory, Inference, and Learning Algorithms}, ch. 2.6, eq. 2.45, p. 34; URL: \url{https://www.inference.org.uk/itprnn/book.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Non-negativity}]{Non-negativity} \label{sec:kl-nonneg}
\setcounter{equation}{0}

\textbf{Theorem:} The Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is always non-negative

\begin{equation} \label{eq:kl-nonneg-KL-nonneg}
\mathrm{KL}[P||Q] \geq 0
\end{equation}

with $\mathrm{KL}[P \vert \vert Q] = 0$, if and only if $P = Q$.


\vspace{1em}
\textbf{Proof:} The discrete Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is defined as

\begin{equation} \label{eq:kl-nonneg-KL}
\mathrm{KL}[P||Q] = \sum_{x \in \mathcal{X}} p(x) \cdot \log \frac{p(x)}{q(x)}
\end{equation}

which can be reformulated into

\begin{equation} \label{eq:kl-nonneg-KL-dev}
\mathrm{KL}[P||Q] = \sum_{x \in \mathcal{X}} p(x) \cdot \log p(x) - \sum_{x \in \mathcal{X}} p(x) \cdot \log q(x) \; .
\end{equation}

Gibbs' inequality ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gibbs-ineq}) states that the entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) of a probability distribution is always less than or equal to the cross-entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent-cross}) with another probability distribution â€“ with equality only if the distributions are identical â€“,

\begin{equation} \label{eq:kl-nonneg-Gibbs-ineq}
- \sum_{i=1}^n p(x_i) \, \log p(x_i) \leq - \sum_{i=1}^n p(x_i) \, \log q(x_i)
\end{equation}

which can be reformulated into

\begin{equation} \label{eq:kl-nonneg-Gibbs-ineq-dev}
\sum_{i=1}^n p(x_i) \, \log p(x_i) - \sum_{i=1}^n p(x_i) \, \log q(x_i) \geq 0 \; .
\end{equation}

Applying \eqref{eq:kl-nonneg-Gibbs-ineq-dev} to \eqref{eq:kl-nonneg-KL-dev}, this proves equation \eqref{eq:kl-nonneg-KL-nonneg}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Kullback-Leibler divergence"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-31; URL: \url{https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Non-negativity}]{Non-negativity} \label{sec:kl-nonneg2}
\setcounter{equation}{0}

\textbf{Theorem:} The Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is always non-negative

\begin{equation} \label{eq:kl-nonneg2-KL-nonneg}
\mathrm{KL}[P||Q] \geq 0
\end{equation}

with $\mathrm{KL}[P \vert \vert Q] = 0$, if and only if $P = Q$.


\vspace{1em}
\textbf{Proof:} The discrete Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is defined as

\begin{equation} \label{eq:kl-nonneg2-KL}
\mathrm{KL}[P||Q] = \sum_{x \in \mathcal{X}} p(x) \cdot \log \frac{p(x)}{q(x)} \; .
\end{equation}

The log sum inequality ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:logsum-ineq}) states that

\begin{equation} \label{eq:kl-nonneg2-logsum-ineq}
\sum_{i=1}^n a_i \, \log_c \frac{a_i}{b_i} \geq a \, \log_c \frac{a}{b} \; .
\end{equation}

where $a_1, \ldots, a_n$ and $b_1, \ldots, b_n$ be non-negative real numbers and $a = \sum_{i=1}^{n} a_i$ and $b = \sum_{i=1}^{n} b_i$. Because $p(x)$ and $q(x)$ are probability mass functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}), such that

\begin{equation} \label{eq:kl-nonneg2-p-q-pmf}
\begin{split}
p(x) \geq 0, \quad \sum_{x \in \mathcal{X}} p(x) &= 1 \quad \text{and} \\
q(x) \geq 0, \quad \sum_{x \in \mathcal{X}} q(x) &= 1 \; ,
\end{split}
\end{equation}

theorem \eqref{eq:kl-nonneg2-KL-nonneg} is simply a special case of \eqref{eq:kl-nonneg2-logsum-ineq}, i.e.

\begin{equation} \label{eq:kl-nonneg2-KL-nonneg-qed}
\mathrm{KL}[P||Q] \overset{\eqref{eq:kl-nonneg2-KL}}{=} \sum_{x \in \mathcal{X}} p(x) \cdot \log \frac{p(x)}{q(x)} \overset{\eqref{eq:kl-nonneg2-logsum-ineq}}{\geq} 1 \, \log \frac{1}{1} = 0 \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Log sum inequality"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-09-09; URL: \url{https://en.wikipedia.org/wiki/Log_sum_inequality#Applications}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Non-symmetry}]{Non-symmetry} \label{sec:kl-nonsymm}
\setcounter{equation}{0}

\textbf{Theorem:}  The Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is non-symmetric, i.e.

\begin{equation} \label{eq:kl-nonsymm-KL-nonsymm}
\mathrm{KL}[P||Q] \neq \mathrm{KL}[Q||P]
\end{equation}

for some probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) $P$ and $Q$.


\vspace{1em}
\textbf{Proof:} Let $X \in \mathcal{X} = \left\lbrace 0, 1, 2 \right\rbrace$ be a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) and consider the two probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist})

\begin{equation} \label{eq:kl-nonsymm-P-Q}
\begin{split}
P : \, X &\sim \mathrm{Bin}(2, 0.5) \\
Q : \, X &\sim \mathcal{U}(0, 2)
\end{split}
\end{equation}

where $\mathrm{Bin}(n, p)$ indicates a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) and $\mathcal{U}(a, b)$ indicates a discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni}).

Then, the probability mass function of the binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-pmf}) entails that

\begin{equation} \label{eq:kl-nonsymm-p(x)}
p(x) = \left\{
\begin{array}{rl}
1/4 \; , & \text{if} \; x = 0 \\
1/2 \; , & \text{if} \; x = 1 \\
1/4 \; , & \text{if} \; x = 2
\end{array}
\right.
\end{equation}

and the probability mass function of the discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni-pmf}) entails that

\begin{equation} \label{eq:kl-nonsymm-q(x)}
q(x) = \frac{1}{3} \; ,
\end{equation}

such that the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ is

\begin{equation} \label{eq:kl-nonsymm-KL-P-Q}
\begin{split}
\mathrm{KL}[P||Q] &= \sum_{x \in \mathcal{X}} p(x) \cdot \log \frac{p(x)}{q(x)} \\
&= \frac{1}{4} \log \frac{3}{4} + \frac{1}{2} \log \frac{3}{2} + \frac{1}{4} \log \frac{3}{4} \\
&= \frac{1}{2} \log \frac{3}{4} + \frac{1}{2} \log \frac{3}{2} \\
&= \frac{1}{2} \left( \log \frac{3}{4} + \log \frac{3}{2} \right) \\
&= \frac{1}{2} \log \left( \frac{3}{4} \cdot \frac{3}{2} \right) \\
&= \frac{1}{2} \log \frac{9}{8} = 0.0589
\end{split}
\end{equation}

and the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $Q$ from $P$ is

\begin{equation} \label{eq:kl-nonsymm-KL-Q-P}
\begin{split}
\mathrm{KL}[Q||P] &= \sum_{x \in \mathcal{X}} q(x) \cdot \log \frac{q(x)}{p(x)} \\
&= \frac{1}{3} \log \frac{4}{3} + \frac{1}{3} \log \frac{2}{3} + \frac{1}{3} \log \frac{4}{3} \\
&= \frac{1}{3} \left( \log \frac{4}{3} + \log \frac{2}{3} + \log \frac{4}{3} \right) \\
&= \frac{1}{3} \log \left( \frac{4}{3} \cdot \frac{2}{3} \cdot \frac{4}{3} \right) \\
&= \frac{1}{3} \log \frac{32}{27} = 0.0566
\end{split}
\end{equation}

which provides an example for

\begin{equation} \label{eq:kl-nonsymm-KL-nonsymm-qed}
\mathrm{KL}[P||Q] \neq \mathrm{KL}[Q||P]
\end{equation}

and thus proves the theorem.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Kullback, Solomon (1959): "Divergence"; in: \textit{Information Theory and Statistics}, ch. 1.3, pp. 6ff.; URL: \url{http://index-of.co.uk/Information-Theory/Information%20theory%20and%20statistics%20-%20Solomon%20Kullback.pdf}.
\item Wikipedia (2020): "Kullback-Leibler divergence"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-08-11; URL: \url{https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Basic_example}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Convexity}]{Convexity} \label{sec:kl-conv}
\setcounter{equation}{0}

\textbf{Theorem:}  The Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is convex in the pair of probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) $(p,q)$, i.e.

\begin{equation} \label{eq:kl-conv-KL-conv}
\mathrm{KL}[\lambda p_1 + (1-\lambda) p_2||\lambda q_1 + (1-\lambda) q_2] \leq \lambda \mathrm{KL}[p_1||q_1] + (1-\lambda) \mathrm{KL}[p_2||q_2]
\end{equation}

where $(p_1,q_1)$ and $(p_2,q_2)$ are two pairs of probability distributions and $0 \leq \lambda \leq 1$.


\vspace{1em}
\textbf{Proof:} The Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ is defined as

\begin{equation} \label{eq:kl-conv-KL}
\mathrm{KL}[P||Q] = \sum_{x \in \mathcal{X}} p(x) \cdot \log \frac{p(x)}{q(x)}
\end{equation}

and the log sum inequality ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:logsum-ineq}) states that

\begin{equation} \label{eq:kl-conv-logsum-ineq}
\sum_{i=1}^n a_i \log \frac{a_i}{b_i} \geq \left( \sum_{i=1}^n a_i \right) \log \frac{\sum_{i=1}^n a_i}{\sum_{i=1}^n b_i}
\end{equation}

where $a_1, \ldots, a_n$ and $b_1, \ldots, b_n$ are non-negative real numbers.

Thus, we can rewrite the KL divergence of the mixture distribution as

\begin{equation} \label{eq:kl-conv-KL-conv-qed}
\begin{split}
&\mathrm{KL}[\lambda p_1 + (1-\lambda) p_2||\lambda q_1 + (1-\lambda) q_2] \\
\overset{\eqref{eq:kl-conv-KL}}{=} &\sum_{x \in \mathcal{X}} \left[ \left[ \lambda p_1(x) + (1-\lambda) p_2(x) \right] \cdot \log \frac{\lambda p_1(x) + (1-\lambda) p_2(x)}{\lambda q_1(x) + (1-\lambda) q_2(x)} \right] \\
\overset{\eqref{eq:kl-conv-logsum-ineq}}{\leq} &\sum_{x \in \mathcal{X}} \left[ \lambda p_1(x) \cdot \log \frac{\lambda p_1(x)}{\lambda q_1(x)} + (1-\lambda) p_2(x) \cdot \log \frac{(1-\lambda) p_2(x)}{(1-\lambda) q_2(x)} \right] \\
= &\lambda \sum_{x \in \mathcal{X}} p_1(x) \cdot \log \frac{p_1(x)}{q_1(x)} + (1-\lambda) \sum_{x \in \mathcal{X}} p_2(x) \cdot \log \frac{p_2(x)}{q_2(x)} \\
\overset{\eqref{eq:kl-conv-KL}}{=} &\lambda \, \mathrm{KL}[p_1||q_1] + (1-\lambda) \, \mathrm{KL}[p_2||q_2]
\end{split}
\end{equation}

which is equivalent to \eqref{eq:kl-conv-KL-conv}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Kullback-Leibler divergence"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-08-11; URL: \url{https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties}.
\item Xie, Yao (2012): "Chain Rules and Inequalities"; in: \textit{ECE587: Information Theory}, Lecture 3, Slides 22/24; URL: \url{https://www2.isye.gatech.edu/~yxie77/ece587/Lecture3.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Additivity for independent distributions}]{Additivity for independent distributions} \label{sec:kl-add}
\setcounter{equation}{0}

\textbf{Theorem:} The Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is additive for independent distributions, i.e.

\begin{equation} \label{eq:kl-add-KL-add}
\mathrm{KL}[P||Q] = \mathrm{KL}[P_1||Q_1] + \mathrm{KL}[P_2||Q_2]
\end{equation}

where $P_1$ and $P_2$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) with the joint distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) $P$, such that $p(x,y) = p_1(x) \, p_2(y)$, and equivalently for $Q_1$, $Q_2$ and $Q$.


\vspace{1em}
\textbf{Proof:} The continuous Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is defined as

\begin{equation} \label{eq:kl-add-KL}
\mathrm{KL}[P||Q] = \int_{\mathcal{X}} p(x) \cdot \log \frac{p(x)}{q(x)} \, \mathrm{d}x
\end{equation}

which, applied to the joint distributions $P$ and $Q$, yields

\begin{equation} \label{eq:kl-add-KL-s1}
\mathrm{KL}[P||Q] = \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x,y) \cdot \log \frac{p(x,y)}{q(x,y)} \, \mathrm{d}y \, \mathrm{d}x \; .
\end{equation}

Applying $p(x,y) = p_1(x) \, p_2(y)$ and $q(x,y) = q_1(x) \, q_2(y)$, we have

\begin{equation} \label{eq:kl-add-KL-s2}
\mathrm{KL}[P||Q] = \int_{\mathcal{X}} \int_{\mathcal{Y}} p_1(x) \, p_2(y) \cdot \log \frac{p_1(x) \, p_2(y)}{q_1(x) \, q_2(y)} \, \mathrm{d}y \, \mathrm{d}x \; .
\end{equation}

Now we can separate the logarithm and evaluate the integrals:

\begin{equation} \label{eq:kl-add-KL-qed}
\begin{split}
\mathrm{KL}[P||Q] &= \int_{\mathcal{X}} \int_{\mathcal{Y}} p_1(x) \, p_2(y) \cdot \left( \log \frac{p_1(x)}{q_1(x)} + \log \frac{p_2(y)}{q_2(y)} \right) \, \mathrm{d}y \, \mathrm{d}x \\
&= \int_{\mathcal{X}} \int_{\mathcal{Y}} p_1(x) \, p_2(y) \cdot \log \frac{p_1(x)}{q_1(x)} \, \mathrm{d}y \, \mathrm{d}x + \int_{\mathcal{X}} \int_{\mathcal{Y}} p_1(x) \, p_2(y) \cdot \log \frac{p_2(y)}{q_2(y)} \, \mathrm{d}y \, \mathrm{d}x \\
&= \int_{\mathcal{X}} p_1(x) \cdot \log \frac{p_1(x)}{q_1(x)} \int_{\mathcal{Y}} p_2(y) \, \mathrm{d}y \, \mathrm{d}x + \int_{\mathcal{Y}} p_2(y) \cdot \log \frac{p_2(y)}{q_2(y)} \int_{\mathcal{X}} p_1(x) \, \mathrm{d}x \, \mathrm{d}y \\
&= \int_{\mathcal{X}} p_1(x) \cdot \log \frac{p_1(x)}{q_1(x)} \, \mathrm{d}x + \int_{\mathcal{Y}} p_2(y) \cdot \log \frac{p_2(y)}{q_2(y)} \, \mathrm{d}y \\
&\overset{\eqref{eq:kl-add-KL}}{=} \mathrm{KL}[P_1||Q_1] + \mathrm{KL}[P_2||Q_2] \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Kullback-Leibler divergence"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-31; URL: \url{https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Invariance under parameter transformation}]{Invariance under parameter transformation} \label{sec:kl-inv}
\setcounter{equation}{0}

\textbf{Theorem:} The Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is invariant under parameter transformation, i.e.

\begin{equation} \label{eq:kl-inv-KL-inv}
\mathrm{KL}[p(x)||q(x)] = \mathrm{KL}[p(y)||q(y)]
\end{equation}

where $y(x) = mx + n$ is an affine transformation of $x$ and $p(x)$ and $q(x)$ are the probability density functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of the probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) $P$ and $Q$ on the continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$.


\vspace{1em}
\textbf{Proof:} The continuous Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) (KL divergence) is defined as

\begin{equation} \label{eq:kl-inv-KL}
\mathrm{KL}[p(x)||q(x)] = \int_{a}^{b} p(x) \cdot \log \frac{p(x)}{q(x)} \, \mathrm{d}x
\end{equation}

where $a = \mathrm{min}(\mathcal{X})$ and $b = \mathrm{max}(\mathcal{X})$ are the lower and upper bound of the possible outcomes $\mathcal{X}$ of $X$.

Due to the identity of the differentials

\begin{equation} \label{eq:kl-inv-diff}
\begin{split}
p(x) \, \mathrm{d}x &= p(y) \, \mathrm{d}y \\
q(x) \, \mathrm{d}x &= q(y) \, \mathrm{d}y
\end{split}
\end{equation}

which can be rearranged into

\begin{equation} \label{eq:kl-inv-diff-dev}
\begin{split}
p(x) &= p(y) \, \frac{\mathrm{d}y}{\mathrm{d}x} \\
q(x) &= q(y) \, \frac{\mathrm{d}y}{\mathrm{d}x} \; ,
\end{split}
\end{equation}

the KL divergence can be evaluated as follows:

\begin{equation} \label{eq:kl-inv-MDE-DCE}
\begin{split}
\mathrm{KL}[p(x)||q(x)] &= \int_{a}^{b} p(x) \cdot \log \frac{p(x)}{q(x)} \, \mathrm{d}x \\
&= \int_{y(a)}^{y(b)} p(y) \, \frac{\mathrm{d}y}{\mathrm{d}x} \cdot \log \left( \frac{p(y) \, \frac{\mathrm{d}y}{\mathrm{d}x}}{q(y) \, \frac{\mathrm{d}y}{\mathrm{d}x}} \right) \, \mathrm{d}x \\
&= \int_{y(a)}^{y(b)} p(y) \cdot \log \frac{p(y)}{q(y)} \, \mathrm{d}y \\
&= \mathrm{KL}[p(y)||q(y)] \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Kullback-Leibler divergence"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-27; URL: \url{https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Properties}.
\item shimao (2018): "KL divergence invariant to affine transformation?"; in: \textit{StackExchange CrossValidated}, retrieved on 2020-05-28; URL: \url{https://stats.stackexchange.com/questions/341922/kl-divergence-invariant-to-affine-transformation}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relation to discrete entropy}]{Relation to discrete entropy} \label{sec:kl-ent}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and let $P$ and $Q$ be two probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) on $X$. Then, the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ can be expressed as

\begin{equation} \label{eq:kl-ent-kl-ent}
\mathrm{KL}[P||Q] = \mathrm{H}(P,Q) - \mathrm{H}(P)
\end{equation}

where $\mathrm{H}(P,Q)$ is the cross-entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent-cross}) of $P$ and $Q$ and $\mathrm{H}(P)$ is the marginal entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) of $P$.


\vspace{1em}
\textbf{Proof:} The discrete Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is defined as

\begin{equation} \label{eq:kl-ent-KL}
\mathrm{KL}[P||Q] = \sum_{x \in \mathcal{X}} p(x) \cdot \log \frac{p(x)}{q(x)}
\end{equation}

where $p(x)$ and $q(x)$ are the probability mass functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $P$ and $Q$.

Separating the logarithm, we have:

\begin{equation} \label{eq:kl-ent-KL-dev}
\mathrm{KL}[P||Q] = - \sum_{x \in \mathcal{X}} p(x) \, \log q(x) + \sum_{x \in \mathcal{X}} p(x) \, \log p(x) \; .
\end{equation}

Now considering the definitions of marginal entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) and cross-entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent-cross})

\begin{equation} \label{eq:kl-ent-ME-CE}
\begin{split}
\mathrm{H}(P) &= - \sum_{x \in \mathcal{X}} p(x) \, \log p(x) \\
\mathrm{H}(P,Q) &= - \sum_{x \in \mathcal{X}} p(x) \, \log q(x) \; ,
\end{split}
\end{equation}

we can finally show:

\begin{equation} \label{eq:kl-ent-KL-qed}
\mathrm{KL}[P||Q] = \mathrm{H}(P,Q) - \mathrm{H}(P) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Kullback-Leibler divergence"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-27; URL: \url{https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Motivation}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relation to differential entropy}]{Relation to differential entropy} \label{sec:kl-dent}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with possible outcomes $\mathcal{X}$ and let $P$ and $Q$ be two probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) on $X$. Then, the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ can be expressed as

\begin{equation} \label{eq:kl-dent-kl-dent}
\mathrm{KL}[P||Q] = \mathrm{h}(P,Q) - \mathrm{h}(P)
\end{equation}

where $\mathrm{h}(P,Q)$ is the differential cross-entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent-cross}) of $P$ and $Q$ and $\mathrm{h}(P)$ is the marginal differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $P$.


\vspace{1em}
\textbf{Proof:} The continuous Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is defined as

\begin{equation} \label{eq:kl-dent-KL}
\mathrm{KL}[P||Q] = \int_{\mathcal{X}} p(x) \cdot \log \frac{p(x)}{q(x)} \, \mathrm{d}x
\end{equation}

where $p(x)$ and $q(x)$ are the probability density functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $P$ and $Q$.

Separating the logarithm, we have:

\begin{equation} \label{eq:kl-dent-KL-dev}
\mathrm{KL}[P||Q] = - \int_{\mathcal{X}} p(x) \, \log q(x) \, \mathrm{d}x + \int_{\mathcal{X}} p(x) \, \log p(x) \, \mathrm{d}x \; .
\end{equation}

Now considering the definitions of marginal differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) and differential cross-entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent-cross})

\begin{equation} \label{eq:kl-dent-MDE-DCE}
\begin{split}
\mathrm{h}(P) &= - \int_{\mathcal{X}} p(x) \, \log p(x) \, \mathrm{d}x \\
\mathrm{h}(P,Q) &= - \int_{\mathcal{X}} p(x) \, \log q(x) \, \mathrm{d}x \; ,
\end{split}
\end{equation}

we can finally show:

\begin{equation} \label{eq:kl-dent-KL-qed}
\mathrm{KL}[P||Q] = \mathrm{h}(P,Q) - \mathrm{h}(P) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Kullback-Leibler divergence"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-27; URL: \url{https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Motivation}.
\end{itemize}
\vspace{1em}



\pagebreak
\section{Estimation theory}

\subsection{Point estimates}

\subsubsection[\textit{Mean squared error}]{Mean squared error} \label{sec:mse}
\setcounter{equation}{0}

\textbf{Definition:} Let $\hat{\theta}$ be an estimator of an unknown parameter $\hat{\theta}$ based on measured data $y$. Then, the mean squared error is defined as the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the squared difference between the estimated value and the true value of the parameter:

\begin{equation} \label{eq:mse-mse}
\mathrm{MSE} = \mathrm{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \theta \right)^2 \right] \; .
\end{equation}

where $\mathrm{E}_{\hat{\theta}}\left[ \cdot \right]$ is expectation calculated over all possible samples $y$ leading to values of $\hat{\theta}$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Estimator"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-03-27; URL: \url{https://en.wikipedia.org/wiki/Estimator#Mean_squared_error}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Partition of the mean squared error into bias and variance}]{Partition of the mean squared error into bias and variance} \label{sec:mse-bnv}
\setcounter{equation}{0}

\textbf{Theorem:} The mean squared error ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mse}) can be partitioned into variance and squared bias

\begin{equation} \label{eq:mse-bnv-MSE}
\mathrm{MSE}(\hat{\theta}) = \mathrm{Var}(\hat{\theta}) + \mathrm{Bias}(\hat{\theta},\theta)^2
\end{equation}

where the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is given by

\begin{equation} \label{eq:mse-bnv-Var}
\mathrm{Var}(\hat{\theta}) = \mathrm{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \mathrm{E}_{\hat{\theta}}(\hat{\theta}) \right)^2 \right]
\end{equation}

and the bias is given by

\begin{equation} \label{eq:mse-bnv-Bias}
\mathrm{Bias}(\hat{\theta},\theta) = \left( \mathrm{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The mean squared error ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mse}) (MSE) is defined as the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the squared deviation of the estimated value $\hat{\theta}$ from the true value $\theta$ of a parameter, over all values $\hat{\theta}$:

\begin{equation} \label{eq:mse-bnv-MSE-def}
\mathrm{MSE}(\hat{\theta}) = \mathrm{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \theta \right)^2 \right] \; .
\end{equation}

This formula can be evaluated in the following way:

\begin{equation} \label{eq:mse-bnv-MSE-ref1}
\begin{split}
\mathrm{MSE}(\hat{\theta}) &= \mathrm{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \theta \right)^2 \right] \\
&= \mathrm{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \mathrm{E}_{\hat{\theta}}(\hat{\theta}) + \mathrm{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right)^2 \right] \\
&= \mathrm{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \mathrm{E}_{\hat{\theta}}(\hat{\theta}) \right)^2 + 2 \left( \hat{\theta} - \mathrm{E}_{\hat{\theta}}(\hat{\theta}) \right) \left( \mathrm{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right) + \left( \mathrm{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right)^2 \right] \\
&= \mathrm{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \mathrm{E}_{\hat{\theta}}(\hat{\theta}) \right)^2 \right] + \mathrm{E}_{\hat{\theta}}\left[ 2 \left( \hat{\theta} - \mathrm{E}_{\hat{\theta}}(\hat{\theta}) \right) \left( \mathrm{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right) \right] + \mathrm{E}_{\hat{\theta}}\left[ \left( \mathrm{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right)^2 \right] \; . \\
\end{split}
\end{equation}

Because $\mathrm{E}_{\hat{\theta}}(\hat{\theta}) - \theta$ is constant as a function of $\hat{\theta}$, we have:

\begin{equation} \label{eq:mse-bnv-MSE-ref2}
\begin{split}
\mathrm{MSE}(\hat{\theta}) &= \mathrm{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \mathrm{E}_{\hat{\theta}}(\hat{\theta}) \right)^2 \right] + 2  \left( \mathrm{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right) \mathrm{E}_{\hat{\theta}}\left[ \hat{\theta} - \mathrm{E}_{\hat{\theta}}(\hat{\theta}) \right] + \left( \mathrm{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right)^2 \\
&= \mathrm{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \mathrm{E}_{\hat{\theta}}(\hat{\theta}) \right)^2 \right] + 2  \left( \mathrm{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right) \left( \mathrm{E}_{\hat{\theta}}(\hat{\theta}) - \mathrm{E}_{\hat{\theta}}(\hat{\theta}) \right) + \left( \mathrm{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right)^2 \\
&= \mathrm{E}_{\hat{\theta}}\left[ \left( \hat{\theta} - \mathrm{E}_{\hat{\theta}}(\hat{\theta}) \right)^2 \right] + \left( \mathrm{E}_{\hat{\theta}}(\hat{\theta}) - \theta \right)^2 \; . \\
\end{split}
\end{equation}

This proofs the partition given by \eqref{eq:mse-bnv-MSE}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2019): "Mean squared error"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2019-11-27; URL: \url{https://en.wikipedia.org/wiki/Mean_squared_error#Proof_of_variance_and_bias_relationship}.
\end{itemize}
\vspace{1em}



\subsection{Interval estimates}

\subsubsection[\textit{Confidence interval}]{Confidence interval} \label{sec:ci}
\setcounter{equation}{0}

\textbf{Definition:} Let $y$ be a random sample from a probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) governed by a parameter of interest $\theta$ and quantities not of interest $\varphi$. A confidence interval for $\theta$ is defined as an interval $[u(y), v(y)]$ determined by the random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $u(y)$ and $v(y)$ with the property

\begin{equation} \label{eq:ci-ci}
\mathrm{Pr}(u(y) < \theta < v(y) \, \vert \, \theta, \varphi) = \gamma \quad \text{for all} \quad (\theta, \varphi) \; .
\end{equation}

where $\gamma = 1 - \alpha$ is called the confidence level.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Confidence interval"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-03-27; URL: \url{https://en.wikipedia.org/wiki/Confidence_interval#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Construction of confidence intervals using Wilks' theorem}]{Construction of confidence intervals using Wilks' theorem} \label{sec:ci-wilks}
\setcounter{equation}{0}

\textbf{Theorem:} Let $m$ be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) for measured data $y$ with model parameters $\theta \in \Theta$, consisting of a parameter of interest $\phi \in \Phi$ and nuisance parameters $\lambda \in \Lambda$:

\begin{equation} \label{eq:ci-wilks-mod-par}
m: p(y|\theta) = \mathcal{D}(y; \theta), \quad \theta = \left\lbrace \phi, \lambda \right\rbrace \; .
\end{equation}

Further, let $\hat{\theta}$ be an estimate of $\theta$, obtained using maximum-likelihood-estimation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}):

\begin{equation} \label{eq:ci-wilks-theta-mle}
\hat{\theta} = \operatorname*{arg\,max}_{\theta} \log p(y|\theta), \quad \hat{\theta} = \left\lbrace \hat{\phi}, \hat{\lambda} \right\rbrace \; .
\end{equation}

Then, an asymptotic confidence interval ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ci}) for $\theta$ is given by

\begin{equation} \label{eq:ci-wilks-ci-wilks}
\mathrm{CI}_{1-\alpha}(\hat{\phi}) = \left\lbrace \phi \, \vert \, \log p(y|\phi,\hat{\lambda}) \geq \log p(y|\hat{\phi},\hat{\lambda}) - \frac{1}{2} \chi^2_{1,1-\alpha} \right\rbrace
\end{equation}

where $1-\alpha$ is the confidence level and $\chi^2_{1,1-\alpha}$ is the $(1-\alpha)$-quantile of the chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) with 1 degree of freedom.


\vspace{1em}
\textbf{Proof:} The confidence interval ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ci}) is defined as the interval that, under infinitely repeated random experiments ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rexp}), contains the true parameter value with a certain probability.

Let us define the likelihood ratio ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lr})

\begin{equation} \label{eq:ci-wilks-lr}
\Lambda(\phi) = \frac{p(y|\phi,\hat{\lambda})}{p(y|\hat{\phi},\hat{\lambda})} \quad \text{for all} \quad \phi \in \Phi
\end{equation}

and compute the log-likelihood ratio ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llr})

\begin{equation} \label{eq:ci-wilks-llr}
\log \Lambda(\phi) = \log p(y|\phi,\hat{\lambda}) - \log p(y|\hat{\phi},\hat{\lambda}) \; .
\end{equation}

Wilks' theorem states that, when comparing two statistical models with parameter spaces $\Theta_1$ and $\Theta_0 \subset \Theta_1$, as the sample size approaches infinity, the quantity calculated as $-2$ times the log-ratio of maximum likelihoods follows a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}), if the null hypothesis is true:

\begin{equation} \label{eq:ci-wilks-wilks}
H_0: \theta \in \Theta_0 \quad \Rightarrow \quad -2 \log \frac{\operatorname*{max}_{\theta \in \Theta_0} p(y|\theta)}{\operatorname*{max}_{\theta \in \Theta_1} p(y|\theta)} \sim \chi^2_{\Delta k}  \quad \text{as} \quad n \rightarrow \infty
\end{equation}

where $\Delta k$ is the difference in dimensionality between $\Theta_0$ and $\Theta_1$. Applied to our example in \eqref{eq:ci-wilks-llr}, we note that $\Theta_1 = \left\lbrace \phi, \hat{\phi} \right\rbrace$ and $\Theta_0 = \left\lbrace \phi \right\rbrace$, such that $\Delta k = 1$ and Wilks' theorem implies:

\begin{equation} \label{eq:ci-wilks-llr-wilks}
-2 \log \Lambda(\phi) \sim  \chi^2_1 \; .
\end{equation}

Using the quantile function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) $\chi^2_{k,p}$ of the chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}), an $(1-\alpha)$-confidence interval is therefore given by all values $\phi$ that satisfy

\begin{equation} \label{eq:ci-wilks-llr-chi2}
-2 \log \Lambda(\phi) \leq \chi^2_{1,1-\alpha} \; .
\end{equation}

Applying \eqref{eq:ci-wilks-llr} and rearranging, we can evaluate

\begin{equation} \label{eq:ci-wilks-llr-chi2-dev}
\begin{split}
-2 \left[ \log p(y|\phi,\hat{\lambda}) - \log p(y|\hat{\phi},\hat{\lambda}) \right] &\leq \chi^2_{1,1-\alpha} \\
\log p(y|\phi,\hat{\lambda}) - \log p(y|\hat{\phi},\hat{\lambda}) &\geq -\frac{1}{2} \chi^2_{1,1-\alpha} \\
\log p(y|\phi,\hat{\lambda}) &\geq \log p(y|\hat{\phi},\hat{\lambda}) - \frac{1}{2} \chi^2_{1,1-\alpha}
\end{split}
\end{equation}

which is equivalent to the confidence interval given by \eqref{eq:ci-wilks-ci-wilks}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Confidence interval"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-19; URL: \url{https://en.wikipedia.org/wiki/Confidence_interval#Methods_of_derivation}.
\item Wikipedia (2020): "Likelihood-ratio test"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-19; URL: \url{https://en.wikipedia.org/wiki/Likelihood-ratio_test#Definition}.
\item Wikipedia (2020): "Wilks' theorem"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-19; URL: \url{https://en.wikipedia.org/wiki/Wilks%27_theorem}.
\end{itemize}
\vspace{1em}



\pagebreak
\section{Frequentist statistics}

\subsection{Likelihood theory}

\subsubsection[\textit{Likelihood function}]{Likelihood function} \label{sec:lf}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) $m$ describing measured data $y$ using model parameters $\theta$. Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of the distribution of $y$ given $\theta$ is called the likelihood function of $m$:

\begin{equation} \label{eq:lf-lf}
\mathcal{L}_m(\theta) = p(y|\theta,m) = \mathcal{D}(y; \theta) \; .
\end{equation}


\subsubsection[\textit{Log-likelihood function}]{Log-likelihood function} \label{sec:llf}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) $m$ describing measured data $y$ using model parameters $\theta$. Then, the logarithm of the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of the distribution of $y$ given $\theta$ is called the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) of $m$:

\begin{equation} \label{eq:llf-llf}
\mathrm{LL}_m(\theta) = \log p(y|\theta,m) = \log \mathcal{D}(y; \theta) \; .
\end{equation}


\subsubsection[\textit{Maximum likelihood estimation}]{Maximum likelihood estimation} \label{sec:mle}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) $m$ describing measured data $y$ using model parameters $\theta$. Then, the parameter values maximizing the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) or log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) are called "maximum likelihood estimates" of $\theta$:

\begin{equation} \label{eq:mle-mle}
\hat{\theta} = \operatorname*{arg\,max}_\theta \mathcal{L}_m(\theta) = \operatorname*{arg\,max}_\theta \mathrm{LL}_m(\theta) \; .
\end{equation}

The process of calculating $\hat{\theta}$ is called "maximum likelihood estimation" and the functional form leading from $y$ to $\hat{\theta}$ given $m$ is called "maximum likelihood estimator". Maximum likelihood estimation, estimator and estimates may all be abbreviated as "MLE".


\subsubsection[\textit{Maximum log-likelihood}]{Maximum log-likelihood} \label{sec:mll}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) $m$ describing measured data $y$ using model parameters $\theta$. Then, the maximum log-likelihood (MLL) of $m$ is the maximal value of the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) of this model:

\begin{equation} \label{eq:mll-mll}
\mathrm{MLL}(m) = \operatorname*{max}_\theta \mathrm{LL}_m(\theta) \; .
\end{equation}

The maximum log-likelihood can be obtained by plugging the maximum likelihood estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) into the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}).


\subsubsection[\textbf{MLE can be biased}]{MLE can be biased} \label{sec:mle-bias}
\setcounter{equation}{0}

\textbf{Theorem:} Maximum likelihood estimation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) can result in biased estimates of model parameters, i.e. estimates whose long-term expected value is unequal to the quantities they estimate:

\begin{equation} \label{eq:mle-bias-aicc-aic}
\mathrm{E}\left[ \hat{\theta}_\mathrm{MLE} \right] = \mathrm{E}\left[ \operatorname*{arg\,max}_\theta \mathrm{LL}_m(\theta) \right] \neq \theta \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Consider a set of independent and identical normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) observations $x = \left\lbrace x_1, \ldots, x_n \right\rbrace$ with unknown mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu$ and variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) $\sigma^2$:

\begin{equation} \label{eq:mle-bias-ug}
x_i \overset{\text{i.i.d.}}{\sim} \mathcal{N}(\mu, \sigma^2), \quad i = 1,\ldots,n \; .
\end{equation}

Then, we know that the maximum likelihood estimator ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) for the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) $\sigma^2$ is underestimating the true variance of the data distribution ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:resvar-bias}):

\begin{equation} \label{eq:mle-bias-resvar-bias}
\mathrm{E}\left[ \hat{\sigma}^2_\mathrm{MLE} \right] = \frac{n-1}{n} \sigma^2 \neq \sigma^2 \; .
\end{equation}

This proofs the existence of cases such as those stated by the theorem.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textit{Likelihood ratio}]{Likelihood ratio} \label{sec:lr}
\setcounter{equation}{0}

\textbf{Definition:} Let $m_0$ and $m_1$ be two generative models ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) describing the same measured data $y$ using different model parameters $\theta_0 \in \Theta_0$ and $\theta_1 \in \Theta_1$. Then, the quotient of the maximized likelihood functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) of these two models is denoted as $\Lambda_{01}$ and is called the likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) ratio of $m_0$ relative to $m_1$:

\begin{equation} \label{eq:lr-lr}
\Lambda_{01}
= \frac{\operatorname*{max}_{\theta_0 \in \Theta_0} \mathcal{L}_{m_0}(\theta_0)}{\operatorname*{max}_{\theta_1 \in \Theta_1} \mathcal{L}_{m_1}(\theta_1)}
= \frac{p(y|\hat{\theta}_0,m_0)}{p(y|\hat{\theta}_1,m_1)} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2024): "Neyman-Pearson lemma"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2024-06-14; URL: \url{https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma#Example}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Log-likelihood ratio}]{Log-likelihood ratio} \label{sec:llr}
\setcounter{equation}{0}

\textbf{Definition:} Let $m_0$ and $m_1$ be two generative models ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) describing the same measured data $y$ using different model parameters $\theta_0 \in \Theta_0$ and $\theta_1 \in \Theta_1$. Then, the logarithmized quotient of the maximized likelihood functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) of these two models is denoted as $\log \Lambda_{01}$ and is called the log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) ratio of $m_0$ relative to $m_1$:

\begin{equation} \label{eq:llr-llr}
\log \Lambda_{01}
= \log \frac{\operatorname*{max}_{\theta_0 \in \Theta_0} \mathcal{L}_{m_0}(\theta_0)}{\operatorname*{max}_{\theta_1 \in \Theta_1} \mathcal{L}_{m_1}(\theta_1)}
= \log p(y|\hat{\theta}_0,m_0) - \log p(y|\hat{\theta}_1,m_1) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2024): "Likelihood-ratio test"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2024-06-14; URL: \url{https://en.wikipedia.org/wiki/Likelihood-ratio_test#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Method of moments}]{Method of moments} \label{sec:mome}
\setcounter{equation}{0}

\textbf{Definition:} Let measured data $y$ follow a probability distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) with probability mass ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) or probability density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) $p(y \vert \theta)$ governed by unknown parameters $\theta_1, \ldots, \theta_k$. Then, method-of-moments estimation, also referred to as "method of moments" or "matching the moments", consists in

\vspace{1em}
1) expressing the first $k$ moments ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom}) of $y$ in terms of $\theta$

\begin{equation} \label{eq:mome-mom}
\begin{split}
\mu_1 &= f_1(\theta_1, \ldots, \theta_k) \\
&\vdots \\
\mu_k &= f_k(\theta_1, \ldots, \theta_k) \; ,
\end{split}
\end{equation}

\vspace{1em}
2) calculating the first $k$ sample moments ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom}) from $y$

\begin{equation} \label{eq:mome-mom-samp}
\hat{\mu}_1(y), \ldots, \hat{\mu}_k(y)
\end{equation}

\vspace{1em}
3) and solving the system of $k$ equations

\begin{equation} \label{eq:mome-mome}
\begin{split}
\hat{\mu}_1(y) &= f_1(\hat{\theta}_1, \ldots, \hat{\theta}_k) \\
&\vdots \\
\hat{\mu}_k(y) &= f_k(\hat{\theta}_1, \ldots, \hat{\theta}_k)
\end{split}
\end{equation}

for $\hat{\theta}_1, \ldots, \hat{\theta}_k$, which are subsequently refered to as "method-of-moments estimates".

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Method of moments (statistics)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-04-29; URL: \url{https://en.wikipedia.org/wiki/Method_of_moments_(statistics)#Method}.
\end{itemize}
\vspace{1em}



\subsection{Statistical hypotheses}

\subsubsection[\textit{Statistical hypothesis}]{Statistical hypothesis} \label{sec:hyp}
\setcounter{equation}{0}

\textbf{Definition:} A statistical hypothesis is a statement about the parameters of a distribution describing a population from which observations can be sampled as measured data.

More precisely, let $m$ be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) describing measured data $y$ in terms of a distribution $\mathcal{D}(\theta)$ with model parameters $\theta \in \Theta$. Then, a statistical hypothesis is formally specified as

\begin{equation} \label{eq:hyp-hyp}
H: \; \theta \in \Theta^{*} \quad \text{where} \quad \Theta^{*} \subset \Theta \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Statistical hypothesis testing"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-19; URL: \url{https://en.wikipedia.org/wiki/Statistical_hypothesis_testing#Definition_of_terms}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Simple vs. composite}]{Simple vs. composite} \label{sec:hyp-simp}
\setcounter{equation}{0}

\textbf{Definition:} Let $H$ be a statistical hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp}). Then,

\begin{itemize}

\item $H$ is called a simple hypothesis, if it completely specifies the population distribution; in this case, the sampling distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-samp}) of the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat}) is a function of sample size alone.

\item $H$ is called a composite hypothesis, if it does not completely specify the population distribution; for example, the hypothesis may only specify one parameter of the distribution and leave others unspecified.

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Exclusion of the null hypothesis"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-19; URL: \url{https://en.wikipedia.org/wiki/Exclusion_of_the_null_hypothesis#Terminology}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Point/exact vs. set/inexact}]{Point/exact vs. set/inexact} \label{sec:hyp-point}
\setcounter{equation}{0}

\textbf{Definition:} Let $H$ be a statistical hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp}). Then,

\begin{itemize}

\item $H$ is called a point hypothesis or exact hypothesis, if it specifies an exact parameter value:

\end{itemize}

\begin{equation} \label{eq:hyp-point-hyp-point}
H: \; \theta = \theta^{*} \; ;
\end{equation}

\begin{itemize}

\item $H$ is called a set hypothesis or inexact hypothesis, if it specifies a set of possible values with more than one element for the parameter value (e.g. a range or an interval):

\end{itemize}

\begin{equation} \label{eq:hyp-point-hyp-non-point}
H: \; \theta \in \Theta^{*} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Exclusion of the null hypothesis"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-19; URL: \url{https://en.wikipedia.org/wiki/Exclusion_of_the_null_hypothesis#Terminology}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{One-tailed vs. two-tailed}]{One-tailed vs. two-tailed} \label{sec:hyp-tail}
\setcounter{equation}{0}

\textbf{Definition:} Let $H_0$ be a point ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp-point}) null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})

\begin{equation} \label{eq:hyp-tail-h0-point}
H_0: \; \theta = \theta_0 \;
\end{equation}

and consider a set ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp-point}) alternative hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}) $H_1$. Then,

\begin{itemize}

\item $H_1$ is called a left-sided one-tailed hypothesis, if $\theta$ is assumed to be smaller than $\theta_0$:

\end{itemize}

\begin{equation} \label{eq:hyp-tail-h1-tail1-left}
H_1: \; \theta < \theta_0 \; ;
\end{equation}

\begin{itemize}

\item $H_1$ is called a right-sided one-tailed hypothesis, if $\theta$ is assumed to be larger than $\theta_0$:

\end{itemize}

\begin{equation} \label{eq:hyp-tail-h1-tail1-right}
H_1: \; \theta > \theta_0 \; ;
\end{equation}

\begin{itemize}

\item $H_1$ is called a two-tailed hypothesis, if $\theta$ is assumed to be unequal to $\theta_0$:

\end{itemize}

\begin{equation} \label{eq:hyp-tail-h1-tail2}
H_1: \; \theta \neq \theta_0 \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "One- and two-tailed tests"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-31; URL: \url{https://en.wikipedia.org/wiki/One-_and_two-tailed_tests}.
\end{itemize}
\vspace{1em}



\subsection{Hypothesis testing}

\subsubsection[\textit{Statistical test}]{Statistical test} \label{sec:test}
\setcounter{equation}{0}

\textbf{Definition:} Let $y$ be a set of measured data. Then, a statistical hypothesis test consists of the following:

\begin{itemize}

\item an assumption about the distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) of the data, often expressed in terms of a statistical model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) $m$;

\item a null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) $H_0$ and an alternative hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}) $H_1$ which make specific statements about the distribution of the data; 

\item a test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat}) $T(Y)$ which is a function of the data and whose distribution under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) is known;

\item a significance level ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:alpha}) $\alpha$ which imposes an upper bound on the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of rejecting $H_0$, given that $H_0$ is true.

\end{itemize}

Procedurally, the statistical hypothesis test works as follows:

\begin{itemize}

\item Given the null hypothesis $H_0$ and the significance level $\alpha$, a critical value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cval}) $t_\mathrm{crit}$ is determined which partitions the set of possible values of $T(Y)$ into "acceptance region" and "rejection region".

\item Then, the observed test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat}) $t_\mathrm{obs} = T(y)$ is calculated from the actually measured data $y$. If it is in the rejection region, $H_0$ is rejected in favor of $H_1$. Otherwise, the test fails to reject $H_0$.

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Statistical hypothesis testing"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-19; URL: \url{https://en.wikipedia.org/wiki/Statistical_hypothesis_testing#The_testing_process}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Null hypothesis}]{Null hypothesis} \label{sec:h0}
\setcounter{equation}{0}

\textbf{Definition:} The statement which is tested in a statistical hypothesis test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}) is called the "null hypothesis", denoted as $H_0$. The test is designed to assess the strength of evidence against $H_0$ and possibly reject it. The opposite of $H_0$ is called the "alternative hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1})". Usually, $H_0$ is a statement that a particular parameter is zero, that there is no effect of a particular treatment or that there is no difference between particular conditions.

More precisely, let $m$ be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) describing measured data $y$ using model parameters $\theta \in \Theta$. Then, a null hypothesis is formally specified as

\begin{equation} \label{eq:h0-h0}
H_0: \; \theta \in \Theta_0 \quad \text{where} \quad \Theta_0 \subset \Theta \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Exclusion of the null hypothesis"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-12; URL: \url{https://en.wikipedia.org/wiki/Exclusion_of_the_null_hypothesis#Basic_definitions}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Alternative hypothesis}]{Alternative hypothesis} \label{sec:h1}
\setcounter{equation}{0}

\textbf{Definition:} Let $H_0$ be a null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) of a statistical hypothesis test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}). Then, the corresponding alternative hypothesis, denoted as $H_1$, is either the negation of $H_0$ or an interesting sub-case in the negation of $H_0$, depending on context. The test is designed to assess the strength of evidence against $H_0$ and possibly reject it in favor of $H_1$. Usually, $H_1$ is a statement that a particular parameter is non-zero, that there is an effect of a particular treatment or that there is a difference between particular conditions.

More precisely, let $m$ be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) describing measured data $y$ using model parameters $\theta \in \Theta$. Then, null and alternative hypothesis are formally specified as

\begin{equation} \label{eq:h1-h0}
\begin{split}
H_0&: \; \theta \in \Theta_0 \quad \text{where} \quad \Theta_0 \subset \Theta \\
H_1&: \; \theta \in \Theta_1 \quad \text{where} \quad \Theta_1 = \Theta \setminus \Theta_0 \; .
\end{split}
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Exclusion of the null hypothesis"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-12; URL: \url{https://en.wikipedia.org/wiki/Exclusion_of_the_null_hypothesis#Basic_definitions}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{One-tailed vs. two-tailed}]{One-tailed vs. two-tailed} \label{sec:test-tail}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a statistical test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}) of an alternative hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}) $H_1$ against a null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) $H_0$. Then,

\begin{itemize}

\item the test is called a one-tailed test, if $H_1$ is a one-tailed hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp-tail});

\item the test is called a two-tailed test, if $H_1$ is a two-tailed hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp-tail}).

\end{itemize}

The fact whether a test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}) is one-tailed or two-tailed has consequences for the computation of critical value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cval}) and p-value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pval}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "One- and two-tailed tests"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-31; URL: \url{https://en.wikipedia.org/wiki/One-_and_two-tailed_tests}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Test statistic}]{Test statistic} \label{sec:tstat}
\setcounter{equation}{0}

\textbf{Definition:} In a statistical hypothesis test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}), the test statistic $T(Y)$ is a scalar function of the measured data $y$ whose distribution under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) $H_0$ can be established. Together with a significance level ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:alpha}) $\alpha$, this distribution implies a critical value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cval}) $t_\mathrm{crit}$ of the test statistic which determines whether the test rejects or fails to reject $H_0$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Statistical hypothesis testing"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-19; URL: \url{https://en.wikipedia.org/wiki/Statistical_hypothesis_testing#The_testing_process}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Size of a test}]{Size of a test} \label{sec:size}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a statistical hypothesis test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}) with null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) $H_0$. Then, the size of the test is the probability of a false-positive result or making a type I error, i.e. the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of rejecting the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) $H_0$, given that $H_0$ is actually true.

For a simple null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp-simp}), the size is determined by the following conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}):

\begin{equation} \label{eq:size-size-h0-simp}
\mathrm{Pr}(\text{test rejects } H_0 \vert H_0) \; .
\end{equation}

For a composite null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp-simp}), the size is the supremum over all possible realizations of the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}):

\begin{equation} \label{eq:size-size-h0-comp}
\operatorname*{sup}_{h \in H_0} \mathrm{Pr}(\text{test rejects } H_0 \vert h) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Size (statistics)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-19; URL: \url{https://en.wikipedia.org/wiki/Size_(statistics)}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Power of a test}]{Power of a test} \label{sec:power}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a statistical hypothesis test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}) with null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) $H_0$ and alternative hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}) $H_1$. Then, the power of the test is the probability of a true-positive result or not making a type II error, i.e. the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of rejecting $H_0$, given that $H_1$ is actually true.

For given null ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) and alternative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}) hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp}), the size is determined by the following conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}):

\begin{equation} \label{eq:power-power}
\mathrm{Pr}(\text{test rejects } H_0 \vert H_1) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Power of a test"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-31; URL: \url{https://en.wikipedia.org/wiki/Power_of_a_test#Description}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Significance level}]{Significance level} \label{sec:alpha}
\setcounter{equation}{0}

\textbf{Definition:} Let the size ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:size}) of a statistical hypothesis test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}) be the probability of a false-positive result or making a type I error, i.e. the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of rejecting the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) $H_0$, given that $H_0$ is actually true:

\begin{equation} \label{eq:alpha-size}
\mathrm{Pr}(\text{test rejects } H_0 \vert H_0) \; .
\end{equation}

Then, the test is said to have significance level $\alpha$, if the size is less than or equal to $\alpha$:

\begin{equation} \label{eq:alpha-alpha}
\mathrm{Pr}(\text{test rejects } H_0 \vert H_0) \leq \alpha \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Statistical hypothesis testing"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-19; URL: \url{https://en.wikipedia.org/wiki/Statistical_hypothesis_testing#Definition_of_terms}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Critical value}]{Critical value} \label{sec:cval}
\setcounter{equation}{0}

\textbf{Definition:} In a statistical hypothesis test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}), the critical value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cval}) $t_\mathrm{crit}$ is that value of the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat}) $T(Y)$ which partitions the set of possible test statistics into "acceptance region" and "rejection region" based on a significance level ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:alpha}) $\alpha$. Put differently, if the observed test statistic $t_\mathrm{obs} = T(y)$ computed from actually measured data $y$ is as extreme or more extreme than the critical value, the test rejects the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) $H_0$ in favor of the alternative hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Statistical hypothesis testing"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-19; URL: \url{https://en.wikipedia.org/wiki/Statistical_hypothesis_testing#Definition_of_terms}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{p-value}]{p-value} \label{sec:pval}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a statistical test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}) of the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) $H_0$ and the alternative hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}) $H_1$ using the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat}) $T(Y)$. Let $y$ be the measured data and let $t_\mathrm{obs} = T(y)$ be the observed test statistic computed from $y$. Moreover, assume that $F_T(t)$ is the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) (CDF) of the distribution of $T(Y)$ under $H_0$.

Then, the p-value is the probability of obtaining a test statistic more extreme than or as extreme as $t_\mathrm{obs}$, given that the null hypothesis $H_0$ is true:

\begin{itemize}

\item $p = F_T(t_\mathrm{obs})$, if $H_1$ is a left-sided one-tailed hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp-tail});

\item $p = 1 - F_T(t_\mathrm{obs})$, if $H_1$ is a right-sided one-tailed hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp-tail});

\item $p = 2 \cdot \min \left( \left[ F_T(t_\mathrm{obs}), \, 1 - F_T(t_\mathrm{obs}) \right] \right)$, if $H_1$ is a two-tailed hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp-tail}).

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Statistical hypothesis testing"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-19; URL: \url{https://en.wikipedia.org/wiki/Statistical_hypothesis_testing#Definition_of_terms}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Distribution of p-value under null hypothesis}]{Distribution of p-value under null hypothesis} \label{sec:pval-h0}
\setcounter{equation}{0}

\textbf{Theorem:} Under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}), the p-value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pval}) in a statistical test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}) follows a continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}):

\begin{equation} \label{eq:pval-h0-pval-h0}
p \sim \mathcal{U}(0,1) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Without loss of generality, consider a left-sided one-tailed hypothesis test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp-tail}). Then, the p-value is a function of the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pval})

\begin{equation} \label{eq:pval-h0-pval}
\begin{split}
P &= F_T(T) \\
p &= F_T(t_\mathrm{obs})
\end{split}
\end{equation}

where $t_\mathrm{obs}$ is the observed test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat}) and $F_T(t)$ is the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat}) under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}).

Then, we can obtain the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of the p-value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pval}) as

\begin{equation} \label{eq:pval-h0-pval-cdf-s1}
\begin{split}
F_P(p) &= \mathrm{Pr}(P < p) \\
&= \mathrm{Pr}(F_T(T) < p) \\
&= \mathrm{Pr}(T < F_T^{-1}(p)) \\
&= F_T(F_T^{-1}(p)) \\
&= p
\end{split}
\end{equation}

which is the cumulative distribution function of a continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-cdf}) over the interval $[0,1]$:

\begin{equation} \label{eq:pval-h0-cuni-cdf}
F_X(x) = \int_{-\infty}^{x} \mathcal{U}(z; 0, 1) \, \mathrm{d}z = x \quad \text{where} \quad 0 \leq x \leq 1 \; .
\end{equation}

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item jll (2018): "Why are p-values uniformly distributed under the null hypothesis?"; in: \textit{StackExchange CrossValidated}, retrieved on 2022-03-18; URL: \url{https://stats.stackexchange.com/a/345763/270304}.
\end{itemize}
\vspace{1em}



\pagebreak
\section{Bayesian statistics}

\subsection{Probabilistic modeling}

\subsubsection[\textit{Generative model}]{Generative model} \label{sec:gm}
\setcounter{equation}{0}

\textbf{Definition:} Consider measured data $y$ and some unknown latent parameters $\theta$. A statement about the distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) of $y$ given $\theta$ is called a generative model $m$

\begin{equation} \label{eq:gm-gm}
m: \, y \sim \mathcal{D}(\theta) \; ,
\end{equation}

where $\mathcal{D}$ denotes an arbitrary probability distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) and $\theta$ are the parameters of this distribution.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Friston et al. (2008): "Bayesian decoding of brain images"; in: \textit{NeuroImage}, vol. 39, pp. 181-205; URL: \url{https://www.sciencedirect.com/science/article/abs/pii/S1053811907007203}; DOI: 10.1016/j.neuroimage.2007.08.013.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Likelihood function}]{Likelihood function} \label{sec:lf}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) $m$ describing measured data $y$ using model parameters $\theta$. Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of the distribution of $y$ given $\theta$ is called the likelihood function of $m$:

\begin{equation} \label{eq:lf-lf}
\mathcal{L}_m(\theta) = p(y|\theta,m) = \mathcal{D}(y; \theta) \; .
\end{equation}


\subsubsection[\textit{Prior distribution}]{Prior distribution} \label{sec:prior}
\setcounter{equation}{0}

\textbf{Definition:} Consider measured data $y$ and some unknown latent parameters $\theta$. A distribution of $\theta$ unconditional on $y$ is called a prior distribution:

\begin{equation} \label{eq:prior-prior}
\theta \sim \mathcal{D}(\lambda) \; .
\end{equation}

The parameters $\lambda$ of this distribution are called the prior hyperparameters and the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) is called the prior density:

\begin{equation} \label{eq:prior-prior-pdf}
p(\theta|m) = \mathcal{D}(\theta; \lambda) \; .
\end{equation}


\subsubsection[\textit{Full probability model}]{Full probability model} \label{sec:fpm}
\setcounter{equation}{0}

\textbf{Definition:} Consider measured data $y$ and some unknown latent parameters $\theta$. The combination of a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) for $y$ and a prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) on $\theta$ is called a full probability model $m$:

\begin{equation} \label{eq:fpm-fpm}
m: \, y \sim \mathcal{D}(\theta), \, \theta \sim \mathcal{D}(\lambda) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Gelman A, Carlin JB, Stern HS, Dunson DB, Vehtari A, Rubin DB (2014): "Probability and inference"; in: \textit{Bayesian Data Analysis}, ch. 1, p. 3; URL: \url{http://www.stat.columbia.edu/~gelman/book/}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Joint likelihood}]{Joint likelihood} \label{sec:jl}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) $m$ describing measured data $y$ using model parameters $\theta$ and a prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) on $\theta$. Then, the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) of $y$ and $\theta$ is called the joint likelihood:

\begin{equation} \label{eq:jl-jl}
p(y,\theta|m) = p(y|\theta,m) \, p(\theta|m) \; .
\end{equation}


\subsubsection[\textbf{Joint likelihood is product of likelihood and prior}]{Joint likelihood is product of likelihood and prior} \label{sec:jl-lfnprior}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) $m$ describing measured data $y$ using model parameters $\theta$ and a prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) on $\theta$. Then, the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) is equal to the product of likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) and prior density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}):

\begin{equation} \label{eq:jl-lfnprior-jl}
p(y,\theta|m) = p(y|\theta,m) \, p(\theta|m) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) is defined as the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) of data $y$ and parameters $\theta$:

\begin{equation} \label{eq:jl-lfnprior-jl-def}
p(y,\theta|m) \; .
\end{equation}

Applying the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), we have:

\begin{equation} \label{eq:jl-lfnprior-jl-qed}
\begin{split}
p(y|\theta,m) &= \frac{p(y,\theta|m)}{p(\theta|m)} \\
&\Leftrightarrow \\
p(y,\theta|m) &= p(y|\theta,m) \, p(\theta|m) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textit{Posterior distribution}]{Posterior distribution} \label{sec:post}
\setcounter{equation}{0}

\textbf{Definition:} Consider measured data $y$ and some unknown latent parameters $\theta$. The distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) of $\theta$ conditional ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-cond}) on $y$ is called the posterior distribution:

\begin{equation} \label{eq:post-post}
\theta|y \sim \mathcal{D}(\phi) \; .
\end{equation}

The parameters $\phi$ of this distribution are called the posterior hyperparameters and the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) is called the posterior density:

\begin{equation} \label{eq:post-prior-pdf}
p(\theta|y,m) = \mathcal{D}(\theta; \phi) \; .
\end{equation}


\subsubsection[\textit{Maximum-a-posteriori estimation}]{Maximum-a-posteriori estimation} \label{sec:map}
\setcounter{equation}{0}

\textbf{Definition:} Consider a posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) of an unknown parameter $\theta$, given measured data $y$, parametrized by posterior hyperparameters $\phi$:

\begin{equation} \label{eq:map-post}
\theta|y \sim \mathcal{D}(\phi) \; .
\end{equation}

Then, the value of $\theta$ at which the posterior density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) attains its maximum is called the "maximum-a-posteriori estimate", "MAP estimate" or "posterior mode" of $\theta$:

\begin{equation} \label{eq:map-prior-pdf}
\hat{\theta}_\mathrm{MAP} = \operatorname*{arg\,max}_\theta \mathcal{D}(\theta; \phi) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2023): "Maximum a posteriori estimation"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2023-12-01; URL: \url{https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation#Description}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Posterior density is proportional to joint likelihood}]{Posterior density is proportional to joint likelihood} \label{sec:post-jl}
\setcounter{equation}{0}

\textbf{Theorem:} In a full probability model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fpm}) $m$ describing measured data $y$ using model parameters $\theta$, the posterior density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) over the model parameters is proportional to the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}):

\begin{equation} \label{eq:post-jl-post}
p(\theta|y,m) \propto p(y,\theta|m) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} In a full probability model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fpm}), the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) can be expressed using Bayes' theorem ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:bayes-th}):

\begin{equation} \label{eq:post-jl-post-s1}
p(\theta|y,m) = \frac{p(y|\theta,m) \, p(\theta|m)}{p(y|m)} \; .
\end{equation}

Applying the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}) to the numerator, we have:

\begin{equation} \label{eq:post-jl-post-s2}
p(\theta|y,m) = \frac{p(y,\theta|m)}{p(y|m)} \; .
\end{equation}

Because the denominator does not depend on $\theta$, it is constant in $\theta$ and thus acts a proportionality factor between the posterior distribution and the joint likelihood:

\begin{equation} \label{eq:post-jl-post-qed}
p(\theta|y,m) \propto p(y,\theta|m) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Combined posterior distribution from independent data}]{Combined posterior distribution from independent data} \label{sec:post-ind}
\setcounter{equation}{0}

\textbf{Theorem:} Let $p(\theta \vert y_1)$ and $p(\theta \vert y_2)$ be posterior distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}), obtained using the same prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) from conditionally independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind-cond}) data sets $y_1$ and $y_2$:

\begin{equation} \label{eq:post-ind-ind-cond}
p(y_1,y_2|\theta) = p(y_1|\theta) \cdot p(y_2|\theta) \; .
\end{equation}

Then, the combined posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) is proportional to the product of the individual posterior densities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}), divided by the prior density:

\begin{equation} \label{eq:post-ind-post-ind}
p(\theta|y_1,y_2) \propto \frac{p(\theta|y_1) \cdot p(\theta|y_2)}{p(\theta)} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Since $p(\theta \vert y_1)$ and $p(\theta \vert y_2)$ are posterior distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}), Bayes' theorem ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:bayes-th}) holds for them:

\begin{equation} \label{eq:post-ind-bayes-th}
\begin{split}
p(\theta|y_1) &= \frac{p(y_1|\theta) \cdot p(\theta)}{p(y_1)} \\
p(\theta|y_2) &= \frac{p(y_2|\theta) \cdot p(\theta)}{p(y_2)} \; .
\end{split}
\end{equation}

Moreover, Bayes' theorem must also hold for the combined posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl}):

\begin{equation} \label{eq:post-ind-post-ind-s1}
p(\theta|y_1,y_2) = \frac{p(y_1,y_2|\theta) \cdot p(\theta)}{p(y_1,y_2)} \; .
\end{equation}

With that, we can express the combined posterior distribution as follows:

\begin{equation} \label{eq:post-ind-post-ind-s2}
\begin{split}
p(\theta|y_1,y_2) &\overset{\eqref{eq:post-ind-post-ind-s1}}{=} \frac{p(y_1,y_2|\theta) \cdot p(\theta)}{p(y_1,y_2)} \\
&\overset{\eqref{eq:post-ind-ind-cond}}{=} p(y_1|\theta) \cdot p(y_2|\theta) \cdot \frac{p(\theta)}{p(y_1,y_2)} \\
&\overset{\eqref{eq:post-ind-bayes-th}}{=} \frac{p(\theta|y_1) \cdot p(y_1)}{p(\theta)} \cdot \frac{p(\theta|y_2) \cdot p(y_2)}{p(\theta)} \cdot \frac{p(\theta)}{p(y_1,y_2)} \\
&= \frac{p(\theta|y_1) \cdot p(\theta|y_2)}{p(\theta)} \cdot \frac{p(y_1) \cdot p(y_2)}{p(y_1,y_2)} \; .
\end{split}
\end{equation}

Note that the second fraction does not depend on $\theta$ and thus, the posterior distribution over $\theta$ is proportional to the first fraction:

\begin{equation} \label{eq:post-ind-post-ind-s3}
p(\theta|y_1,y_2) \propto \frac{p(\theta|y_1) \cdot p(\theta|y_2)}{p(\theta)} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textit{Marginal likelihood}]{Marginal likelihood} \label{sec:ml}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) $m$ describing measured data $y$ using model parameters $\theta$ and a prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) on $\theta$. Then, the marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}) distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) of $y$ across the parameter space $\Theta$ is called the marginal likelihood:

\begin{equation} \label{eq:ml-ml}
p(y|m) = \int_{\Theta} p(y,\theta|m) \, \mathrm{d}\theta \; .
\end{equation}


\subsubsection[\textbf{Marginal likelihood is integral of joint likelihood}]{Marginal likelihood is integral of joint likelihood} \label{sec:ml-jl}
\setcounter{equation}{0}

\textbf{Theorem:} In a full probability model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fpm}) $m$ describing measured data $y$ using model parameters $\theta$, the marginal likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml}) is the integral of the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) across the parameter space $\Theta$

\begin{equation} \label{eq:ml-jl-ml-jl}
p(y|m) = \int_{\Theta} p(y,\theta|m) \, \mathrm{d}\theta
\end{equation}

and related to likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) and prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) as follows:

\begin{equation} \label{eq:ml-jl-ml-lf}
p(y|m) = \int_{\Theta} p(y|\theta,m) \, p(\theta|m) \, \mathrm{d}\theta \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} In a full probability model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fpm}), the marginal likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml}) is defined as the marginal probability of the data $y$, given only the model $m$:

\begin{equation} \label{eq:ml-jl-ml-def}
p(y|m) \; .
\end{equation}

Using the law of marginal probabililty ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), this can be obtained by integrating the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) function over the entire parameter space:

\begin{equation} \label{eq:ml-jl-ml-jl-qed}
p(y|m) = \int_{\Theta} p(y,\theta|m) \, \mathrm{d}\theta \; .
\end{equation}

Applying the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), the integrand can also be written as the product of likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) and prior density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}):

\begin{equation} \label{eq:ml-jl-ml-lf-qed}
p(y|m) = \int_{\Theta} p(y|\theta,m) \, p(\theta|m) \, \mathrm{d}\theta \; .
\end{equation}

\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{Prior distributions}

\subsubsection[\textit{Flat vs. hard vs. soft}]{Flat vs. hard vs. soft} \label{sec:prior-flat}
\setcounter{equation}{0}

\textbf{Definition:} Let $p(\theta \vert m)$ be a prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) for the parameter $\theta$ of a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) $m$. Then,

\begin{itemize}

\item the distribution is called a "flat prior", if its precision ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prec}) is zero or variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is infinite;

\item the distribution is called a "hard prior", if its precision ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prec}) is infinite or variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is zero;

\item the distribution is called a "soft prior", if its precision ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prec}) and variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) are non-zero and finite.

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Friston et al. (2002): "Classical and Bayesian Inference in Neuroimaging: Theory"; in: \textit{NeuroImage}, vol. 16, iss. 2, pp. 465-483, fn. 1; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811902910906}; DOI: 10.1006/nimg.2002.1090.
\item Friston et al. (2002): "Classical and Bayesian Inference in Neuroimaging: Applications"; in: \textit{NeuroImage}, vol. 16, iss. 2, pp. 484-512, fn. 10; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811902910918}; DOI: 10.1006/nimg.2002.1091.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Uniform vs. non-uniform}]{Uniform vs. non-uniform} \label{sec:prior-uni}
\setcounter{equation}{0}

\textbf{Definition:} Let $p(\theta \vert m)$ be a prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) for the parameter $\theta$ of a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) $m$ where $\theta$ belongs to the parameter space $\Theta$. Then,

\begin{itemize}

\item the distribution is called a "uniform prior", if its density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) or mass ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) is constant over $\Theta$;

\item the distribution is called a "non-uniform prior", if its density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) or mass ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) is not constant over $\Theta$.

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Lindley's paradox"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-25; URL: \url{https://en.wikipedia.org/wiki/Lindley%27s_paradox#Bayesian_approach}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Informative vs. non-informative}]{Informative vs. non-informative} \label{sec:prior-inf}
\setcounter{equation}{0}

\textbf{Definition:} Let $p(\theta \vert m)$ be a prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) for the parameter $\theta$ of a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) $m$. Then,

\begin{itemize}

\item the distribution is called an "informative prior", if it biases the parameter towards particular values;

\item the distribution is called a "weakly informative prior", if it mildly influences the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl});

\item the distribution is called a "non-informative prior", if it does not influence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl}) the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}).

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Soch J, Allefeld C, Haynes JD (2016): "How to avoid mismodelling in GLM-based fMRI data analysis: cross-validated Bayesian model selection"; in: \textit{NeuroImage}, vol. 141, pp. 469-489, eq. 15, p. 473; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811916303615}; DOI: 10.1016/j.neuroimage.2016.07.047.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Empirical vs. non-empirical}]{Empirical vs. non-empirical} \label{sec:prior-emp}
\setcounter{equation}{0}

\textbf{Definition:} Let $p(\theta \vert m)$ be a prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) for the parameter $\theta$ of a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) $m$. Then,

\begin{itemize}

\item the distribution is called an "empirical prior", if it has been derived from empirical data ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl});

\item the distribution is called a "theoretical prior", if it was specified without regard to empirical data.

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Soch J, Allefeld C, Haynes JD (2016): "How to avoid mismodelling in GLM-based fMRI data analysis: cross-validated Bayesian model selection"; in: \textit{NeuroImage}, vol. 141, pp. 469-489, eq. 13, p. 473; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811916303615}; DOI: 10.1016/j.neuroimage.2016.07.047.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Conjugate vs. non-conjugate}]{Conjugate vs. non-conjugate} \label{sec:prior-conj}
\setcounter{equation}{0}

\textbf{Definition:} Let $m$ be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) with likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) $p(y \vert \theta, m)$ and prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) $p(\theta \vert m)$. Then,

\begin{itemize}

\item the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) is called "conjugate", if it, when combined with the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}), leads to a posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) that belongs to the same family of probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist});

\item the prior distribution is called "non-conjugate", if this is not the case.

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Conjugate prior"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-12-02; URL: \url{https://en.wikipedia.org/wiki/Conjugate_prior}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Maximum entropy priors}]{Maximum entropy priors} \label{sec:prior-maxent}
\setcounter{equation}{0}

\textbf{Definition:} Let $m$ be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) with likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) $p(y \vert \theta, m)$ and prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) $p(\theta \vert \lambda, m)$ using prior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) $\lambda$. Then, the prior distribution is called a "maximum entropy prior", if

1) when $\theta$ is a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}), it maximizes the entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) of the prior probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}):

\begin{equation} \label{eq:prior-maxent-prior-maxent-disc}
\lambda_{\mathrm{maxent}} = \operatorname*{arg\,max}_{\lambda} \mathrm{H}\left[ p(\theta \vert \lambda, m) \right] \; ;
\end{equation}

2) when $\theta$ is a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}), it maximizes the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of the prior probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}):

\begin{equation} \label{eq:prior-maxent-prior-maxent-cont}
\lambda_{\mathrm{maxent}} = \operatorname*{arg\,max}_{\lambda} \mathrm{h}\left[ p(\theta \vert \lambda, m) \right] \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Prior probability"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-12-02; URL: \url{https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Empirical Bayes priors}]{Empirical Bayes priors} \label{sec:prior-eb}
\setcounter{equation}{0}

\textbf{Definition:} Let $m$ be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) with likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) $p(y \vert \theta, m)$ and prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) $p(\theta \vert \lambda, m)$ using prior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) $\lambda$. Let $p(y \vert \lambda, m)$ be the marginal likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml}) when integrating the parameters out of the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml-jl}). Then, the prior distribution is called an "Empirical Bayes ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:eb}) prior", if it maximizes the logarithmized marginal likelihood:

\begin{equation} \label{eq:prior-eb-prior-eb}
\lambda_{\mathrm{EB}} = \operatorname*{arg\,max}_{\lambda} \log p(y \vert \lambda, m) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Empirical Bayes method"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-12-02; URL: \url{https://en.wikipedia.org/wiki/Empirical_Bayes_method#Introduction}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Reference priors}]{Reference priors} \label{sec:prior-ref}
\setcounter{equation}{0}

\textbf{Definition:} Let $m$ be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) with likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) $p(y \vert \theta, m)$ and prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) $p(\theta \vert \lambda, m)$ using prior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) $\lambda$. Let $p(\theta \vert y, \lambda, m)$ be the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) that is proportional to the the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl}). Then, the prior distribution is called a "reference prior", if it maximizes the expected ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of the posterior distribution relative to the prior distribution:

\begin{equation} \label{eq:prior-ref-prior-ref}
\lambda_{\mathrm{ref}} = \operatorname*{arg\,max}_{\lambda} \left\langle \mathrm{KL} \left[ p(\theta \vert y, \lambda, m) \, || \, p(\theta \vert \lambda, m) \right] \right\rangle \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Prior probability"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-12-02; URL: \url{https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors}.
\end{itemize}
\vspace{1em}



\subsection{Bayesian inference}

\subsubsection[\textbf{Bayes' theorem}]{Bayes' theorem} \label{sec:bayes-th}
\setcounter{equation}{0}

\textbf{Theorem:} Let $A$ and $B$ be two arbitrary statements about random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}), such as statements about the presence or absence of an event or about the value of a scalar, vector or matrix. Then, the conditional probability that $A$ is true, given that $B$ is true, is equal to

\begin{equation} \label{eq:bayes-th-BT}
p(A|B) = \frac{p(B|A) \, p(A)}{p(B)} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}) is defined as the ratio of joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}), i.e. the probability of both statements being true, and marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), i.e. the probability of only the second one being true:

\begin{equation} \label{eq:bayes-th-LCP}
p(A|B) = \frac{p(A,B)}{p(B)} \; .
\end{equation}

It can also be written down for the reverse situation, i.e. to calculate the probability that $B$ is true, given that $A$ is true:

\begin{equation} \label{eq:bayes-th-LCP-rev}
p(B|A) = \frac{p(A,B)}{p(A)} \; .
\end{equation}

Both equations can be rearranged for the joint probability

\begin{equation} \label{eq:bayes-th-JP}
p(A|B) \, p(B) \overset{\eqref{eq:bayes-th-LCP}}{=} p(A,B) \overset{\eqref{eq:bayes-th-LCP-rev}}{=} p(B|A) \, p(A)
\end{equation}

from which Bayes' theorem can be directly derived:

\begin{equation} \label{eq:bayes-th-BT-proof}
p(A|B) \overset{\eqref{eq:bayes-th-JP}}{=} \frac{p(B|A) \, p(A)}{p(B)} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Koch, Karl-Rudolf (2007): "Rules of Probability"; in: \textit{Introduction to Bayesian Statistics}, Springer, Berlin/Heidelberg, 2007, pp. 6/13, eqs. 2.12/2.38; URL: \url{https://www.springer.com/de/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Bayes' rule}]{Bayes' rule} \label{sec:bayes-rule}
\setcounter{equation}{0}

\textbf{Theorem:} Let $A_1$, $A_2$ and $B$ be arbitrary statements about random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) where $A_1$ and $A_2$ are mutually exclusive. Then, Bayes' rule states that the posterior odds are equal to the Bayes factor ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:bf}) times the prior odds, i.e.

\begin{equation} \label{eq:bayes-rule-bayes-rule}
\frac{p(A_1|B)}{p(A_2|B)} = \frac{p(B|A_1)}{p(B|A_2)} \cdot \frac{p(A_1)}{p(A_2)} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Using Bayes' theorem ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:bayes-th}), the conditional probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}) on the left are given by

\begin{equation} \label{eq:bayes-rule-bayes-th-A1}
p(A_1|B) = \frac{p(B|A_1) \cdot p(A_1)}{p(B)}
\end{equation}

\begin{equation} \label{eq:bayes-rule-bayes-th-A2}
p(A_2|B) = \frac{p(B|A_2) \cdot p(A_2)}{p(B)} \; .
\end{equation}

Dividing the two conditional probabilities by each other

\begin{equation} \label{eq:bayes-rule-bayes-rule-qed}
\begin{split}
\frac{p(A_1|B)}{p(A_2|B)} &= \frac{p(B|A_1) \cdot p(A_1) / p(B)}{p(B|A_2) \cdot p(A_2) / p(B)} \\
&= \frac{p(B|A_1)}{p(B|A_2)} \cdot \frac{p(A_1)}{p(A_2)} \; ,
\end{split}
\end{equation}

one obtains the posterior odds ratio as given by the theorem.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2019): "Bayes' theorem"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-06; URL: \url{https://en.wikipedia.org/wiki/Bayes%27_theorem#Bayes%E2%80%99_rule}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Empirical Bayes}]{Empirical Bayes} \label{sec:eb}
\setcounter{equation}{0}

\textbf{Definition:} Let $m$ be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) with model parameters $\theta$ and hyper-parameters $\lambda$ implying the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) $p(y \vert \theta, \lambda, m)$ and prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) $p(\theta \vert \lambda, m)$. Then, an Empirical Bayes treatment of $m$, also referred to as "type II maximum likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle})" or "evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) approximation", consists in

\vspace{1em}
1) evaluating the marginal likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml}) of the model $m$

\begin{equation} \label{eq:eb-ML}
p(y \vert \lambda, m) = \int p(y \vert \theta, \lambda, m) \, (\theta \vert \lambda, m) \, \mathrm{d}\theta \; ,
\end{equation}

\vspace{1em}
2) maximizing the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) with respect to $\lambda$

\begin{equation} \label{eq:eb-EB}
\hat{\lambda} = \operatorname*{arg\,max}_{\lambda} \log p(y \vert \lambda, m)
\end{equation}

\vspace{1em}
3) and using the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) at this maximum

\begin{equation} \label{eq:eb-prior-eb}
p(\theta \vert m) = p(\theta \vert \hat{\lambda}, m)
\end{equation}

for Bayesian inference ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:bayes-th}), i.e. obtaining the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl}) and computing the marginal likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml-jl}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Empirical Bayes method"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-04-29; URL: \url{https://en.wikipedia.org/wiki/Empirical_Bayes_method#Introduction}.
\item Bishop CM (2006): "The Evidence Approximation"; in: \textit{Pattern Recognition for Machine Learning}, ch. 3.5, pp. 165-172; URL: \url{https://www.springer.com/gp/book/9780387310732}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Variational Bayes}]{Variational Bayes} \label{sec:vb}
\setcounter{equation}{0}

\textbf{Definition:} Let $m$ be a generative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:gm}) with model parameters $\theta$ implying the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) $p(y \vert \theta, m)$ and prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) $p(\theta \vert m)$. Then, a Variational Bayes treatment of $m$, also referred to as "approximate inference" or "variational inference", consists in

\vspace{1em}
1) constructing an approximate posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post})

\begin{equation} \label{eq:vb-post-vb}
q(\theta) \approx p(\theta \vert y, m) \; ,
\end{equation}

\vspace{1em}
2) evaluating the variational free energy ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:vblme})

\begin{equation} \label{eq:vb-FE}
F_q(m) = \int q(\theta) \log p(y|\theta,m) \, \mathrm{d}\theta - \int q(\theta) \frac{q(\theta)}{p(\theta|m)} \, \mathrm{d}\theta
\end{equation}

\vspace{1em}
3) and maximizing this function with respect to $q(\theta)$

\begin{equation} \label{eq:vb-VB}
\hat{q}(\theta) = \operatorname*{arg\,max}_{q} F_q(m) \; .
\end{equation}

for Bayesian inference ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:bayes-th}), i.e. obtaining the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) (from eq. \eqref{eq:vb-VB}) and approximating the marginal likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml}) (by plugging eq. \eqref{eq:vb-VB} into eq. \eqref{eq:vb-FE}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Variational Bayesian methods"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-04-29; URL: \url{https://en.wikipedia.org/wiki/Variational_Bayesian_methods#Evidence_lower_bound}.
\item Penny W, Flandin G, Trujillo-Barreto N (2007): "Bayesian Comparison of Spatially Regularised General Linear Models"; in: \textit{Human Brain Mapping}, vol. 28, pp. 275â€“293, eqs. 2-9; URL: \url{https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.20327}; DOI: 10.1002/hbm.20327.
\end{itemize}
\vspace{1em}



\pagebreak
\section{Machine learning}

\subsection{Scoring rules}

\subsubsection[\textit{Scoring rule}]{Scoring rule} \label{sec:sr}
\setcounter{equation}{0}

\textbf{Definition:} A scoring rule is any extended real-valued function $\mathbf{S}: \mathcal{Q} \times \Omega \rightarrow \mathbb {R}$ where $\mathcal{Q}$ is a family of probability distributions over the space $\Omega$, such that $\mathbf{S}(Q, \cdot) $ is $\mathcal{Q}$-quasi-integrable for all $Q \in \mathcal{Q}$. Output of the function $\mathbf{S}(Q, y)$ represents the loss or penalty when the forecast $Q \in \mathcal{Q}$ is issued and the observation $y \in \Omega$ is realized.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item BÃ¡lint MucsÃ¡nyi, Michael Kirchhof, Elisa Nguyen, Alexander Rubinstein, Seong Joon Oh (2023): "Proper/Strictly Proper Scoring Rule"; in: \textit{Trustworthy Machine Learning}; URL: \url{https://trustworthyml.io/}; DOI: 10.48550/arXiv.2310.08215.
\item Wikipedia (2024): "Scoring rule"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2024-02-28; URL: \url{https://en.wikipedia.org/wiki/Scoring_rule    }.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Proper scoring rule}]{Proper scoring rule} \label{sec:psr}
\setcounter{equation}{0}

\textbf{Definition:} A scoring rule ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:sr}) $\mathbf{S}$ is called a proper scoring rule, if and only if 

\begin{equation} \label{eq:psr-psr}
\max_{Q \in \mathcal{Q}} \mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] =  \mathbb{E}_{Y \sim P}[\mathbf{S}(P, Y)] \; .
\end{equation}

In other words, score function $\mathbf{S}$ is a proper scoring rule, if it is maximized when the forecaster gives exactly the ground truth distribution $P(Y)$ as its probabilistic forecast $Q \in \mathcal{Q}$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item BÃ¡lint MucsÃ¡nyi, Michael Kirchhof, Elisa Nguyen, Alexander Rubinstein, Seong Joon Oh (2023): "Proper/Strictly Proper Scoring Rule"; in: \textit{Trustworthy Machine Learning}; URL: \url{https://trustworthyml.io/}; DOI: 10.48550/arXiv.2310.08215.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Strictly proper scoring rule}]{Strictly proper scoring rule} \label{sec:spsr}
\setcounter{equation}{0}

\textbf{Definition:} A scoring rule ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:sr}) $\mathbf{S}$ is called a strictly proper scoring rule, if and only if

\begin{itemize}

\item $\mathbf{S}$ is a proper scoring rule ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:psr}), and

\item $\operatorname*{arg\,max}\_{Q \in \mathcal{Q}} \mathbb{E}\_{Y \sim P}[\mathbf{S}(Q, Y)] = P$ is the unique maximizer of $\mathbf{S}$ in $Q$.

\end{itemize}

In other words, a strictly proper scoring rule is maximized only when the the forecaster gives exactly the ground truth distribution $P(Y)$ as its probabilistic forecast $Q \in \mathcal{Q}$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item BÃ¡lint MucsÃ¡nyi, Michael Kirchhof, Elisa Nguyen, Alexander Rubinstein, Seong Joon Oh (2023): "Proper/Strictly Proper Scoring Rule"; in: \textit{Trustworthy Machine Learning}; URL: \url{https://trustworthyml.io/}; DOI: 10.48550/arXiv.2310.08215.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Log probability scoring rule}]{Log probability scoring rule} \label{sec:lpsr}
\setcounter{equation}{0}

\textbf{Definition:} A log (probability) scoring rule ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:sr}) $S(q, y)$ is as a scoring rule that measures the quality of a probabilistic forecast in decision theory. Formally, it can be defined in discrete or continuous form as follows:

1) Log scoring rule for binary classification:

\begin{equation} \label{eq:lpsr-binary-lpsr-cases}
S(q, y) = \left\{
\begin{array}{rl}
\log q \; , & \text{if} \; y = 1 \\
\log(1-q) \; , & \text{if} \; y = 0
\end{array}
\right.
\end{equation}

which can be expressed as

\begin{equation} \label{eq:lpsr-binary-lpsr}
S(q, y) = y \log q + (1-y) \log (1-q)
\end{equation}

Note that the expressions given above have slightly different domains. For the first equation, the domain is $D_1 = ([0,1) \times \left\lbrace 0 \right\rbrace) \cup ((0, 1] \times \left\lbrace 1 \right\rbrace)$, while for the second equation, the domain is $D_2 = (0,1) \times \left\lbrace 0,1 \right\rbrace$.

2) Log scoring rule for multiclass classification:

\begin{equation} \label{eq:lpsr-multiclass-lpsr}
S(q, y) = \sum_k y_k \log q_k(x) = \log q_{y^*}(x)
\end{equation}

where $y^*$ is the true class and $q$ is the predicted probability distribution over the classes. We have $y_k = 1$, if the true class is $k$ and $y_k = 0$ otherwise.

3) Log scoring rule for regression (continuous case):

\begin{equation} \label{eq:lpsr-regression-lpsr}
S(q, y) = \log q(y)
\end{equation}

where $q$ is the predicted probability distribution over the continuous space and $y$ is the true value.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item BÃ¡lint MucsÃ¡nyi, Michael Kirchhof, Elisa Nguyen, Alexander Rubinstein, Seong Joon Oh (2023): "Proper/Strictly Proper Scoring Rule"; in: \textit{Trustworthy Machine Learning}; URL: \url{https://trustworthyml.io/}; DOI: 10.48550/arXiv.2310.08215.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Log probability is strictly proper scoring rule}]{Log probability is strictly proper scoring rule} \label{sec:lpsr-spsr}
\setcounter{equation}{0}

\textbf{Theorem:} The log (probability) scoring rule ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lpsr}) is a strictly proper scoring rule ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:spsr}).

\vspace{1em}
\textbf{Proof:} We will show that all versions of the log probability scoring rule (binary/multiclass/regression) are strictly proper scoring rules.

1) Binary log probability scoring rule:

\begin{equation}
\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] = P(Y = 1) \log q + P(Y = 0) \log (1 - q)
\end{equation}

Let $p$ be the true probability of the event $Y = 1$. Then, the expected score is:

\begin{equation}
\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] = p \log q + (1 - p) \log (1 - q)
\end{equation}

To find the maxima, take the derivative with respect to $q$ and set it to zero:

\begin{equation}
\begin{split}
\frac{\partial}{\partial q}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &= \frac{p}{q} - \frac{1-p}{1 - q} \\
0 &= \frac{p  - pq - q + pq}{q (1-q)} \\
0 &= \frac{p - q}{q (1-q)} \\
\Rightarrow p - q &= 0 \\
\Rightarrow p &= q
\end{split}
\end{equation}

Now, we need to check the second derivative to see, if it is a maximum for the properness condition and if it is the only maximizer for the strictness condition:

\begin{equation}
\begin{split}
\frac{\partial^2}{\partial q^2}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &= -\frac{p}{q^2} - \frac{1-p}{(1 - q)^2} \\
&= - \left( \underbrace{\frac{p}{q^2}}_\textrm{> 0} + \underbrace{\frac{1-p}{(1 - q)^2}}_\textrm{> 0} \right) < 0
\end{split}
\end{equation}

Except for the cases $q=0$ and $q=1$, the second derivative is always negative, which means that the function is concave and the maximum is unique. For $q = 1$, maximum is achieved only if $p = 1$, and similarly for $q = 0$, maximum is achieved only if $p = 0$. Therefore, $p = q$ is the only maximizer and the log probability scoring rule for binary classification is strictly proper. 


2a) Multiclass log probability scoring rule (Proof 1):

\begin{equation}
S(q, y) = \sum_k^K y_k \log q_k(x)
\end{equation}

Let $p_k$ be the true probability of the event $Y = k$. Since $q_k$ is the predicted probability for class $k$, we know that $\sum_i q_i = 1$. Then, the expected score is:

\begin{equation}
\begin{split}
\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &= \sum_k P(Y = k|x) \log(q_k(x)) \\
&= p_1 \log(q_1(x)) + p_2\log(q_2(x)) + ... + p_K \log(q_K(x)) \\
&= p_1 \log(q_1(x)) + p_2\log(q_2(x)) + ... + p_K \log(1 - \sum_{i \neq K} q_i(x))
\end{split}
\end{equation}

Taking the derivative with respect to $q_j$ and setting it to zero:

\begin{equation}
\begin{split}
\frac{\partial}{\partial q_j}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &= \frac{p_j}{q_j} -\frac{p_K}{1- \sum_{i \neq K} q_i(x)} \\
0 &= \frac{p_j}{q_j} - \frac{p_K}{q_K} \\
\Rightarrow \frac{p_j}{q_j} &= \frac{p_K}{q_K}
\end{split}
\end{equation}

This equality holds for any $j$:

\begin{equation}
\frac{p_1}{q_1} = \frac{p_2}{q_2} = ... = \frac{p_K}{q_K} = \lambda
\end{equation}

Each $q_i$ can be represented as a constant multiple of $p_i$ as follows: $q_i = \lambda \ p_i$

\begin{equation}
\begin{split}
\sum_i q_i &= 1 \\
\sum_i \lambda \ p_i &= 1 \\
\lambda \sum_i p_i &= 1 \\
\lambda = 1
\end{split}
\end{equation}

Since $\lambda = 1$, we have $p_i = q_i$ for all $i$. Now, we need to check the second derivative to see, if it is a maximum for the properness condition and if it is the only maximizer for the strictness condition:

\begin{equation}
\begin{split}
\frac{\partial^2}{\partial q_j^2}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &= -\frac{p_j}{q_j^2} - \frac{p_K}{(1- \sum_{i \neq K} q_i(x))^2}  \\
&= - \left( \underbrace{\frac{p_j}{q_j^2}}_\textrm{> 0} + \underbrace{\frac{p_K}{(1- \sum_{i \neq K} q_i(x))^2}}_\textrm{> 0} \right) < 0
\end{split}
\end{equation}

Except for the cases $q_j=0$ and $q_j=1$, the second derivative is always negative, which means that the function is concave and the maximum is unique. For $q_j = 1$, maximum is achieved only if $p_j = 1$, and similarly for $q_j = 0$ maximum is achieved only if $p_j = 0$. Therefore, $p_j = q_j$ is the only maximizer and the log probability scoring rule for multiclass classification is strictly proper.


2b) Multiclass log probability scoring rule (Proof 2):

Alternatively, we can solve the optimization problem with Lagrange multipliers. The Lagrangian is:

\begin{equation}
\mathcal{L}(q, \lambda) = \sum_k P(Y = k|x) \log(q_k(x)) + \lambda \left(1 - \sum_k q_k(x)\right)
\end{equation}

Taking the derivative with respect to $q_j$ and setting it to zero:

\begin{equation}
\begin{split}
\frac{\partial}{\partial q_j}\mathcal{L}(q, \lambda) &= \frac{p_j}{q_j} - \lambda \\
0 &= \frac{p_j}{q_j} - \lambda \\
\Rightarrow \frac{p_j}{q_j} &= \lambda
\end{split}
\end{equation}

The rest of the proof follows as in the first proof.


3) Continuous log probability scoring rule:

\begin{equation}
S(q, y) = \log q(y)
\end{equation}

Let $p(y)$ be the true probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of the event $Y = y$. Then, the expected ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) score is:

\begin{equation}
\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] = \int p(y) \log q(y) dy
\end{equation}

Let $X = \frac{q(y)}{p(y)}$ and $\phi = \log(\cdot)$ (a concave function). By Jensen's inequality, we know that $f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]$, if $f$ is concave. Therefore, we have:

\begin{equation}
\begin{split}
\int p(y) \log \frac{q(y)}{p(y)} dy &\leq \log \int p(y)\frac{q(y)}{p(y)} dy \\
\int p(y) \log \frac{q(y)}{p(y)} dy &\leq \log \int q(y) dy \\
\int p(y) \log \frac{q(y)}{p(y)} dy &\leq \log(1) \\
\int p(y) \log \frac{q(y)}{p(y)} dy &\leq 0
\end{split}
\end{equation}

The same result can be obtained by using the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}). The Kullback-Leibler divergence is always non-negative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl-nonneg2}), therefore $\text{E} - \text{CE} = \text{KL} \geq 0$. The resulting expression is $-\text{KL}$, which is always non-positive. It is maximized only when $q(y) = p(y)$ which means that the log probability scoring rule for continuous classification is strictly proper.

An alternative argument for uniqueness of the maximum point can be proposed as follows: $\int p(y) \log \frac{q(y)}{p(y)} dy$ can be equal to $0$ in two cases: Either $\frac{q(y)}{p(y)}$ is equal to $1$ for each value or the expression $\log ( \frac{q(y)}{p(y)})$ takes positive and negative values summing up to $0$ at the end. The second case cannot occur, because it means that there exists a $y_0$ such that $q(y_0) > p(y_0)$, implying that Jensenâ€™s inequality is violated. Therefore, the maximum is achieved, if and only if $q = p$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item BÃ¡lint MucsÃ¡nyi, Michael Kirchhof, Elisa Nguyen, Alexander Rubinstein, Seong Joon Oh (2023): "Proper/Strictly Proper Scoring Rule"; in: \textit{Trustworthy Machine Learning}; URL: \url{https://trustworthyml.io/}; DOI: 10.48550/arXiv.2310.08215.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Brier scoring rule}]{Brier scoring rule} \label{sec:bsr}
\setcounter{equation}{0}

\textbf{Definition:} A Brier scoring rule ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:sr}) $S(q, y)$ is as a scoring rule that measures the quality of a probabilistic forecast in decision theory. Formally, it can be defined for binary or multiclass classification as follows:

1) Brier scoring rule for binary classification:

\begin{equation} \label{eq:bsr-binary-bsr}
S(q, y) = -(q - y)^2
\end{equation}

$q$ represents the predicted probability of the positive class ($Y = 1$) and $y$ is the true class label. Since we want the output of the scoring rule to be maximized when the predicted probability is close to the true class label, we use the negative of the squared difference between the predicted probability and the true class label.

2) Brier scoring rule for multiclass classification:

\begin{equation} \label{eq:bsr-multiclass-bsr}
S(q, y) = -\sum_k (q_k - y_k)^2 = -(q_{y^*} - 1)^2 -\sum_{k \neq y^*} q_k^2
\end{equation}

where $q_k$ is the predicted probability of class $k$ and $y^*$ is the true class label. Similar to the log probability scoring rule, we have $y_k = 1$, if the true class is $k$ and $y_k = 0$ otherwise.

3) Regression (continuous case):

Although there is no direct version of Brier score for regression, we can use the squared error loss as a scoring rule for regression problems as well.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item BÃ¡lint MucsÃ¡nyi, Michael Kirchhof, Elisa Nguyen, Alexander Rubinstein, Seong Joon Oh (2023): "Proper/Strictly Proper Scoring Rule"; in: \textit{Trustworthy Machine Learning}; URL: \url{https://trustworthyml.io/}; DOI: 10.48550/arXiv.2310.08215.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Brier scoring rule is strictly proper scoring rule}]{Brier scoring rule is strictly proper scoring rule} \label{sec:bsr-spsr}
\setcounter{equation}{0}

\textbf{Theorem:} The brier scoring rule ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:bsr}) is a strictly proper scoring rule ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:spsr}).

\vspace{1em}
\textbf{Proof:} We will show that both versions of the brier scoring rule (binary/multiclass) are strictly proper scoring rules.

1) Brier scoring rule for binary classification:

\begin{equation} \label{eq:bsr-spsr-binary-bsr-s1}
\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] = - P(Y = 1) (q - 1)^2 + P(Y = 0) -q^2
\end{equation}

Let $p$ be the true probability of the event $Y = 1$. Then, the expected score is:

\begin{equation} \label{eq:bsr-spsr-binary-bsr-s2}
\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] = - p (q - 1)^2 - (1 - p) q^2
\end{equation}

To find the maxima, take the derivative with respect to $q$ and set it to zero:

\begin{equation}
\begin{split}
\frac{\partial}{\partial q}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &= -2p(q - 1) - 2(1 - p)q \\
&= -2pq + 2p - 2q + 2pq \\
&= 2p - 2q \\
0 &= 2p - 2q \\
\Rightarrow p &= q
\end{split}
\end{equation}

We need to check the second derivative to see if it is a maximum (for the properness condition) and if it is the only maximizer (for the strictness condition):

\begin{equation}
\begin{split}
\frac{\partial^2}{\partial q^2}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &= -2 < 0 \\
\end{split}
\end{equation}

The second derivative is always negative which means that the function is concave and the maximum is unique. Therefore, $p = q$ is the only maximizer and the Brier scoring rule for binary classification is strictly proper.


2) Brier scoring rule for multiclass classification:

\begin{equation}
\begin{split}
\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &= \sum_k P(Y = k) \bigg[ -\sum_i (q_i - y_i)^2 \bigg]\\
&= \sum_k P(Y = k) \bigg[  -(q_{k} - 1)^2 -\sum_{i \neq k} q_i^2 \bigg] \\
&= \sum_k P(Y = k) \bigg[  -(q_{k} - 1)^2 + q_k^2 -\sum_{i} q_i^2 \bigg] \\
&= \sum_k P(Y = k) \bigg[  -q_{k}^2 - 1 + 2q_k + q_k^2 -\sum_{i} q_i^2 \bigg] \\
&= \sum_k P(Y = k) \bigg[  2q_k - 1 -\sum_{i} q_i^2 \bigg] \\
&= \sum_k   P(Y = k)(2q_k - 1) - \sum_k P(Y = k) \bigg(\sum_i q_i^2\bigg) \\
&= \sum_k  P(Y = k)(2q_k - 1) - \sum_i q_i^2 \bigg(\underbrace{\sum_k P(Y = k)}_\textrm{1}\bigg) \\
&= \sum_k  P(Y = k)(2q_k - 1) - \sum_i q_i^2 \\
&= \sum_k  P(Y = k)(2q_k - 1)  - q_k^2
\end{split}
\end{equation}

Similar to what we did for log probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lpsr-spsr}), this expression can be expressed as follows (replacing $q_K$ with $1 - \sum_{i \neq K} q_i$):

\begin{equation}
\begin{split}
\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &= p_1(2q_1 - 1) - q_1^2 + p_2(2q_2 - 1) - q_2^2 + \ldots + p_K(2q_K - 1) - q_K^2 \\
&= p_1(2q_1 - 1) -q_1^2 + p_2(2q_2 - 1) -q_2^2 + \ldots + p_K \left( 1 - 2\sum_{i \neq K} q_i \right) - \left( 1 - \sum_{i \neq K} q_i \right)^2
\end{split}
\end{equation}

Taking the derivative with respect to $q_j$ and setting it to zero, we obtain:

\begin{equation}
\begin{split}
\frac{\partial}{\partial q_j}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] &= 2p_j - 2q_j - 2p_K + 2 \left( 1 - \sum_{i \neq K} q_i \right) \\
&= 2p_j - 2q_j - 2p_K + 2 q_K \\
&= (p_j - q_j) + (q_K - p_K) \\
(p_j - q_j) &= (p_K - q_K) \\
\end{split}
\end{equation}

We know that $\sum_i p_i = 1$ and $\sum_i q_i = 1$, therefore:

\begin{equation}
\begin{split}
p_1 - q_1 = p_2 - q_2 &= \ldots = p_K - q_K = \lambda \\
\sum_i p_i - q_i &= K \cdot \lambda = 0 \\  
\Rightarrow \lambda &= 0 \quad \text{since} \; K \neq 0 \\
\Rightarrow p_i &= q_i \quad \text{for all} \; i = 1, \ldots, K \\
\end{split}
\end{equation}

Now, we need to check the second derivative to see, if it is a maximum for the properness condition and if it is the only maximizer for the strictness condition:

\begin{equation}
\frac{\partial^2}{\partial q_j^2}\mathbb{E}_{Y \sim P}[\mathbf{S}(Q, Y)] = - 2 - 2 = -4 < 0 \\
\end{equation}

The second derivative is always negative which means that the function is concave and the maximum is unique. Therefore, $p = q$ is the only maximizer and the Brier scoring rule for multiclass classification is strictly proper.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item BÃ¡lint MucsÃ¡nyi, Michael Kirchhof, Elisa Nguyen, Alexander Rubinstein, Seong Joon Oh (2023): "Proper/Strictly Proper Scoring Rule"; in: \textit{Trustworthy Machine Learning}; URL: \url{https://trustworthyml.io/}; DOI: 10.48550/arXiv.2310.08215.
\end{itemize}
\vspace{1em}





% Chapter 2 %
\chapter{Probability Distributions} \label{sec:Probability Distributions} \newpage

\pagebreak
\section{Univariate discrete distributions}

\subsection{Discrete uniform distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:duni}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, $X$ is said to be uniformly distributed with minimum $a$ and maximum $b$

\begin{equation} \label{eq:duni-duni}
X \sim \mathcal{U}(a, b) \; ,
\end{equation}

if and only if each integer between and including $a$ and $b$ occurs with the same probability.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Discrete uniform distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-07-28; URL: \url{https://en.wikipedia.org/wiki/Discrete_uniform_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability mass function}]{Probability mass function} \label{sec:duni-pmf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni}):

\begin{equation} \label{eq:duni-pmf-duni}
X \sim \mathcal{U}(a, b) \; .
\end{equation}

Then, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$ is

\begin{equation} \label{eq:duni-pmf-duni-pmf}
f_X(x) = \frac{1}{b-a+1} \quad \text{where} \quad x \in \left\lbrace a, a+1, \ldots, b-1, b \right\rbrace \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} A discrete uniform variable is defined as ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni}) having the same probability for each integer between and including $a$ and $b$. The number of integers between and including $a$ and $b$ is

\begin{equation} \label{eq:duni-pmf-n}
n = b - a + 1
\end{equation}

and because the sum across all probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) is

\begin{equation} \label{eq:duni-pmf-1}
\sum_{x=a}^{b} f_X(x) = 1 \; ,
\end{equation}

we have

\begin{equation} \label{eq:duni-pmf-duni-pmf-qed}
f_X(x) = \frac{1}{n} = \frac{1}{b-a+1} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Cumulative distribution function}]{Cumulative distribution function} \label{sec:duni-cdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni}):

\begin{equation} \label{eq:duni-cdf-duni}
X \sim \mathcal{U}(a, b) \; .
\end{equation}

Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$ is

\begin{equation} \label{eq:duni-cdf-duni-cdf}
F_X(x) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < a \\
\frac{\left\lfloor{x}\right\rfloor - a + 1}{b - a + 1} \; , & \text{if} \; a \leq x \leq b \\
1 \; , & \text{if} \; x > b \; .
\end{array}
\right.
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability mass function of the discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni-pmf}) is

\begin{equation} \label{eq:duni-cdf-duni-pmf}
\mathcal{U}(x; a, b) = \frac{1}{b-a+1} \quad \text{where} \quad x \in \left\lbrace a, a+1, \ldots, b-1, b \right\rbrace \; .
\end{equation}

Thus, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) is:

\begin{equation} \label{eq:duni-cdf-duni-cdf-s1}
F_X(x) = \int_{-\infty}^{x} \mathcal{U}(z; a, b) \, \mathrm{d}z
\end{equation}

From \eqref{eq:duni-cdf-duni-pmf}, it follows that the cumulative probability increases step-wise by $1/n$ at each integer between and including $a$ and $b$ where

\begin{equation} \label{eq:duni-cdf-n}
n = b - a + 1
\end{equation}

is the number of integers between and including $a$ and $b$. This can be expressed by noting that

\begin{equation} \label{eq:duni-cdf-duni-cdf-s2b}
F_X(x) \overset{\eqref{eq:duni-cdf-duni-pmf}}{=} \frac{\left\lfloor{x}\right\rfloor - a + 1}{n}, \; \text{if} \; a \leq x \leq b \; .
\end{equation}

Also, because $\mathrm{Pr}(X < a) = 0$, we have

\begin{equation} \label{eq:duni-cdf-duni-cdf-s2a}
F_X(x) \overset{\eqref{eq:duni-cdf-duni-cdf-s1}}{=} \int_{-\infty}^{x} 0 \, \mathrm{d}z = 0, \; \text{if} \; x < a
\end{equation}

and because $\mathrm{Pr}(X > b) = 0$, we have

\begin{equation} \label{eq:duni-cdf-duni-cdf-s2c}
\begin{split}
F_X(x) &\overset{\eqref{eq:duni-cdf-duni-cdf-s1}}{=} \int_{-\infty}^{x} \mathcal{U}(z; a, b) \, \mathrm{d}z \\
&= \int_{-\infty}^{b} \mathcal{U}(z; a, b) \, \mathrm{d}z + \int_{b}^{x} \mathcal{U}(z; a, b) \, \mathrm{d}z \\
&= F_X(b) + \int_{b}^{x} 0 \, \mathrm{d}z \overset{\eqref{eq:duni-cdf-duni-cdf-s2b}}{=} 1 + 0 \\
&= 1, \; \text{if} \; x > b \; .
\end{split}
\end{equation}

This completes the proof.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Quantile function}]{Quantile function} \label{sec:duni-qf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni}):

\begin{equation} \label{eq:duni-qf-duni}
X \sim \mathcal{U}(a, b) \; .
\end{equation}

Then, the quantile function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) of $X$ is

\begin{equation} \label{eq:duni-qf-duni-qf}
Q_X(p) = \left\{
\begin{array}{rl}
-\infty \; , & \text{if} \; p = 0 \\
a (1-p) + (b+1) p - 1 \; , & \text{when} \; p \in \left\lbrace \frac{1}{n}, \frac{2}{n}, \ldots, \frac{b-a}{n}, 1 \right\rbrace \; .
\end{array}
\right.
\end{equation}

with $n = b - a + 1$.


\vspace{1em}
\textbf{Proof:} The cumulative distribution function of the discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni-cdf}) is:

\begin{equation} \label{eq:duni-qf-duni-cdf}
F_X(x) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < a \\
\frac{\left\lfloor{x}\right\rfloor - a + 1}{b - a + 1} \; , & \text{if} \; a \leq x \leq b \\
1 \; , & \text{if} \; x > b \; .
\end{array}
\right.
\end{equation}

The quantile function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) $Q_X(p)$ is defined as the smallest $x$, such that $F_X(x) = p$:

\begin{equation} \label{eq:duni-qf-qf}
Q_X(p) = \min \left\lbrace x \in \mathbb{R} \, \vert \, F_X(x) = p \right\rbrace \; .
\end{equation}

Because the CDF only returns ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni-cdf}) multiples of $1/n$ with $n = b - a + 1$, the quantile function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) is only defined for such values. First, we have $Q_X(p) = -\infty$, if $p = 0$. Second, since the cumulative probability increases step-wise ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni-cdf}) by $1/n$ at each integer between and including $a$ and $b$, the minimum $x$ at which

\begin{equation} \label{eq:duni-qf-duni-cdf-p}
F_X(x) = \frac{c}{n} \quad \text{where} \quad c \in \left\lbrace 1, \ldots, n \right\rbrace
\end{equation}

is given by

\begin{equation} \label{eq:duni-qf-duni-qf-p}
Q_X\left( \frac{c}{n} \right) = a + \frac{c}{n} \cdot n - 1 \; .
\end{equation}

Substituting $p = c/n$ and $n = b - a + 1$, we can finally show:

\begin{equation} \label{eq:duni-qf-duni-qf-qed}
\begin{split}
Q_X(p) &= a + p \cdot (b-a+1) - 1 \\
&= a + pb - pa + p - 1 \\
&= a (1-p) + (b+1) p - 1 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Shannon entropy}]{Shannon entropy} \label{sec:duni-ent}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni}):

\begin{equation} \label{eq:duni-ent-duni}
X \sim \mathcal{U}(a,b) \; .
\end{equation}

Then, the (Shannon) entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) of $X$ in nats is

\begin{equation} \label{eq:duni-ent-duni-ent}
\mathrm{H}(X) = \ln(b-a+1) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) is defined as the probability-weighted average of the logarithmized probabilities for all possible values:

\begin{equation} \label{eq:duni-ent-ent}
\mathrm{H}(X) = - \sum_{x \in \mathcal{X}} p(x) \cdot \log_b p(x) \; .
\end{equation}

Entropy is measured in nats by setting $b = e$. Then, with the probability mass function of the discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni-pmf}), we have:

\begin{equation} \label{eq:duni-ent-duni-ent-qed}
\begin{split}
\mathrm{H}(X) &= - \sum_{x \in \mathcal{X}} p(x) \cdot \log_e p(x) \\
&= - \sum_{x=a}^{b} p(x) \cdot \ln p(x) \\
&= - \sum_{x=a}^{b} \frac{1}{b-a+1} \cdot \ln{\frac{1}{b-a+1}} \\
&= - (b-a+1) \cdot \frac{1}{b-a+1} \cdot \ln{\frac{1}{b-a+1}} \\
&= - \ln{\frac{1}{b-a+1}} \\
&= \ln(b-a+1) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Kullback-Leibler divergence}]{Kullback-Leibler divergence} \label{sec:duni-kl}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Assume two discrete uniform distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:Duni}) $P$ and $Q$ specifying the probability distribution of $X$ as

\begin{equation} \label{eq:duni-kl-dunis}
\begin{split}
P: \; X &\sim \mathcal{U}(a_1, b_1) \\
Q: \; X &\sim \mathcal{U}(a_2, b_2) \; .
\end{split}
\end{equation}

Then, the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ is given by

\begin{equation} \label{eq:duni-kl-duni-KL}
\mathrm{KL}[P\,||\,Q] = \ln \frac{b_2-a_2+1}{b_1-a_1+1} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The KL divergence for a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is given by

\begin{equation} \label{eq:duni-kl-KL-disc}
\mathrm{KL}[P\,||\,Q] = \sum_{x \in \mathcal{X}} p(x) \, \ln \frac{p(x)}{q(x)} \; .
\end{equation}

This means that the KL divergence of $P$ from $Q$ is only defined, if for all $x \in \mathcal{X}$, $q(x) = 0$ implies $p(x) = 0$. Thus, $\mathrm{KL}[P\,\vert\vert\,Q]$ only exists, if $a_2 \leq a_1$ and $b_1 \leq b_2$, i.e. if $P$ only places non-zero probability where $Q$ also places non-zero probability, such that $q(x)$ is not zero for any $x \in \mathcal{X}$ where $p(x)$ is positive.

If this requirement is fulfilled, we can write

\begin{equation} \label{eq:duni-kl-duni-KL-s1}
\mathrm{KL}[P\,||\,Q] = \sum_{x=-\infty}^{a_1} p(x) \, \ln \frac{p(x)}{q(x)} + \sum_{x=a_1}^{b_1} p(x) \, \ln \frac{p(x)}{q(x)} + \sum_{x=b_1}^{+\infty} p(x) \, \ln \frac{p(x)}{q(x)}
\end{equation}

and because $p(x) = 0$ for any $x < a_1$ and any $x > b_1$, we have

\begin{equation} \label{eq:duni-kl-duni-KL-s2}
\mathrm{KL}[P\,||\,Q] = \sum_{x=-\infty}^{a_1} 0 \cdot \ln \frac{0}{q(x)} + \sum_{x=a_1}^{b_1} p(x) \, \ln \frac{p(x)}{q(x)} + \sum_{x=b_1}^{+\infty} 0 \cdot \ln \frac{0}{q(x)} \; .
\end{equation}

Now, $(0 \cdot \ln 0)$ is taken to be $0$ by convention ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}), such that

\begin{equation} \label{eq:duni-kl-duni-KL-s3}
\mathrm{KL}[P\,||\,Q] = \sum_{x=a_1}^{b_1} p(x) \, \ln \frac{p(x)}{q(x)}
\end{equation}

and we can use the probability mass function of the discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni-pmf}) to evaluate:

\begin{equation} \label{eq:duni-kl-duni-KL-s4}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \sum_{x=a_1}^{b_1} \frac{1}{b_1-a_1+1} \cdot \ln \frac{\frac{1}{b_1-a_1+1}}{\frac{1}{b_2-a_2+1}} \\
&= \frac{1}{b_1-a_1+1} \cdot \ln \frac{b_2-a_2+1}{b_1-a_1+1} \sum_{x=a_1}^{b_1} 1 \\
&= \frac{1}{b_1-a_1+1} \cdot \ln \frac{b_2-a_2+1}{b_1-a_1+1} \cdot (b_1-a_1+1) \\
&= \ln \frac{b_2-a_2+1}{b_1-a_1+1} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Maximum entropy distribution}]{Maximum entropy distribution} \label{sec:duni-maxent}
\setcounter{equation}{0}

\textbf{Theorem:} The discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni}) maximizes (Shannon) entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) for a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with finite support.


\vspace{1em}
\textbf{Proof:} A random variable with finite support is a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}). Let $X$ be such a random variable. Without loss of generality, we can assume that the possible values of the $X$ can be enumerated from $1$ to $n$.

Let $g(x)$ be the discrete uniform distribution with minimum $a=1$ and maximum $b=n$ which assigns to equal probability to all $n$ possible values and let $f(x)$ be an arbitrary discrete ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) probability distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) on the set $\left\lbrace 1, 2, \ldots, n-1, n \right\rbrace$.

For a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) $X$ with set of possible values $\mathcal{X}$ and probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) $p(x)$, the Shannon entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) is defined as:

\begin{equation} \label{eq:duni-maxent-ent}
\mathrm{H}(X) = - \sum_{x \in \mathcal{X}} p(x) \log p(x)
\end{equation}

Consider the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of distribution $f(x)$ from distribution $g(x)$ which is non-negative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl-nonneg}):

\begin{equation} \label{eq:duni-maxent-kl-fg}
\begin{split}
0 \leq \mathrm{KL}[f||g] &= \sum_{x \in \mathcal{X}} f(x) \log \frac{f(x)}{g(x)} \\
&= \sum_{x \in \mathcal{X}} f(x) \log f(x) - \sum_{x \in \mathcal{X}} f(x) \log g(x) \\
&\overset{\eqref{eq:duni-maxent-ent}}{=} - \mathrm{H}[f(x)] - \sum_{x \in \mathcal{X}} f(x) \log g(x) \; .
\end{split}
\end{equation}

By plugging the probability mass function of the discrte uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni-pmf}) into the second term, we obtain:

\begin{equation} \label{eq:duni-maxent-sum-fg-s1}
\begin{split}
\sum_{x \in \mathcal{X}} f(x) \log g(x) &= \sum_{x=1}^{n} f(x) \log \frac{1}{n-1+1} \\
&= \log \frac{1}{n} \sum_{x=1}^{n} f(x) \\
&= -\log(n) \; .
\end{split}
\end{equation}

This is actually the negative of the entropy of the discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni-ent}), such that:

\begin{equation} \label{eq:duni-maxent-sum-fg-s2}
\sum_{x \in \mathcal{X}} f(x) \log g(x) = -\mathrm{H}[\mathcal{U}(1,n)] = -\mathrm{H}[g(x)] \; .
\end{equation}

Combining \eqref{eq:duni-maxent-kl-fg} with \eqref{eq:duni-maxent-sum-fg-s2}, we can show that

\begin{equation} \label{eq:duni-maxent-duni-maxent}
\begin{split}
0 &\leq \mathrm{KL}[f||g] \\
0 &\leq - \mathrm{H}[f(x)] - \left( -\mathrm{H}[g(x)] \right) \\
\mathrm{H}[g(x)] &\geq \mathrm{H}[f(x)]
\end{split}
\end{equation}

which means that the entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) of the discrete uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:duni}) $\mathcal{U}(a,b)$ will be larger than or equal to any other distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) defined on the same set of values $\left\lbrace a, \ldots, b \right\rbrace$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Probability Fact (2023): "The entropy of a distribution with finite domain"; in: \textit{Twitter}, retrieved on 2023-08-18; URL: \url{https://twitter.com/ProbFact/status/1673787091610750980}.
\end{itemize}
\vspace{1em}



\subsection{Bernoulli distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:bern}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, $X$ is said to follow a Bernoulli distribution with success probability $p$

\begin{equation} \label{eq:bern-bern}
X \sim \mathrm{Bern}(p) \; ,
\end{equation}

if $X = 1$ with probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) $p$ and $X = 0$ with probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) $q = 1-p$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Bernoulli distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-22; URL: \url{https://en.wikipedia.org/wiki/Bernoulli_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability mass function}]{Probability mass function} \label{sec:bern-pmf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a Bernoulli distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}):

\begin{equation} \label{eq:bern-pmf-Bern}
X \sim \mathrm{Bern}(p) \; .
\end{equation}

Then, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$ is

\begin{equation} \label{eq:bern-pmf-Bern-pmf}
f_X(x) = \left\{
\begin{array}{rl}
p \; , & \text{if} \; x = 1 \\
1-p \; , & \text{if} \; x = 0 \; . \\
\end{array}
\right. \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the Bernoulli distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:bern-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a Bernoulli distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}):

\begin{equation} \label{eq:bern-mean-bern}
X \sim \mathrm{Bern}(p) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation} \label{eq:bern-mean-bern-mean}
\mathrm{E}(X) = p \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is the probability-weighted average of all possible values:

\begin{equation} \label{eq:bern-mean-mean}
\mathrm{E}(X) = \sum_{x \in \mathcal{X}} x \cdot \mathrm{Pr}(X = x) \; .
\end{equation}

Since there are only two possible outcomes for a Bernoulli random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern-pmf}), we have:

\begin{equation} \label{eq:bern-mean-bern-mean-qed}
\begin{split}
\mathrm{E}(X) &= 0 \cdot \mathrm{Pr}(X = 0) + 1 \cdot \mathrm{Pr}(X = 1) \\
&= 0 \cdot (1-p) + 1 \cdot p \\
&= p \; . \\
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Bernoulli distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-16; URL: \url{https://en.wikipedia.org/wiki/Bernoulli_distribution#Mean}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Variance}]{Variance} \label{sec:bern-var}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a Bernoulli distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}):

\begin{equation} \label{eq:bern-var-bern}
X \sim \mathrm{Bern}(p) \; .
\end{equation}

Then, the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation} \label{eq:bern-var-bern-var}
\mathrm{Var}(X) = p \, (1-p) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is the probability-weighted average of the squared deviation from the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) across all possible values

\begin{equation} \label{eq:bern-var-var}
\mathrm{Var}(X) = \sum_{x \in \mathcal{X}} (x - \mathrm{E}(X))^2 \cdot \mathrm{Pr}(X = x)
\end{equation}

and can also be written in terms of the expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-mean}):

\begin{equation} \label{eq:bern-var-var-mean}
\mathrm{Var}(X) = \mathrm{E}\left( X^2 \right) - \mathrm{E}(X)^2 \; .
\end{equation}

The mean of a Bernoulli random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern-mean}) is

\begin{equation} \label{eq:bern-var-bern-mean}
X \sim \mathrm{Bern}(p) \quad \Rightarrow \quad \mathrm{E}(X) = p
\end{equation}

and the mean of a squared Bernoulli random variable is

\begin{equation} \label{eq:bern-var-bern-sqr-mean}
\mathrm{E}\left( X^2 \right) = 0^2 \cdot \mathrm{Pr}(X = 0) + 1^2 \cdot \mathrm{Pr}(X = 1) = 0 \cdot (1-p) + 1 \cdot p = p \; .
\end{equation}

Combining \eqref{eq:bern-var-var-mean}, \eqref{eq:bern-var-bern-mean} and \eqref{eq:bern-var-bern-sqr-mean}, we have:

\begin{equation} \label{eq:bern-var-bern-var-qed}
\mathrm{Var}(X) = p - p^2 = p \, (1-p) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Bernoulli distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-01-20; URL: \url{https://en.wikipedia.org/wiki/Bernoulli_distribution#Variance}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Range of variance}]{Range of variance} \label{sec:bern-varrange}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a Bernoulli distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}):

\begin{equation} \label{eq:bern-varrange-bern}
X \sim \mathrm{Bern}(p) \; .
\end{equation}

Then, the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is necessarily between 0 and 1/4:

\begin{equation} \label{eq:bern-varrange-bern-var-range}
0 \leq \mathrm{Var}(X) \leq \frac{1}{4} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance of a Bernoulli random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern-var}) is

\begin{equation} \label{eq:bern-varrange-bern-var}
X \sim \mathrm{Bern}(p) \quad \Rightarrow \quad \mathrm{Var}(X) = p \, (1-p)
\end{equation}

which can also be understood as a function of the success probability ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}) $p$:

\begin{equation} \label{eq:bern-varrange-bern-var-p}
\mathrm{Var}(X) = \mathrm{Var}(p) = -p^2 + p \; .
\end{equation}

The first derivative of this function is

\begin{equation} \label{eq:bern-varrange-dVar-dp}
\frac{\mathrm{d}\mathrm{Var}(p)}{\mathrm{d}p} = -2 \, p + 1
\end{equation}

and setting this deriative to zero

\begin{equation} \label{eq:bern-varrange-dVar-dp-0}
\begin{split}
\frac{\mathrm{d}\mathrm{Var}(p_M)}{\mathrm{d}p} &= 0 \\
0 &= -2 \, p_M + 1 \\
p_M &= \frac{1}{2} \; ,
\end{split}
\end{equation}

we obtain the maximum possible variance

\begin{equation} \label{eq:bern-varrange-bern-var-max}
\mathrm{max}\left[\mathrm{Var}(X)\right] = \mathrm{Var}(p_M) = -\left( \frac{1}{2} \right)^2 + \frac{1}{2} = \frac{1}{4} \; .
\end{equation}

The function $\mathrm{Var}(p)$ is monotonically increasing for $0 < p < p_M$ as $\mathrm{d}\mathrm{Var}(p)/\mathrm{d}p > 0$ in this interval and it is monotonically decreasing for $p_M < p < 1$ as $\mathrm{d}\mathrm{Var}(p)/\mathrm{d}p < 0$ in this interval. Moreover, as variance is always non-negative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-nonneg}), the minimum variance is

\begin{equation} \label{eq:bern-varrange-bern-var-min}
\mathrm{min}\left[\mathrm{Var}(X)\right] = \mathrm{Var}(0) = \mathrm{Var}(1) = 0 \; .
\end{equation}

Thus, we have:

\begin{equation} \label{eq:bern-varrange-bern-var-int}
\mathrm{Var}(p) \in \left[ 0, \; \frac{1}{4} \right] \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Bernoulli distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-01-27; URL: \url{https://en.wikipedia.org/wiki/Bernoulli_distribution#Variance}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Shannon entropy}]{Shannon entropy} \label{sec:bern-ent}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a Bernoulli distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}):

\begin{equation} \label{eq:bern-ent-bern}
X \sim \mathrm{Bern}(p) \; .
\end{equation}

Then, the (Shannon) entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) of $X$ in bits is

\begin{equation} \label{eq:bern-ent-bern-ent}
\mathrm{H}(X) = -p \log_2 p - (1-p) \log_2 (1-p) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) is defined as the probability-weighted average of the logarithmized probabilities for all possible values:

\begin{equation} \label{eq:bern-ent-ent}
\mathrm{H}(X) = - \sum_{x \in \mathcal{X}} p(x) \cdot \log_b p(x) \; .
\end{equation}

Entropy is measured in bits by setting $b = 2$. Since there are only two possible outcomes for a Bernoulli random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern-pmf}), we have:

\begin{equation} \label{eq:bern-ent-bern-ent-qed}
\begin{split}
\mathrm{H}(X) &= - \mathrm{Pr}(X = 0) \cdot \log_2 \mathrm{Pr}(X = 0) - \mathrm{Pr}(X = 1) \cdot \log_2 \mathrm{Pr}(X = 1) \\
&= -p \log_2 p - (1-p) \log_2 (1-p) \; . \\
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Bernoulli distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-09-02; URL: \url{https://en.wikipedia.org/wiki/Bernoulli_distribution}.
\item Wikipedia (2022): "Binary entropy function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-09-02; URL: \url{https://en.wikipedia.org/wiki/Binary_entropy_function}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Kullback-Leibler divergence}]{Kullback-Leibler divergence} \label{sec:bern-kl}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Assume two Bernoulli distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}) $P$ and $Q$ specifying the probability distribution of $X$ as

\begin{equation} \label{eq:bern-kl-berns}
\begin{split}
P: \; X &\sim \mathrm{Bern}(p_1) \\
Q: \; X &\sim \mathrm{Bern}(p_2) \; .
\end{split}
\end{equation}

Then, the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ is given by

\begin{equation} \label{eq:bern-kl-bern-KL}
\mathrm{KL}[P\,||\,Q] = \ln \frac{1-p_1}{1-p_2} + p_1 \cdot \ln \frac{p_1 \, (1-p_2)}{p_2 \, (1-p_1)} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The KL divergence for a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is given by 

\begin{equation} \label{eq:bern-kl-KL-disc}
\mathrm{KL}[P\,||\,Q] = \sum_{x \in \mathcal{X}} p(x) \, \ln \frac{p(x)}{q(x)}
\end{equation}

which, applied to the Bernoulli distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}) in \eqref{eq:bern-kl-berns}, yields

\begin{equation} \label{eq:bern-kl-bern-KL-s1}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \sum_{x \in \left\lbrace 0,1 \right\rbrace} p(x) \, \ln \frac{p(x)}{q(x)} \\
&= p(X=0) \cdot \ln \frac{p(X=0)}{q(X=0)} + p(X=1) \cdot \ln \frac{p(X=1)}{q(X=1)} \; .
\end{split}
\end{equation}

Using the probability mass function of the Bernoulli distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern-pmf}), this becomes:

\begin{equation} \label{eq:bern-kl-bern-KL-s2}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= (1-p_1) \cdot \ln \frac{1-p_1}{1-p_2} + p_1 \cdot \ln \frac{p_1}{p_2} \\
&= \ln \frac{1-p_1}{1-p_2} + p_1 \cdot \ln \frac{p_1}{p_2} - p_1 \cdot \ln \frac{1-p_1}{1-p_2} \\
&= \ln \frac{1-p_1}{1-p_2} + p_1 \cdot \left( \ln \frac{p_1}{p_2} + \ln \frac{1-p_2}{1-p_1} \right) \\
&= \ln \frac{1-p_1}{1-p_2} + p_1 \cdot \ln \frac{p_1 \, (1-p_2)}{p_2 \, (1-p_1)}
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{Binomial distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:bin}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, $X$ is said to follow a binomial distribution with number of trials $n$ and success probability $p$

\begin{equation} \label{eq:bin-bin}
X \sim \mathrm{Bin}(n, p) \; ,
\end{equation}

if $X$ is the number of successes observed in $n$ independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) trials, where each trial has two possible outcomes ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}) (success/failure) and the probability of success and failure are identical across trials ($p$/$q = 1-p$).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-22; URL: \url{https://en.wikipedia.org/wiki/Binomial_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability mass function}]{Probability mass function} \label{sec:bin-pmf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-pmf-bin}
X \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Then, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$ is

\begin{equation} \label{eq:bin-pmf-bin-pmf}
f_X(x) = {n \choose x} \, p^x \, (1-p)^{n-x} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} A binomial variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) is defined as the number of successes observed in $n$ independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) trials, where each trial has two possible outcomes ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}) (success/failure) and the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of success and failure are identical across trials ($p$, $q = 1-p$).

If one has obtained $x$ successes in $n$ trials, one has also obtained $(n-x)$ failures. The probability of a particular series of $x$ successes and $(n-x)$ failures, when order does matter, is

\begin{equation} \label{eq:bin-pmf-bin-prob}
p^x \, (1-p)^{n-x} \; .
\end{equation}

When order does not matter, there is a number of series consisting of $x$ successes and $(n-x)$ failures. This number is equal to the number of possibilities in which $x$ objects can be choosen from $n$ objects which is given by the binomial coefficient:

\begin{equation} \label{eq:bin-pmf-bin-coeff}
{n \choose x} \; .
\end{equation}

In order to obtain the probability of $x$ successes and $(n-x)$ failures, when order does not matter, the probability in \eqref{eq:bin-pmf-bin-prob} has to be multiplied with the number of possibilities in \eqref{eq:bin-pmf-bin-coeff} which gives

\begin{equation} \label{eq:bin-pmf-bin-pmf-qed}
p(X=x|n,p) = {n \choose x} \, p^x \, (1-p)^{n-x}
\end{equation}

which is equivalent to the expression above.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Probability-generating function}]{Probability-generating function} \label{sec:bin-pgf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-pgf-bin}
X \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Then, the probability-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pgf}) of $X$ is

\begin{equation} \label{eq:bin-pgf-bin-pgf}
G_X(z) = (q + pz)^n
\end{equation}

where $q = 1-p$.


\vspace{1em}
\textbf{Proof:} The probability-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pgf}) of $X$ is defined as

\begin{equation} \label{eq:bin-pgf-pgf}
G_X(z) = \sum_{x=0}^{\infty} f_X(x) \, z^x
\end{equation}

With the probability mass function of the binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-pmf})

\begin{equation} \label{eq:bin-pgf-bin-pmf}
f_X(x) = {n \choose x} \, p^x \, (1-p)^{n-x} \; ,
\end{equation}

we obtain:

\begin{equation} \label{eq:bin-pgf-pgf-zero-s1}
\begin{split}
G_X(z) &= \sum_{x=0}^{n} {n \choose x} \, p^x \, (1-p)^{n-x} \, z^x \\
&= \sum_{x=0}^{n} {n \choose x} \, (pz)^x \, (1-p)^{n-x} \; .
\end{split}
\end{equation}

According to the binomial theorem

\begin{equation} \label{eq:bin-pgf-bin-th}
(x+y)^n = \sum_{k=0}^{n} {n \choose k} \, x^{n-k} \, y^k \; ,
\end{equation}

the sum in equation \eqref{eq:bin-pgf-pgf-zero-s1} equals

\begin{equation} \label{eq:bin-pgf-pgf-zero-s2}
G_X(z) = \left( (1-p) + (pz) \right)^n
\end{equation}

which is equivalent to the result in \eqref{eq:bin-pgf-bin-pgf}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item ProofWiki (2022): "Probability Generating Function of Binomial Distribution"; in: \textit{ProofWiki}, retrieved on 2022-10-11; URL: \url{https://proofwiki.org/wiki/Probability_Generating_Function_of_Binomial_Distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:bin-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-mean-bin}
X \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation} \label{eq:bin-mean-bin-mean}
\mathrm{E}(X) = n p \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} By definition, a binomial random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) is the sum of $n$ independent and identical Bernoulli trials ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}) with success probability $p$. Therefore, the expected value is

\begin{equation} \label{eq:bin-mean-bin-mean-s1}
\mathrm{E}(X) = \mathrm{E}(X_1 + \ldots + X_n)
\end{equation}

and because the expected value is a linear operator ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), this is equal to

\begin{equation} \label{eq:bin-mean-bin-mean-s2}
\mathrm{E}(X) = \mathrm{E}(X_1) + \ldots + \mathrm{E}(X_n) = \sum_{i=1}^{n} \mathrm{E}(X_i) \; .
\end{equation}

With the expected value of the Bernoulli distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern-mean}), we have:

\begin{equation} \label{eq:bin-mean-bin-mean-s3}
\mathrm{E}(X) = \sum_{i=1}^{n} p = n p \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-16; URL: \url{https://en.wikipedia.org/wiki/Binomial_distribution#Expected_value_and_variance}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Variance}]{Variance} \label{sec:bin-var}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-var-bin}
X \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Then, the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is

\begin{equation} \label{eq:bin-var-bin-var}
\mathrm{Var}(X) = n p \, (1-p) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} By definition, a binomial random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) is the sum of $n$ independent and identical Bernoulli trials ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}) with success probability $p$. Therefore, the variance is

\begin{equation} \label{eq:bin-var-bin-var-s1}
\mathrm{Var}(X) = \mathrm{Var}(X_1 + \ldots + X_n)
\end{equation}

and because variances add up under independence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-add}), this is equal to

\begin{equation} \label{eq:bin-var-bin-var-s2}
\mathrm{Var}(X) = \mathrm{Var}(X_1) + \ldots + \mathrm{Var}(X_n) = \sum_{i=1}^{n} \mathrm{Var}(X_i) \; .
\end{equation}

With the variance of the Bernoulli distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern-var}), we have:

\begin{equation} \label{eq:bin-var-bin-var-s3}
\mathrm{Var}(X) = \sum_{i=1}^{n} p \, (1-p) = n p \, (1-p) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-01-20; URL: \url{https://en.wikipedia.org/wiki/Binomial_distribution#Expected_value_and_variance}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Range of variance}]{Range of variance} \label{sec:bin-varrange}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-varrange-bin}
X \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Then, the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is necessarily between 0 and $n/4$:

\begin{equation} \label{eq:bin-varrange-bin-var-range}
0 \leq \mathrm{Var}(X) \leq \frac{n}{4} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} By definition, a binomial random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) is the sum of $n$ independent and identical Bernoulli trials ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}) with success probability $p$. Therefore, the variance is

\begin{equation} \label{eq:bin-varrange-bin-var-s1}
\mathrm{Var}(X) = \mathrm{Var}(X_1 + \ldots + X_n)
\end{equation}

and because variances add up under independence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-add}), this is equal to

\begin{equation} \label{eq:bin-varrange-bin-var-s2}
\mathrm{Var}(X) = \mathrm{Var}(X_1) + \ldots + \mathrm{Var}(X_n) = \sum_{i=1}^{n} \mathrm{Var}(X_i) \; .
\end{equation}

As the variance of a Bernoulli random variable is always between 0 and 1/4 ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern-varrange})

\begin{equation} \label{eq:bin-varrange-bern-var-range}
0 \leq \mathrm{Var}(X_i) \leq \frac{1}{4} \quad \text{for all} \quad i = 1,\ldots,n \; ,
\end{equation}

the minimum variance of $X$ is

\begin{equation} \label{eq:bin-varrange-bin-var-min}
\mathrm{min}\left[\mathrm{Var}(X)\right] = n \cdot 0 = 0
\end{equation}

and the maximum variance of $X$ is

\begin{equation} \label{eq:bin-varrange-bin-var-max}
\mathrm{max}\left[\mathrm{Var}(X)\right] = n \cdot \frac{1}{4} = \frac{n}{4} \; .
\end{equation}

Thus, we have:

\begin{equation} \label{eq:bin-varrange-bin-var-int}
\mathrm{Var}(X) \in \left[ 0, \; \frac{n}{4} \right] \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Shannon entropy}]{Shannon entropy} \label{sec:bin-ent}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-ent-bin}
X \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Then, the (Shannon) entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) of $X$ in bits is

\begin{equation} \label{eq:bin-ent-bin-ent}
\mathrm{H}(X) = n \cdot \mathrm{H}_\mathrm{bern}(p) - \mathrm{E}_\mathrm{lbc}(n,p)
\end{equation}

where $\mathrm{H}_\mathrm{bern}(p)$ is the binary entropy function, i.e. the (Shannon) entropy of the Bernoulli distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern-ent}) with success probability $p$

\begin{equation} \label{eq:bin-ent-H-bern}
\mathrm{H}_\mathrm{bern}(p) = - p \cdot \log_2 p - (1-p) \log_2 (1-p)
\end{equation}

and $\mathrm{E}_\mathrm{lbc}(n,p)$ is the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the logarithmized binomial coefficient ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-pmf}) with superset size $n$

\begin{equation} \label{eq:bin-ent-E-lbf}
\mathrm{E}_\mathrm{lbc}(n,p) = \mathrm{E}\left[ \log_2 {n \choose X} \right] \quad \text{where} \quad X \sim \mathrm{Bin}(n,p) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) is defined as the probability-weighted average of the logarithmized probabilities for all possible values:

\begin{equation} \label{eq:bin-ent-ent}
\mathrm{H}(X) = - \sum_{x \in \mathcal{X}} p(x) \cdot \log_b p(x) \; .
\end{equation}

Entropy is measured in bits by setting $b = 2$. Then, with the probability mass function of the binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-pmf}), we have:

\begin{equation} \label{eq:bin-ent-bin-ent-s1}
\begin{split}
\mathrm{H}(X) &= - \sum_{x \in \mathcal{X}} f_X(x) \cdot \log_2 f_X(x) \\
&= - \sum_{x=0}^{n} {n \choose x} \, p^x \, (1-p)^{n-x} \cdot \log_2 \left[ {n \choose x} \, p^x \, (1-p)^{n-x} \right] \\
&= - \sum_{x=0}^{n} {n \choose x} \, p^x \, (1-p)^{n-x} \cdot \left[ \log_2 {n \choose x} + x \cdot \log_2 p + (n-x) \cdot \log_2 (1-p) \right] \\
&= - \sum_{x=0}^{n} {n \choose x} \, p^x \, (1-p)^{n-x} \cdot \left[ \log_2 {n \choose x} + x \cdot \log_2 p + n \cdot \log_2 (1-p) - x \cdot \log_2 (1-p) \right] \; .
\end{split}
\end{equation}

Since the first factor in the sum corresponds to the probability mass ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X=x$, we can rewrite this as the sum of the expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lotus}) of the discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) $x$ in the square bracket:

\begin{equation} \label{eq:bin-ent-bin-ent-s2}
\begin{split}
\mathrm{H}(X) &= - \left\langle \log_2 {n \choose x} \right\rangle_{p(x)} - \left\langle x \cdot \log_2 p \right\rangle_{p(x)} - \left\langle n \cdot \log_2 (1-p) \right\rangle_{p(x)} + \left\langle x \cdot \log_2 (1-p) \right\rangle_{p(x)} \\
&= - \left\langle \log_2 {n \choose x} \right\rangle_{p(x)} - \log_2 p \cdot \left\langle x \right\rangle_{p(x)} - n \cdot \log_2 (1-p) +  \log_2 (1-p) \cdot \left\langle x \right\rangle_{p(x)} \; .
\end{split}
\end{equation}

Using the expected value of the binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-mean}), i.e. $X \sim \mathrm{Bin}(n,p) \Rightarrow \left\langle x \right\rangle = n p$, this gives:

\begin{equation} \label{eq:bin-ent-bin-ent-s3}
\begin{split}
\mathrm{H}(X) &= - \left\langle \log_2 {n \choose x} \right\rangle_{p(x)} - n p \cdot \log_2 p - n \cdot \log_2 (1-p) +  n p \cdot \log_2 (1-p) \\
&= - \left\langle \log_2 {n \choose x} \right\rangle_{p(x)} + n \left[ - p \cdot \log_2 p - (1-p) \log_2 (1-p) \right] \; .
\end{split}
\end{equation}

Finally, we note that the first term is the negative expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the logarithm of a binomial coefficient ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-pmf}) and that the term in square brackets is the entropy of the Bernoulli distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-ent}), such that we finally get:

\begin{equation} \label{eq:bin-ent-bin-ent-s4}
\mathrm{H}(X) = n \cdot \mathrm{H}_\mathrm{bern}(p) - \mathrm{E}_\mathrm{lbc}(n,p) \; .
\end{equation}

Note that, because $0 \leq \mathrm{H}\_\mathrm{bern}(p) \leq 1$, we have $0 \leq n \cdot \mathrm{H}\_\mathrm{bern}(p) \leq n$, and because the entropy is non-negative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent-nonneg}), it must hold that $n \geq \mathrm{E}\_\mathrm{lbc}(n,p) \geq 0$.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Kullback-Leibler divergence}]{Kullback-Leibler divergence} \label{sec:bin-kl}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Assume two binomial distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) $P$ and $Q$ specifying the probability distribution of $X$ as

\begin{equation} \label{eq:bin-kl-bins}
\begin{split}
P: \; X &\sim \mathrm{Bin}(n, p_1) \\
Q: \; X &\sim \mathrm{Bin}(n, p_2) \; .
\end{split}
\end{equation}

Then, the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ is given by

\begin{equation} \label{eq:bin-kl-bin-KL}
\mathrm{KL}[P\,||\,Q] = n p_1 \cdot \ln \frac{p_1}{p_2} + n (1-p_1) \cdot \ln \frac{1-p_1}{1-p_2} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The KL divergence for a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is given by 

\begin{equation} \label{eq:bin-kl-KL-disc}
\mathrm{KL}[P\,||\,Q] = \sum_{x \in \mathcal{X}} p(x) \, \ln \frac{p(x)}{q(x)}
\end{equation}

which, applied to the binomial distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) in \eqref{eq:bin-kl-bins}, yields

\begin{equation} \label{eq:bin-kl-bin-KL-s1}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \sum_{x=0}^{n} p(x) \, \ln \frac{p(x)}{q(x)} \\
&= p(X=0) \cdot \ln \frac{p(X=0)}{q(X=0)} + \ldots + p(X=n) \cdot \ln \frac{p(X=n)}{q(X=n)} \; .
\end{split}
\end{equation}

Using the probability mass function of the binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-pmf}), this becomes:

\begin{equation} \label{eq:bin-kl-bin-KL-s2}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \sum_{x=0}^{n} {n \choose x} \, p_1^x \, (1-p_1)^{n-x} \cdot \ln \frac{ {n \choose x} \, p_1^x \, (1-p_1)^{n-x} }{ {n \choose x} \, p_2^x \, (1-p_2)^{n-x} } \\
&= \sum_{x=0}^{n} {n \choose x} \, p_1^x \, (1-p_1)^{n-x} \cdot \left[ x \cdot \ln \frac{p_1}{p_2} + (n-x) \cdot \ln \frac{1-p_1}{1-p_2} \right] \\
&= \ln \frac{p_1}{p_2} \cdot \sum_{x=0}^{n} {n \choose x} \, p_1^x \, (1-p_1)^{n-x} x + \ln \frac{1-p_1}{1-p_2} \cdot \sum_{x=0}^{n} {n \choose x} \, p_1^x \, (1-p_1)^{n-x} (n-x) \; .
\end{split}
\end{equation}

We can now see that some terms in this sum are expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) with respect to binomial distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-kl-bin-means-s1}
\begin{split}
\sum_{x=0}^{n} {n \choose x} \, p_1^x \, (1-p_1)^{n-x} x &= \mathrm{E}\left[ x \right]_{\mathrm{Bin}(n,p_1)} \\
\sum_{x=0}^{n} {n \choose x} \, p_1^x \, (1-p_1)^{n-x} (n-x) &= \mathrm{E}\left[ n-x \right]_{\mathrm{Bin}(n,p_1)} \; .
\end{split}
\end{equation}

Using the expected value of the binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-mean}), these can be simplified to

\begin{equation} \label{eq:bin-kl-bin-means-s2}
\begin{split}
\mathrm{E}\left[ x \right]_{\mathrm{Bin}(n,p_1)} &= n p_1 \\
\mathrm{E}\left[ n-x \right]_{\mathrm{Bin}(n,p_1)} &= n - n p_ 1 \; ,
\end{split}
\end{equation}

such that the Kullback-Leibler divergence finally becomes:

\begin{equation} \label{eq:bin-kl-bin-KL-qed}
\mathrm{KL}[P\,||\,Q] = n p_1 \cdot \ln \frac{p_1}{p_2} + n (1-p_1) \cdot \ln \frac{1-p_1}{1-p_2} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item PSPACEhard (2017): "Kullback-Leibler divergence for binomial distributions P and Q"; in: \textit{StackExchange Mathematics}, retrieved on 2023-10-20; URL: \url{https://math.stackexchange.com/a/2215384/480910}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Conditional binomial}]{Conditional binomial} \label{sec:bin-margcond}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be two random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) where $Y$ is binomially distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) conditional on ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-cond}) $X$

\begin{equation} \label{eq:bin-margcond-Y-X-dist}
Y \vert X \sim \mathrm{Bin}(X, q)
\end{equation}

and $X$ also follows a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}), but with different success frequency ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-margcond-X-dist}
X \sim \mathrm{Bin}(n, p) \; .
\end{equation}

Then, the maginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of $Y$ unconditional on $X$ is again a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-margcond-Y-dist}
Y \sim \mathrm{Bin}(n, pq) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} We are interested in the probability that $Y$ equals a number $m$. According to the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}) or the law of total probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-tot}), this probability can be expressed as:

\begin{equation} \label{eq:bin-margcond-Y-dist-s1}
\mathrm{Pr}(Y = m) = \sum_{k=0}^{\infty} \mathrm{Pr}(Y = m \vert X = k) \cdot \mathrm{Pr}(X = k) \; .
\end{equation}

Since, by definitions \eqref{eq:bin-margcond-X-dist} and \eqref{eq:bin-margcond-Y-X-dist}, $\mathrm{Pr}(X = k) = 0$ when $k > n$ and $\mathrm{Pr}(Y = m \vert X = k) = 0$ when $k < m$, we have:

\begin{equation} \label{eq:bin-margcond-Y-dist-s2}
\mathrm{Pr}(Y = m) = \sum_{k=m}^{n} \mathrm{Pr}(Y = m \vert X = k) \cdot \mathrm{Pr}(X = k) \; .
\end{equation}

Now we can take the probability mass function of the binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-pmf}) and plug it in for the terms in the sum of \eqref{eq:bin-margcond-Y-dist-s2} to get:

\begin{equation} \label{eq:bin-margcond-Y-dist-s3}
\mathrm{Pr}(Y = m) = \sum_{k=m}^{n} {k \choose m} \, q^m \, (1-q)^{k-m} \cdot {n \choose k} \, p^k \, (1-p)^{n-k} \; .
\end{equation}

Applying the binomial coefficient identity ${n \choose k} {k \choose m} = {n \choose m} {n-m \choose k-m}$ and rearranging the terms, we have:

\begin{equation} \label{eq:bin-margcond-Y-dist-s4}
\mathrm{Pr}(Y = m) = \sum_{k=m}^{n} {n \choose m} \, {n-m \choose k-m} \, p^k \, q^m \, (1-p)^{n-k} \, (1-q)^{k-m} \; .
\end{equation}

Now we partition $p^k = p^m \cdot p^{k-m}$ and pull all terms dependent on $k$ out of the sum:

\begin{equation} \label{eq:bin-margcond-Y-dist-s5}
\begin{split}
\mathrm{Pr}(Y = m) &= {n \choose m} \, p^m \, q^m \sum_{k=m}^{n} {n-m \choose k-m} \, p^{k-m} \, (1-p)^{n-k} \, (1-q)^{k-m} \\
&= {n \choose m} \, (p q)^m \sum_{k=m}^{n} {n-m \choose k-m} \, \left( p (1-q) \right)^{k-m} \, (1-p)^{n-k} \; .
\end{split}
\end{equation}

Then we subsititute $i = k - m$, such that $k = i + m$:

\begin{equation} \label{eq:bin-margcond-Y-dist-s6}
\mathrm{Pr}(Y = m) = {n \choose m} \, (p q)^m \sum_{i=0}^{n-m} {n-m \choose i} \, \left( p - pq \right)^{i} \, (1-p)^{n-m-i} \; .
\end{equation}

According to the binomial theorem

\begin{equation} \label{eq:bin-margcond-bin-th}
(x+y)^n = \sum_{k=0}^{n} {n \choose k} \, x^{n-k} \, y^k \; ,
\end{equation}

the sum in equation \eqref{eq:bin-margcond-Y-dist-s6} is equal to

\begin{equation} \label{eq:bin-margcond-Y-dist-sum}
\sum_{i=0}^{n-m} {n-m \choose i} \, \left( p - pq \right)^{i} \, (1-p)^{n-m-i} = \left( (p-pq)+(1-p) \right)^{n-m} \; .
\end{equation}

Thus, \eqref{eq:bin-margcond-Y-dist-s6} develops into

\begin{equation} \label{eq:bin-margcond-Y-dist-s7}
\begin{split}
\mathrm{Pr}(Y = m) &= {n \choose m} \, (p q)^m (p - pq + 1 - p)^{n-m} \\
&= {n \choose m} \, (p q)^m (1 - pq)^{n-m}
\end{split}
\end{equation}

which is the probability mass function of the binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-pmf}) with parameters $n$ and $pq$, such that

\begin{equation} \label{eq:bin-margcond-Y-dist-qed}
Y \sim \mathrm{Bin}(n, pq) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-10-07; URL: \url{https://en.wikipedia.org/wiki/Binomial_distribution#Conditional_binomials}.
\end{itemize}
\vspace{1em}



\subsection{Beta-binomial distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:betabin}
\setcounter{equation}{0}

\textbf{Definition:} Let $p$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta})

\begin{equation} \label{eq:betabin-beta}
p \sim \mathrm{Bet}(\alpha, \beta)
\end{equation}

and let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) conditional on $p$

\begin{equation} \label{eq:betabin-bin}
X \mid p \sim \mathrm{Bin}(n, p) \; .
\end{equation}

Then, the marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of $X$ is called a beta-binomial distribution

\begin{equation} \label{eq:betabin-betabin}
X \sim \mathrm{BetBin}(n, \alpha, \beta)
\end{equation}

with number of trials ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) $n$ and shape parameters ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta}) $\alpha$ and $\beta$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Beta-binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-10-20; URL: \url{https://en.wikipedia.org/wiki/Beta-binomial_distribution#Motivation_and_derivation}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability mass function}]{Probability mass function} \label{sec:betabin-pmf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a beta-binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:betabin}):

\begin{equation} \label{eq:betabin-pmf-betabin}
X \sim \mathrm{BetBin}(n,\alpha,\beta) \; .
\end{equation}

Then, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$ is

\begin{equation} \label{eq:betabin-pmf-betabin-pmf}
f_X(x) = {n \choose x} \cdot \frac{\mathrm{B}(\alpha+x,\beta+n-x)}{\mathrm{B}(\alpha,\beta)}
\end{equation}

where $\mathrm{B}(x,y)$ is the beta function.


\vspace{1em}
\textbf{Proof:} A beta-binomial random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:betabin}) is defined as a binomial variate ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) for which the success probability is following a beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta}):

\begin{equation} \label{eq:betabin-pmf-betabin-bin-beta}
\begin{split}
X \mid p &\sim \mathrm{Bin}(n, p) \\
p &\sim \mathrm{Bet}(\alpha, \beta) \; .
\end{split}
\end{equation}

Thus, we can combine the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}) and the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}) to derive the probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of $X$ as

\begin{equation} \label{eq:betabin-pmf-betabin-pmf-s1}
\begin{split}
p(x) &= \int_\mathcal{P} \mathrm{p}(x,p) \, \mathrm{d}p \\
&= \int_\mathcal{P} \mathrm{p}(x \vert p) \, \mathrm{p}(p) \, \mathrm{d}p \; .
\end{split}
\end{equation}

Now, we can plug in the probability mass function of the binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-pmf}) and the probability density function of the beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-pdf}) to get

\begin{equation} \label{eq:betabin-pmf-betabin-pmf-s2}
\begin{split}
p(x) &= \int_0^1 {n \choose x} \, p^x \, (1-p)^{n-x} \cdot \frac{1}{\mathrm{B}(\alpha, \beta)} \, p^{\alpha-1} \, (1-p)^{\beta-1} \, \mathrm{d}p \\
&= {n \choose x} \cdot \frac{1}{\mathrm{B}(\alpha, \beta)} \, \int_0^1 p^{\alpha+x-1} \, (1-p)^{\beta+n-x-1} \, \mathrm{d}p \\
&= {n \choose x} \cdot \frac{\mathrm{B}(\alpha+x,\beta+n-x)}{\mathrm{B}(\alpha, \beta)} \, \int_0^1 \frac{1}{\mathrm{B}(\alpha+x,\beta+n-x)} \, p^{\alpha+x-1} \, (1-p)^{\beta+n-x-1} \, \mathrm{d}p \; .
\end{split}
\end{equation}

Finally, we recognize that the integrand is equal to the probability density function of a beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-pdf}) and because probability density integrates to one ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}), we have

\begin{equation} \label{eq:betabin-pmf-betabin-pmf-qed}
p(x) = {n \choose x} \cdot \frac{\mathrm{B}(\alpha+x,\beta+n-x)}{\mathrm{B}(\alpha,\beta)} = f_X(x) \; .
\end{equation}

This completes the proof.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Beta-binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-10-20; URL: \url{https://en.wikipedia.org/wiki/Beta-binomial_distribution#As_a_compound_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability mass function in terms of gamma function}]{Probability mass function in terms of gamma function} \label{sec:betabin-pmfitogf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a beta-binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:betabin}):

\begin{equation} \label{eq:betabin-pmfitogf-betabin}
X \sim \mathrm{BetBin}(n,\alpha,\beta) \; .
\end{equation}

Then, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$ can be expressed as

\begin{equation} \label{eq:betabin-pmfitogf-betabin-pmfitogf}
f_X(x) = \frac{\Gamma(n+1)}{\Gamma(x+1) \, \Gamma(n-x+1)} \cdot \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \, \Gamma(\beta)} \cdot \frac{\Gamma(\alpha+x) \, \Gamma(\beta+n-x)}{\Gamma(\alpha+\beta+n)}
\end{equation}

where $\Gamma(x)$ is the gamma function.


\vspace{1em}
\textbf{Proof:} The probability mass function of the beta-binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:betabin-pmf}) is given by

\begin{equation} \label{eq:betabin-pmfitogf-betabin-pmf}
f_X(x) = {n \choose x} \cdot \frac{\mathrm{B}(\alpha+x,\beta+n-x)}{\mathrm{B}(\alpha,\beta)} \; .
\end{equation}

Note that the binomial coefficient can be expressed in terms of factorials

\begin{equation} \label{eq:betabin-pmfitogf-bincoeff-facts}
{n \choose x} = \frac{n!}{x! \, (n-x)!} \; ,
\end{equation}

that factorials are related to the gamma function via $n! = \Gamma(n+1)$

\begin{equation} \label{eq:betabin-pmfitogf-facts-gamfct}
\frac{n!}{x! \, (n-x)!} = \frac{\Gamma(n+1)}{\Gamma(x+1) \, \Gamma(n-x+1)}
\end{equation}

and that the beta function is related to the gamma function via

\begin{equation} \label{eq:betabin-pmfitogf-betafct-gamfct}
\mathrm{B}(\alpha,\beta) = \frac{\Gamma(\alpha) \, \Gamma(\beta)}{\Gamma(\alpha+\beta)} \; .
\end{equation}

Applying \eqref{eq:betabin-pmfitogf-bincoeff-facts}, \eqref{eq:betabin-pmfitogf-facts-gamfct} and \eqref{eq:betabin-pmfitogf-betafct-gamfct} to \eqref{eq:betabin-pmfitogf-betabin-pmf}, we get

\begin{equation} \label{eq:betabin-pmfitogf-betabin-pmfitogf-qed}
f_X(x) = \frac{\Gamma(n+1)}{\Gamma(x+1) \, \Gamma(n-x+1)} \cdot \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \, \Gamma(\beta)} \cdot \frac{\Gamma(\alpha+x) \, \Gamma(\beta+n-x)}{\Gamma(\alpha+\beta+n)} \; .
\end{equation}

This completes the proof.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Beta-binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-10-20; URL: \url{https://en.wikipedia.org/wiki/Beta-binomial_distribution#As_a_compound_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Cumulative distribution function}]{Cumulative distribution function} \label{sec:betabin-cdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a beta-binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:betabin}):

\begin{equation} \label{eq:betabin-cdf-betabin}
X \sim \mathrm{BetBin}(n,\alpha,\beta) \; .
\end{equation}

Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$ is

\begin{equation} \label{eq:betabin-cdf-betabin-cdf}
F_X(x) = \frac{1}{\mathrm{B}(\alpha,\beta)} \cdot \frac{\Gamma(n+1)}{\Gamma(\alpha+\beta+n)} \cdot \sum_{i=0}^{x} \frac{\Gamma(\alpha+i) \cdot \Gamma(\beta+n-i)}{\Gamma(i+1) \cdot \Gamma(n-i+1)}
\end{equation}

where $\mathrm{B}(x,y)$ is the beta function and $\Gamma(x)$ is the gamma function.


\vspace{1em}
\textbf{Proof:} The cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) is defined as

\begin{equation} \label{eq:betabin-cdf-cdf}
F_X(x) = \mathrm{Pr}(X \leq x)
\end{equation}

which, for a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}), evaluates to

\begin{equation} \label{eq:betabin-cdf-cdf-disc}
F_X(x) = \sum_{i=-\infty}^{x} f_X(i) \; .
\end{equation}

With the probability mass function of the beta-binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:betabin-pmf}), this becomes

\begin{equation} \label{eq:betabin-cdf-betabin-cdf-s1}
F_X(x) = \sum_{i=0}^{x} {n \choose i} \cdot \frac{\mathrm{B}(\alpha+i,\beta+n-i)}{\mathrm{B}(\alpha,\beta)} \; .
\end{equation}

Using the expression of binomial coefficients in terms of factorials

\begin{equation} \label{eq:betabin-cdf-bincoeff-facts}
{n \choose k} = \frac{n!}{k! \, (n-k)!} \; ,
\end{equation}

the relationship between factorials and the gamma function

\begin{equation} \label{eq:betabin-cdf-facts-gamfct}
n! = \Gamma(n+1)
\end{equation}

and the link between gamma function and beta function

\begin{equation} \label{eq:betabin-cdf-betafct-gamfct}
\mathrm{B}(\alpha,\beta) = \frac{\Gamma(\alpha) \, \Gamma(\beta)}{\Gamma(\alpha+\beta)} \; ,
\end{equation}

equation \eqref{eq:betabin-cdf-betabin-cdf-s1} can be further developped as follows:

\begin{equation} \label{eq:betabin-cdf-betabin-cdf-s2}
\begin{split}
F_X(x) &\overset{\eqref{eq:betabin-cdf-bincoeff-facts}}{=} \frac{1}{\mathrm{B}(\alpha,\beta)} \cdot \sum_{i=0}^{x} \frac{n!}{i! \, (n-i)!} \cdot \mathrm{B}(\alpha+i,\beta+n-i) \\
&\overset{\eqref{eq:betabin-cdf-betafct-gamfct}}{=} \frac{1}{\mathrm{B}(\alpha,\beta)} \cdot \sum_{i=0}^{x} \frac{n!}{i! \, (n-i)!} \cdot
\frac{\Gamma(\alpha+i) \cdot \Gamma(\beta+n-i)}{\Gamma(\alpha+\beta+n)} \\
&= \frac{1}{\mathrm{B}(\alpha,\beta)} \cdot \frac{n!}{\Gamma(\alpha+\beta+n)} \cdot \sum_{i=0}^{x}
\frac{\Gamma(\alpha+i) \cdot \Gamma(\beta+n-i)}{i! \, (n-i)!} \\
&\overset{\eqref{eq:betabin-cdf-facts-gamfct}}{=} \frac{1}{\mathrm{B}(\alpha,\beta)} \cdot \frac{\Gamma(n+1)}{\Gamma(\alpha+\beta+n)} \cdot \sum_{i=0}^{x}
\frac{\Gamma(\alpha+i) \cdot \Gamma(\beta+n-i)}{\Gamma(i+1) \cdot \Gamma(n-i+1)} \; .
\end{split}
\end{equation}

This completes the proof.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{Poisson distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:poiss}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, $X$ is said to follow a Poisson distribution with rate $\lambda$

\begin{equation} \label{eq:poiss-poiss}
X \sim \mathrm{Poiss}(\lambda) \; ,
\end{equation}

if and only if its probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) is given by

\begin{equation} \label{eq:poiss-poiss-pmf}
\mathrm{Poiss}(x; \lambda) = \frac{\lambda^x \, e^{-\lambda}}{x!}
\end{equation}

where $x \in \mathbb{N}_0$ and $\lambda > 0$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Poisson distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-25; URL: \url{https://en.wikipedia.org/wiki/Poisson_distribution#Definitions}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability mass function}]{Probability mass function} \label{sec:poiss-pmf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss}):

\begin{equation} \label{eq:poiss-pmf-Poiss}
X \sim \mathrm{Poiss}(\lambda) \; .
\end{equation}

Then, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$ is

\begin{equation} \label{eq:poiss-pmf-Poiss-pmf}
f_X(x) = \frac{\lambda^x \, e^{-\lambda}}{x!}, \; x \in \mathbb{N}_0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:poiss-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss}):

\begin{equation} \label{eq:poiss-mean-poiss}
X \sim \mathrm{Poiss}(\lambda) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation} \label{eq:poiss-mean-poiss-mean}
\mathrm{E}(X) = \lambda \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The expected value of a discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is defined as

\begin{equation} \label{eq:poiss-mean-mean}
\mathrm{E}(X) = \sum_{x \in \mathcal{X}} x \cdot f_X(x) \; ,
\end{equation}

such that, with the probability mass function of the Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss-pmf}), we have:

\begin{equation} \label{eq:poiss-mean-poiss-mean-s1}
\begin{split}
\mathrm{E}(X) &= \sum_{x=0}^\infty x \cdot \frac{\lambda^x \, e^{-\lambda}}{x!} \\
&= \sum_{x=1}^\infty x \cdot \frac{\lambda^x \, e^{-\lambda}}{x!} \\
&= e^{-\lambda} \cdot \sum_{x=1}^\infty \frac{x}{x!} \lambda^x \\
&= \lambda e^{-\lambda} \cdot \sum_{x=1}^\infty \frac{\lambda^{x-1}}{(x-1)!} \; .
\end{split}
\end{equation}

Substituting $z = x-1$, such that $x = z+1$, we get:

\begin{equation} \label{eq:poiss-mean-poiss-mean-s2}
\mathrm{E}(X) = \lambda e^{-\lambda} \cdot \sum_{z=0}^\infty \frac{\lambda^z}{z!} \; .
\end{equation}

Using the power series expansion of the exponential function

\begin{equation} \label{eq:poiss-mean-exp-ps}
e^x = \sum_{n=0}^\infty \frac{x^n}{n!} \; ,
\end{equation}

the expected value of $X$ finally becomes

\begin{equation} \label{eq:poiss-mean-poiss-mean-s3}
\begin{split}
\mathrm{E}(X) &= \lambda e^{-\lambda} \cdot e^{\lambda} \\
&= \lambda \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item ProofWiki (2020): "Expectation of Poisson Distribution"; in: \textit{ProofWiki}, retrieved on 2020-08-19; URL: \url{https://proofwiki.org/wiki/Expectation_of_Poisson_Distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Variance}]{Variance} \label{sec:poiss-var}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss}):

\begin{equation} \label{eq:poiss-var-poiss}
X \sim \mathrm{Poiss}(\lambda) \; .
\end{equation}

Then, the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is

\begin{equation} \label{eq:poiss-var-poiss-var}
\mathrm{Var}(X) = \lambda \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) can be expressed in terms of expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-mean}) as

\begin{equation} \label{eq:poiss-var-var-mean}
\mathrm{Var}(X) = \mathrm{E}(X^2) - \mathrm{E}(X)^2 \; .
\end{equation}

The expected value of a Poisson random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss-mean}) is

\begin{equation} \label{eq:poiss-var-poiss-mean}
\mathrm{E}(X) = \lambda \; .
\end{equation}

Let us now consider the expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X \, (X-1)$ which is defined as

\begin{equation} \label{eq:poiss-var-mean}
\mathrm{E}[X \, (X-1)] = \sum_{x \in \mathcal{X}} x \, (x-1) \cdot f_X(x) \; ,
\end{equation}

such that, with the probability mass function of the Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss-pmf}), we have:

\begin{equation} \label{eq:poiss-var-poiss-x2x-mean-s1}
\begin{split}
\mathrm{E}[X \, (X-1)] &= \sum_{x=0}^\infty x \, (x-1) \cdot \frac{\lambda^x \, e^{-\lambda}}{x!} \\
&= \sum_{x=2}^\infty x \, (x-1) \cdot \frac{\lambda^x \, e^{-\lambda}}{x!} \\
&= e^{-\lambda} \cdot \sum_{x=2}^\infty x \, (x-1) \cdot \frac{\lambda^x}{x \cdot (x-1) \cdot (x-2)!} \\
&= \lambda^2 \cdot e^{-\lambda} \cdot \sum_{x=2}^\infty \frac{\lambda^{x-2}}{(x-2)!} \; .
\end{split}
\end{equation}

Substituting $z = x-2$, such that $x = z+2$, we get:

\begin{equation} \label{eq:poiss-var-poiss-x2x-mean-s2}
\mathrm{E}[X \, (X-1)] = \lambda^2 \cdot e^{-\lambda} \cdot \sum_{z=0}^\infty \frac{\lambda^z}{z!} \; .
\end{equation}

Using the power series expansion of the exponential function

\begin{equation} \label{eq:poiss-var-exp-ps}
e^x = \sum_{n=0}^\infty \frac{x^n}{n!} \; ,
\end{equation}

the expected value of $X \, (X-1)$ finally becomes

\begin{equation} \label{eq:poiss-var-poiss-x2x-mean-s3}
\mathrm{E}[X \, (X-1)] = \lambda^2 \cdot e^{-\lambda} \cdot e^{\lambda} = \lambda^2 \; .
\end{equation}

Note that this expectation can be written as

\begin{equation} \label{eq:poiss-var-poiss-x2-mean-s1}
\mathrm{E}[X \, (X-1)] = \mathrm{E}(X^2 - X) = \mathrm{E}(X^2) - \mathrm{E}(X) \; ,
\end{equation}

such that, with \eqref{eq:poiss-var-poiss-x2x-mean-s3} and \eqref{eq:poiss-var-poiss-mean}, we have:

\begin{equation} \label{eq:poiss-var-poiss-x2-mean-s2}
\mathrm{E}(X^2) - \mathrm{E}(X) = \lambda^2 \quad \Rightarrow \quad \mathrm{E}(X^2) = \lambda^2 + \lambda \; .
\end{equation}

Plugging \eqref{eq:poiss-var-poiss-x2-mean-s2} and \eqref{eq:poiss-var-poiss-mean} into \eqref{eq:poiss-var-var-mean}, the variance of a Poisson random variable finally becomes

\begin{equation} \label{eq:poiss-var-poiss-var-qed}
\mathrm{Var}(X) = \lambda^2 + \lambda - \lambda^2 = \lambda \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item jbstatistics (2013): "The Poisson Distribution: Mathematically Deriving the Mean and Variance"; in: \textit{YouTube}, retrieved on 2021-04-29; URL: \url{https://www.youtube.com/watch?v=65n_v92JZeE}.
\end{itemize}
\vspace{1em}



\pagebreak
\section{Multivariate discrete distributions}

\subsection{Categorical distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:cat}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, $X$ is said to follow a categorical distribution with success probability $p_1, \ldots, p_k$

\begin{equation} \label{eq:cat-cat}
X \sim \mathrm{Cat}(\left[p_1, \ldots, p_k \right]) \; ,
\end{equation}

if $X = e_i$ with probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) $p_i$ for all $i = 1, \ldots, k$, where $e_i$ is the $i$-th elementary row vector, i.e. a $1 \times k$ vector of zeros with a one in $i$-th position.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Categorical distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-22; URL: \url{https://en.wikipedia.org/wiki/Categorical_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability mass function}]{Probability mass function} \label{sec:cat-pmf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a categorical distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat}):

\begin{equation} \label{eq:cat-pmf-cat}
X \sim \mathrm{Cat}(\left[p_1, \ldots, p_k \right]) \; .
\end{equation}

Then, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$ is

\begin{equation} \label{eq:cat-pmf-cat-pmf}
f_X(x) = \left\{
\begin{array}{rl}
p_1 \; , & \text{if} \; x = e_1 \\
\vdots \; \hphantom{,} & \quad \vdots \\
p_k \; , & \text{if} \; x = e_k \; . \\
\end{array}
\right.
\end{equation}

where $e_1, \ldots, e_k$ are the $1 \times k$ elementary row vectors.


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the categorical distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:cat-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a categorical distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat}):

\begin{equation} \label{eq:cat-mean-cat}
X \sim \mathrm{Cat}(\left[p_1, \ldots, p_k \right]) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation} \label{eq:cat-mean-cat-mean}
\mathrm{E}(X) = \left[p_1, \ldots, p_k \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} If we conceive the outcome of a categorical distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat}) to be a $1 \times k$ vector, then the elementary row vectors $e_1 = \left[1, 0, \ldots, 0 \right]$, ..., $e_k = \left[0, \ldots, 0, 1 \right]$ are all the possible outcomes and they occur with probabilities $\mathrm{Pr}(X = e_1) = p_1$, ..., $\mathrm{Pr}(X = e_k) = p_k$. Consequently, the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is

\begin{equation} \label{eq:cat-mean-cat-mean-qed}
\begin{split}
\mathrm{E}(X) &= \sum_{x \in \mathcal{X}} x \cdot \mathrm{Pr}(X = x) \\
&= \sum_{i=1}^k e_i \cdot \mathrm{Pr}(X = e_i) \\
&= \sum_{i=1}^k e_i \cdot p_i \\
&= \left[p_1, \ldots, p_k \right] \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Covariance}]{Covariance} \label{sec:cat-cov}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a categorical distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat}):

\begin{equation} \label{eq:cat-cov-cat}
X \sim \mathrm{Cat}(n,p) \; .
\end{equation}

Then, the covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of $X$ is

\begin{equation} \label{eq:cat-cov-cat-cov}
\mathrm{Cov}(X) = \mathrm{diag}(p) - pp^\mathrm{T} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The categorical distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat}) is a special case of the multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}) in which $n = 1$:

\begin{equation} \label{eq:cat-cov-cat-mult}
X \sim \mathrm{Mult}(n,p) \quad \text{and} \quad n = 1 \quad \Rightarrow \quad X \sim \mathrm{Cat}(p) \; .
\end{equation}

The covariance matrix of the multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult-cov}) is

\begin{equation} \label{eq:cat-cov-mult-cov}
\mathrm{Cov}(X) = n \left(\mathrm{diag}(p) - pp^\mathrm{T} \right) \; ,
\end{equation}

thus the covariance matrix of the categorical distribution is

\begin{equation} \label{eq:cat-cov-cat-cov-qed}
\mathrm{Cov}(X) = \mathrm{diag}(p) - pp^\mathrm{T} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Shannon entropy}]{Shannon entropy} \label{sec:cat-ent}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a categorical distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat}):

\begin{equation} \label{eq:cat-ent-cat}
X \sim \mathrm{Cat}(p) \; .
\end{equation}

Then, the (Shannon) entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) of $X$ is

\begin{equation} \label{eq:cat-ent-cat-ent}
\mathrm{H}(X) = - \sum_{i=1}^{k} p_i \cdot \log p_i \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) is defined as the probability-weighted average of the logarithmized probabilities for all possible values:

\begin{equation} \label{eq:cat-ent-ent}
\mathrm{H}(X) = - \sum_{x \in \mathcal{X}} p(x) \cdot \log_b p(x) \; .
\end{equation}

Since there are $k$ possible values for a categorical random vector ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat}) with probabilities given by the entries ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat-pmf}) of the $1 \times k$ vector $p$, we have:

\begin{equation} \label{eq:cat-ent-cat-ent-qed}
\begin{split}
\mathrm{H}(X) &= - \mathrm{Pr}(X = e_1) \cdot \log \mathrm{Pr}(X = e_1) - \ldots - \mathrm{Pr}(X = e_k) \cdot \log \mathrm{Pr}(X = e_k) \\
\mathrm{H}(X) &= - \sum_{i=1}^{k} \mathrm{Pr}(X = e_i) \cdot \log \mathrm{Pr}(X = e_i) \\
\mathrm{H}(X) &= - \sum_{i=1}^{k} p_i \cdot \log p_i \; . \\
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{Multinomial distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:mult}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, $X$ is said to follow a multinomial distribution with number of trials $n$ and category probabilities $p_1, \ldots, p_k$

\begin{equation} \label{eq:mult-mult}
X \sim \mathrm{Mult}(n, \left[p_1, \ldots, p_k \right]) \; ,
\end{equation}

if $X$ are the numbers of observations belonging to $k$ distinct categories in $n$ independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) trials, where each trial has $k$ possible outcomes ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat}) and the category probabilities are identical across trials.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-22; URL: \url{https://en.wikipedia.org/wiki/Multinomial_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability mass function}]{Probability mass function} \label{sec:mult-pmf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}):

\begin{equation} \label{eq:mult-pmf-mult}
X \sim \mathrm{Mult}(n, \left[p_1, \ldots, p_k \right]) \; .
\end{equation}

Then, the probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X$ is

\begin{equation} \label{eq:mult-pmf-mult-pmf}
f_X(x) = {n \choose {x_1, \ldots, x_k}} \, \prod_{i=1}^k {p_i}^{x_i} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} A multinomial variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}) is defined as a vector of the numbers of observations belonging to $k$ distinct categories in $n$ independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) trials, where each trial has $k$ possible outcomes ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat}) and the category probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) are identical across trials.

Since the individual trials are independent ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}) and joint probability factorizes under independence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ind}), the probability of a particular series of $x_1$ observations for category $1$, $x_2$ observations for category $2$, ... etc., when order does matter, is

\begin{equation} \label{eq:mult-pmf-mult-prob}
\prod_{i=1}^k {p_i}^{x_i} \; .
\end{equation}

When order does not matter, there is a number of series consisting of $x_1$ observations for category $1$, $x_2$ observations for category $2$, ... etc. This number is equal to the number of possibilities in which $x_1$ category $1$ objects, $x_2$ category $2$ objects, ... etc. can be distributed in a sequence of $n$ objects which is given by the multinomial coefficient that can be expressed in terms of factorials:

\begin{equation} \label{eq:mult-pmf-mult-coeff}
{n \choose {x_1, \ldots, x_k}} = \frac{n!}{x_1! \cdot \ldots \cdot x_k!} \; .
\end{equation}

In order to obtain the probability of $x_1$ observations for category $1$, $x_2$ observations for category $2$, ... etc., when order does not matter, the probability in \eqref{eq:mult-pmf-mult-prob} has to be multiplied with the number of possibilities in \eqref{eq:mult-pmf-mult-coeff} which gives

\begin{equation} \label{eq:mult-pmf-mult-pmf-qed}
p(X=x|n,\left[p_1, \ldots, p_k \right]) = {n \choose {x_1, \ldots, x_k}} \, \prod_{i=1}^k {p_i}^{x_i}
\end{equation}

which is equivalent to the expression above.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:mult-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}):

\begin{equation} \label{eq:mult-mean-mult}
X \sim \mathrm{Mult}(n,\left[p_1, \ldots, p_k \right]) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation} \label{eq:mult-mean-bin-mean}
\mathrm{E}(X) = \left[n p_1, \ldots, n p_k \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} By definition, a multinomial random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}) is the sum of $n$ independent and identical categorical trials ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat}) with category probabilities $p_1, \ldots, p_k$. Therefore, the expected value is

\begin{equation} \label{eq:mult-mean-mult-mean-s1}
\mathrm{E}(X) = \mathrm{E}(X_1 + \ldots + X_n)
\end{equation}

and because the expected value is a linear operator ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), this is equal to

\begin{equation} \label{eq:mult-mean-mult-mean-s2}
\begin{split}
\mathrm{E}(X) &= \mathrm{E}(X_1) + \ldots + \mathrm{E}(X_n) \\
&= \sum_{i=1}^{n} \mathrm{E}(X_i) \; .
\end{split}
\end{equation}

With the expected value of the categorical distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat-mean}), we have:

\begin{equation} \label{eq:mult-mean-mult-mean-s3}
\mathrm{E}(X) = \sum_{i=1}^{n} \left[p_1, \ldots, p_k \right] = n \cdot \left[p_1, \ldots, p_k \right] = \left[n p_1, \ldots, n p_k \right] \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Covariance}]{Covariance} \label{sec:mult-cov}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}):

\begin{equation} \label{eq:mult-cov-mult}
\left[X_1, \ldots, X_k \right] = X \sim \mathrm{Mult}(n, p), \; n \in \mathbb{N}, \; p = \left[p_1, \ldots, p_k \right]^\mathrm{T} \; .
\end{equation}

Then, the covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of $X$ is

\begin{equation} \label{eq:mult-cov-mult-cov}
\mathrm{Cov}(X) = n \left(\mathrm{diag}(p) - pp^\mathrm{T} \right) \; .
\end{equation}

\vspace{1em}
\textbf{Proof:} We first observe that the sample space ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:samp-spc}) of each coordinate $X_i$ is $\left\lbrace 0, 1, \ldots, n \right\rbrace$ and $X_i$ is the sum of independent draws of category $i$, which is drawn with probability $p_i$. Thus each coordinate follows a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:mult-cov-Marginal}
X_i \stackrel{\mathrm{i.i.d.}}{\sim} \mathrm{Bin}(n, p_i), \; i = 1,\ldots, k \; ,
\end{equation}

which has the variance ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-var}) $\mathrm{Var}(X_i) = n p_i(1-p_i) = n (p_i - p_i^2)$, constituting the elements of the main diagonal in $\mathrm{Cov}(X)$ in \eqref{eq:mult-cov-mult-cov}. To prove $\mathrm{Cov}(X_i, X_j) = -n p_i p_j$ for $i \ne j$ (which constitutes the off-diagonal elements of the covariance matrix), we first recognize that

\begin{equation} \label{eq:mult-cov-bin-sum}
X_i = \sum_{k=1}^n \mathbb{I}_i(k), \quad \text{with} \quad \mathbb{I}_i(k) = \begin{cases}
    1 & \text{if $k$-th draw was of category $i$}, \\
    0 & \text{otherwise} \; ,
\end{cases}
\end{equation}

where the indicator function $\mathbb{I}_i$ is a Bernoulli-distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern}) random variable with the expected value ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bern-mean}) $p_i$. Then, we have 

\begin{equation} \label{eq:mult-cov-mult-cov-qed}
\begin{split}
\mathrm{Cov}(X_i, X_j) &= \mathrm{Cov}\left(\sum_{k=1}^n \mathbb{I}_i(k), \sum_{l=1}^n \mathbb{I}_j(l)\right) \\
&= \sum_{k=1}^n\sum_{l=1}^n \mathrm{Cov}\left(\mathbb{I}_i(k), \mathbb{I}_j(l)\right) \\
&= \sum_{k=1}^n \left[ \mathrm{Cov}\left(\mathbb{I}_i(k), \mathbb{I}_j(k)\right) + \sum_{\substack{l=1 \\ l \ne k}}^n \underbrace{\mathrm{Cov}\left(\mathbb{I}_i(k), \mathbb{I}_j(l)\right)}_{=0} \right] \\
& \stackrel{i \ne j}{=} \;\; \sum_{k=1}^n \left(\mathrm{E}\Big( \underbrace{\mathbb{I}_i(k) \,\mathbb{I}_j(k)}_{=0} \Big) - \mathrm{E}\big(\mathbb{I}_i(k)\big) \mathrm{E}\big(\mathbb{I}_j(k)\big) \right) \\
&= -\sum_{k=1}^n \mathrm{E}\big(\mathbb{I}_i(k)\big) \mathrm{E}\big(\mathbb{I}_j(k)\big) \\
&= -n p_i p_j \; ,
\end{split}
\end{equation}

as desired.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Tutz G (2012): "Multinomial Response Models"; in: \textit{Regression for Categorical Data}, pp. 209ff.; URL: \url{https://www.cambridge.org/core/books/regression-for-categorical-data/B71F71F2A484E2DF88256C8DF004108C}; DOI: 10.1017/CBO9780511842061.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Shannon entropy}]{Shannon entropy} \label{sec:mult-ent}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}):

\begin{equation} \label{eq:mult-ent-mult}
X \sim \mathrm{Mult}(n,p) \; .
\end{equation}

Then, the (Shannon) entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) of $X$ is

\begin{equation} \label{eq:mult-ent-mult-ent}
\mathrm{H}(X) = n \cdot \mathrm{H}_\mathrm{cat}(p) - \mathrm{E}_\mathrm{lmc}(n,p)
\end{equation}

where $\mathrm{H}_\mathrm{cat}(p)$ is the categorical entropy function, i.e. the (Shannon) entropy of the categorical distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat-ent}) with category probabilities $p$

\begin{equation} \label{eq:mult-ent-H-cat}
\mathrm{H}_\mathrm{cat}(p) = - \sum_{i=1}^{k} p_i \cdot \log p_i
\end{equation}

and $\mathrm{E}_\mathrm{lmc}(n,p)$ is the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the logarithmized multinomial coefficient ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult-pmf}) with superset size $n$

\begin{equation} \label{eq:mult-ent-E-lmf}
\mathrm{E}_\mathrm{lmf}(n,p) = \mathrm{E}\left[ \log {n \choose {X_1, \ldots, X_k}} \right] \quad \text{where} \quad X \sim \mathrm{Mult}(n,p) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}) is defined as the probability-weighted average of the logarithmized probabilities for all possible values:

\begin{equation} \label{eq:mult-ent-ent}
\mathrm{H}(X) = - \sum_{x \in \mathcal{X}} p(x) \cdot \log_b p(x) \; .
\end{equation}

The probability mass function of the multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult-pmf}) is

\begin{equation} \label{eq:mult-ent-mult-pmf}
f_X(x) = {n \choose {x_1, \ldots, x_k}} \, \prod_{i=1}^k {p_i}^{x_i}
\end{equation}

Let $\mathcal{X}_{n,k}$ be the set of all vectors $x \in \mathbb{N}^{1 \times k}$ satisfying $\sum_{i=1}^{k} x_i = n$. Then, we have:

\begin{equation} \label{eq:mult-ent-mult-ent-s1}
\begin{split}
\mathrm{H}(X) &= - \sum_{x \in \mathcal{X}_{n,k}} f_X(x) \cdot \log f_X(x) \\
&= - \sum_{x \in \mathcal{X}_{n,k}} f_X(x) \cdot \log \left[ {n \choose {x_1, \ldots, x_k}} \, \prod_{i=1}^k {p_i}^{x_i} \right] \\
&= - \sum_{x \in \mathcal{X}_{n,k}} f_X(x) \cdot \left[ \log {n \choose {x_1, \ldots, x_k}} + \sum_{i=1}^{k} x_i \cdot \log p_i \right] \; .
\end{split}
\end{equation}

Since the first factor in the sum corresponds to the probability mass ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) of $X=x$, we can rewrite this as the sum of the expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lotus}) of the discrete random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar-disc}) $x$ in the square bracket:

\begin{equation} \label{eq:mult-ent-mult-ent-s2}
\begin{split}
\mathrm{H}(X) &= - \left\langle \log {n \choose {x_1, \ldots, x_k}} \right\rangle_{p(x)} - \left\langle \sum_{i=1}^{k} x_i \cdot \log p_i \right\rangle_{p(x)} \\
&= - \left\langle \log {n \choose {x_1, \ldots, x_k}} \right\rangle_{p(x)} - \sum_{i=1}^{k} \left\langle x_i \cdot \log p_i \right\rangle_{p(x)} \; .
\end{split}
\end{equation}

Using the expected value of the multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult-mean}), i.e. $X \sim \mathrm{Mult}(n,p) \Rightarrow \left\langle x_i \right\rangle = n p_i$, this gives:

\begin{equation} \label{eq:mult-ent-mult-ent-s3}
\begin{split}
\mathrm{H}(X) &= - \left\langle \log {n \choose {x_1, \ldots, x_k}} \right\rangle_{p(x)} - \sum_{i=1}^{k} n p_i \cdot \log p_i \\
&= - \left\langle\log {n \choose {x_1, \ldots, x_k}} \right\rangle_{p(x)} - n \sum_{i=1}^{k} p_i \cdot \log p_i \; .
\end{split}
\end{equation}

Finally, we note that the first term is the negative expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the logarithm of a multinomial coefficient ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult-pmf}) and that the second term is the entropy of the categorical distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cat-ent}), such that we finally get:

\begin{equation} \label{eq:mult-ent-mult-ent-s4}
\mathrm{H}(X) = n \cdot \mathrm{H}_\mathrm{cat}(p) - \mathrm{E}_\mathrm{lmc}(n,p) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\pagebreak
\section{Univariate continuous distributions}

\subsection{Continuous uniform distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:cuni}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, $X$ is said to be uniformly distributed with minimum $a$ and maximum $b$

\begin{equation} \label{eq:cuni-cuni}
X \sim \mathcal{U}(a, b) \; ,
\end{equation}

if and only if each value between and including $a$ and $b$ occurs with the same probability.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Uniform distribution (continuous)"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-27; URL: \url{https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Standard uniform distribution}]{Standard uniform distribution} \label{sec:suni}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, $X$ is said to be standard uniformly distributed, if $X$ follows a continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}) with minimum $a = 0$ and maximum $b = 1$:

\begin{equation} \label{eq:suni-suni}
X \sim \mathcal{U}(0, 1) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Continuous uniform distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-07-23; URL: \url{https://en.wikipedia.org/wiki/Continuous_uniform_distribution#Standard_uniform}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:cuni-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}):

\begin{equation} \label{eq:cuni-pdf-cuni}
X \sim \mathcal{U}(a, b) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ is

\begin{equation} \label{eq:cuni-pdf-cuni-pdf}
f_X(x) = \left\{
\begin{array}{rl}
\frac{1}{b-a} \; , & \text{if} \; a \leq x \leq b \\
0 \; , & \text{otherwise} \; .
\end{array}
\right.
\end{equation}


\vspace{1em}
\textbf{Proof:} A continuous uniform variable is defined as ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}) having a constant probability density between minimum $a$ and maximum $b$. Therefore,

\begin{equation} \label{eq:cuni-pdf-cuni-pdf-s1}
\begin{split}
f_X(x) &\propto 1 \quad \text{for all} \quad x \in [a,b] \quad \text{and} \\
f_X(x) &= 0, \quad\!\! \text{if} \quad x < a \quad \text{or} \quad x > b \; .
\end{split}
\end{equation}

To ensure that $f_X(x)$ is a proper probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}), the integral over all non-zero probabilities has to sum to $1$. Therefore,

\begin{equation} \label{eq:cuni-pdf-cuni-pdf-s2}
f_X(x) = \frac{1}{c(a,b)} \quad \text{for all} \quad x \in [a,b]
\end{equation}

where the normalization factor $c(a,b)$ is specified, such that

\begin{equation} \label{eq:cuni-pdf-cuni-pdf-s3}
\frac{1}{c(a,b)} \int_{a}^{b} 1 \, \mathrm{d}x = 1 \; .
\end{equation}

Solving this for $c(a,b)$, we obtain:

\begin{equation} \label{eq:cuni-pdf-cuni-pdf-s4}
\begin{split}
\int_{a}^{b} 1 \, \mathrm{d}x &= c(a,b) \\
[x]_a^b &= c(a,b) \\
c(a,b) &= b-a \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Cumulative distribution function}]{Cumulative distribution function} \label{sec:cuni-cdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}):

\begin{equation} \label{eq:cuni-cdf-cuni}
X \sim \mathcal{U}(a, b) \; .
\end{equation}

Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$ is

\begin{equation} \label{eq:cuni-cdf-cuni-cdf}
F_X(x) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < a \\
\frac{x-a}{b-a} \; , & \text{if} \; a \leq x \leq b \\
1 \; , & \text{if} \; x > b \; .
\end{array}
\right.
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density function of the continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-pdf}) is:

\begin{equation} \label{eq:cuni-cdf-cuni-pdf}
\mathcal{U}(x; a, b) = \left\{
\begin{array}{rl}
\frac{1}{b-a} \; , & \text{if} \; a \leq x \leq b \\
0 \; , & \text{otherwise} \; .
\end{array}
\right.
\end{equation}

Thus, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) is:

\begin{equation} \label{eq:cuni-cdf-cuni-cdf-s1}
F_X(x) = \int_{-\infty}^{x} \mathcal{U}(z; a, b) \, \mathrm{d}z
\end{equation}

First of all, if $x < a$, we have

\begin{equation} \label{eq:cuni-cdf-cuni-cdf-s2a}
F_X(x) = \int_{-\infty}^{x} 0 \, \mathrm{d}z = 0 \; .
\end{equation}

Moreover, if $a \leq x \leq b$, we have using \eqref{eq:cuni-cdf-cuni-pdf}

\begin{equation} \label{eq:cuni-cdf-cuni-cdf-s2b}
\begin{split}
F_X(x) &= \int_{-\infty}^{a} \mathcal{U}(z; a, b) \, \mathrm{d}z + \int_{a}^{x} \mathcal{U}(z; a, b) \, \mathrm{d}z \\
&= \int_{-\infty}^{a} 0 \, \mathrm{d}z + \int_{a}^{x} \frac{1}{b-a} \, \mathrm{d}z \\
&= 0 + \frac{1}{b-a} [z]_a^x \\
&= \frac{x-a}{b-a} \; .
\end{split}
\end{equation}

Finally, if $x > b$, we have

\begin{equation} \label{eq:cuni-cdf-cuni-cdf-s2c}
\begin{split}
F_X(x) &= \int_{-\infty}^{b} \mathcal{U}(z; a, b) \, \mathrm{d}z + \int_{b}^{x} \mathcal{U}(z; a, b) \, \mathrm{d}z \\
&= F_X(b) + \int_{b}^{x} 0 \, \mathrm{d}z \\
&= \frac{b-a}{b-a} + 0 \\
&= 1 \; .
\end{split}
\end{equation}

This completes the proof.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Quantile function}]{Quantile function} \label{sec:cuni-qf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}):

\begin{equation} \label{eq:cuni-qf-cuni}
X \sim \mathcal{U}(a, b) \; .
\end{equation}

Then, the quantile function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) of $X$ is

\begin{equation} \label{eq:cuni-qf-cuni-qf}
Q_X(p) = \left\{
\begin{array}{rl}
-\infty \; , & \text{if} \; p = 0 \\
bp + a(1-p) \; , & \text{if} \; p > 0 \; .
\end{array}
\right.
\end{equation}


\vspace{1em}
\textbf{Proof:} The cumulative distribution function of the continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-cdf}) is:

\begin{equation} \label{eq:cuni-qf-cuni-cdf}
F_X(x) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < a \\
\frac{x-a}{b-a} \; , & \text{if} \; a \leq x \leq b \\
1 \; , & \text{if} \; x > b \; .
\end{array}
\right.
\end{equation}

The quantile function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) $Q_X(p)$ is defined as the smallest $x$, such that $F_X(x) = p$:

\begin{equation} \label{eq:cuni-qf-qf}
Q_X(p) = \min \left\lbrace x \in \mathbb{R} \, \vert \, F_X(x) = p \right\rbrace \; .
\end{equation}

Thus, we have $Q_X(p) = -\infty$, if $p = 0$. When $p > 0$, it holds that ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf-cdf})

\begin{equation} \label{eq:cuni-qf-exp-qf-s1}
Q_X(p) = F_X^{-1}(x) \; .
\end{equation}

This can be derived by rearranging equation \eqref{eq:cuni-qf-cuni-cdf}:

\begin{equation} \label{eq:cuni-qf-cuni-cdf-s2}
\begin{split}
p &= \frac{x-a}{b-a} \\
x &= p(b-a) + a \\
x &= bp + a(1-p) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:cuni-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}):

\begin{equation} \label{eq:cuni-mean-cuni}
X \sim \mathcal{U}(a, b) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation} \label{eq:cuni-mean-cuni-mean}
\mathrm{E}(X) = \frac{1}{2} (a+b) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is the probability-weighted average over all possible values:

\begin{equation} \label{eq:cuni-mean-mean}
\mathrm{E}(X) = \int_{\mathcal{X}} x \cdot f_X(x) \, \mathrm{d}x \; .
\end{equation}

With the probability density function of the continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-pdf}), this becomes:

\begin{equation} \label{eq:cuni-mean-cuni-mean-qed}
\begin{split}
\mathrm{E}(X) &= \int_a^b x \cdot \frac{1}{b-a} \, \mathrm{d}x \\
&= \left[ \frac{1}{2} \, \frac{x^2}{b-a} \right]_a^b \\
&= \frac{1}{2} \, \frac{b^2 - a^2}{b-a} \\
&= \frac{1}{2} \, \frac{(b+a)(b-a)}{b-a} \\
&= \frac{1}{2} (a+b) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Median}]{Median} \label{sec:cuni-med}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}):

\begin{equation} \label{eq:cuni-med-cuni}
X \sim \mathcal{U}(a, b) \; .
\end{equation}

Then, the median ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:med}) of $X$ is

\begin{equation} \label{eq:cuni-med-cuni-med}
\mathrm{median}(X) = \frac{1}{2} (a+b) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The median ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:med}) is the value at which the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) is $1/2$:

\begin{equation} \label{eq:cuni-med-median}
F_X(\mathrm{median}(X)) = \frac{1}{2} \; .
\end{equation}

The cumulative distribution function of the continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-cdf}) is

\begin{equation} \label{eq:cuni-med-cuni-cdf}
F_X(x) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < a \\
\frac{x-a}{b-a} \; , & \text{if} \; a \leq x \leq b \\
1 \; , & \text{if} \; x > b \; .
\end{array}
\right.
\end{equation}

Thus, the inverse CDF ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-qf}) is

\begin{equation} \label{eq:cuni-med-cuni-cdf-inv}
x = bp + a(1-p) \; .
\end{equation}

Setting $p = 1/2$, we obtain:

\begin{equation} \label{eq:cuni-med-cuni-med-qed}
\mathrm{median}(X) = b \cdot \frac{1}{2} + a \cdot \left( 1-\frac{1}{2} \right) = \frac{1}{2} (a+b) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mode}]{Mode} \label{sec:cuni-med}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}):

\begin{equation} \label{eq:cuni-med-cuni}
X \sim \mathcal{U}(a, b) \; .
\end{equation}

Then, the mode ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mode}) of $X$ is

\begin{equation} \label{eq:cuni-med-cuni-mode}
\mathrm{mode}(X) \in [a,b] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:}  The mode ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mode}) is the value which maximizes the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}):

\begin{equation} \label{eq:cuni-med-mode}
\mathrm{mode}(X) = \operatorname*{arg\,max}_x f_X(x) \; .
\end{equation}

The probability density function of the continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-pdf}) is:

\begin{equation} \label{eq:cuni-med-cuni-pdf}
f_X(x) = \left\{
\begin{array}{rl}
\frac{1}{b-a} \; , & \text{if} \; a \leq x \leq b \\
0 \; , & \text{otherwise} \; .
\end{array}
\right.
\end{equation}

Since the PDF attains its only non-zero value whenever $a \leq x \leq b$,

\begin{equation} \label{eq:cuni-med-cuni-pdf-max}
\operatorname*{max}_x f_X(x) = \frac{1}{b-a} \; ,
\end{equation}

any value in the interval $[a,b]$ may be considered the mode of $X$.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Variance}]{Variance} \label{sec:cuni-var}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}):

\begin{equation} \label{eq:cuni-var-cuni}
X \sim \mathcal{U}(a, b) \; .
\end{equation}

Then, the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is

\begin{equation} \label{eq:cuni-var-cuni-var}
\mathrm{Var}(X) = \frac{1}{12} (b-a)^2 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is the probability-weighted average of the squared deviation from the mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}):

\begin{equation} \label{eq:cuni-var-var}
\mathrm{Var}(X) = \int_{\mathbb{R}} (x - \mathrm{E}(X))^2 \cdot f_\mathrm{X}(x) \, \mathrm{d}x \; .
\end{equation}

With the expected value ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-mean}) and probability density function ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-pdf}) of the continuous uniform distribution, this reads:

\begin{equation} \label{eq:cuni-var-cuni-var-qed}
\begin{split}
\mathrm{Var}(X) &= \int_a^b \left( x - \frac{1}{2} (a+b) \right)^2 \cdot \frac{1}{b-a} \, \mathrm{d}x \\
&= \frac{1}{b-a} \cdot \int_a^b \left( x - \frac{a+b}{2} \right)^2 \, \mathrm{d}x \\
&= \frac{1}{b-a} \cdot \left[ \frac{1}{3} \left( x - \frac{a+b}{2} \right)^3 \right]_a^b \\
&= \frac{1}{3(b-a)} \cdot \left[ \left( \frac{2x-(a+b)}{2} \right)^3 \right]_a^b \\
&= \frac{1}{3(b-a)} \cdot \left[ \frac{1}{8} ( 2x-a-b )^3 \right]_a^b \\
&= \frac{1}{24(b-a)} \cdot \left[ ( 2x-a-b )^3 \right]_a^b \\
&= \frac{1}{24(b-a)} \cdot \left[ ( 2b-a-b )^3 - ( 2a-a-b )^3 \right] \\
&= \frac{1}{24(b-a)} \cdot \left[ ( b-a )^3 - ( a-b )^3 \right] \\
&= \frac{1}{24(b-a)} \cdot \left[ ( b-a )^3 + (-1)^3 ( a-b )^3 \right] \\
&= \frac{1}{24(b-a)} \cdot \left[ ( b-a )^3 + ( b-a )^3 \right] \\
&= \frac{2}{24(b-a)} (b-a)^3 \\
&= \frac{1}{12} (b-a)^2 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Differential entropy}]{Differential entropy} \label{sec:cuni-dent}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}):

\begin{equation} \label{eq:cuni-dent-cuni}
X \sim \mathcal{U}(a, b) \; .
\end{equation}

Then, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $X$ is

\begin{equation} \label{eq:cuni-dent-cuni-dent}
\mathrm{h}(X) = \ln(b-a) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of a random variable is defined as

\begin{equation} \label{eq:cuni-dent-dent}
\mathrm{h}(X) = - \int_{\mathcal{X}} p(x) \, \log_b p(x) \, \mathrm{d}x \; .
\end{equation}

To measure $h(X)$ in nats, we set $b = e$, such that

\begin{equation} \label{eq:cuni-dent-dent-nats}
\mathrm{h}(X) = - \int_{\mathcal{X}} p(x) \, \ln p(x) \, \mathrm{d}x \; .
\end{equation}

With the probability density function of the continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-pdf}), the differential entropy of $X$ is:

\begin{equation} \label{eq:cuni-dent-cuni-dent-qed}
\begin{split}
\mathrm{h}(X) &= - \int_a^b \frac{1}{b-a} \, \ln \left( \frac{1}{b-a} \right) \, \mathrm{d}x \\
&= \frac{1}{b-a} \cdot \int_a^b \ln(b-a) \, \mathrm{d}x \\
&= \frac{1}{b-a} \cdot \left[ x \cdot \ln(b-a) \right]_a^b \\
&= \frac{1}{b-a} \cdot \left[ b \cdot \ln(b-a) - a \cdot \ln(b-a) \right] \\
&= \frac{1}{b-a} (b-a) \ln(b-a) \\
&= \ln(b-a) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Kullback-Leibler divergence}]{Kullback-Leibler divergence} \label{sec:cuni-kl}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Assume two continuous uniform distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}) $P$ and $Q$ specifying the probability distribution of $X$ as

\begin{equation} \label{eq:cuni-kl-cunis}
\begin{split}
P: \; X &\sim \mathcal{U}(a_1, b_1) \\
Q: \; X &\sim \mathcal{U}(a_2, b_2) \; .
\end{split}
\end{equation}

Then, the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ is given by

\begin{equation} \label{eq:cuni-kl-cuni-KL}
\mathrm{KL}[P\,||\,Q] = \ln \frac{b_2-a_2}{b_1-a_1} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The KL divergence for a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is given by 

\begin{equation} \label{eq:cuni-kl-KL-cont}
\mathrm{KL}[P\,||\,Q] = \int_{\mathcal{X}} p(x) \, \ln \frac{p(x)}{q(x)} \, \mathrm{d}x \; .
\end{equation}

This means that the KL divergence of $P$ from $Q$ is only defined, if for all $x \in \mathcal{X}$, $q(x) = 0$ implies $p(x) = 0$. Thus, $\mathrm{KL}[P\,\vert\vert\,Q]$ only exists, if $a_2 \leq a_1$ and $b_1 \leq b_2$, i.e. if $P$ only places non-zero probability where $Q$ also places non-zero probability, such that $q(x)$ is not zero for any $x \in \mathcal{X}$ where $p(x)$ is positive.

If this requirement is fulfilled, we can write

\begin{equation} \label{eq:cuni-kl-cuni-KL-s1}
\mathrm{KL}[P\,||\,Q] = \int_{-\infty}^{a_1} p(x) \, \ln \frac{p(x)}{q(x)} \, \mathrm{d}x + \int_{a_1}^{b_1} p(x) \, \ln \frac{p(x)}{q(x)} \, \mathrm{d}x + \int_{b_1}^{+\infty} p(x) \, \ln \frac{p(x)}{q(x)} \, \mathrm{d}x
\end{equation}

and because $p(x) = 0$ for any $x < a_1$ and any $x > b_1$, we have

\begin{equation} \label{eq:cuni-kl-cuni-KL-s2}
\mathrm{KL}[P\,||\,Q] = \int_{-\infty}^{a_1} 0 \cdot \ln \frac{0}{q(x)} \, \mathrm{d}x + \int_{a_1}^{b_1} p(x) \, \ln \frac{p(x)}{q(x)} \, \mathrm{d}x + \int_{b_1}^{+\infty} 0 \cdot \ln \frac{0}{q(x)} \, \mathrm{d}x \; .
\end{equation}

Now, $(0 \cdot \ln 0)$ is taken to be zero by convention ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ent}), such that

\begin{equation} \label{eq:cuni-kl-cuni-KL-s3}
\mathrm{KL}[P\,||\,Q] = \int_{a_1}^{b_1} p(x) \, \ln \frac{p(x)}{q(x)} \, \mathrm{d}x
\end{equation}

and we can use the probability density function of the continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-pdf}) to evaluate:

\begin{equation} \label{eq:cuni-kl-cuni-KL-s4}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \int_{a_1}^{b_1} \frac{1}{b_1-a_1}\, \ln \frac{\frac{1}{b_1-a_1}}{\frac{1}{b_2-a_2}} \, \mathrm{d}x \\
&= \frac{1}{b_1-a_1}\, \ln \frac{b_2-a_2}{b_1-a_1} \int_{a_1}^{b_1} \, \mathrm{d}x \\
&= \frac{1}{b_1-a_1}\, \ln \frac{b_2-a_2}{b_1-a_1} \left[ x \right]_{a_1}^{b_1} \\
&= \frac{1}{b_1-a_1}\, \ln \frac{b_2-a_2}{b_1-a_1} (b_1-a_1) \\
&= \ln \frac{b_2-a_2}{b_1-a_1} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Maximum entropy distribution}]{Maximum entropy distribution} \label{sec:cuni-maxent}
\setcounter{equation}{0}

\textbf{Theorem:} The continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}) maximizes differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) for a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with a fixed range.


\vspace{1em}
\textbf{Proof:} Without loss of generality, let us assume that the random variable $X$ is in the following range: $a \leq X \leq b$.

Let $g(x)$ be the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of a continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}) with minimum $a$ and maximum $b$ and let $f(x)$ be an arbitrary probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) defined on the same support $\mathcal{X} = [a,b]$.

For a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ with set of possible values $\mathcal{X}$ and probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) $p(x)$, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) is defined as:

\begin{equation} \label{eq:cuni-maxent-dent}
\mathrm{h}(X) = - \int_{\mathcal{X}} p(x) \log p(x) \, \mathrm{d}x
\end{equation}

Consider the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of distribution $f(x)$ from distribution $g(x)$ which is non-negative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl-nonneg}):

\begin{equation} \label{eq:cuni-maxent-kl-fg}
\begin{split}
0 \leq \mathrm{KL}[f||g] &= \int_{\mathcal{X}} f(x) \log \frac{f(x)}{g(x)} \, \mathrm{d}x \\
&= \int_{\mathcal{X}} f(x) \log f(x) \, \mathrm{d}x - \int_{\mathcal{X}} f(x) \log g(x) \, \mathrm{d}x \\
&\overset{\eqref{eq:cuni-maxent-dent}}{=} - \mathrm{h}[f(x)] - \int_{\mathcal{X}} f(x) \log g(x) \, \mathrm{d}x \; .
\end{split}
\end{equation}

By plugging the probability density function of the continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-pdf}) into the second term, we obtain:

\begin{equation} \label{eq:cuni-maxent-int-fg-s1}
\begin{split}
\int_{\mathcal{X}} f(x) \log g(x) \, \mathrm{d}x &= \int_{\mathcal{X}} f(x) \log \frac{1}{b-a} \, \mathrm{d}x \\
&= \log \frac{1}{b-a} \int_{\mathcal{X}} f(x) \, \mathrm{d}x \\
&= -\log(b-a) \; .
\end{split}
\end{equation}

This is actually the negative of the differential entropy of the continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni-dent}), such that:

\begin{equation} \label{eq:cuni-maxent-int-fg-s2}
\int_{\mathcal{X}} f(x) \log g(x) \, \mathrm{d}x = -\mathrm{h}[\mathcal{U}(a,b)] = -\mathrm{h}[g(x)] \; .
\end{equation}

Combining \eqref{eq:cuni-maxent-kl-fg} with \eqref{eq:cuni-maxent-int-fg-s2}, we can show that

\begin{equation} \label{eq:cuni-maxent-cuni-maxent}
\begin{split}
0 &\leq \mathrm{KL}[f||g] \\
0 &\leq - \mathrm{h}[f(x)] - \left( -\mathrm{h}[g(x)] \right) \\
\mathrm{h}[g(x)] &\geq \mathrm{h}[f(x)]
\end{split}
\end{equation}

which means that the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of the continuous uniform distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:cuni}) $\mathcal{U}(a,b)$ will be larger than or equal to any other distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) defined in the same range.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2023): "Maximum entropy probability distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2023-08-25; URL: \url{https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution#Uniform_and_piecewise_uniform_distributions}.
\end{itemize}
\vspace{1em}



\subsection{Normal distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:norm}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, $X$ is said to be normally distributed with mean $\mu$ and variance $\sigma^2$ (or, standard deviation $\sigma$)

\begin{equation} \label{eq:norm-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; ,
\end{equation}

if and only if its probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) is given by

\begin{equation} \label{eq:norm-norm-pdf}
\mathcal{N}(x; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right]
\end{equation}

where $\mu \in \mathbb{R}$ and $\sigma^2 > 0$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-27; URL: \url{https://en.wikipedia.org/wiki/Normal_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Special case of multivariate normal distribution}]{Special case of multivariate normal distribution} \label{sec:norm-mvn}
\setcounter{equation}{0}

\textbf{Theorem:} The normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) is a special case of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) with number of variables $n = 1$, i.e. random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $x \in \mathbb{R}$, mean $\mu \in \mathbb{R}$ and covariance matrix $\Sigma = \sigma^2$.


\vspace{1em}
\textbf{Proof:} The probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}) is

\begin{equation} \label{eq:norm-mvn-mvn-pdf}
\mathcal{N}(x; \mu, \Sigma) = \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right] \; .
\end{equation}

Setting $n = 1$, such that $x, \mu \in \mathbb{R}$, and $\Sigma = \sigma^2$, we obtain

\begin{equation} \label{eq:norm-mvn-norm-pdf}
\begin{split}
\mathcal{N}(x; \mu, \sigma^2) &= \frac{1}{\sqrt{(2 \pi)^1 |\sigma^2|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} (\sigma^2)^{-1} (x-\mu) \right] \\
&= \frac{1}{\sqrt{(2\pi) \sigma^2}} \cdot \exp\left[-\frac{1}{2 \sigma^2} (x-\mu)^2 \right] \\
&= \frac{1}{\sqrt{2\pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right]
\end{split}
\end{equation}

which is equivalent to the probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}).
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Multivariate normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-08-19; URL: \url{https://en.wikipedia.org/wiki/Multivariate_normal_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Standard normal distribution}]{Standard normal distribution} \label{sec:snorm}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, $X$ is said to be standard normally distributed, if $X$ follows a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with mean $\mu = 0$ and variance $\sigma^2 = 1$:

\begin{equation} \label{eq:snorm-snorm}
X \sim \mathcal{N}(0, 1) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-26; URL: \url{https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relationship to standard normal distribution}]{Relationship to standard normal distribution} \label{sec:norm-snorm}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with mean $\mu$ and variance $\sigma^2$:

\begin{equation} \label{eq:norm-snorm-X-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the quantity $Z = (X-\mu)/\sigma$ will have a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) with mean $0$ and variance $1$:

\begin{equation} \label{eq:norm-snorm-Z-snorm}
Z = \frac{X-\mu}{\sigma} \sim \mathcal{N}(0, 1) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Note that $Z$ is a function of $X$

\begin{equation} \label{eq:norm-snorm-Z-X}
Z = g(X) = \frac{X-\mu}{\sigma}
\end{equation}

with the inverse function

\begin{equation} \label{eq:norm-snorm-X-Z}
X = g^{-1}(Z) = \sigma Z + \mu \; .
\end{equation}

Because $\sigma$ is positive, $g(X)$ is strictly increasing and we can calculate the cumulative distribution function of a strictly increasing function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf-sifct}) as

\begin{equation} \label{eq:norm-snorm-cdf-sifct}
F_Y(y) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; y < \mathrm{min}(\mathcal{Y}) \\
F_X(g^{-1}(y)) \; , & \text{if} \; y \in \mathcal{Y} \\
1 \; , & \text{if} \; y > \mathrm{max}(\mathcal{Y}) \; .
\end{array}
\right.
\end{equation}

The cumulative distribution function of the normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-cdf}) $X$ is

\begin{equation} \label{eq:norm-snorm-norm-cdf}
F_X(x) = \int_{-\infty}^{x} \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{t-\mu}{\sigma} \right)^2 \right] \, \mathrm{d}t \; .
\end{equation}

Applying \eqref{eq:norm-snorm-cdf-sifct} to \eqref{eq:norm-snorm-norm-cdf}, we have:

\begin{equation} \label{eq:norm-snorm-Z-cdf-s1}
\begin{split}
F_Z(z) &\overset{\eqref{eq:norm-snorm-cdf-sifct}}{=} F_X(g^{-1}(z)) \\
&\overset{\eqref{eq:norm-snorm-norm-cdf}}{=} \int_{-\infty}^{\sigma z + \mu} \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{t-\mu}{\sigma} \right)^2 \right] \, \mathrm{d}t \; .
\end{split}
\end{equation}

Substituting $s = (t - \mu)/\sigma$, such that $t = \sigma s + \mu$, we obtain

\begin{equation} \label{eq:norm-snorm-Z-cdf-s2}
\begin{split}
F_Z(z) &= \int_{(-\infty - \mu)/\sigma}^{([\sigma z + \mu] - \mu)/\sigma} \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{(\sigma s + \mu)-\mu}{\sigma} \right)^2 \right] \, \mathrm{d}(\sigma s + \mu) \\
&= \int_{-\infty}^{z} \frac{\sigma}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} s^2 \right] \, \mathrm{d}s \\
&= \int_{-\infty}^{z} \frac{1}{\sqrt{2 \pi}} \cdot \exp \left[ -\frac{1}{2} s^2 \right] \, \mathrm{d}s
\end{split}
\end{equation}

which is the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of the standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Relationship to standard normal distribution}]{Relationship to standard normal distribution} \label{sec:norm-snorm2}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with mean $\mu$ and variance $\sigma^2$:

\begin{equation} \label{eq:norm-snorm2-X-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the quantity $Z = (X-\mu)/\sigma$ will have a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) with mean $0$ and variance $1$:

\begin{equation} \label{eq:norm-snorm2-Z-snorm}
Z = \frac{X-\mu}{\sigma} \sim \mathcal{N}(0, 1) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Note that $Z$ is a function of $X$

\begin{equation} \label{eq:norm-snorm2-Z-X}
Z = g(X) = \frac{X-\mu}{\sigma}
\end{equation}

with the inverse function

\begin{equation} \label{eq:norm-snorm2-X-Z}
X = g^{-1}(Z) = \sigma Z + \mu \; .
\end{equation}

Because $\sigma$ is positive, $g(X)$ is strictly increasing and we can calculate the probability density function of a strictly increasing function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-sifct}) as

\begin{equation} \label{eq:norm-snorm2-pdf-sifct}
f_Y(y) = \left\{
\begin{array}{rl}
f_X(g^{-1}(y)) \, \frac{\mathrm{d}g^{-1}(y)}{\mathrm{d}y} \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y \notin \mathcal{Y}
\end{array}
\right.
\end{equation}

where $\mathcal{Y} = \left\lbrace y = g(x): x \in \mathcal{X} \right\rbrace$. With the probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}), we have

\begin{equation} \label{eq:norm-snorm2-pdf-Z}
\begin{split}
f_Z(z) &= \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{g^{-1}(z)-\mu}{\sigma} \right)^2 \right] \cdot \frac{\mathrm{d}g^{-1}(z)}{\mathrm{d}z} \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{(\sigma z + \mu)-\mu}{\sigma} \right)^2 \right] \cdot \frac{\mathrm{d}(\sigma z + \mu)}{\mathrm{d}z} \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} z^2 \right] \cdot \sigma \\
&= \frac{1}{\sqrt{2 \pi}} \cdot \exp \left[ -\frac{1}{2} z^2 \right]
\end{split}
\end{equation}

which is the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of the standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Relationship to standard normal distribution}]{Relationship to standard normal distribution} \label{sec:norm-snorm3}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with mean $\mu$ and variance $\sigma^2$:

\begin{equation} \label{eq:norm-snorm3-X-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the quantity $Z = (X-\mu)/\sigma$ will have a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) with mean $0$ and variance $1$:

\begin{equation} \label{eq:norm-snorm3-Z-snorm}
Z = \frac{X-\mu}{\sigma} \sim \mathcal{N}(0, 1) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The linear transformation theorem for multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}) states

\begin{equation} \label{eq:norm-snorm3-mvn-ltt}
x \sim \mathcal{N}(\mu, \Sigma) \quad \Rightarrow \quad y = Ax + b \sim \mathcal{N}(A\mu + b, A \Sigma A^\mathrm{T})
\end{equation}

where $x$ is an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) with mean $\mu$ and covariance $\Sigma$, $A$ is an $m \times n$ matrix and $b$ is an $m \times 1$ vector. Note that

\begin{equation} \label{eq:norm-snorm3-Z-X}
Z = \frac{X-\mu}{\sigma} = \frac{X}{\sigma} - \frac{\mu}{\sigma}
\end{equation}

is a special case of \eqref{eq:norm-snorm3-mvn-ltt} with $x = X$, $\mu = \mu$, $\Sigma = \sigma^2$, $A = 1/\sigma$ and $b = \mu/\sigma$. Applying theorem \eqref{eq:norm-snorm3-mvn-ltt} to $Z$ as a function of $X$, we have

\begin{equation} \label{eq:norm-snorm3-mvn-ltt-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \quad \Rightarrow \quad Z = \frac{X}{\sigma} - \frac{\mu}{\sigma} \sim \mathcal{N}\left( \frac{\mu}{\sigma} - \frac{\mu}{\sigma}, \frac{1}{\sigma} \cdot \sigma^2 \cdot \frac{1}{\sigma} \right)
\end{equation}

which results in the distribution:

\begin{equation} \label{eq:norm-snorm3-Z-snorm-qed}
Z \sim \mathcal{N}(0, 1) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Relationship to chi-squared distribution}]{Relationship to chi-squared distribution} \label{sec:norm-chi2}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X_1, \ldots, X_n$ be independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) where each of them is following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with mean $\mu$ and variance $\sigma^2$:

\begin{equation} \label{eq:norm-chi2-norm}
X_i \sim \mathcal{N}(\mu, \sigma^2) \quad \text{for} \quad i = 1, \ldots, n \; .
\end{equation}

Define the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp})

\begin{equation} \label{eq:norm-chi2-mean-samp}
\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i
\end{equation}

and the unbiased sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp})

\begin{equation} \label{eq:norm-chi2-var-samp}
s^2 = \frac{1}{n-1} \sum_{i=1}^{n} \left( X_i - \bar{X} \right)^2 \; .
\end{equation}

Then, the sampling distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-samp}) of the sample variance is given by a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) with $n-1$ degrees of freedom:

\begin{equation} \label{eq:norm-chi2-norm-chi2}
V = (n-1) \, \frac{s^2}{\sigma^2} \sim \chi^2(n-1) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Consider the random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $U_i$ defined as

\begin{equation} \label{eq:norm-chi2-Ui}
U_i = \frac{X_i - \mu}{\sigma}
\end{equation}

which follows a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-snorm})

\begin{equation} \label{eq:norm-chi2-norm-snorm}
U_i \sim \mathcal{N}(0,1) \; .
\end{equation}

Then, the sum of squared random variables $U_i$ can be rewritten as

\begin{equation} \label{eq:norm-chi2-sum-Ui2-s1}
\begin{split}
\sum_{i=1}^{n} U_i^2 &= \sum_{i=1}^{n} \left( \frac{X_i - \mu}{\sigma} \right)^2 \\
&= \sum_{i=1}^{n} \left( \frac{(X_i - \bar{X}) + (\bar{X} - \mu)}{\sigma} \right)^2 \\
&= \sum_{i=1}^{n} \frac{(X_i - \bar{X})^2}{\sigma^2} + \sum_{i=1}^{n} \frac{(\bar{X} - \mu)^2}{\sigma^2} + 2 \sum_{i=1}^{n} \frac{(X_i - \bar{X})(\bar{X} - \mu)}{\sigma^2} \\
&= \sum_{i=1}^{n} \left( \frac{X_i - \bar{X}}{\sigma} \right)^2 + \sum_{i=1}^{n} \left( \frac{\bar{X} - \mu}{\sigma} \right)^2 + 2\frac{(\bar{X} - \mu)}{\sigma^2} \sum_{i=1}^{n} (X_i - \bar{X}) \; .
\end{split}
\end{equation}

Because the following sum is zero

\begin{equation} \label{eq:norm-chi2-Xi-Xb}
\begin{split}
\sum_{i=1}^{n} (X_i - \bar{X}) &= \sum_{i=1}^{n} X_i - n \bar{X} \\
&= \sum_{i=1}^{n} X_i - n \cdot \frac{1}{n} \sum_{i=1}^{n} X_i \\
&= \sum_{i=1}^{n} X_i - \sum_{i=1}^{n} X_i \\
&= 0 \; ,
\end{split}
\end{equation}

the third term disappears, i.e.

\begin{equation} \label{eq:norm-chi2-sum-Ui2-s2}
\sum_{i=1}^{n} U_i^2 = \sum_{i=1}^{n} \left( \frac{X_i - \bar{X}}{\sigma} \right)^2 + \sum_{i=1}^{n} \left( \frac{\bar{X} - \mu}{\sigma} \right)^2 \; .
\end{equation}

Cochran's theorem states that, if a sum of squared standard normal ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) can be written as a sum of squared forms

\begin{equation} \label{eq:norm-chi2-cochran-p1}
\begin{split}
\sum_{i=1}^{n} U_i^2 = \sum_{j=1}^{m} Q_j \quad &\text{where} \quad Q_j = \sum_{k=1}^{n} \sum_{l=1}^{n} U_k B^{(j)}_{kl} U_l \\
&\text{with} \quad \sum_{j=1}^{m} B^{(j)} = I_n \\
&\text{and} \quad r_j = \mathrm{rank}(B^{(j)}) \; ,
\end{split}
\end{equation}

then the terms $Q_j$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) and each term $Q_j$ follows a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) with $r_j$ degrees of freedom:

\begin{equation} \label{eq:norm-chi2-cochran-p2}
Q_j \sim \chi^2(r_j) \; .
\end{equation}

We observe that \eqref{eq:norm-chi2-sum-Ui2-s2} can be represented as

\begin{equation} \label{eq:norm-chi2-sum-Ui2-s3}
\begin{split}
\sum_{i=1}^{n} U_i^2 &= \sum_{i=1}^{n} \left( \frac{X_i - \bar{X}}{\sigma} \right)^2 + \sum_{i=1}^{n} \left( \frac{\bar{X} - \mu}{\sigma} \right)^2 \\
= Q_1 + Q_2 &= \sum_{i=1}^{n} \left( U_i - \frac{1}{n} \sum_{j=1}^n U_j \right)^2 + \frac{1}{n} \left( \sum_{i=1}^{n} U_i \right)^2
\end{split}
\end{equation}

where, with the $n \times n$ matrix of ones $J_n$, the matrices $B^{(j)}$ are

\begin{equation} \label{eq:norm-chi2-sum-Ui2-s3-Bj}
B^{(1)} = I_n - \frac{J_n}{n} \quad \text{and} \quad B^{(2)} = \frac{J_n}{n} \; .
\end{equation}

Because all columns of $B^{(2)}$ are identical, it has rank $r_2 = 1$. Because the $n$ columns of $B^{(1)}$ add up to zero, it has rank $r_1 = n-1$. Thus, the conditions of Cochran's theorem are met and the squared form

\begin{equation} \label{eq:norm-chi2-Q1}
Q_1 = \sum_{i=1}^{n} \left( \frac{X_i - \bar{X}}{\sigma} \right)^2 = (n-1) \, \frac{1}{\sigma^2} \, \frac{1}{n-1} \sum_{i=1}^{n} \left( X_i - \bar{X} \right)^2 = (n-1) \, \frac{s^2}{\sigma^2}
\end{equation}

follows a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) with $n-1$ degrees of freedom:

\begin{equation} \label{eq:norm-chi2-norm-chi2-qed}
(n-1) \, \frac{s^2}{\sigma^2} \sim \chi^2(n-1) \; .
\end{equation}

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Glen-b (2014): "Why is the sampling distribution of variance a chi-squared distribution?"; in: \textit{StackExchange CrossValidated}, retrieved on 2021-05-20; URL: \url{https://stats.stackexchange.com/questions/121662/why-is-the-sampling-distribution-of-variance-a-chi-squared-distribution}.
\item Wikipedia (2021): "Cochran's theorem"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-20; URL: \url{https://en.wikipedia.org/wiki/Cochran%27s_theorem#Sample_mean_and_sample_variance}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relationship to t-distribution}]{Relationship to t-distribution} \label{sec:norm-t}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X_1, \ldots, X_n$ be independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) where each of them is following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with mean $\mu$ and variance $\sigma^2$:

\begin{equation} \label{eq:norm-t-norm}
X_i \sim \mathcal{N}(\mu, \sigma^2) \quad \text{for} \quad i = 1, \ldots, n \; .
\end{equation}

Define the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp})

\begin{equation} \label{eq:norm-t-mean-samp}
\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i
\end{equation}

and the unbiased sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp})

\begin{equation} \label{eq:norm-t-var-samp}
s^2 = \frac{1}{n-1} \sum_{i=1}^{n} \left( X_i - \bar{X} \right)^2 \; .
\end{equation}

Then, subtracting $\mu$ from the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), dividing by the sample standard deviation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:std}) and multiplying with $\sqrt{n}$ results in a qunatity that follows a t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) with $n-1$ degrees of freedom:

\begin{equation} \label{eq:norm-t-norm-t}
t = \sqrt{n} \, \frac{\bar{X}-\mu}{s} \sim \mathrm{t}(n-1) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Note that $\bar{X}$ is a linear combination of $X_1, \ldots, X_n$:

\begin{equation} \label{eq:norm-t-X-bar-lincomb}
\bar{X} = \frac{1}{n} X_1, + \ldots + \frac{1}{n} X_n \; .
\end{equation}

Because the linear combination of independent normal random variables is also normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-lincomb}), we have:

\begin{equation} \label{eq:norm-t-X-bar-dist}
\bar{X} \sim \mathcal{N}\left( \frac{1}{n} \, n \mu, \left(\frac{1}{n}\right)^2 n \sigma^2 \right) = \mathcal{N}\left( \mu, \sigma^2/n \right) \; .
\end{equation}

Let $Z = \sqrt{n} \, (\bar{X}-\mu)/\sigma$. Because $Z$ is a linear transformation ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}) of $\bar{X}$, it also follows a normal distribution:

\begin{equation} \label{eq:norm-t-Z-dist}
Z = \sqrt{n} \frac{\bar{X}-\mu}{\sigma} \sim \mathcal{N}\left( \frac{\sqrt{n}}{\sigma} (\mu - \mu), \left(\frac{\sqrt{n}}{\sigma}\right)^2 \sigma^2/n \right) = \mathcal{N}\left( 0, 1 \right) \; .
\end{equation}

Let $V = (n-1) \, s^2/\sigma^2$. We know that this function of the sample variance follows a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-chi2}) with $n-1$ degrees of freedom:

\begin{equation} \label{eq:norm-t-V-dist}
V = (n-1) \frac{s^2}{\sigma^2} \sim \chi^2(n-1) \; .
\end{equation}

Observe that $t$ is the ratio of a standard normal random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) and the square root of a chi-squared random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}), divided by its degrees of freedom:

\begin{equation} \label{eq:norm-t-t-Z-V}
t = \sqrt{n} \, \frac{\bar{X}-\mu}{s} = \frac{\sqrt{n} \frac{\bar{X}-\mu}{\sigma}}{\sqrt{(n-1) \frac{s^2}{\sigma^2}/(n-1)}} =  \frac{Z}{\sqrt{V/(n-1)}} \; .
\end{equation}

Thus, by definition of the t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}), this ratio follows a t-distribution with $n-1$ degrees of freedom:

\begin{equation} \label{eq:norm-t-norm-t-qed}
t \sim \mathrm{t}(n-1) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Student's t-distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-05-27; URL: \url{https://en.wikipedia.org/wiki/Student%27s_t-distribution#Characterization}.
\item Wikipedia (2021): "Normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-05-27; URL: \url{https://en.wikipedia.org/wiki/Normal_distribution#Operations_on_multiple_independent_normal_variables}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Gaussian integral}]{Gaussian integral} \label{sec:norm-gi}
\setcounter{equation}{0}

\textbf{Theorem:} The definite integral of $\mathrm{exp}\left[ -x^2 \right]$ from $-\infty$ to $+\infty$ is equal to the square root of $\pi$:

\begin{equation} \label{eq:norm-gi-norm-gi}
\int_{-\infty}^{+\infty} \mathrm{exp}\left[ -x^2 \right] \, \mathrm{d}x = \sqrt{\pi} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Let

\begin{equation} \label{eq:norm-gi-I}
I = \int_{0}^{\infty} \mathrm{exp}\left[ -x^2 \right] \, \mathrm{d}x
\end{equation}

and 

\begin{equation} \label{eq:norm-gi-IP}
I_P = \int_{0}^{P} \mathrm{exp}\left[ -x^2 \right] \, \mathrm{d}x = \int_{0}^{P} \mathrm{exp}\left[ -y^2 \right] \, \mathrm{d}y \; .
\end{equation}

Then, we have

\begin{equation} \label{eq:norm-gi-IP-I}
\lim\limits_{P \rightarrow \infty} I_P = I
\end{equation}

and

\begin{equation} \label{eq:norm-gi-IP2-I2}
\lim\limits_{P \rightarrow \infty} I_P^2 = I^2 \; .
\end{equation}

Moreover, we can write

\begin{equation} \label{eq:norm-gi-IP2}
\begin{split}
I_P^2 &\overset{\eqref{eq:norm-gi-IP}}{=} \left( \int_{0}^{P} \mathrm{exp}\left[ -x^2 \right] \, \mathrm{d}x \right) \left( \int_{0}^{P} \mathrm{exp}\left[ -y^2 \right] \, \mathrm{d}y \right) \\
&= \int_{0}^{P} \int_{0}^{P} \mathrm{exp}\left[ - \left( x^2 + y^2 \right) \right] \, \mathrm{d}x \, \mathrm{d}y \\
&= \iint_{S_P} \mathrm{exp}\left[ - \left( x^2 + y^2 \right) \right] \, \mathrm{d}x \, \mathrm{d}y
\end{split}
\end{equation}

where $S_P$ is the square with corners $(0,0)$, $(0,P)$, $(P,P)$ and $(P,0)$. For this integral, we can write down the following inequality

\begin{equation} \label{eq:norm-gi-IP2-ineq}
\iint_{C_1} \mathrm{exp}\left[ - \left( x^2 + y^2 \right) \right] \, \mathrm{d}x \, \mathrm{d}y \leq I_P^2 \leq \iint_{C_2} \mathrm{exp}\left[ - \left( x^2 + y^2 \right) \right] \, \mathrm{d}x \, \mathrm{d}y
\end{equation}

where $C_1$ and $C_2$ are the regions in the first quadrant bounded by circles with center at $(0,0)$ and going through the points $(0,P)$ and $(P,P)$, respectively. The radii of these two circles are $r_1 = \sqrt{P^2} = P$ and $r_2 = \sqrt{2 P^2} = P \sqrt{2}$, such that we can rewrite equation \eqref{eq:norm-gi-IP2-ineq} using polar coordinates as

\begin{equation} \label{eq:norm-gi-IP2-ineq-PC}
\int_{0}^{\frac{\pi}{2}} \int_{0}^{r_1} \mathrm{exp}\left[ -r^2 \right] \, r \, \mathrm{d}r \, \mathrm{d}\theta \leq I_P^2 \leq \int_{0}^{\frac{\pi}{2}} \int_{0}^{r_2} \mathrm{exp}\left[ -r^2 \right] \, r \, \mathrm{d}r \, \mathrm{d}\theta \; .
\end{equation}

Solving the definite integrals yields:

\begin{equation} \label{eq:norm-gi-IP2-ineq-PC-int}
\begin{split}
\int_{0}^{\frac{\pi}{2}} \int_{0}^{r_1} \mathrm{exp}\left[ -r^2 \right] r \, \mathrm{d}r \, \mathrm{d}\theta &\leq I_P^2 \leq \int_{0}^{\frac{\pi}{2}} \int_{0}^{r_2} \mathrm{exp}\left[ -r^2 \right] r \, \mathrm{d}r \, \mathrm{d}\theta \\
\int_{0}^{\frac{\pi}{2}} \left[ -\frac{1}{2} \mathrm{exp}\left[ -r^2 \right] \right]_{0}^{r_1} \, \mathrm{d}\theta &\leq I_P^2 \leq \int_{0}^{\frac{\pi}{2}} \left[ -\frac{1}{2} \mathrm{exp}\left[ -r^2 \right] \right]_{0}^{r_2} \, \mathrm{d}\theta \\
-\frac{1}{2} \int_{0}^{\frac{\pi}{2}} \left( \mathrm{exp}\left[ -r_1^2 \right] - 1 \right) \, \mathrm{d}\theta &\leq I_P^2 \leq -\frac{1}{2} \int_{0}^{\frac{\pi}{2}} \left( \mathrm{exp}\left[ -r_2^2 \right] - 1 \right) \, \mathrm{d}\theta \\
-\frac{1}{2} \left[ \left( \mathrm{exp}\left[ -r_1^2 \right] - 1 \right) \theta \right]_{0}^{\frac{\pi}{2}} &\leq I_P^2 \leq -\frac{1}{2} \left[ \left( \mathrm{exp}\left[ -r_2^2 \right] - 1 \right) \theta \right]_{0}^{\frac{\pi}{2}} \\
\frac{1}{2} \left( 1 - \mathrm{exp}\left[ -r_1^2 \right] \right) \frac{\pi}{2} &\leq I_P^2 \leq \frac{1}{2} \left( 1 - \mathrm{exp}\left[ -r_2^2 \right] \right) \frac{\pi}{2} \\
\frac{\pi}{4} \left( 1 - \mathrm{exp}\left[ -P^2 \right] \right) &\leq I_P^2 \leq \frac{\pi}{4} \left( 1 - \mathrm{exp}\left[ -2 P^2 \right] \right)
\end{split}
\end{equation}

Calculating the limit for $P \rightarrow \infty$, we obtain

\begin{equation} \label{eq:norm-gi-IP2-ineq-PC-int-lim}
\begin{split}
\lim\limits_{P \rightarrow \infty} \frac{\pi}{4} \left( 1 - \mathrm{exp}\left[ -P^2 \right] \right) \leq \lim\limits_{P \rightarrow \infty} I_P^2 &\leq \lim\limits_{P \rightarrow \infty} \frac{\pi}{4} \left( 1 - \mathrm{exp}\left[ -2 P^2 \right] \right) \\
\frac{\pi}{4} \leq I^2 &\leq \frac{\pi}{4} \; ,
\end{split}
\end{equation}

such that we have a preliminary result for $I$:

\begin{equation} \label{eq:norm-gi-I-qed}
I^2 = \frac{\pi}{4} \quad \Rightarrow \quad I = \frac{\sqrt{\pi}}{2} \; .
\end{equation}

Because the integrand in \eqref{eq:norm-gi-norm-gi} is an even function, we can calculate the final result as follows:

\begin{equation} \label{eq:norm-gi-norm-gi-qed}
\begin{split}
\int_{-\infty}^{+\infty} \mathrm{exp}\left[ -x^2 \right] \, \mathrm{d}x &= 2 \int_{0}^{\infty} \mathrm{exp}\left[ -x^2 \right] \, \mathrm{d}x \\
&\overset{\eqref{eq:norm-gi-I-qed}}{=} 2 \, \frac{\sqrt{\pi}}{2} \\
&= \sqrt{\pi} \; . 
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item ProofWiki (2020): "Gaussian Integral"; in: \textit{ProofWiki}, retrieved on 2020-11-25; URL: \url{https://proofwiki.org/wiki/Gaussian_Integral}.
\item ProofWiki (2020): "Integral to Infinity of Exponential of minus t squared"; in: \textit{ProofWiki}, retrieved on 2020-11-25; URL: \url{https://proofwiki.org/wiki/Integral_to_Infinity_of_Exponential_of_-t%5E2}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:norm-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-pdf-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ is

\begin{equation} \label{eq:norm-pdf-norm-pdf}
f_X(x) = \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Moment-generating function}]{Moment-generating function} \label{sec:norm-mgf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-mgf-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) of $X$ is

\begin{equation} \label{eq:norm-mgf-norm-mgf}
M_X(t) = \exp\left[ \mu t + \frac{1}{2} \sigma^2 t^2 \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) is

\begin{equation} \label{eq:norm-mgf-norm-pdf}
f_X(x) = \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right]
\end{equation}

and the moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) is defined as

\begin{equation} \label{eq:norm-mgf-mgf-var}
M_X(t) = \mathrm{E} \left[ e^{tX} \right] \; .
\end{equation}

Using the expected value for continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), the moment-generating function of $X$ therefore is

\begin{equation} \label{eq:norm-mgf-norm-mgf-s1}
\begin{split}
M_X(t) &= \int_{-\infty}^{+\infty} \exp[tx] \cdot \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \, \mathrm{d}x \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{+\infty} \exp\left[ tx - \frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \, \mathrm{d}x \; .
\end{split}
\end{equation}

Substituting $u = (x-\mu)/(\sqrt{2}\sigma)$, i.e. $x = \sqrt{2}\sigma u + \mu$, we have

\begin{equation} \label{eq:norm-mgf-norm-mgf-s2}
\begin{split}
M_X(t) &= \frac{1}{\sqrt{2 \pi} \sigma} \int_{(-\infty-\mu)/(\sqrt{2}\sigma)}^{(+\infty-\mu)/(\sqrt{2}\sigma)} \exp\left[ t\left( \sqrt{2} \sigma u + \mu \right) - \frac{1}{2} \left( \frac{ \sqrt{2} \sigma u + \mu - \mu}{\sigma} \right)^2 \right] \, \mathrm{d}\left( \sqrt{2} \sigma u + \mu \right) \\
&= \frac{\sqrt{2} \sigma}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{+\infty} \exp\left[ \left( \sqrt{2} \sigma u + \mu \right) t - u^2 \right] \, \mathrm{d}u \\
&= \frac{\exp(\mu t)}{\sqrt{\pi}} \int_{-\infty}^{+\infty} \exp\left[ \sqrt{2} \sigma u t - u^2 \right] \, \mathrm{d}u \\
&= \frac{\exp(\mu t)}{\sqrt{\pi}} \int_{-\infty}^{+\infty} \exp\left[ - \left( u^2 - \sqrt{2} \sigma u t \right) \right] \, \mathrm{d}u \\
&= \frac{\exp(\mu t)}{\sqrt{\pi}} \int_{-\infty}^{+\infty} \exp\left[ - \left( u - \frac{\sqrt{2}}{2} \sigma t \right)^2 + \frac{1}{2} \sigma^2 t^2 \right] \, \mathrm{d}u \\
&= \frac{\exp\left[ \mu t + \frac{1}{2} \sigma^2 t^2 \right]}{\sqrt{\pi}} \int_{-\infty}^{+\infty} \exp\left[ - \left( u - \frac{\sqrt{2}}{2} \sigma t \right)^2 \right] \, \mathrm{d}u
\end{split}
\end{equation}

Now substituting $v = u - \sqrt{2}/2 \, \sigma t$, i.e. $u = v + \sqrt{2}/2 \, \sigma t$, we have

\begin{equation} \label{eq:norm-mgf-norm-mgf-s3}
\begin{split}
M_X(t) &= \frac{\exp\left[ \mu t + \frac{1}{2} \sigma^2 t^2 \right]}{\sqrt{\pi}} \int_{-\infty - \sqrt{2}/2 \, \sigma t}^{+\infty - \sqrt{2}/2 \, \sigma t} \exp\left[ -v^2 \right] \, \mathrm{d}\left( v + \sqrt{2}/2 \, \sigma t \right) \\
&= \frac{\exp\left[ \mu t + \frac{1}{2} \sigma^2 t^2 \right]}{\sqrt{\pi}} \int_{-\infty}^{+\infty} \exp\left[ -v^2 \right] \, \mathrm{d}v \; .
\end{split}
\end{equation}

With the Gaussian integral ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-gi})

\begin{equation} \label{eq:norm-mgf-gauss}
\int_{-\infty}^{+\infty} \exp\left[ -x^2 \right] \, \mathrm{d}x = \sqrt{\pi} \; ,
\end{equation}

this finally becomes

\begin{equation} \label{eq:norm-mgf-norm-mgf-qed}
M_X(t) = \exp\left[ \mu t + \frac{1}{2} \sigma^2 t^2 \right] \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item ProofWiki (2020): "Moment Generating Function of Gaussian Distribution"; in: \textit{ProofWiki}, retrieved on 2020-03-03; URL: \url{https://proofwiki.org/wiki/Moment_Generating_Function_of_Gaussian_Distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Cumulative distribution function}]{Cumulative distribution function} \label{sec:norm-cdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-cdf-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$ is

\begin{equation} \label{eq:norm-cdf-norm-cdf}
F_X(x) = \frac{1}{2} \left[ 1 + \mathrm{erf}\left( \frac{x-\mu}{\sqrt{2} \sigma} \right) \right]
\end{equation}

where $\mathrm{erf}(x)$ is the error function defined as

\begin{equation} \label{eq:norm-cdf-erf}
\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} \exp(-t^2) \, \mathrm{d}t \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) is:

\begin{equation} \label{eq:norm-cdf-norm-pdf}
f_X(x) = \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \; .
\end{equation}

Thus, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) is:

\begin{equation} \label{eq:norm-cdf-norm-cdf-s1}
\begin{split}
F_X(x) &= \int_{-\infty}^{x} \mathcal{N}(z; \mu, \sigma^2) \, \mathrm{d}z \\
&= \int_{-\infty}^{x} \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{z-\mu}{\sigma} \right)^2 \right] \, \mathrm{d}z \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{x} \exp \left[ -\left( \frac{z-\mu}{\sqrt{2} \sigma} \right)^2 \right] \, \mathrm{d}z \; .
\end{split}
\end{equation}

Substituting $t = (z-\mu)/(\sqrt{2} \sigma)$, i.e. $z = \sqrt{2} \sigma t + \mu$, this becomes:

\begin{equation} \label{eq:norm-cdf-norm-cdf-s2}
\begin{split}
F_X(x) &= \frac{1}{\sqrt{2 \pi} \sigma} \int_{(-\infty-\mu)/(\sqrt{2} \sigma)}^{(x-\mu)/(\sqrt{2} \sigma)} \exp(-t^2) \, \mathrm{d}\left( \sqrt{2} \sigma t + \mu \right) \\
&= \frac{\sqrt{2} \sigma}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{\frac{x-\mu}{\sqrt{2} \sigma}} \exp(-t^2) \, \mathrm{d}t \\
&= \frac{1}{\sqrt{\pi}} \int_{-\infty}^{\frac{x-\mu}{\sqrt{2} \sigma}} \exp(-t^2) \, \mathrm{d}t \\
&= \frac{1}{\sqrt{\pi}} \int_{-\infty}^{0} \exp(-t^2) \, \mathrm{d}t + \frac{1}{\sqrt{\pi}} \int_{0}^{\frac{x-\mu}{\sqrt{2} \sigma}} \exp(-t^2) \, \mathrm{d}t \\
&= \frac{1}{\sqrt{\pi}} \int_{0}^{\infty} \exp(-t^2) \, \mathrm{d}t + \frac{1}{\sqrt{\pi}} \int_{0}^{\frac{x-\mu}{\sqrt{2} \sigma}} \exp(-t^2) \, \mathrm{d}t \; .
\end{split}
\end{equation}

Applying \eqref{eq:norm-cdf-erf} to \eqref{eq:norm-cdf-norm-cdf-s2}, we have:

\begin{equation} \label{eq:norm-cdf-norm-cdf-s3}
\begin{split}
F_X(x) &= \frac{1}{2} \lim_{x \to \infty} \mathrm{erf}(x) + \frac{1}{2} \, \mathrm{erf}\left( \frac{x-\mu}{\sqrt{2} \sigma} \right) \\
&= \frac{1}{2} + \frac{1}{2} \, \mathrm{erf}\left( \frac{x-\mu}{\sqrt{2} \sigma} \right) \\
&= \frac{1}{2} \left[ 1 + \mathrm{erf}\left( \frac{x-\mu}{\sqrt{2} \sigma} \right) \right] \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-20; URL: \url{https://en.wikipedia.org/wiki/Normal_distribution#Cumulative_distribution_function}.
\item Wikipedia (2020): "Error function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-20; URL: \url{https://en.wikipedia.org/wiki/Error_function}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Cumulative distribution function without error function}]{Cumulative distribution function without error function} \label{sec:norm-cdfwerf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-cdfwerf-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$ can be expressed as

\begin{equation} \label{eq:norm-cdfwerf-norm-cdf}
F_X(x) = \Phi_{\mu,\sigma}(x) = \varphi\left( \frac{x-\mu}{\sigma} \right) \cdot \sum_{i=1}^{\infty} \frac{\left( \frac{x-\mu}{\sigma} \right)^{2i-1}}{(2i-1)!!} + \frac{1}{2}
\end{equation}

where $\varphi(x)$ is the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of the standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) and $n!!$ is a double factorial.


\vspace{1em}
\textbf{Proof:}

1) First, consider the standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) $\mathcal{N}(0, 1)$ which has the probability density function ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf})

\begin{equation} \label{eq:norm-cdfwerf-snorm-pdf}
\varphi(x) = \frac{1}{\sqrt{2 \pi}} \cdot e^{-\frac{1}{2} x^2} \; .
\end{equation}

Let $T(x)$ be the indefinite integral of this function. It can be obtained using infinitely repeated integration by parts as follows:

\begin{equation} \label{eq:norm-cdfwerf-snorm-pdf-ii-s1}
\begin{split}
T(x) &= \int \varphi(x) \, \mathrm{d}x \\
&= \int \frac{1}{\sqrt{2 \pi}} \cdot e^{-\frac{1}{2} x^2} \, \mathrm{d}x \\
&= \frac{1}{\sqrt{2 \pi}} \int 1 \cdot e^{-\frac{1}{2} x^2} \, \mathrm{d}x \\
&= \frac{1}{\sqrt{2 \pi}} \cdot \left[ x \cdot e^{-\frac{1}{2} x^2} + \int x^2 \cdot e^{-\frac{1}{2} x^2} \, \mathrm{d}x \right] \\
&= \frac{1}{\sqrt{2 \pi}} \cdot \left[ x \cdot e^{-\frac{1}{2} x^2} + \left[ \frac{1}{3} x^3 \cdot e^{-\frac{1}{2} x^2} + \int \frac{1}{3} x^4 \cdot e^{-\frac{1}{2} x^2} \, \mathrm{d}x \right] \right] \\
&= \frac{1}{\sqrt{2 \pi}} \cdot \left[ x \cdot e^{-\frac{1}{2} x^2} + \left[ \frac{1}{3} x^3 \cdot e^{-\frac{1}{2} x^2} + \left[ \frac{1}{15} x^5 \cdot e^{-\frac{1}{2} x^2} + \int \frac{1}{15} x^6 \cdot e^{-\frac{1}{2} x^2} \, \mathrm{d}x \right] \right] \right] \\
&= \ldots \\
&= \frac{1}{\sqrt{2 \pi}} \cdot \left[ \sum_{i=1}^{n} \left( \frac{x^{2i-1}}{(2i-1)!!} \cdot e^{-\frac{1}{2} x^2} \right) + \int \left( \frac{x^{2n}}{(2n-1)!!} \cdot e^{-\frac{1}{2} x^2} \right) \, \mathrm{d}x \right] \\
&= \frac{1}{\sqrt{2 \pi}} \cdot \left[ \sum_{i=1}^{\infty} \left( \frac{x^{2i-1}}{(2i-1)!!} \cdot e^{-\frac{1}{2} x^2} \right) + \lim_{n \to \infty} \int \left( \frac{x^{2n}}{(2n-1)!!} \cdot e^{-\frac{1}{2} x^2} \right) \, \mathrm{d}x \right] \; .
\end{split}
\end{equation}

Since $(2n-1)!!$ grows faster than $x^{2n}$, it holds that

\begin{equation} \label{eq:norm-cdfwerf-int-const}
\frac{1}{\sqrt{2 \pi}} \cdot \lim_{n \to \infty} \int \left( \frac{x^{2n}}{(2n-1)!!} \cdot e^{-\frac{1}{2} x^2} \right) \, \mathrm{d}x = \int 0 \, \mathrm{d}x = c
\end{equation}

for constant $c$, such that the indefinite integral becomes

\begin{equation} \label{eq:norm-cdfwerf-snorm-pdf-ii-s2}
\begin{split}
T(x) &= \frac{1}{\sqrt{2 \pi}} \cdot \sum_{i=1}^{\infty} \left( \frac{x^{2i-1}}{(2i-1)!!} \cdot e^{-\frac{1}{2} x^2} \right) + c \\
&= \frac{1}{\sqrt{2 \pi}} \cdot e^{-\frac{1}{2} x^2} \cdot \sum_{i=1}^{\infty} \frac{x^{2i-1}}{(2i-1)!!} + c \\
&\overset{\eqref{eq:norm-cdfwerf-snorm-pdf}}{=} \varphi(x) \cdot \sum_{i=1}^{\infty} \frac{x^{2i-1}}{(2i-1)!!} + c \; .
\end{split}
\end{equation}

2) Next, let $\Phi(x)$ be the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of the standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}):

\begin{equation} \label{eq:norm-cdfwerf-snorm-cdf}
\Phi(x) = \int_{-\infty}^x \varphi(x) \, \mathrm{d}x \; .
\end{equation}

It can be obtained by matching $T(0)$ to $\Phi(0)$ which is $1/2$, because the standard normal distribution is symmetric around zero:

\begin{equation} \label{eq:norm-cdfwerf-snorm-cdf-c}
\begin{split}
T(0) = \varphi(0) \cdot \sum_{i=1}^{\infty} \frac{0^{2i-1}}{(2i-1)!!} + c &= \frac{1}{2} = \Phi(0) \\
\Leftrightarrow c &= \frac{1}{2} \\
\Rightarrow \Phi(x) = \varphi(x) \cdot \sum_{i=1}^{\infty} \frac{x^{2i-1}}{(2i-1)!!} + \frac{1}{2} \! &\; .
\end{split}
\end{equation}

3) Finally, the cumulative distribution functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of the standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) and the general normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) are related to each other ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-snorm}) as

\begin{equation} \label{eq:norm-cdfwerf-norm-snorm-cdf}
\Phi_{\mu,\sigma}(x) = \Phi\left( \frac{x-\mu}{\sigma} \right) \; .
\end{equation}

Combining \eqref{eq:norm-cdfwerf-norm-snorm-cdf} with \eqref{eq:norm-cdfwerf-snorm-cdf-c}, we have:

\begin{equation} \label{eq:norm-cdfwerf-norm-cdf-qed}
\Phi_{\mu,\sigma}(x) = \varphi\left( \frac{x-\mu}{\sigma} \right) \cdot \sum_{i=1}^{\infty} \frac{\left( \frac{x-\mu}{\sigma} \right)^{2i-1}}{(2i-1)!!} + \frac{1}{2} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Soch J (2015): "Solution for the Indefinite Integral of the Standard Normal Probability Density Function"; in: \textit{arXiv stat.OT}, 1512.04858; URL: \url{https://arxiv.org/abs/1512.04858}.
\item Wikipedia (2020): "Normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-20; URL: \url{https://en.wikipedia.org/wiki/Normal_distribution#Cumulative_distribution_function}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability of being within standard deviations from mean}]{Probability of being within standard deviations from mean} \label{sec:norm-probstd}
\setcounter{equation}{0}

\textbf{Theorem:} (also called "68-95-99.7 rule") Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu$ and variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) $\sigma^2$. Then, about $68\%$, $95\%$ and $99.7\%$ of the values of $X$ will fall within 1, 2 and 3 standard deviations ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:std}) from the mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), respectively:

\begin{equation} \label{eq:norm-probstd-norm-probstd}
\begin{split}
\mathrm{Pr}(\mu-1\sigma \leq X \leq \mu+1\sigma) &\approx 68 \% \\
\mathrm{Pr}(\mu-2\sigma \leq X \leq \mu+2\sigma) &\approx 95 \% \\
\mathrm{Pr}(\mu-3\sigma \leq X \leq \mu+3\sigma) &\approx 99.7 \% \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} The cumulative distribution function of a normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-cdf}) random variable $X$ is

\begin{equation} \label{eq:norm-probstd-norm-cdf}
F_X(x) = \frac{1}{2} \left[ 1 + \mathrm{erf}\left( \frac{x-\mu}{\sqrt{2} \sigma} \right) \right]
\end{equation}

where $\mathrm{erf}(x)$ is the error function defined as

\begin{equation} \label{eq:norm-probstd-erf}
\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} \exp(-t^2) \, \mathrm{d}t
\end{equation}

which exhibits a point-symmetry property:

\begin{equation} \label{eq:norm-probstd-erf-symm}
\mathrm{erf}(-x) = -\mathrm{erf}(x) \; .
\end{equation}

Thus, the probability that $X$ falls between $\mu - a \cdot \sigma$ and $\mu + a \cdot \sigma$ is equal to:

\begin{equation} \label{eq:norm-probstd-prob-std}
\begin{split}
p(a) &= \mathrm{Pr}(\mu-a\sigma \leq X \leq \mu+a\sigma) \\
&= F_X(\mu+a\sigma) - F_X(\mu-a\sigma) \\
&\overset{\eqref{eq:norm-probstd-norm-cdf}}{=} \frac{1}{2} \left[ 1 + \mathrm{erf}\left( \frac{\mu+a\sigma-\mu}{\sqrt{2} \sigma} \right) \right] - \frac{1}{2} \left[ 1 + \mathrm{erf}\left( \frac{\mu-a\sigma-\mu}{\sqrt{2} \sigma} \right) \right] \\
&= \frac{1}{2} \left[ \mathrm{erf}\left( \frac{\mu+a\sigma-\mu}{\sqrt{2} \sigma} \right) - \mathrm{erf}\left( \frac{\mu-a\sigma-\mu}{\sqrt{2} \sigma} \right) \right] \\
&= \frac{1}{2} \left[ \mathrm{erf}\left( \frac{a}{\sqrt{2}} \right) - \mathrm{erf}\left( -\frac{a}{\sqrt{2}} \right) \right] \\
&\overset{\eqref{eq:norm-probstd-erf-symm}}{=} \frac{1}{2} \left[ \mathrm{erf}\left( \frac{a}{\sqrt{2}} \right) + \mathrm{erf}\left( \frac{a}{\sqrt{2}} \right) \right] \\
&= \mathrm{erf}\left( \frac{a}{\sqrt{2}} \right) \\
\end{split}
\end{equation}

With that, we can use numerical implementations of the error function to calculate:

\begin{equation} \label{eq:norm-probstd-norm-probstd-qed}
\begin{split}
\mathrm{Pr}(\mu-1\sigma \leq X \leq \mu+1\sigma) &= p(1) = 68.27 \% \\
\mathrm{Pr}(\mu-2\sigma \leq X \leq \mu+2\sigma) &= p(2) = 95.45 \% \\
\mathrm{Pr}(\mu-3\sigma \leq X \leq \mu+3\sigma) &= p(3) = 99.73 \% \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "68-95-99.7 rule"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-05.08; URL: \url{https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Quantile function}]{Quantile function} \label{sec:norm-qf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-qf-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the quantile function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) of $X$ is

\begin{equation} \label{eq:norm-qf-norm-qf}
Q_X(p) = \sqrt{2}\sigma \cdot \mathrm{erf}^{-1}(2p-1) + \mu
\end{equation}

where $\mathrm{erf}^{-1}(x)$ is the inverse error function.


\vspace{1em}
\textbf{Proof:} The cumulative distribution function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-cdf}) is:

\begin{equation} \label{eq:norm-qf-norm-cdf}
F_X(x) = \frac{1}{2} \left[ 1 + \mathrm{erf}\left( \frac{x-\mu}{\sqrt{2} \sigma} \right) \right] \; .
\end{equation}

Because the cumulative distribution function (CDF) is strictly monotonically increasing, the quantile function is equal to the inverse of the CDF ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf-cdf}):

\begin{equation} \label{eq:norm-qf-norm-qf-s1}
Q_X(p) = F_X^{-1}(x) \; .
\end{equation}

This can be derived by rearranging equation \eqref{eq:norm-qf-norm-cdf}:

\begin{equation} \label{eq:norm-qf-norm-qf-s2}
\begin{split}
p &= \frac{1}{2} \left[ 1 + \mathrm{erf}\left( \frac{x-\mu}{\sqrt{2} \sigma} \right) \right] \\
2 p - 1 &= \mathrm{erf}\left( \frac{x-\mu}{\sqrt{2} \sigma} \right) \\
\mathrm{erf}^{-1}(2p-1) &= \frac{x-\mu}{\sqrt{2} \sigma} \\
x &= \sqrt{2}\sigma \cdot \mathrm{erf}^{-1}(2p-1) + \mu \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-20; URL: \url{https://en.wikipedia.org/wiki/Normal_distribution#Quantile_function}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:norm-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-mean-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation} \label{eq:norm-mean-norm-mean}
\mathrm{E}(X) = \mu \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is the probability-weighted average over all possible values:

\begin{equation} \label{eq:norm-mean-mean}
\mathrm{E}(X) = \int_{\mathcal{X}} x \cdot f_X(x) \, \mathrm{d}x \; .
\end{equation}

With the probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}), this reads:

\begin{equation} \label{eq:norm-mean-norm-mean-s1}
\begin{split}
\mathrm{E}(X) &= \int_{-\infty}^{+\infty} x \cdot \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \, \mathrm{d}x \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{+\infty} x \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \, \mathrm{d}x \; .
\end{split}
\end{equation}

Substituting $z = x -\mu$, we have:

\begin{equation} \label{eq:norm-mean-norm-mean-s2}
\begin{split}
\mathrm{E}(X) &= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty-\mu}^{+\infty-\mu} (z + \mu) \cdot \exp \left[ -\frac{1}{2} \left( \frac{z}{\sigma} \right)^2 \right] \, \mathrm{d}(z + \mu) \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{+\infty} (z + \mu) \cdot \exp \left[ -\frac{1}{2} \left( \frac{z}{\sigma} \right)^2 \right] \, \mathrm{d}z \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \left( \int_{-\infty}^{+\infty} z \cdot \exp \left[ -\frac{1}{2} \left( \frac{z}{\sigma} \right)^2 \right] \, \mathrm{d}z + \mu \int_{-\infty}^{+\infty} \exp \left[ -\frac{1}{2} \left( \frac{z}{\sigma} \right)^2 \right] \, \mathrm{d}z \right) \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \left( \int_{-\infty}^{+\infty} z \cdot \exp \left[ -\frac{1}{2 \sigma^2} \cdot z^2 \right] \, \mathrm{d}z + \mu \int_{-\infty}^{+\infty} \exp \left[ -\frac{1}{2 \sigma^2} \cdot z^2 \right] \, \mathrm{d}z \right) \; .
\end{split}
\end{equation}

The general antiderivatives are

\begin{equation} \label{eq:norm-mean-exp-erf-anti-der}
\begin{split}
\int x \cdot \exp \left[ -a x^2 \right] \mathrm{d}x &= -\frac{1}{2a} \cdot \exp \left[ -a x^2 \right] \\
\int \exp \left[ -a x^2 \right] \mathrm{d}x &= \frac{1}{2} \sqrt{\frac{\pi}{a}} \cdot \mathrm{erf} \left[ \sqrt{a} x \right]
\end{split}
\end{equation}

where $\mathrm{erf}(x)$ is the error function. Using this, the integrals can be calculated as:

\begin{equation} \label{eq:norm-mean-norm-mean-s3}
\begin{split}
\mathrm{E}(X) &= \frac{1}{\sqrt{2 \pi} \sigma} \left( \left[ -\sigma^2 \cdot \exp \left[ -\frac{1}{2 \sigma^2} \cdot z^2 \right] \right]_{-\infty}^{+\infty} + \mu \left[ \sqrt{\frac{\pi}{2}} \sigma \cdot \mathrm{erf} \left[ \frac{1}{\sqrt{2} \sigma} z \right] \right]_{-\infty}^{+\infty} \right) \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \left( \left[ \lim_{z \to \infty} \left( -\sigma^2 \cdot \exp \left[ -\frac{1}{2 \sigma^2} \cdot z^2 \right] \right) - \lim_{z \to -\infty} \left( -\sigma^2 \cdot \exp \left[ -\frac{1}{2 \sigma^2} \cdot z^2 \right] \right) \right] \right. \\
&\hphantom{\sqrt{2 \pi}\sigma \;} + \mu \left. \left[ \lim_{z \to \infty} \left( \sqrt{\frac{\pi}{2}} \sigma \cdot \mathrm{erf} \left[ \frac{1}{\sqrt{2} \sigma} z \right] \right) - \lim_{z \to -\infty} \left( \sqrt{\frac{\pi}{2}} \sigma \cdot \mathrm{erf} \left[ \frac{1}{\sqrt{2} \sigma} z \right] \right) \right] \right) \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \left( [0 - 0] + \mu \left[ \sqrt{\frac{\pi}{2}} \sigma - \left(- \sqrt{\frac{\pi}{2}} \sigma \right) \right] \right) \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \cdot \mu \cdot 2 \sqrt{\frac{\pi}{2}} \sigma \\
&= \mu \; .
\end{split}
\end{equation}

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Papadopoulos, Alecos (2013): "How to derive the mean and variance of Gaussian random variable?"; in: \textit{StackExchange Mathematics}, retrieved on 2020-01-09; URL: \url{https://math.stackexchange.com/questions/518281/how-to-derive-the-mean-and-variance-of-a-gaussian-random-variable}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Median}]{Median} \label{sec:norm-med}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-med-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the median ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:med}) of $X$ is

\begin{equation} \label{eq:norm-med-norm-median}
\mathrm{median}(X) = \mu \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The median ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:med}) is the value at which the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) is $1/2$:

\begin{equation} \label{eq:norm-med-median}
F_X(\mathrm{median}(X)) = \frac{1}{2} \; .
\end{equation}

The cumulative distribution function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-cdf}) is

\begin{equation} \label{eq:norm-med-norm-cdf}
F_X(x) = \frac{1}{2} \left[ 1 + \mathrm{erf} \left( \frac{x-\mu}{\sqrt{2}\sigma} \right) \right]
\end{equation}

where $\mathrm{erf}(x)$ is the error function. Thus, the inverse CDF is

\begin{equation} \label{eq:norm-med-norm-cdf-inv}
x = \sqrt{2}\sigma \cdot \mathrm{erf}^{-1}(2p-1) + \mu
\end{equation}

where $\mathrm{erf}^{-1}(x)$ is the inverse error function. Setting $p = 1/2$, we obtain:

\begin{equation} \label{eq:norm-med-norm-med-qed}
\mathrm{median}(X) = \sqrt{2}\sigma \cdot \mathrm{erf}^{-1}(0) + \mu = \mu \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mode}]{Mode} \label{sec:norm-mode}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-mode-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the mode ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mode}) of $X$ is

\begin{equation} \label{eq:norm-mode-norm-mode}
\mathrm{mode}(X) = \mu \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The mode ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mode}) is the value which maximizes the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}):

\begin{equation} \label{eq:norm-mode-mode}
\mathrm{mode}(X) = \operatorname*{arg\,max}_x f_X(x) \; .
\end{equation}

The probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) is:

\begin{equation} \label{eq:norm-mode-norm-pdf}
f_X(x) = \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \; .
\end{equation}

The first two deriatives of this function are:

\begin{equation} \label{eq:norm-mode-norm-pdf-der1}
f'_X(x) = \frac{\mathrm{d}f_X(x)}{\mathrm{d}x} = \frac{1}{\sqrt{2 \pi} \sigma^3} \cdot (-x + \mu) \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right]
\end{equation}

\begin{equation} \label{eq:norm-mode-norm-pdf-der2}
f''_X(x) = \frac{\mathrm{d}^2f_X(x)}{\mathrm{d}x^2} = -\frac{1}{\sqrt{2 \pi} \sigma^3} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] + \frac{1}{\sqrt{2 \pi} \sigma^5} \cdot (-x + \mu)^2 \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \; .
\end{equation}

We now calculate the root of the first derivative \eqref{eq:norm-mode-norm-pdf-der1}:

\begin{equation} \label{eq:norm-mode-norm-mode-s1}
\begin{split}
f'_X(x) = 0 &= \frac{1}{\sqrt{2 \pi} \sigma^3} \cdot (-x + \mu) \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \\
0 &= -x + \mu \\
x &= \mu \; .
\end{split}
\end{equation}

By plugging this value into the second deriative \eqref{eq:norm-mode-norm-pdf-der2},

\begin{equation} \label{eq:norm-mode-norm-mode-s2}
\begin{split}
f''_X(\mu) &= -\frac{1}{\sqrt{2 \pi} \sigma^3} \cdot \exp(0) + \frac{1}{\sqrt{2 \pi} \sigma^5} \cdot (0)^2 \cdot \exp(0) \\
&= -\frac{1}{\sqrt{2 \pi} \sigma^3} < 0 \; ,
\end{split}
\end{equation}

we confirm that it is in fact a maximum which shows that

\begin{equation} \label{eq:norm-mode-norm-mode-qed}
\mathrm{mode}(X) = \mu \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Variance}]{Variance} \label{sec:norm-var}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-var-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is

\begin{equation} \label{eq:norm-var-norm-var}
\mathrm{Var}(X) = \sigma^2 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) is the probability-weighted average of the squared deviation from the mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}):

\begin{equation} \label{eq:norm-var-var}
\mathrm{Var}(X) = \int_{\mathbb{R}} (x - \mathrm{E}(X))^2 \cdot f_\mathrm{X}(x) \, \mathrm{d}x \; .
\end{equation}

With the expected value ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-mean}) and probability density function ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) of the normal distribution, this reads:

\begin{equation} \label{eq:norm-var-norm-var-s1}
\begin{split}
\mathrm{Var}(X) &= \int_{-\infty}^{+\infty} (x - \mu)^2 \cdot \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \, \mathrm{d}x \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{+\infty} (x - \mu)^2 \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \, \mathrm{d}x \; .
\end{split}
\end{equation}

Substituting $z = x -\mu$, we have:

\begin{equation} \label{eq:norm-var-norm-var-s2}
\begin{split}
\mathrm{Var}(X) &= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty-\mu}^{+\infty-\mu} z^2 \cdot \exp \left[ -\frac{1}{2} \left( \frac{z}{\sigma} \right)^2 \right] \, \mathrm{d}(z + \mu) \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{+\infty} z^2 \cdot \exp \left[ -\frac{1}{2} \left( \frac{z}{\sigma} \right)^2 \right] \, \mathrm{d}z \; .
\end{split}
\end{equation}

Now substituting $z = \sqrt{2} \sigma x$, we have:

\begin{equation} \label{eq:norm-var-norm-var-s3}
\begin{split}
\mathrm{Var}(X) &= \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{+\infty} (\sqrt{2} \sigma x)^2 \cdot \exp \left[ -\frac{1}{2} \left( \frac{\sqrt{2} \sigma x}{\sigma} \right)^2 \right] \, \mathrm{d}(\sqrt{2} \sigma x) \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \cdot 2 \sigma^2 \cdot \sqrt{2} \sigma \int_{-\infty}^{+\infty} x^2 \cdot \exp \left[ -x^2 \right] \, \mathrm{d}x \\
&= \frac{2 \sigma^2}{\sqrt{\pi}} \int_{-\infty}^{+\infty} x^2 \cdot e^{-x^2} \, \mathrm{d}x \; .
\end{split}
\end{equation}

Since the integrand is symmetric with respect to $x = 0$, we can write:

\begin{equation} \label{eq:norm-var-norm-var-s4}
\mathrm{Var}(X) = \frac{4 \sigma^2}{\sqrt{\pi}} \int_{0}^{\infty} x^2 \cdot e^{-x^2} \, \mathrm{d}x \; .
\end{equation}

If we define $z = x^2$, then $x = \sqrt{z}$ and $\mathrm{d}x = 1/2 \, z^{-1/2} \, \mathrm{d}z$. Substituting this into the integral

\begin{equation} \label{eq:norm-var-norm-var-s5}
\mathrm{Var}(X) = \frac{4 \sigma^2}{\sqrt{\pi}} \int_{0}^{\infty} z \cdot e^{-z} \cdot \frac{1}{2} z^{-\frac{1}{2}} \, \mathrm{d}z = \frac{2 \sigma^2}{\sqrt{\pi}} \int_{0}^{\infty} z^{\frac{3}{2}-1} \cdot e^{-z} \, \mathrm{d}z
\end{equation}

and using the definition of the gamma function

\begin{equation} \label{eq:norm-var-gam-fct}
\Gamma(x) = \int_{0}^{\infty} z^{x-1} \cdot e^{-z} \, \mathrm{d}z \; ,
\end{equation}

we can finally show that

\begin{equation} \label{eq:norm-var-norm-var-s6}
\mathrm{Var}(X) = \frac{2 \sigma^2}{\sqrt{\pi}} \cdot \Gamma\!\left(\frac{3}{2}\right) = \frac{2 \sigma^2}{\sqrt{\pi}} \cdot \frac{\sqrt{\pi}}{2} = \sigma^2 \; .
\end{equation}

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Papadopoulos, Alecos (2013): "How to derive the mean and variance of Gaussian random variable?"; in: \textit{StackExchange Mathematics}, retrieved on 2020-01-09; URL: \url{https://math.stackexchange.com/questions/518281/how-to-derive-the-mean-and-variance-of-a-gaussian-random-variable}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Full width at half maximum}]{Full width at half maximum} \label{sec:norm-fwhm}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-fwhm-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the full width at half maximum ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fwhm}) (FWHM) of $X$ is

\begin{equation} \label{eq:norm-fwhm-norm-fwhm}
\mathrm{FWHM}(X) = 2 \sqrt{2 \ln 2} \sigma \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) is

\begin{equation} \label{eq:norm-fwhm-norm-pdf}
f_X(x) = \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right]
\end{equation}

and the mode of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-mode}) is

\begin{equation} \label{eq:norm-fwhm-norm-mode}
\mathrm{mode}(X) = \mu \; ,
\end{equation}

such that

\begin{equation} \label{eq:norm-fwhm-norm-pdf-max}
f_\mathrm{max} = f_X(\mathrm{mode}(X)) \overset{\eqref{eq:norm-fwhm-norm-mode}}{=} f_X(\mu) \overset{\eqref{eq:norm-fwhm-norm-pdf}}{=} \frac{1}{\sqrt{2 \pi} \sigma} \; .
\end{equation}

The FWHM bounds satisfy the equation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fwhm})

\begin{equation} \label{eq:norm-fwhm-x-FHWM}
f_X(x_\mathrm{FWHM}) = \frac{1}{2} f_\mathrm{max} \overset{\eqref{eq:norm-fwhm-norm-pdf-max}}{=} \frac{1}{2 \sqrt{2 \pi} \sigma} \; .
\end{equation}

Using \eqref{eq:norm-fwhm-norm-pdf}, we can develop this equation as follows:

\begin{equation} \label{eq:norm-fwhm-x-FHWM-s1}
\begin{split}
\frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x_\mathrm{FWHM}-\mu}{\sigma} \right)^2 \right] &= \frac{1}{2 \sqrt{2 \pi} \sigma} \\
\exp \left[ -\frac{1}{2} \left( \frac{x_\mathrm{FWHM}-\mu}{\sigma} \right)^2 \right] &= \frac{1}{2} \\
-\frac{1}{2} \left( \frac{x_\mathrm{FWHM}-\mu}{\sigma} \right)^2 &= \ln \frac{1}{2} \\
\left( \frac{x_\mathrm{FWHM}-\mu}{\sigma} \right)^2 &= -2 \ln \frac{1}{2} \\
\frac{x_\mathrm{FWHM}-\mu}{\sigma} &= \pm \sqrt{2 \ln 2} \\
x_\mathrm{FWHM}-\mu &= \pm \sqrt{2 \ln 2} \sigma \\
x_\mathrm{FWHM} &= \pm \sqrt{2 \ln 2} \sigma + \mu \; .
\end{split}
\end{equation}

This implies the following two solutions for $x_\mathrm{FWHM}$

\begin{equation} \label{eq:norm-fwhm-x-FHWM-s2}
\begin{split}
x_1 &= \mu - \sqrt{2 \ln 2} \sigma \\
x_2 &= \mu + \sqrt{2 \ln 2} \sigma \; ,
\end{split}
\end{equation}

such that the full width at half maximum ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fwhm}) of $X$ is

\begin{equation} \label{eq:norm-fwhm-norm-fwhm-qed}
\begin{split}
\mathrm{FWHM}(X) &= \Delta x = x_2 - x_1 \\
&\overset{\eqref{eq:norm-fwhm-x-FHWM-s2}}{=} \left( \mu + \sqrt{2 \ln 2} \sigma \right) - \left( \mu - \sqrt{2 \ln 2} \sigma \right) \\
&= 2 \sqrt{2 \ln 2} \sigma \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Full width at half maximum"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-08-19; URL: \url{https://en.wikipedia.org/wiki/Full_width_at_half_maximum}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Extreme points}]{Extreme points} \label{sec:norm-extr}
\setcounter{equation}{0}

\textbf{Theorem:} The probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with mean $\mu$ and variance $\sigma^2$ has a maximum at $x = \mu$ and no other extrema. Consequently, the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) is a unimodal probability distribution.


\vspace{1em}
\textbf{Proof:} The probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) is:

\begin{equation} \label{eq:norm-extr-norm-pdf}
f_X(x) = \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \; .
\end{equation}

The first two deriatives of this function ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-mode}) are:

\begin{equation} \label{eq:norm-extr-norm-pdf-der1}
f'_X(x) = \frac{\mathrm{d}f_X(x)}{\mathrm{d}x} = \frac{1}{\sqrt{2 \pi} \sigma^3} \cdot (-x + \mu) \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right]
\end{equation}

\begin{equation} \label{eq:norm-extr-norm-pdf-der2}
f''_X(x) = \frac{\mathrm{d}^2f_X(x)}{\mathrm{d}x^2} = -\frac{1}{\sqrt{2 \pi} \sigma^3} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] + \frac{1}{\sqrt{2 \pi} \sigma^5} \cdot (-x + \mu)^2 \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \; .
\end{equation}

The first derivative is zero, if and only if

\begin{equation} \label{eq:norm-extr-norm-pdf-der1-zero}
-x + \mu = 0 \quad \Leftrightarrow \quad x = \mu \; .
\end{equation}

Since the second derivative is negative at this value

\begin{equation} \label{eq:norm-extr-norm-pdf-der2-extr}
f''_X(\mu) = -\frac{1}{\sqrt{2 \pi} \sigma^3} < 0 \; ,
\end{equation}

there is a maximum at $x = \mu$. From \eqref{eq:norm-extr-norm-pdf-der1}, it can be seen that $f'_X(x)$ is positive for $x < \mu$ and negative for $x > \mu$. Thus, there are no further extrema and $\mathcal{N}(\mu, \sigma^2)$ is unimodal ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-mode}).
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-08-25; URL: \url{https://en.wikipedia.org/wiki/Normal_distribution#Symmetries_and_derivatives}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Inflection points}]{Inflection points} \label{sec:norm-infl}
\setcounter{equation}{0}

\textbf{Theorem:} The probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with mean $\mu$ and variance $\sigma^2$ has two inflection points at $x = \mu - \sigma$ and $x = \mu + \sigma$, i.e. exactly one standard deviation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:std}) away from the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}).


\vspace{1em}
\textbf{Proof:} The probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) is:

\begin{equation} \label{eq:norm-infl-norm-pdf}
f_X(x) = \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \; .
\end{equation}

The first three deriatives of this function are:

\begin{equation} \label{eq:norm-infl-norm-pdf-der1}
f'_X(x) = \frac{\mathrm{d}f_X(x)}{\mathrm{d}x} = \frac{1}{\sqrt{2 \pi} \sigma} \cdot \left( - \frac{x - \mu}{\sigma^2} \right) \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right]
\end{equation}

\begin{equation} \label{eq:norm-infl-norm-pdf-der2}
\begin{split}
f''_X(x) = \frac{\mathrm{d}^2f_X(x)}{\mathrm{d}x^2} &= \frac{1}{\sqrt{2 \pi} \sigma} \cdot \left( - \frac{1}{\sigma^2} \right) \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] + \frac{1}{\sqrt{2 \pi} \sigma} \cdot \left( \frac{x-\mu}{\sigma^2} \right)^2 \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \cdot \left[ \left( \frac{x-\mu}{\sigma^2} \right)^2 - \frac{1}{\sigma^2} \right] \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right]
\end{split}
\end{equation}

\begin{equation} \label{eq:norm-infl-norm-pdf-der3}
\begin{split}
f'''_X(x) = \frac{\mathrm{d}^3f_X(x)}{\mathrm{d}x^3} &= \frac{1}{\sqrt{2 \pi} \sigma} \cdot \left[ \frac{2}{\sigma^2} \left( \frac{x-\mu}{\sigma^2} \right) \right] \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] - \frac{1}{\sqrt{2 \pi} \sigma} \cdot \left[ \left( \frac{x-\mu}{\sigma^2} \right)^2 - \frac{1}{\sigma^2} \right] \cdot \left( \frac{x - \mu}{\sigma^2}  \right) \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \cdot \left[ -\left( \frac{x - \mu}{\sigma^2} \right)^3 + 3 \left( \frac{x - \mu}{\sigma^4} \right) \right] \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \; .
\end{split}
\end{equation}

The second derivative is zero, if and only if

\begin{equation} \label{eq:norm-infl-norm-pdf-der2-zero}
\begin{split}
0 &= \left[ \left( \frac{x-\mu}{\sigma^2} \right)^2 - \frac{1}{\sigma^2} \right] \\
0 &= \frac{x^2}{\sigma^4} - \frac{2 \mu x}{\sigma^4} + \frac{\mu^2}{\sigma^4} - \frac{1}{\sigma^2} \\
0 &= x^2 - 2 \mu x + (\mu^2 - \sigma^2) \\
x_{1/2} &= -\frac{-2 \mu}{2} \pm \sqrt{ \left(\frac{-2 \mu}{2}\right)^2 - (\mu^2 - \sigma^2)} \\
x_{1/2} &= \mu \pm \sqrt{ \mu^2 - \mu^2 + \sigma^2} \\
x_{1/2} &= \mu \pm \sigma \; .
\end{split}
\end{equation}

Since the third derivative is non-zero at this value

\begin{equation} \label{eq:norm-infl-norm-pdf-der3-infl}
\begin{split}
f'''_X(\mu \pm \sigma) &= \frac{1}{\sqrt{2 \pi} \sigma} \cdot \left[ -\left( \frac{\pm \sigma}{\sigma^2} \right)^3 + 3 \left( \frac{\pm \sigma}{\sigma^4} \right) \right] \cdot \exp \left[ -\frac{1}{2} \left( \frac{\pm \sigma}{\sigma} \right)^2 \right] \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \cdot \left( \pm \frac{2}{\sigma^3} \right) \cdot \exp \left( - \frac{1}{2} \right) \neq 0 \; ,
\end{split}
\end{equation}

there are inflection points at $x_{1/2} = \mu \pm \sigma$. Because $\mu$ is the mean and $\sigma^2$ is the variance of a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}), these points are exactly one standard deviation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:std}) away from the mean.

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-08-25; URL: \url{https://en.wikipedia.org/wiki/Normal_distribution#Symmetries_and_derivatives}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Differential entropy}]{Differential entropy} \label{sec:norm-dent}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:norm-dent-norm}
X \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $X$ is

\begin{equation} \label{eq:norm-dent-norm-dent}
\mathrm{h}(X) = \frac{1}{2} \ln\left( 2 \pi \sigma^2 e \right) \; .
\end{equation}

\vspace{1em}
\textbf{Proof:} The differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of a random variable is defined as

\begin{equation} \label{eq:norm-dent-dent}
\mathrm{h}(X) = - \int_{\mathcal{X}} p(x) \, \log_b p(x) \, \mathrm{d}x \; .
\end{equation}

To measure $h(X)$ in nats, we set $b = e$, such that ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean})

\begin{equation} \label{eq:norm-dent-dent-nats}
\mathrm{h}(X) = - \mathrm{E}\left[ \ln p(x) \right] \; .
\end{equation}

With the probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}), the differential entropy of $X$ is:

\begin{equation} \label{eq:norm-dent-norm-dent-s1}
\begin{split}
\mathrm{h}(X) &= - \mathrm{E}\left[ \ln \left( \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \right) \right] \\
&= - \mathrm{E}\left[ - \frac{1}{2} \ln(2\pi\sigma^2) - \frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \\
&= \frac{1}{2} \ln(2 \pi \sigma^2) + \frac{1}{2} \, \mathrm{E}\left[ \left( \frac{x-\mu}{\sigma} \right)^2 \right] \\
&= \frac{1}{2} \ln(2 \pi \sigma^2) + \frac{1}{2} \cdot \frac{1}{\sigma^2} \cdot \mathrm{E}\left[ (x-\mu)^2 \right] \; .
\end{split}
\end{equation}

Note that $\mathrm{E}\left[ (x-\mu)^2 \right]$ corresponds to the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ and the variance of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-var}) is $\sigma^2$. Thus, we can proceed:

\begin{equation} \label{eq:norm-dent-norm-dent-s2}
\begin{split}
\mathrm{h}(X) &= \frac{1}{2} \ln(2 \pi \sigma^2) + \frac{1}{2} \cdot \frac{1}{\sigma^2} \cdot \sigma^2 \\
&= \frac{1}{2} \ln(2 \pi \sigma^2) + \frac{1}{2} \\
&= \frac{1}{2} \ln(2 \pi \sigma^2 e) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wang, Peng-Hua (2012): "Differential Entropy"; in: \textit{National Taipei University}; URL: \url{https://web.ntpu.edu.tw/~phwang/teaching/2012s/IT/slides/chap08.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Kullback-Leibler divergence}]{Kullback-Leibler divergence} \label{sec:norm-kl}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Assume two normal distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) $P$ and $Q$ specifying the probability distribution of $X$ as

\begin{equation} \label{eq:norm-kl-norms}
\begin{split}
P: \; X &\sim \mathcal{N}(\mu_1, \sigma_1^2) \\
Q: \; X &\sim \mathcal{N}(\mu_2, \sigma_2^2) \; .
\end{split}
\end{equation}

Then, the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ is given by

\begin{equation} \label{eq:norm-kl-norm-KL}
\mathrm{KL}[P\,||\,Q] = \frac{1}{2} \left[ \frac{(\mu_2 - \mu_1)^2}{\sigma_2^2} + \frac{\sigma_1^2}{\sigma_2^2} - \ln \frac{\sigma_1^2}{\sigma_2^2} - 1 \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The KL divergence for a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is given by 

\begin{equation} \label{eq:norm-kl-KL-cont}
\mathrm{KL}[P\,||\,Q] = \int_{\mathcal{X}} p(x) \, \ln \frac{p(x)}{q(x)} \, \mathrm{d}x
\end{equation}

which, applied to the normal distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) in \eqref{eq:norm-kl-norms}, yields

\begin{equation} \label{eq:norm-kl-norm-KL-s1}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \int_{-\infty}^{+\infty} \mathcal{N}(x; \mu_1, \sigma_1^2) \, \ln \frac{\mathcal{N}(x; \mu_1, \sigma_1^2)}{\mathcal{N}(x; \mu_2, \sigma_2^2)} \, \mathrm{d}x \\
&= \left\langle \ln \frac{\mathcal{N}(x; \mu_1, \sigma_1^2)}{\mathcal{N}(x; \mu_2, \sigma_2^2)} \right\rangle_{p(x)} \; .
\end{split}
\end{equation}

Using the probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}), this becomes:

\begin{equation} \label{eq:norm-kl-norm-KL-s2}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \left\langle \ln \frac{ \frac{1}{\sqrt{2 \pi} \sigma_1} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu_1}{\sigma_1} \right)^2 \right] }{ \frac{1}{\sqrt{2 \pi} \sigma_2} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu_2}{\sigma_2} \right)^2 \right] } \right\rangle_{p(x)} \\
&= \left\langle \ln \left( \sqrt \frac{\sigma_2^2}{\sigma_1^2} \cdot \exp\left[ -\frac{1}{2} \left( \frac{x-\mu_1}{\sigma_1} \right)^2 + \frac{1}{2} \left( \frac{x-\mu_2}{\sigma_2} \right)^2 \right] \right) \right\rangle_{p(x)} \\
&= \left\langle \frac{1}{2} \ln \frac{\sigma_2^2}{\sigma_1^2} -\frac{1}{2} \left( \frac{x-\mu_1}{\sigma_1} \right)^2 + \frac{1}{2} \left( \frac{x-\mu_2}{\sigma_2} \right)^2 \right\rangle_{p(x)} \\
&= \frac{1}{2} \left\langle - \left( \frac{x-\mu_1}{\sigma_1} \right)^2 + \left( \frac{x-\mu_2}{\sigma_2} \right)^2 - \ln \frac{\sigma_1^2}{\sigma_2^2} \right\rangle_{p(x)} \\
&= \frac{1}{2} \left\langle - \frac{(x-\mu_1)^2}{\sigma_1^2} + \frac{x^2 - 2 \mu_2 x + \mu_2^2}{\sigma_2^2} - \ln \frac{\sigma_1^2}{\sigma_2^2} \right\rangle_{p(x)} \; .
\end{split}
\end{equation}

Because the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is a linear operator ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), the expectation can be moved into the sum:

\begin{equation} \label{eq:norm-kl-norm-KL-s3}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \frac{1}{2} \left[ - \frac{\left\langle (x-\mu_1)^2 \right\rangle}{\sigma_1^2} + \frac{\left\langle x^2 - 2 \mu_2 x + \mu_2^2 \right\rangle}{\sigma_2^2} - \left\langle \ln \frac{\sigma_1^2}{\sigma_2^2} \right\rangle \right] \\
&= \frac{1}{2} \left[ - \frac{\left\langle (x-\mu_1)^2 \right\rangle}{\sigma_1^2} + \frac{\left\langle x^2 \right\rangle - \left\langle 2 \mu_2 x \right\rangle + \left\langle \mu_2^2 \right\rangle}{\sigma_2^2} - \ln \frac{\sigma_1^2}{\sigma_2^2} \right] \; .
\end{split}
\end{equation}

The first expectation corresponds to the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var})

\begin{equation} \label{eq:norm-kl-var}
\left\langle (X-\mu)^2 \right\rangle = \mathrm{E}[(X-\mathrm{E}(X))^2] = \mathrm{Var}(X)
\end{equation}

and the variance of a normally distributed random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-var}) is

\begin{equation} \label{eq:norm-kl-norm-var}
X \sim \mathcal{N}(\mu, \sigma^2) \quad \Rightarrow \quad \mathrm{Var}(X) = \sigma^2 \; .
\end{equation}

Additionally applying the raw moments of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-mgf})

\begin{equation} \label{eq:norm-kl-norm-mom-raw}
X \sim \mathcal{N}(\mu, \sigma^2) \quad \Rightarrow \quad \left\langle x \right\rangle = \mu \quad \text{and} \quad \left\langle x^2 \right\rangle = \mu^2 + \sigma^2 \; ,
\end{equation}

the Kullback-Leibler divergence in \eqref{eq:norm-kl-norm-KL-s3} becomes

\begin{equation} \label{eq:norm-kl-norm-KL-s4}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \frac{1}{2} \left[ - \frac{\sigma_1^2}{\sigma_1^2} + \frac{\mu_1^2 + \sigma_1^2 - 2 \mu_2 \mu_1 + \mu_2^2}{\sigma_2^2} - \ln \frac{\sigma_1^2}{\sigma_2^2} \right] \\
&= \frac{1}{2} \left[ \frac{\mu_1^2 - 2 \mu_1 \mu_2 + \mu_2^2}{\sigma_2^2} + \frac{\sigma_1^2}{\sigma_2^2} - \ln \frac{\sigma_1^2}{\sigma_2^2} - 1 \right] \\
&= \frac{1}{2} \left[ \frac{(\mu_1 - \mu_2)^2}{\sigma_2^2} + \frac{\sigma_1^2}{\sigma_2^2} - \ln \frac{\sigma_1^2}{\sigma_2^2} - 1 \right]
\end{split}
\end{equation}

which is equivalent to \eqref{eq:norm-kl-norm-KL}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Maximum entropy distribution}]{Maximum entropy distribution} \label{sec:norm-maxent}
\setcounter{equation}{0}

\textbf{Theorem:} The normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) maximizes differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) for a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with fixed variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}).


\vspace{1em}
\textbf{Proof:} For a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X$ with set of possible values $\mathcal{X}$ and probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) $p(x)$, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) is defined as:

\begin{equation} \label{eq:norm-maxent-dent}
\mathrm{h}(X) = - \int_{\mathcal{X}} p(x) \log p(x) \, \mathrm{d}x
\end{equation}

Let $g(x)$ be the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu$ and variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) $\sigma^2$ and let $f(x)$ be an arbitrary probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) with the same variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}). Since differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) is translation-invariant ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent-inv}), we can assume that $f(x)$ has the same mean as $g(x)$.

Consider the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of distribution $f(x)$ from distribution $g(x)$ which is non-negative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl-nonneg}):

\begin{equation} \label{eq:norm-maxent-kl-fg}
\begin{split}
0 \leq \mathrm{KL}[f||g] &= \int_{\mathcal{X}} f(x) \log \frac{f(x)}{g(x)} \, \mathrm{d}x \\
&= \int_{\mathcal{X}} f(x) \log f(x) \, \mathrm{d}x - \int_{\mathcal{X}} f(x) \log g(x) \, \mathrm{d}x \\
&\overset{\eqref{eq:norm-maxent-dent}}{=} - \mathrm{h}[f(x)] - \int_{\mathcal{X}} f(x) \log g(x) \, \mathrm{d}x \; .
\end{split}
\end{equation}

By plugging the probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) into the second term, we obtain:

\begin{equation} \label{eq:norm-maxent-int-fg-s1}
\begin{split}
\int_{\mathcal{X}} f(x) \log g(x) \, \mathrm{d}x &= \int_{\mathcal{X}} f(x) \log \left( \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right] \right) \, \mathrm{d}x \\
&= \int_{\mathcal{X}} f(x) \log \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right) \, \mathrm{d}x + \int_{\mathcal{X}} f(x) \log \left( \exp \left[ -\frac{(x-\mu)^2}{2 \sigma^2} \right] \right) \, \mathrm{d}x \\
&= -\frac{1}{2} \log \left( 2 \pi \sigma^2 \right) \int_{\mathcal{X}} f(x) \, \mathrm{d}x - \frac{\log(e)}{2 \sigma^2} \int_{\mathcal{X}} f(x) (x-\mu)^2 \, \mathrm{d}x \; .
\end{split}
\end{equation}

Because the entire integral over a probability density function is one ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) and the second central moment is equal to the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:momcent-2nd}), we have:

\begin{equation} \label{eq:norm-maxent-int-fg-s2}
\begin{split}
\int_{\mathcal{X}} f(x) \log g(x) \, \mathrm{d}x &= -\frac{1}{2} \log \left( 2 \pi \sigma^2 \right) - \frac{\log(e) \sigma^2}{2 \sigma^2} \\
&= -\frac{1}{2} \left[ \log \left( 2 \pi \sigma^2 \right) + \log(e) \right] \\
&= -\frac{1}{2} \log \left( 2 \pi \sigma^2 e \right) \; .
\end{split}
\end{equation}

This is actually the negative of the differential entropy of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-dent}), such that:

\begin{equation} \label{eq:norm-maxent-int-fg-s3}
\int_{\mathcal{X}} f(x) \log g(x) \, \mathrm{d}x = -\mathrm{h}[\mathcal{N}(\mu,\sigma^2)] = -\mathrm{h}[g(x)] \; .
\end{equation}

Combining \eqref{eq:norm-maxent-kl-fg} with \eqref{eq:norm-maxent-int-fg-s3}, we can show that

\begin{equation} \label{eq:norm-maxent-norm-maxent}
\begin{split}
0 &\leq \mathrm{KL}[f||g] \\
0 &\leq - \mathrm{h}[f(x)] - \left( -\mathrm{h}[g(x)] \right) \\
\mathrm{h}[g(x)] &\geq \mathrm{h}[f(x)]
\end{split}
\end{equation}

which means that the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) $\mathcal{N}(\mu, \sigma^2)$ will be larger than or equal to any other distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) with the same variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) $\sigma^2$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Differential entropy"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-08-25; URL: \url{https://en.wikipedia.org/wiki/Differential_entropy#Maximization_in_the_normal_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Linear combination}]{Linear combination} \label{sec:norm-lincomb}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X_1, \ldots, X_n$ be independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) with means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu_1, \ldots, \mu_n$ and variances ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) $\sigma^2_1, \ldots, \sigma^2_n$:

\begin{equation} \label{eq:norm-lincomb-norm}
X_i \sim \mathcal{N}(\mu_i, \sigma^2_i) \quad \text{for} \quad i = 1, \ldots, n \; .
\end{equation}

Then, any linear combination of those random variables

\begin{equation} \label{eq:norm-lincomb-lincomb}
Y = \sum_{i=1}^{n} a_i X_i \quad \text{where} \quad a_1, \ldots, a_n \in \mathbb{R}
\end{equation}

also follows a normal distribution

\begin{equation} \label{eq:norm-lincomb-norm-lincomb}
Y \sim \mathcal{N}\left( \sum_{i=1}^{n} a_i \mu_i, \; \sum_{i=1}^{n} a_i^2 \sigma^2_i \right)
\end{equation}

with mean and variance which are functions of the individual means and variances.


\vspace{1em}
\textbf{Proof:} A set of $n$ independent normal random variables $X_1, \ldots, X_n$ is equivalent ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ind}) to an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $x$ following a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) with a diagonal covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}). Therefore, we can write

\begin{equation} \label{eq:norm-lincomb-norm-mvn}
X_i \sim \mathcal{N}(\mu_i, \sigma^2_i), \; i = 1, \ldots, n \quad \Rightarrow \quad x = \left[ \begin{array}{c} X_1 \\ \vdots \\ X_n \end{array} \right] \sim \mathcal{N}(\mu, \Sigma)
\end{equation}

with mean vector and covariance matrix

\begin{equation} \label{eq:norm-lincomb-mu-Sigma}
\mu = \left[ \begin{array}{c} \mu_1 \\ \vdots \\ \mu_n \end{array} \right] \quad \text{and} \quad \Sigma = \left[ \begin{array}{ccc} \sigma^2_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \sigma^2_n \end{array} \right] = \mathrm{diag}\left( \left[ \sigma^2_1, \ldots, \sigma^2_n \right] \right) \; .
\end{equation}

Thus, we can apply the linear transformation theorem for the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt})

\begin{equation} \label{eq:norm-lincomb-mvn-ltt}
x \sim \mathcal{N}(\mu, \Sigma) \quad \Rightarrow \quad y = Ax + b \sim \mathcal{N}(A\mu + b, A \Sigma A^\mathrm{T})
\end{equation}

with the constant matrix and vector

\begin{equation} \label{eq:norm-lincomb-A-b}
A = \left[ a_1, \ldots, a_n \right] \quad \text{and} \quad b = 0 \; .
\end{equation}

This implies the following distribution the linear combination given by equation \eqref{eq:norm-lincomb-lincomb}:

\begin{equation} \label{eq:norm-lincomb-norm-lincomb-p1}
Y = Ax + b \sim \mathcal{N}(A\mu, A \Sigma A^\mathrm{T}) \; .
\end{equation}

Finally, we note that

\begin{equation} \label{eq:norm-lincomb-A-b-mu-Sigma}
\begin{split}
A \mu &= \left[ a_1, \ldots, a_n \right] \left[ \begin{array}{c} \mu_1 \\ \vdots \\ \mu_n \end{array} \right] = \sum_{i=1}^{n} a_i \mu_i \quad \text{and} \quad \\
A \Sigma A^\mathrm{T} &= \left[ a_1, \ldots, a_n \right] \left[ \begin{array}{ccc} \sigma^2_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \sigma^2_n \end{array} \right] \left[ \begin{array}{c} a_1 \\ \vdots \\ a_n \end{array} \right] = \sum_{i=1}^{n} a_i^2 \sigma^2_i \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{t-distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:t}
\setcounter{equation}{0}

\textbf{Definition:} Let $Z$ and $V$ be independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) and a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) with $\nu$ degrees of freedom, respectively:

\begin{equation} \label{eq:t-snorm-chi2}
\begin{split}
Z &\sim \mathcal{N}(0,1) \\
V &\sim \chi^{2}(\nu) \; .
\end{split}
\end{equation}

Then, the ratio of $Z$ to the square root of $V$, divided by the respective degrees of freedom, is said to be $t$-distributed with degrees of freedom $\nu$:

\begin{equation} \label{eq:t-t}
Y = \frac{Z}{\sqrt{V/\nu}} \sim t(\nu) \; .
\end{equation}

The $t$-distribution is also called "Student's $t$-distribution", after William S. Gosset a.k.a. "Student".

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Student's t-distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-04-21; URL: \url{https://en.wikipedia.org/wiki/Student%27s_t-distribution#Characterization}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Special case of multivariate t-distribution}]{Special case of multivariate t-distribution} \label{sec:t-mvt}
\setcounter{equation}{0}

\textbf{Theorem:} The t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) is a special case of the multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt}) with number of variables $n = 1$, i.e. random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $x \in \mathbb{R}$, mean $\mu = 0$ and covariance matrix $\Sigma = 1$.


\vspace{1em}
\textbf{Proof:} The probability density function of the multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt-pdf}) is

\begin{equation} \label{eq:t-mvt-mvt-pdf}
t(x; \mu, \Sigma, \nu) = \sqrt{\frac{1}{(\nu \pi)^{n} |\Sigma|}} \, \frac{\Gamma([\nu+n]/2)}{\Gamma(\nu/2)} \, \left[ 1 + \frac{1}{\nu} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right]^{-(\nu+n)/2} \; .
\end{equation}

Setting $n = 1$, such that $x \in \mathbb{R}$, as well as $\mu = 0$ and $\Sigma = 1$, we obtain

\begin{equation} \label{eq:t-mvt-t-pdf}
\begin{split}
t(x; 0, 1, \nu) &= \sqrt{\frac{1}{(\nu \pi)^{1} |1|}} \, \frac{\Gamma([\nu+1]/2)}{\Gamma(\nu/2)} \, \left[ 1 + \frac{1}{\nu} (x-0)^\mathrm{T} 1^{-1} (x-0) \right]^{-(\nu+1)/2} \\
&= \sqrt{\frac{1}{\nu \pi}} \, \frac{\Gamma([\nu+1]/2)}{\Gamma(\nu/2)} \, \left[ 1 + \frac{x^2}{\nu} \right]^{-(\nu+1)/2} \\
&= \frac{1}{\sqrt{\nu \pi}} \cdot \frac{\Gamma\left( \frac{\nu+1}{2} \right)}{\Gamma\left( \frac{\nu}{2} \right)} \cdot \left[ 1 + \frac{x^2}{\nu} \right]^{-\frac{\nu+1}{2}} \; .
\end{split}
\end{equation}

which is equivalent to the probability density function of the t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t-pdf}).
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Multivariate t-distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-08-25; URL: \url{https://en.wikipedia.org/wiki/Multivariate_t-distribution#Derivation}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Non-standardized t-distribution}]{Non-standardized t-distribution} \label{sec:nst}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a Student's t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) with $\nu$ degrees of freedom. Then, the random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar})

\begin{equation} \label{eq:nst-Y}
Y = \sigma X + \mu
\end{equation}

is said to follow a non-standardized t-distribution with non-centrality $\mu$, scale $\sigma^2$ and degrees of freedom $\nu$:

\begin{equation} \label{eq:nst-nct}
Y \sim \mathrm{nst}(\mu, \sigma^2, \nu) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Student's t-distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-05-20; URL: \url{https://en.wikipedia.org/wiki/Student%27s_t-distribution#Generalized_Student's_t-distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relationship to non-standardized t-distribution}]{Relationship to non-standardized t-distribution} \label{sec:nst-t}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a non-standardized t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:nst}) with mean $\mu$, scale $\sigma^2$ and degrees of freedom $\nu$:

\begin{equation} \label{eq:nst-t-X}
X \sim \mathrm{nst}(\mu, \sigma^2, \nu) \; .
\end{equation}

Then, subtracting the mean and dividing by the square root of the scale results in a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) with degrees of freedom $\nu$:

\begin{equation} \label{eq:nst-t-nst-t}
Y = \frac{X-\mu}{\sigma} \sim t(\nu) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The non-standardized t-distribution is a special case of the multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt}) in which the mean vector and scale matrix are scalars:

\begin{equation} \label{eq:nst-t-nst-mvt}
X \sim \mathrm{nst}(\mu, \sigma^2, \nu) \quad \Rightarrow \quad X \sim t(\mu, \sigma^2, \nu) \; .
\end{equation}

Therefore, we can apply the linear transformation theorem for the multivariate t-distribution for an $n \times 1$ random vector $x$:

\begin{equation} \label{eq:nst-t-mvt-ltt}
x \sim t(\mu, \Sigma, \nu) \quad \Rightarrow \quad y = Ax + b \sim t(A\mu + b, A \Sigma A^\mathrm{T}, \nu) \; .
\end{equation}

Comparing with equation \eqref{eq:nst-t-nst-t}, we have $A = 1/\sigma$, $b = -\mu/\sigma$ and the variable $Y$ is distributed as:

\begin{equation} \label{eq:nst-t-Y-dist}
\begin{split}
Y &= \frac{X-\mu}{\sigma} = \frac{X}{\sigma} - \frac{\mu}{\sigma} \\
&\sim t\left( \frac{\mu}{\sigma} - \frac{\mu}{\sigma}, \left( \frac{1}{\sigma} \right)^2 \sigma^2, \nu \right) \\
&= t\left( 0, 1, \nu \right) \; .
\end{split}
\end{equation}

Plugging $\mu = 0$, $\Sigma = 1$ and $n = 1$ into the probability density function of the multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt-pdf}),

\begin{equation} \label{eq:nst-t-mvt-pdf}
p(x) = \sqrt{\frac{1}{(\nu \pi)^{n} |\Sigma|}} \, \frac{\Gamma([\nu+n]/2)}{\Gamma(\nu/2)} \, \left[ 1 + \frac{1}{\nu} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right] \; ,
\end{equation}

we get

\begin{equation} \label{eq:nst-t-t-pdf}
p(x) = \sqrt{\frac{1}{\nu \pi}} \, \frac{\Gamma([\nu+1]/2)}{\Gamma(\nu/2)} \, \left[ 1 + \frac{x^2}{\nu} \right]
\end{equation}

which is the probability density function of Student's t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t-pdf}) with $\nu$ degrees of freedom.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:t-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $T$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}):

\begin{equation} \label{eq:t-pdf-t}
T \sim t(\nu) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $T$ is

\begin{equation} \label{eq:t-pdf-t-pdf}
f_T(t) = \frac{\Gamma\left( \frac{\nu+1}{2} \right)}{\Gamma\left( \frac{\nu}{2} \right) \cdot \sqrt{\nu \pi}} \cdot \left( \frac{t^2}{\nu}+1 \right)^{-\frac{\nu+1}{2}} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} A t-distributed random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) is defined as the ratio of a standard normal random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) and the square root of a chi-squared random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}), divided by its degrees of freedom

\begin{equation} \label{eq:t-pdf-t-def}
X \sim \mathcal{N}(0,1), \; Y \sim \chi^2(\nu) \quad \Rightarrow \quad T = \frac{X}{\sqrt{Y/\nu}} \sim t(\nu)
\end{equation}

where $X$ and $Y$ are independent of each other ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}).

The probability density function ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) of the standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) is

\begin{equation} \label{eq:t-pdf-snorm-pdf}
f_X(x) = \frac{1}{\sqrt{2 \pi}} \cdot e^{-\frac{x^2}{2}}
\end{equation}

and the probability density function of the chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2-pdf}) is

\begin{equation} \label{eq:t-pdf-chi2-pdf}
f_Y(y) = \frac{1}{\Gamma\left( \frac{\nu}{2} \right) \cdot 2^{\nu/2}} \cdot y^{\frac{\nu}{2}-1} \cdot e^{-\frac{y}{2}} \; .
\end{equation}

Define the random variables $T$ and $W$ as functions of $X$ and $Y$

\begin{equation} \label{eq:t-pdf-TW-XY}
\begin{split}
T &= X \cdot \sqrt{\frac{\nu}{Y}} \\
W &= Y \; ,
\end{split}
\end{equation}

such that the inverse functions $X$ and $Y$ in terms of $T$ and $W$ are

\begin{equation} \label{eq:t-pdf-XY-TW}
\begin{split}
X &= T \cdot \sqrt{\frac{W}{\nu}} \\
Y &= W \; .
\end{split}
\end{equation}

This implies the following Jacobian matrix and determinant:

\begin{equation} \label{eq:t-pdf-XY-TW-jac}
\begin{split}
J &= \left[ \begin{matrix}
\frac{\mathrm{d}X}{\mathrm{d}T} & \frac{\mathrm{d}X}{\mathrm{d}W} \\
\frac{\mathrm{d}Y}{\mathrm{d}T} & \frac{\mathrm{d}Y}{\mathrm{d}W}
\end{matrix} \right]
= \left[ \begin{matrix}
\sqrt{\frac{W}{\nu}} & \frac{T}{2 \nu \sqrt{W/\nu}} \\
0 & 1
\end{matrix} \right] \\
\lvert J \rvert  &= \sqrt{\frac{W}{\nu}} \; .
\end{split}
\end{equation}

Because $X$ and $Y$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the joint density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) of $X$ and $Y$ is equal to the product ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ind}) of the marginal densities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}):

\begin{equation} \label{eq:t-pdf-f-XY}
f_{X,Y}(x,y) = f_X(x) \cdot f_Y(y) \; .
\end{equation}

With the probability density function of an invertible function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-invfct}), the joint density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) of $T$ and $W$ can be derived as:

\begin{equation} \label{eq:t-pdf-f-TW-s1}
f_{T,W}(t,w) = f_{X,Y}(x,y) \cdot \lvert J \rvert \; .
\end{equation}

Substituting \eqref{eq:t-pdf-XY-TW} into \eqref{eq:t-pdf-snorm-pdf} and \eqref{eq:t-pdf-chi2-pdf}, and then with \eqref{eq:t-pdf-XY-TW-jac} into \eqref{eq:t-pdf-f-TW-s1}, we get:

\begin{equation} \label{eq:t-pdf-f-TW-s2}
\begin{split}
f_{T,W}(t,w) &= f_X\left( t \cdot \sqrt{\frac{w}{\nu}} \right) \cdot f_Y(w) \cdot \lvert J \rvert \\
&= \frac{1}{\sqrt{2 \pi}} \cdot e^{-\frac{\left( t \cdot \sqrt{\frac{w}{\nu}} \right)^2}{2}} \cdot \frac{1}{\Gamma\left( \frac{\nu}{2} \right) \cdot 2^{\nu/2}} \cdot w^{\frac{\nu}{2}-1} \cdot e^{-\frac{w}{2}} \cdot \sqrt{\frac{w}{\nu}} \\
&= \frac{1}{\sqrt{2 \pi \nu} \cdot \Gamma\left( \frac{\nu}{2} \right) \cdot 2^{\nu/2}} \cdot w^{\frac{\nu+1}{2}-1} \cdot e^{-\frac{w}{2} \left( \frac{t^2}{\nu} + 1 \right)} \; .
\end{split}
\end{equation}

The marginal density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of $T$ can now be obtained by integrating out ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}) $W$:

\begin{equation} \label{eq:t-pdf-f-T-s1}
\begin{split}
f_T(t) &= \int_{0}^{\infty} f_{T,W}(t,w) \, \mathrm{d}w \\
&= \frac{1}{\sqrt{2 \pi \nu} \cdot \Gamma\left( \frac{\nu}{2} \right) \cdot 2^{\nu/2}} \cdot \int_{0}^{\infty} w^{\frac{\nu+1}{2}-1} \cdot \mathrm{exp}\left[ -\frac{1}{2}\left( \frac{t^2}{\nu}+1 \right) w \right] \, \mathrm{d}w \\
&= \frac{1}{\sqrt{2 \pi \nu} \cdot \Gamma\left( \frac{\nu}{2} \right) \cdot 2^{\nu/2}} \cdot \frac{\Gamma\left( \frac{\nu+1}{2} \right)}{\left[ \frac{1}{2}\left( \frac{t^2}{\nu}+1 \right) \right]^{(\nu+1)/2}} \cdot \int_{0}^{\infty} \frac{\left[ \frac{1}{2}\left( \frac{t^2}{\nu}+1 \right) \right]^{(\nu+1)/2}}{\Gamma\left( \frac{\nu+1}{2} \right)} \cdot w^{\frac{\nu+1}{2}-1} \cdot \mathrm{exp}\left[ -\frac{1}{2}\left( \frac{t^2}{\nu}+1 \right) w \right] \, \mathrm{d}w \; .
\end{split}
\end{equation}

At this point, we can recognize that the integrand is equal to the probability density function of a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}) with

\begin{equation} \label{eq:t-pdf-f-W-gam-ab}
a = \frac{\nu+1}{2} \quad \text{and} \quad b = \frac{1}{2}\left( \frac{t^2}{\nu}+1 \right) \; ,
\end{equation}

and because a probability density function integrates to one ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}), we finally have:

\begin{equation} \label{eq:t-pdf-f-T-s2}
\begin{split}
f_T(t) &= \frac{1}{\sqrt{2 \pi \nu} \cdot \Gamma\left( \frac{\nu}{2} \right) \cdot 2^{\nu/2}} \cdot \frac{\Gamma\left( \frac{\nu+1}{2} \right)}{\left[ \frac{1}{2}\left( \frac{t^2}{\nu}+1 \right) \right]^{(\nu+1)/2}} \\
&= \frac{\Gamma\left( \frac{\nu+1}{2} \right)}{\Gamma\left( \frac{\nu}{2} \right) \cdot \sqrt{\nu \pi}} \cdot \left( \frac{t^2}{\nu}+1 \right)^{-\frac{\nu+1}{2}} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Computation Empire (2021): "Student's t Distribution: Derivation of PDF"; in: \textit{YouTube}, retrieved on 2021-10-11; URL: \url{https://www.youtube.com/watch?v=6BraaGEVRY8}.
\end{itemize}
\vspace{1em}



\subsection{Gamma distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:gam}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, $X$ is said to follow a gamma distribution with shape $a$ and rate $b$

\begin{equation} \label{eq:gam-gam}
X \sim \mathrm{Gam}(a, b) \; ,
\end{equation}

if and only if its probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) is given by

\begin{equation} \label{eq:gam-gam-pdf}
\mathrm{Gam}(x; a, b) = \frac{b^a}{\Gamma(a)} x^{a-1} \exp[-b x], \quad x > 0
\end{equation}

where $a > 0$ and $b > 0$, and the density is zero, if $x \leq 0$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Koch, Karl-Rudolf (2007): "Gamma Distribution"; in: \textit{Introduction to Bayesian Statistics}, Springer, Berlin/Heidelberg, 2007, p. 47, eq. 2.172; URL: \url{https://www.springer.com/de/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Special case of Wishart distribution}]{Special case of Wishart distribution} \label{sec:gam-wish}
\setcounter{equation}{0}

\textbf{Theorem:} The gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) is a special case of the Wishart distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wish}) where the number of columns of the random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}) is $p = 1$.


\vspace{1em}
\textbf{Proof:} Let $X$ be a $p \times p$ positive-definite symmetric matrix, such that $X$ follows a Wishart distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wish}):

\begin{equation} \label{eq:gam-wish-wish}
Y \sim \mathcal{W}(V, n) \; .
\end{equation}

Then, $Y$ is described by the probability density function

\begin{equation} \label{eq:gam-wish-wish-pdf}
\begin{split}
p(Y) = \frac{1}{\Gamma_p \left( \frac{n}{2} \right)} \cdot \frac{1}{\sqrt{2^{n p} |V|^n}} \cdot |X|^{(n-p-1)/2} \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( V^{-1} X \right) \right]
\end{split}
\end{equation}

where $\lvert A \rvert$ is a matrix determinant, $A^{-1}$ is a matrix inverse and $\Gamma_p(x)$ is the multivariate gamma function of order $p$. If $p = 1$, then $\Gamma_p(x) = \Gamma(x)$ is the ordinary gamma function, $x = X$ and $v = V$ are real numbers. Thus, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $x$ can be developed as

\begin{equation} \label{eq:gam-wish-gam-pdf-s1}
\begin{split}
p(x) &= \frac{1}{\Gamma\left( \frac{n}{2} \right)} \cdot \frac{1}{\sqrt{2^n \, v^n}} \cdot x^{(n-2)/2} \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( v^{-1} x \right) \right] \\
&= \frac{(2v)^{-n/2}}{\Gamma\left( \frac{n}{2} \right)} \cdot x^{n/2-1} \cdot \exp\left[ -\frac{1}{2v} x \right] \\
\end{split}
\end{equation}

Finally, substituting $a = \frac{n}{2}$ and $b = \frac{1}{2v}$, we get

\begin{equation} \label{eq:gam-wish-gam-pdf-s2}
p(x) = \frac{b^a}{\Gamma(a)} \, x^{a-1} \, \exp[-b x]
\end{equation}

which is the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textit{Standard gamma distribution}]{Standard gamma distribution} \label{sec:sgam}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, $X$ is said to have a standard gamma distribution, if $X$ follows a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) with shape $a > 0$ and rate $b = 1$:

\begin{equation} \label{eq:sgam-sgam}
X \sim \mathrm{Gam}(a, 1) \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item JoramSoch (2017): "Gamma-distributed random numbers"; in: \textit{MACS â€“ a new SPM toolbox for model assessment, comparison and selection}, retrieved on 2020-05-26; URL: \url{https://github.com/JoramSoch/MACS/blob/master/MD_gamrnd.m}; DOI: 10.5281/zenodo.845404.
\item NIST/SEMATECH (2012): "Gamma distribution"; in: \textit{e-Handbook of Statistical Methods}, ch. 1.3.6.6.11; URL: \url{https://www.itl.nist.gov/div898/handbook/eda/section3/eda366b.htm}; DOI: 10.18434/M32189.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relationship to standard gamma distribution}]{Relationship to standard gamma distribution} \label{sec:gam-sgam}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) with shape $a$ and rate $b$:

\begin{equation} \label{eq:gam-sgam-X-gam}
X \sim \mathrm{Gam}(a,b) \; .
\end{equation}

Then, the quantity $Y = b X$ will have a standard gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:sgam}) with shape $a$ and rate $1$:

\begin{equation} \label{eq:gam-sgam-Y-snorm}
Y = b X \sim \mathrm{Gam}(a,1) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Note that $Y$ is a function of $X$

\begin{equation} \label{eq:gam-sgam-Y-X}
Y = g(X) = b X
\end{equation}

with the inverse function

\begin{equation} \label{eq:gam-sgam-X-Y}
X = g^{-1}(Y) = \frac{1}{b} Y \; .
\end{equation}

Because $b$ is positive, $g(X)$ is strictly increasing and we can calculate the cumulative distribution function of a strictly increasing function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf-sifct}) as

\begin{equation} \label{eq:gam-sgam-cdf-sifct}
F_Y(y) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; y < \mathrm{min}(\mathcal{Y}) \\
F_X(g^{-1}(y)) \; , & \text{if} \; y \in \mathcal{Y} \\
1 \; , & \text{if} \; y > \mathrm{max}(\mathcal{Y}) \; .
\end{array}
\right.
\end{equation}

The cumulative distribution function of the gamma-distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-cdf}) $X$ is

\begin{equation} \label{eq:gam-sgam-gam-cdf}
F_X(x) = \int_{-\infty}^{x} \frac{b^a}{\Gamma(a)} t^{a-1} \exp[-b t] \, \mathrm{d}t \; .
\end{equation}

Applying \eqref{eq:gam-sgam-cdf-sifct} to \eqref{eq:gam-sgam-gam-cdf}, we have:

\begin{equation} \label{eq:gam-sgam-Y-cdf-s1}
\begin{split}
F_Y(y) &\overset{\eqref{eq:gam-sgam-cdf-sifct}}{=} F_X(g^{-1}(y)) \\
&\overset{\eqref{eq:gam-sgam-gam-cdf}}{=} \int_{-\infty}^{y/b} \frac{b^a}{\Gamma(a)} t^{a-1} \exp[-b t] \, \mathrm{d}t \; .
\end{split}
\end{equation}

Substituting $s = b t$, such that $t = s/b$, we obtain

\begin{equation} \label{eq:gam-sgam-Z-cdf-s2}
\begin{split}
F_Y(y) &= \int_{-b \infty}^{b (y/b)} \frac{b^a}{\Gamma(a)} \left(\frac{s}{b}\right)^{a-1} \exp\left[-b \left(\frac{s}{b}\right)\right] \, \mathrm{d}\left(\frac{s}{b}\right) \\
&= \int_{-\infty}^{y} \frac{b^a}{\Gamma(a)} \, \frac{1}{b^{a-1} \, b} \, s^{a-1} \exp[-s] \, \mathrm{d}s \\
&= \int_{-\infty}^{y} \frac{1}{\Gamma(a)} s^{a-1} \exp[-s] \, \mathrm{d}s
\end{split}
\end{equation}

which is the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of the standard gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:sgam}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Relationship to standard gamma distribution}]{Relationship to standard gamma distribution} \label{sec:gam-sgam2}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) with shape $a$ and rate $b$:

\begin{equation} \label{eq:gam-sgam2-X-gam}
X \sim \mathrm{Gam}(a,b) \; .
\end{equation}

Then, the quantity $Y = b X$ will have a standard gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:sgam}) with shape $a$ and rate $1$:

\begin{equation} \label{eq:gam-sgam2-Y-snorm}
Y = b X \sim \mathrm{Gam}(a,1) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Note that $Y$ is a function of $X$

\begin{equation} \label{eq:gam-sgam2-Y-X}
Y = g(X) = b X
\end{equation}

with the inverse function

\begin{equation} \label{eq:gam-sgam2-X-Y}
X = g^{-1}(Y) = \frac{1}{b} Y \; .
\end{equation}

Because $b$ is positive, $g(X)$ is strictly increasing and we can calculate the probability density function of a strictly increasing function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-sifct}) as

\begin{equation} \label{eq:gam-sgam2-pdf-sifct}
f_Y(y) = \left\{
\begin{array}{rl}
f_X(g^{-1}(y)) \, \frac{\mathrm{d}g^{-1}(y)}{\mathrm{d}y} \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y \notin \mathcal{Y}
\end{array}
\right.
\end{equation}

where $\mathcal{Y} = \left\lbrace y = g(x): x \in \mathcal{X} \right\rbrace$. With the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), we have

\begin{equation} \label{eq:gam-sgam2-pdf-Y}
\begin{split}
f_Y(y) &= \frac{b^a}{\Gamma(a)} [g^{-1}(y)]^{a-1} \exp[-b \, g^{-1}(y)] \cdot \frac{\mathrm{d}g^{-1}(y)}{\mathrm{d}y} \\
&= \frac{b^a}{\Gamma(a)} \left(\frac{1}{b} y\right)^{a-1} \exp\left[-b \left(\frac{1}{b} y\right)\right] \cdot \frac{\mathrm{d}\left(\frac{1}{b} y\right)}{\mathrm{d}y} \\
&= \frac{b^a}{\Gamma(a)} \, \frac{1}{b^{a-1}} \, y^{a-1} \exp[-y] \cdot \frac{1}{b} \\
&= \frac{1}{\Gamma(a)} \, y^{a-1} \exp[-y]
\end{split}
\end{equation}

which is the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of the standard gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:sgam}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Scaling of a gamma random variable}]{Scaling of a gamma random variable} \label{sec:gam-scal}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) with shape $a$ and rate $b$:

\begin{equation} \label{eq:gam-scal-gam}
X \sim \mathrm{Gam}(a,b) \; .
\end{equation}

Then, the quantity $Y = c X$ will also be gamma-distributed with shape $a$ and rate $b/c$:

\begin{equation} \label{eq:gam-scal-gam-scal}
Y = c X \sim \mathrm{Gam}\left( a, \frac{b}{c} \right) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:}  Note that $Y$ is a function of $X$

\begin{equation} \label{eq:gam-scal-Y-X}
Y = g(X) = c X
\end{equation}

with the inverse function

\begin{equation} \label{eq:gam-scal-X-Y}
X = g^{-1}(Y) = \frac{1}{c} Y \; .
\end{equation}

Because the parameters of a gamma distribution are positive ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}), $c$ must also be positive. Thus, $g(X)$ is strictly increasing and we can calculate the probability density function of a strictly increasing function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-sifct}) as

\begin{equation} \label{eq:gam-scal-pdf-sifct}
f_Y(y) = \left\{
\begin{array}{rl}
f_X(g^{-1}(y)) \, \frac{\mathrm{d}g^{-1}(y)}{\mathrm{d}y} \; , & \text{if} \; y \in \mathcal{Y} \\
0 \; , & \text{if} \; y \notin \mathcal{Y}
\end{array}
\right.
\end{equation}

The probability density function of the gamma-distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}) $X$ is

\begin{equation} \label{eq:gam-scal-gam-pdf}
f_X(x) = \frac{b^a}{\Gamma(a)} x^{a-1} \exp[-b x] \; .
\end{equation}

Applying \eqref{eq:gam-scal-pdf-sifct} to \eqref{eq:gam-scal-gam-pdf}, we have:

\begin{equation} \label{eq:gam-scal-Y-pdf}
\begin{split}
f_Y(y) &= \frac{b^a}{\Gamma(a)} [g^{-1}(y)]^{a-1} \exp[-b g^{-1}(y)] \, \frac{\mathrm{d}g^{-1}(y)}{\mathrm{d}y} \\
&= \frac{b^a}{\Gamma(a)} \left( \frac{1}{c} y \right)^{a-1} \exp\left[-b \left( \frac{1}{c} y \right) \right] \, \frac{\mathrm{d}\left( \frac{1}{c} y \right)}{\mathrm{d}y} \\
&= \frac{b^a}{\Gamma(a)} \left( \frac{1}{c} \right)^{a} \left( \frac{1}{c} \right)^{-1} y^{a-1} \exp\left[- \frac{b}{c} y \right] \, \frac{1}{c} \\
&= \frac{(b/c)^a}{\Gamma(a)} y^{a-1} \exp\left[- \frac{b}{c} y \right]
\end{split}
\end{equation}

which is the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) with shape $a$ and rate $b/c$.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:gam-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a positive random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:gam-pdf-gam}
X \sim \mathrm{Gam}(a, b) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ is

\begin{equation} \label{eq:gam-pdf-gam-pdf}
f_X(x) = \frac{b^a}{\Gamma(a)} x^{a-1} \exp[-b x] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Moment-generating function}]{Moment-generating function} \label{sec:gam-mgf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ follow a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:gam-mgf-gam}
X \sim \mathrm{Gam}(a, b) \; .
\end{equation}

Then, the moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) of $X$ is

\begin{equation} \label{eq:gam-mgf-gam-mgf}
M_X(t) = \left( 1 - \frac{t}{b} \right)^{-a} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The moment-generating function of a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) $X$ is defined as:

\begin{equation} \label{eq:gam-mgf-mgf}
M_X(t) = \mathrm{E} \left[ e^{tX} \right], \quad t \in \mathbb{R} \; .
\end{equation}

Applying the law of the unconscious statistician ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lotus}), we have:

\begin{equation} \label{eq:gam-mgf-gam-mgf-s1}
M_X(t) = \int_{\mathcal{X}} e^{tx} \cdot f_X(x) \, \mathrm{d}x \; .
\end{equation}

With the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), we have:

\begin{equation} \label{eq:gam-mgf-gam-mgf-s2}
M_X(t) = \int_{\mathbb{R}} \exp[t x] \cdot \frac{b^a}{\Gamma(a)} x^{a-1} \exp[-b x] \, \mathrm{d}x \; .
\end{equation}

Now we summarize the two exponential functions inside the integral:

\begin{equation} \label{eq:gam-mgf-gam-mgf-s3}
\begin{split}
M_X(t) &= \int_{\mathbb{R}} \frac{b^a}{\Gamma(a)} x^{a-1} \exp[-(b-t) x] \, \mathrm{d}x \\
&= \int_{\mathbb{R}} \frac{(b-t)^a}{(b-t)^a} \cdot \frac{b^a}{\Gamma(a)} x^{a-1} \exp[-(b-t) x] \, \mathrm{d}x \\
&= \int_{\mathbb{R}} \frac{b^a}{(b-t)^a} \cdot \frac{(b-t)^a}{\Gamma(a)} x^{a-1} \exp[-(b-t) x] \, \mathrm{d}x \\
&= \left( \frac{b}{b-t} \right)^a \int_{\mathbb{R}} \frac{(b-t)^a}{\Gamma(a)} x^{a-1} \exp[-(b-t) x] \, \mathrm{d}x \; .
\end{split}
\end{equation}

The integrand is equal to the probability density function of a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}):

\begin{equation} \label{eq:gam-mgf-gam-mgf-s4}
M_X(t) = \left( \frac{b}{b-t} \right)^a \int_{\mathbb{R}} \mathrm{Gam}(x; a, b-t) \, \mathrm{d}x \; .
\end{equation}

Because the entire probability density integrates to one ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}), we finally have:

\begin{equation} \label{eq:gam-mgf-gam-mgf-s5}
M_X(t) = \left( \frac{b}{b-t} \right)^a = \left( \frac{b-t}{b} \right)^{-a} = \left( \frac{b}{b} - \frac{t}{b} \right)^{-a} = \left( 1 - \frac{t}{b} \right)^{-a} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Cumulative distribution function}]{Cumulative distribution function} \label{sec:gam-cdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a positive random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:gam-cdf-gam}
X \sim \mathrm{Gam}(a, b) \; .
\end{equation}

Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$ is

\begin{equation} \label{eq:gam-cdf-gam-cdf}
F_X(x) = \frac{\gamma(a,bx)}{\Gamma(a)}
\end{equation}

where $\Gamma(x)$ is the gamma function and $\gamma(s,x)$ is the lower incomplete gamma function.


\vspace{1em}
\textbf{Proof:} The probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}) is:

\begin{equation} \label{eq:gam-cdf-gam-pdf}
f_X(x) = \frac{b^a}{\Gamma(a)} x^{a-1} \exp[-b x] \; .
\end{equation}

Thus, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) is:

\begin{equation} \label{eq:gam-cdf-gam-cdf-s1}
\begin{split}
F_X(x) &= \int_{0}^{x} \mathrm{Gam}(z; a, b) \, \mathrm{d}z \\
&= \int_{0}^{x} \frac{b^a}{\Gamma(a)} z^{a-1} \exp[-b z] \, \mathrm{d}z \\
&= \frac{b^a}{\Gamma(a)} \int_{0}^{x} z^{a-1} \exp[-b z] \, \mathrm{d}z \; .
\end{split}
\end{equation}

Substituting $t = b z$, i.e. $z = t/b$, this becomes:

\begin{equation} \label{eq:gam-cdf-gam-cdf-s2}
\begin{split}
F_X(x) &= \frac{b^a}{\Gamma(a)} \int_{b \cdot 0}^{b x} \left(\frac{t}{b}\right)^{a-1} \exp\left[-b \left(\frac{t}{b}\right)\right] \, \mathrm{d}\left(\frac{t}{b}\right) \\
&= \frac{b^a}{\Gamma(a)} \cdot \frac{1}{b^{a-1}} \cdot \frac{1}{b} \int_{0}^{b x} t^{a-1} \exp[-t] \, \mathrm{d}t \\
&= \frac{1}{\Gamma(a)} \int_{0}^{b x} t^{a-1} \exp[-t] \, \mathrm{d}t \; .
\end{split}
\end{equation}

With the definition of the lower incomplete gamma function

\begin{equation} \label{eq:gam-cdf-low-inc-gam-fct}
\gamma(s,x) = \int_{0}^{x} t^{s-1} \exp[-t] \, \mathrm{d}t \; ,
\end{equation}

we arrive at the final result given by equation \eqref{eq:gam-cdf-gam-cdf}:

\begin{equation} \label{eq:gam-cdf-gam-cdf-qed}
F_X(x) = \frac{\gamma(a,bx)}{\Gamma(a)} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Incomplete gamma function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-10-29; URL: \url{https://en.wikipedia.org/wiki/Incomplete_gamma_function#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Quantile function}]{Quantile function} \label{sec:gam-qf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:gam-qf-gam}
X \sim \mathrm{Gam}(a,b) \; .
\end{equation}

Then, the quantile function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) of $X$ is

\begin{equation} \label{eq:gam-qf-gam-qf}
Q_X(p) = \left\{
\begin{array}{rl}
-\infty \; , & \text{if} \; p = 0 \\
\gamma^{-1}(a, \Gamma(a) \cdot p)/b \; , & \text{if} \; p > 0
\end{array}
\right.
\end{equation}

where $\gamma^{-1}(s, y)$ is the inverse of the lower incomplete gamma function $\gamma(s, x)$


\vspace{1em}
\textbf{Proof:} The cumulative distribution function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-cdf}) is:

\begin{equation} \label{eq:gam-qf-gam-cdf}
F_X(x) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < 0 \\
\frac{\gamma(a,bx)}{\Gamma(a)} \; , & \text{if} \; x \geq 0 \; .
\end{array}
\right.
\end{equation}

The quantile function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) $Q_X(p)$ is defined as the smallest $x$, such that $F_X(x) = p$:

\begin{equation} \label{eq:gam-qf-qf}
Q_X(p) = \min \left\lbrace x \in \mathbb{R} \, \vert \, F_X(x) = p \right\rbrace \; .
\end{equation}

Thus, we have $Q_X(p) = -\infty$, if $p = 0$. When $p > 0$, it holds that ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf-cdf})

\begin{equation} \label{eq:gam-qf-gam-qf-s1}
Q_X(p) = F_X^{-1}(x) \; .
\end{equation}

This can be derived by rearranging equation \eqref{eq:gam-qf-gam-cdf}:

\begin{equation} \label{eq:gam-qf-gam-qf-s2}
\begin{split}
p &= \frac{\gamma(a,bx)}{\Gamma(a)} \\
\Gamma(a) \cdot p &= \gamma(a,bx) \\
\gamma^{-1}(a, \Gamma(a) \cdot p) &= bx \\
x &= \frac{\gamma^{-1}(a, \Gamma(a) \cdot p)}{b} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Incomplete gamma function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-19; URL: \url{https://en.wikipedia.org/wiki/Incomplete_gamma_function#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:gam-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:gam-mean-gam}
X \sim \mathrm{Gam}(a, b) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation} \label{eq:gam-mean-gam-mean}
\mathrm{E}(X) = \frac{a}{b} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is the probability-weighted average over all possible values:

\begin{equation} \label{eq:gam-mean-mean}
\mathrm{E}(X) = \int_{\mathcal{X}} x \cdot f_X(x) \, \mathrm{d}x \; .
\end{equation}

With the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), this reads:

\begin{equation} \label{eq:gam-mean-gam-mean-s1}
\begin{split}
\mathrm{E}(X) &= \int_{0}^{\infty} x \cdot \frac{b^a}{\Gamma(a)} x^{a-1} \exp[-b x] \, \mathrm{d}x \\
&= \int_{0}^{\infty} \frac{b^a}{\Gamma(a)} x^{(a+1)-1} \exp[-b x] \, \mathrm{d}x \\
&= \int_{0}^{\infty} \frac{1}{b} \cdot \frac{b^{a+1}}{\Gamma(a)} x^{(a+1)-1} \exp[-b x] \, \mathrm{d}x \; .
\end{split}
\end{equation}

Employing the relation $\Gamma(x+1) = \Gamma(x) \cdot x$, we have

\begin{equation} \label{eq:gam-mean-gam-mean-s2}
\mathrm{E}(X) = \int_{0}^{\infty} \frac{a}{b} \cdot \frac{b^{a+1}}{\Gamma(a+1)} x^{(a+1)-1} \exp[-b x] \, \mathrm{d}x
\end{equation}

and again using the density of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), we get

\begin{equation} \label{eq:gam-mean-gam-mean-s3}
\begin{split}
\mathrm{E}(X) &= \frac{a}{b} \int_{0}^{\infty} \mathrm{Gam}(x; a+1, b) \, \mathrm{d}x \\
&= \frac{a}{b} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Turlapaty, Anish (2013): "Gamma random variable: mean \& variance"; in: \textit{YouTube}, retrieved on 2020-05-19; URL: \url{https://www.youtube.com/watch?v=Sy4wP-Y2dmA}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Variance}]{Variance} \label{sec:gam-var}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:gam-var-gam}
X \sim \mathrm{Gam}(a, b) \; .
\end{equation}

Then, the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is

\begin{equation} \label{eq:gam-var-gam-var}
\mathrm{Var}(X) = \frac{a}{b^2} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) can be expressed in terms of expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-mean}) as

\begin{equation} \label{eq:gam-var-var-mean}
\mathrm{Var}(X) = \mathrm{E}(X^2) - \mathrm{E}(X)^2 \; .
\end{equation}

The expected value of a gamma random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-mean}) is

\begin{equation} \label{eq:gam-var-gam-mean}
\mathrm{E}(X) = \frac{a}{b} \; .
\end{equation}

With the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), the expected value of a squared gamma random variable is

\begin{equation} \label{eq:gam-var-gam-sqr-mean-s1}
\begin{split}
\mathrm{E}(X^2) &= \int_{0}^{\infty} x^2 \cdot \frac{b^a}{\Gamma(a)} x^{a-1} \exp[-b x] \, \mathrm{d}x \\
&= \int_{0}^{\infty} \frac{b^a}{\Gamma(a)} x^{(a+2)-1} \exp[-b x] \, \mathrm{d}x \\
&= \int_{0}^{\infty} \frac{1}{b^2} \cdot \frac{b^{a+2}}{\Gamma(a)} x^{(a+2)-1} \exp[-b x] \, \mathrm{d}x \; .
\end{split}
\end{equation}

Twice-applying the relation $\Gamma(x+1) = \Gamma(x) \cdot x$, we have

\begin{equation} \label{eq:gam-var-gam-sqr-mean-s2}
\mathrm{E}(X^2) = \int_{0}^{\infty} \frac{a \, (a+1)}{b^2} \cdot \frac{b^{a+2}}{\Gamma(a+2)} x^{(a+2)-1} \exp[-b x] \, \mathrm{d}x
\end{equation}

and again using the density of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), we get

\begin{equation} \label{eq:gam-var-gam-sqr-mean-s3}
\begin{split}
\mathrm{E}(X^2) &= \frac{a \, (a+1)}{b^2} \int_{0}^{\infty} \mathrm{Gam}(x; a+2, b) \, \mathrm{d}x \\
&= \frac{a^2+a}{b^2} \; .
\end{split}
\end{equation}

Plugging \eqref{eq:gam-var-gam-sqr-mean-s3} and \eqref{eq:gam-var-gam-mean} into \eqref{eq:gam-var-var-mean}, the variance of a gamma random variable finally becomes

\begin{equation} \label{eq:gam-var-gam-var-qed}
\begin{split}
\mathrm{Var}(X) &= \frac{a^2+a}{b^2} - \left( \frac{a}{b} \right)^2 \\
&= \frac{a}{b^2} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Turlapaty, Anish (2013): "Gamma random variable: mean \& variance"; in: \textit{YouTube}, retrieved on 2020-05-19; URL: \url{https://www.youtube.com/watch?v=Sy4wP-Y2dmA}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Logarithmic expectation}]{Logarithmic expectation} \label{sec:gam-logmean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:gam-logmean-gam}
X \sim \mathrm{Gam}(a, b) \; .
\end{equation}

Then, the expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the natural logarithm of $X$ is

\begin{equation} \label{eq:gam-logmean-gam-logmean}
\mathrm{E}(\ln X) = \psi(a) - \ln(b)
\end{equation}

where $\psi(x)$ is the digamma function.


\vspace{1em}
\textbf{Proof:} Let $Y = \ln(X)$, such that $\mathrm{E}(Y) = \mathrm{E}(\ln X)$ and consider the special case that $b = 1$. In this case, the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}) is

\begin{equation} \label{eq:gam-logmean-X-pdf-s1}
f_X(x) = \frac{1}{\Gamma(a)} \, x^{a-1} \, \mathrm{exp} [-x] \; .
\end{equation}

Multiplying this function with $\mathrm{d}x$, we obtain

\begin{equation} \label{eq:gam-logmean-X-pdf-s2}
f_X(x) \, \mathrm{d}x = \frac{1}{\Gamma(a)} \, x^a \, \mathrm{exp} [-x] \, \frac{\mathrm{d}x}{x} \; .
\end{equation}

Substituting $y = \ln x$, i.e. $x = e^y$, such that $\mathrm{d}x/\mathrm{d}y = x$, i.e. $\mathrm{d}x/x = \mathrm{d}y$, we get

\begin{equation} \label{eq:gam-logmean-Y-pdf-s1}
\begin{split}
f_Y(y) \, \mathrm{d}y &= \frac{1}{\Gamma(a)} \, \left( e^y \right)^a \, \mathrm{exp} [-e^y] \, \mathrm{d}y \\
&= \frac{1}{\Gamma(a)} \, \mathrm{exp}\left[ ay - e^y \right] \, \mathrm{d}y \; .
\end{split}
\end{equation}

Because $f_Y(y)$ integrates to one, we have

\begin{equation} \label{eq:gam-logmean-Y-pdf-s2}
\begin{split}
1 &= \int_{\mathbb{R}} f_Y(y) \, \mathrm{d}y \\
1 &= \int_{\mathbb{R}} \frac{1}{\Gamma(a)} \, \mathrm{exp}\left[ ay - e^y \right] \, \mathrm{d}y \\
\Gamma(a) &= \int_{\mathbb{R}} \mathrm{exp}\left[ ay - e^y \right] \, \mathrm{d}y \; .
\end{split}
\end{equation}

Note that the integrand in \eqref{eq:gam-logmean-Y-pdf-s2} is differentiable with respect to $a$:

\begin{equation} \label{eq:gam-logmean-dfy-da}
\begin{split}
\frac{\mathrm{d}}{\mathrm{d}a} \mathrm{exp}\left[ ay - e^y \right] \, \mathrm{d}y &= y \, \mathrm{exp}\left[ ay - e^y \right] \, \mathrm{d}y \\
&\overset{\eqref{eq:gam-logmean-Y-pdf-s1}}{=} \Gamma(a) \, y \, f_Y(y) \, \mathrm{d}y \; .
\end{split}
\end{equation}

Now we can calculate the expected value of $Y = \ln(X)$:

\begin{equation} \label{eq:gam-logmean-E-Y-s1}
\begin{split}
\mathrm{E}(Y) &= \int_{\mathbb{R}} y \, f_Y(y) \, \mathrm{d}y \\
&\overset{\eqref{eq:gam-logmean-dfy-da}}{=} \frac{1}{\Gamma(a)} \int_{\mathbb{R}} \frac{\mathrm{d}}{\mathrm{d}a} \mathrm{exp}\left[ ay - e^y \right] \, \mathrm{d}y \\
&= \frac{1}{\Gamma(a)} \frac{\mathrm{d}}{\mathrm{d}a} \int_{\mathbb{R}} \mathrm{exp}\left[ ay - e^y \right] \, \mathrm{d}y \\
&\overset{\eqref{eq:gam-logmean-Y-pdf-s2}}{=} \frac{1}{\Gamma(a)} \frac{\mathrm{d}}{\mathrm{d}a} \Gamma(a) \\
&= \frac{\Gamma'(a)}{\Gamma(a)} \; .
\end{split}
\end{equation}

Using the derivative of a logarithmized function

\begin{equation} \label{eq:gam-logmean-log-der}
\frac{\mathrm{d}}{\mathrm{d}x} \ln f(x) = \frac{f'(x)}{f(x)}
\end{equation}

and the definition of the digamma function

\begin{equation} \label{eq:gam-logmean-psi}
\psi(x) = \frac{\mathrm{d}}{\mathrm{d}x} \ln \Gamma(x) \; ,
\end{equation}

we have

\begin{equation} \label{eq:gam-logmean-E-Y-s2}
\mathrm{E}(Y) = \psi(a) \; .
\end{equation}

Finally, noting that $1/b$ acts as a scaling parameter ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-sgam}) on a gamma-distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}),

\begin{equation} \label{eq:gam-logmean-gam-sgam}
X \sim \mathrm{Gam}(a,1) \quad \Rightarrow \quad \frac{1}{b} X \sim \mathrm{Gam}(a,b) \; ,
\end{equation}

and that a scaling parameter acts additively on the logarithmic expectation of a random variable,

\begin{equation} \label{eq:gam-logmean-logmean}
\mathrm{E}\left[\ln(cX)\right] = \mathrm{E}\left[\ln(X) + \ln(c)\right] = \mathrm{E}\left[\ln(X)\right] + \ln(c) \; ,
\end{equation}

it follows that

\begin{equation} \label{eq:gam-logmean-E-Y-s3}
X \sim \mathrm{Gam}(a,b) \quad \Rightarrow \quad \mathrm{E}(\ln X) = \psi(a) - \ln(b) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item whuber (2018): "What is the expected value of the logarithm of Gamma distribution?"; in: \textit{StackExchange CrossValidated}, retrieved on 2020-05-25; URL: \url{https://stats.stackexchange.com/questions/370880/what-is-the-expected-value-of-the-logarithm-of-gamma-distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Expectation of x ln x}]{Expectation of x ln x} \label{sec:gam-xlogx}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:gam-xlogx-gam}
X \sim \mathrm{Gam}(a, b) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $(X \cdot \ln X)$ is

\begin{equation} \label{eq:gam-xlogx-gam-xlogx}
\mathrm{E}(X \ln X) = \frac{a}{b} \left[ \psi(a) - \ln(b) \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} With the definition of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), the law of the unconscious statistician ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lotus}) and the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), we have:

\begin{equation} \label{eq:gam-xlogx-gam-xlogx-s1}
\begin{split}
\mathrm{E}(X \ln X) &= \int_{0}^{\infty} x \ln x \cdot \frac{b^a}{\Gamma(a)} x^{a-1} \exp[-b x] \, \mathrm{d}x \\
&= \frac{1}{\Gamma(a)} \int_{0}^{\infty} \ln x \cdot \frac{b^{a+1}}{b} x^{a} \exp[-b x] \, \mathrm{d}x \\
&= \frac{\Gamma(a+1)}{\Gamma(a) \, b} \int_{0}^{\infty} \ln x \cdot \frac{b^{a+1}}{\Gamma(a+1)} x^{(a+1)-1} \exp[-b x] \, \mathrm{d}x \\
\end{split}
\end{equation}

The integral now corresponds to the logarithmic expectation of a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-logmean}) with shape $a+1$ and rate $b$

\begin{equation} \label{eq:gam-xlogx-logmean-a+1}
\mathrm{E}(\ln Y) \quad \text{where} \quad Y \sim \mathrm{Gam}(a+1,b)
\end{equation}

which is given by ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-logmean})

\begin{equation} \label{eq:gam-xlogx-gam-logmean}
\mathrm{E}(\ln Y) = \psi(a+1) - \ln(b)
\end{equation}

where $\psi(x)$ is the digamma function. Additionally employing the relation

\begin{equation} \label{eq:gam-xlogx-gam-fct}
\Gamma(x+1) = \Gamma(x) \cdot x \quad \Leftrightarrow \quad \frac{\Gamma(x+1)}{\Gamma(x)} = x \; ,
\end{equation}

the expression in equation \eqref{eq:gam-xlogx-gam-xlogx-s1} develops into:

\begin{equation} \label{eq:gam-xlogx-gam-xlogx-qed}
\mathrm{E}(X \ln X) = \frac{a}{b} \left[ \psi(a) - \ln(b) \right] \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item gunes (2020): "What is the expected value of x log(x) of the gamma distribution?"; in: \textit{StackExchange CrossValidated}, retrieved on 2020-10-15; URL: \url{https://stats.stackexchange.com/questions/457357/what-is-the-expected-value-of-x-logx-of-the-gamma-distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Differential entropy}]{Differential entropy} \label{sec:gam-dent}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:gam-dent-gam}
X \sim \mathrm{Gam}(a, b)
\end{equation}

Then, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $X$ in nats is

\begin{equation} \label{eq:gam-dent-gam-dent}
\mathrm{h}(X) = a + \ln \Gamma(a) + (1-a) \cdot \psi(a) + \ln b \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of a random variable is defined as

\begin{equation} \label{eq:gam-dent-dent}
\mathrm{h}(X) = - \int_{\mathcal{X}} p(x) \, \log_b p(x) \, \mathrm{d}x \; .
\end{equation}

To measure $h(X)$ in nats, we set $b = e$, such that ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean})

\begin{equation} \label{eq:gam-dent-dent-nats}
\mathrm{h}(X) = - \mathrm{E}\left[ \ln p(x) \right] \; .
\end{equation}

With the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), the differential entropy of $X$ is:

\begin{equation} \label{eq:gam-dent-gam-dent-s1}
\begin{split}
\mathrm{h}(X) &= - \mathrm{E}\left[ \ln \left( \frac{b^a}{\Gamma(a)} x^{a-1} \exp[-b x] \right) \right] \\
&= - \mathrm{E}\left[ a \cdot \ln b - \ln \Gamma(a) + (a-1) \ln x - b x \right] \\
&= - a \cdot \ln b + \ln \Gamma(a) - (a-1) \cdot \mathrm{E}(\ln x) + b \cdot \mathrm{E}(x) \; .
\end{split}
\end{equation}

Using the mean ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-mean}) and logarithmic expectation ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-logmean}) of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam})

\begin{equation} \label{eq:gam-dent-gam-mean-logmean}
X \sim \mathrm{Gam}(a, b) \quad \Rightarrow \quad \mathrm{E}(X) = \frac{a}{b} \quad \text{and} \quad \mathrm{E}(\ln X) = \psi(a) - \ln(b) \; ,
\end{equation}

the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $X$ becomes:

\begin{equation} \label{eq:gam-dent-gam-dent-s2}
\begin{split}
\mathrm{h}(X) &= - a \cdot \ln b + \ln \Gamma(a) - (a-1) \cdot (\psi(a) - \ln b) + b \cdot \frac{a}{b} \\
&= - a \cdot \ln b + \ln \Gamma(a) + (1-a) \cdot \psi(a) + a \cdot \ln b - \ln b + a \\
&= a + \ln \Gamma(a) + (1-a) \cdot \psi(a) - \ln b \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Gamma distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-07-14; URL: \url{https://en.wikipedia.org/wiki/Gamma_distribution#Information_entropy}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Kullback-Leibler divergence}]{Kullback-Leibler divergence} \label{sec:gam-kl}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Assume two gamma distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) $P$ and $Q$ specifying the probability distribution of $X$ as

\begin{equation} \label{eq:gam-kl-gams}
\begin{split}
P: \; X &\sim \mathrm{Gam}(a_1, b_1) \\
Q: \; X &\sim \mathrm{Gam}(a_2, b_2) \; .
\end{split}
\end{equation}

Then, the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ is given by

\begin{equation} \label{eq:gam-kl-gam-KL}
\mathrm{KL}[P\,||\,Q] = a_2 \, \ln \frac{b_1}{b_2} - \ln \frac{\Gamma(a_1)}{\Gamma(a_2)} + (a_1 - a_2) \, \psi(a_1) - (b_1 - b_2) \, \frac{a_1}{b_1} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The KL divergence for a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is given by 

\begin{equation} \label{eq:gam-kl-KL-cont}
\mathrm{KL}[P\,||\,Q] = \int_{\mathcal{X}} p(x) \, \ln \frac{p(x)}{q(x)} \, \mathrm{d}x
\end{equation}

which, applied to the gamma distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) in \eqref{eq:gam-kl-gams}, yields

\begin{equation} \label{eq:gam-kl-gam-KL-s1}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \int_{-\infty}^{+\infty} \mathrm{Gam}(x; a_1, b_1) \, \ln \frac{\mathrm{Gam}(x; a_1, b_1)}{\mathrm{Gam}(x; a_2, b_2)} \, \mathrm{d}x \\
&= \left\langle \ln \frac{\mathrm{Gam}(x; a_1, b_1)}{\mathrm{Gam}(x; a_2, b_2)} \right\rangle_{p(x)} \; .
\end{split}
\end{equation}

Using the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), this becomes:

\begin{equation} \label{eq:gam-kl-gam-KL-s2}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \left\langle \ln \frac{ \frac{ {b_1}^{a_1}}{\Gamma(a_1)} x^{a_1-1} \exp[-b_1 x] }{ \frac{ {b_2}^{a_2}}{\Gamma(a_2)} x^{a_2-1} \exp[-b_2 x] } \right\rangle_{p(x)} \\
&= \left\langle \ln \left( \frac{ {b_1}^{a_1}}{ {b_2}^{a_2}} \cdot \frac{\Gamma(a_2)}{\Gamma(a_1)} \cdot x^{a_1-a_2} \cdot \exp[-(b_1-b_2) x] \right) \right\rangle_{p(x)} \\
&= \left\langle a_1 \cdot \ln b_1 - a_2 \cdot \ln b_2 - \ln \Gamma(a_1) + \ln \Gamma(a_2) + (a_1-a_2) \cdot \ln x - (b_1-b_2) \cdot x \right\rangle_{p(x)} \; .
\end{split}
\end{equation}

Using the mean of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-mean}) and the expected value of a logarithmized gamma variate ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-logmean})

\begin{equation} \label{eq:gam-kl-gam-means}
\begin{split}
x \sim \mathrm{Gam}(a,b) \quad \Rightarrow \quad &\left\langle x \right\rangle = \frac{a}{b} \quad \text{and} \\
&\left\langle \ln x \right\rangle = \psi(a) - \ln(b) \; ,
\end{split}
\end{equation}

the Kullback-Leibler divergence from \eqref{eq:gam-kl-gam-KL-s2} becomes:

\begin{equation} \label{eq:gam-kl-gam-KL-s3}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= a_1 \cdot \ln b_1 - a_2 \cdot \ln b_2 - \ln \Gamma(a_1) + \ln \Gamma(a_2) + (a_1-a_2) \cdot \left( \psi(a_1) - \ln(b_1) \right) - (b_1-b_2) \cdot \frac{a_1}{b_1} \\
&= a_2 \cdot \ln b_1 - a_2 \cdot \ln b_2 - \ln \Gamma(a_1) + \ln \Gamma(a_2) + (a_1-a_2) \cdot \psi(a_1) - (b_1-b_2) \cdot \frac{a_1}{b_1} \; .
\end{split}
\end{equation}

Finally, combining the logarithms, we get:

\begin{equation} \label{eq:gam-kl-gam-KL-qed}
\mathrm{KL}[P\,||\,Q] = a_2 \, \ln \frac{b_1}{b_2} - \ln \frac{\Gamma(a_1)}{\Gamma(a_2)} + (a_1 - a_2) \, \psi(a_1) - (b_1 - b_2) \, \frac{a_1}{b_1} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Penny, William D. (2001): "KL-Divergences of Normal, Gamma, Dirichlet and Wishart densities"; in: \textit{University College, London}; URL: \url{https://www.fil.ion.ucl.ac.uk/~wpenny/publications/densities.ps}.
\end{itemize}
\vspace{1em}



\subsection{Exponential distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:exp}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, $X$ is said to be exponentially distributed with rate (or, inverse scale) $\lambda$

\begin{equation} \label{eq:exp-exp}
X \sim \mathrm{Exp}(\lambda) \; ,
\end{equation}

if and only if its probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) is given by

\begin{equation} \label{eq:exp-exp-pdf}
\mathrm{Exp}(x; \lambda) = \lambda \exp[-\lambda x], \quad x \geq 0
\end{equation}

where $\lambda > 0$, and the density is zero, if $x < 0$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Exponential distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-08; URL: \url{https://en.wikipedia.org/wiki/Exponential_distribution#Definitions}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Special case of gamma distribution}]{Special case of gamma distribution} \label{sec:exp-gam}
\setcounter{equation}{0}

\textbf{Theorem:} The exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}) is a special case of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) with shape $a = 1$ and rate $b = \lambda$.


\vspace{1em}
\textbf{Proof:} The probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}) is

\begin{equation} \label{eq:exp-gam-gam-pdf}
\mathrm{Gam}(x; a, b) = \frac{b^a}{\Gamma(a)} x^{a-1} \exp[-b x] \; .
\end{equation}

Setting $a = 1$ and $b = \lambda$, we obtain

\begin{equation} \label{eq:exp-gam-exp-pdf}
\begin{split}
\mathrm{Gam}(x; 1, \lambda) &= \frac{\lambda^1}{\Gamma(1)} x^{1-1} \exp[-\lambda x] \\
&= \frac{x^0}{\Gamma(1)} \lambda \exp[-\lambda x] \\
&= \lambda \exp[-\lambda x]
\end{split}
\end{equation}

which is equivalent to the probability density function of the exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp-pdf}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:exp-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a non-negative random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation} \label{eq:exp-pdf-exp}
X \sim \mathrm{Exp}(\lambda) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ is

\begin{equation} \label{eq:exp-pdf-gam-pdf}
f_X(x) = \lambda \exp[-\lambda x] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Moment-generating function}]{Moment-generating function} \label{sec:exp-mgf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation} \label{eq:exp-mgf-exp}
X \sim \mathrm{Exp}(\lambda) \; .
\end{equation}

Then, the moment generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) of $X$ is 

\begin{equation} \label{eq:exp-mgf-exp-mgf}
M_X(t) = \frac{\lambda}{\lambda - t}
\end{equation}

which is well-defined for $t < \lambda$.

\vspace{1em}
\textbf{Proof:} Suppose $X$ follows an exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}) with rate $\lambda$; that is, $X\sim \mathrm{Exp}(\lambda)$. Then, the probability density function ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp-pdf}) is given by 

\begin{equation} \label{eq:exp-mgf-exp-pdf}
f_X(x) = \lambda e^{-\lambda x}
\end{equation}

and the moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) is defined as

\begin{equation} \label{eq:exp-mgf-mgf}
M_X(t) = \mathrm{E} \left[ e^{tX} \right] \; .
\end{equation}

Using the definition of expected value for continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), the moment-generating function of $X$ is thus:

\begin{equation} \label{eq:exp-mgf-exp-mgf-s1}
\begin{split}
M_X(t) &= \int_0^{\infty} e^{tx} \cdot f_X(x) dx\\
&= \int_0^{\infty} e^{tx}\cdot \lambda e^{-\lambda x} dx\\
&= \int_0^{\infty} \lambda e^{x(t-\lambda)} dx\\
&= \frac{\lambda}{t-\lambda} e^{x(t-\lambda)} \Big|_{x = 0}^{x = \infty}\\
&= \lim_{x\rightarrow \infty} \left[ \frac{\lambda}{t-\lambda} e^{x(t-\lambda)} - \frac{\lambda}{t-\lambda}\right]\\
&= \frac{\lambda}{t-\lambda} \left[ \lim_{x\rightarrow \infty} e^{x(t-\lambda)} -1 \right] \; .
\end{split}
\end{equation}

Note that $t$ cannot be equal to $\lambda$, else $M_X(t)$ is undefined. Further, if $t > \lambda$, then $\lim_{x\rightarrow \infty} e^{x(t-\lambda)} = \infty$, which implies that $M_X(t)$ diverges for $t \geq \lambda$. So, we must restrict the domain of $M_X(t)$ to $t<\lambda$. Assuming this, we can further simplify \eqref{eq:exp-mgf-exp-mgf-s1}:

\begin{equation} \label{eq:exp-mgf-exp-mgf-s2}
\begin{split}
M_X(t) &= \frac{\lambda}{t-\lambda} \left[ \lim_{x\rightarrow \infty} e^{x(t-\lambda)} -1 \right] \\
&= \frac{\lambda}{t-\lambda} \left[ 0 - 1 \right] \\
&= \frac{\lambda}{\lambda - t} \; .
\end{split}
\end{equation}

This completes the proof of \eqref{eq:exp-mgf-exp-mgf}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Cumulative distribution function}]{Cumulative distribution function} \label{sec:exp-cdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation} \label{eq:exp-cdf-exp}
X \sim \mathrm{Exp}(\lambda) \; .
\end{equation}

Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$ is

\begin{equation} \label{eq:exp-cdf-exp-cdf}
F_X(x) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < 0 \\
1 - \exp[-\lambda x] \; , & \text{if} \; x \geq 0 \; .
\end{array}
\right.
\end{equation}


\vspace{1em}
\textbf{Proof:}  The probability density function of the exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp-pdf}) is:

\begin{equation} \label{eq:exp-cdf-exp-pdf}
\mathrm{Exp}(x; \lambda) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < 0 \\
\lambda \exp[-\lambda x] \; , & \text{if} \; x \geq 0 \; .
\end{array}
\right.
\end{equation}

Thus, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) is:

\begin{equation} \label{eq:exp-cdf-exp-cdf-s1}
F_X(x) = \int_{-\infty}^{x} \mathrm{Exp}(z; \lambda) \, \mathrm{d}z \; .
\end{equation}

If $x < 0$, we have:

\begin{equation} \label{eq:exp-cdf-exp-cdf-s2a}
F_X(x) = \int_{-\infty}^{x} 0 \, \mathrm{d}z = 0 \; .
\end{equation}

If $x \geq 0$, we have using \eqref{eq:exp-cdf-exp-pdf}:

\begin{equation} \label{eq:exp-cdf-exp-cdf-s2b}
\begin{split}
F_X(x) &= \int_{-\infty}^{0} \mathrm{Exp}(z; \lambda) \, \mathrm{d}z + \int_{0}^{x} \mathrm{Exp}(z; \lambda) \, \mathrm{d}z \\
&= \int_{-\infty}^{0} 0 \, \mathrm{d}z + \int_{0}^{x} \lambda \exp[-\lambda z] \, \mathrm{d}z \\
&= 0 + \lambda \left[ -\frac{1}{\lambda} \exp[-\lambda z] \right]_{0}^{x} \\
&= \lambda \left[ \left( -\frac{1}{\lambda} \exp[-\lambda x] \right) - \left( -\frac{1}{\lambda} \exp[-\lambda \cdot 0] \right) \right] \\
&= 1 - \exp[-\lambda x] \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Quantile function}]{Quantile function} \label{sec:exp-qf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation} \label{eq:exp-qf-exp}
X \sim \mathrm{Exp}(\lambda) \; .
\end{equation}

Then, the quantile function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) of $X$ is

\begin{equation} \label{eq:exp-qf-exp-qf}
Q_X(p) = \left\{
\begin{array}{rl}
-\infty \; , & \text{if} \; p = 0 \\
-\frac{\ln(1-p)}{\lambda} \; , & \text{if} \; p > 0 \; .
\end{array}
\right.
\end{equation}


\vspace{1em}
\textbf{Proof:} The cumulative distribution function of the exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp-cdf}) is:

\begin{equation} \label{eq:exp-qf-exp-cdf}
F_X(x) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < 0 \\
1 - \exp[-\lambda x] \; , & \text{if} \; x \geq 0 \; .
\end{array}
\right.
\end{equation}

The quantile function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) $Q_X(p)$ is defined as the smallest $x$, such that $F_X(x) = p$:

\begin{equation} \label{eq:exp-qf-qf}
Q_X(p) = \min \left\lbrace x \in \mathbb{R} \, \vert \, F_X(x) = p \right\rbrace \; .
\end{equation}

Thus, we have $Q_X(p) = -\infty$, if $p = 0$. When $p > 0$, it holds that ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf-cdf})

\begin{equation} \label{eq:exp-qf-exp-qf-s1}
Q_X(p) = F_X^{-1}(x) \; .
\end{equation}

This can be derived by rearranging equation \eqref{eq:exp-qf-exp-cdf}:

\begin{equation} \label{eq:exp-qf-exp-qf-s2}
\begin{split}
p &= 1 - \exp[-\lambda x] \\
\exp[-\lambda x] &= 1-p \\
-\lambda x &= \ln(1-p) \\
x &= -\frac{\ln(1-p)}{\lambda} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:exp-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation} \label{eq:exp-mean-exp}
X \sim \mathrm{Exp}(\lambda) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation} \label{eq:exp-mean-exp-mean}
\mathrm{E}(X) = \frac{1}{\lambda} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is the probability-weighted average over all possible values:

\begin{equation} \label{eq:exp-mean-mean}
\mathrm{E}(X) = \int_{\mathcal{X}} x \cdot f_\mathrm{X}(x) \, \mathrm{d}x \; .
\end{equation}

With the probability density function of the exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp-pdf}), this reads:

\begin{equation} \label{eq:exp-mean-exp-mean-s1}
\begin{split}
\mathrm{E}(X) &= \int_{0}^{+\infty} x \cdot \lambda \exp(-\lambda x) \, \mathrm{d}x \\
&= \lambda \int_{0}^{+\infty} x \cdot \exp(-\lambda x) \, \mathrm{d}x \; .
\end{split}
\end{equation}

Using the following anti-derivative

\begin{equation} \label{eq:exp-mean-exp-mean-s2}
\int x \cdot \exp(-\lambda x) \, \mathrm{d}x = \left( - \frac{1}{\lambda} x - \frac{1}{\lambda^2} \right) \exp(-\lambda x) \; ,
\end{equation}

the expected value becomes

\begin{equation} \label{eq:exp-mean-exp-mean-s3}
\begin{split}
\mathrm{E}(X) &= \lambda \left[ \left( - \frac{1}{\lambda} x - \frac{1}{\lambda^2} \right) \exp(-\lambda x) \right]_{0}^{+\infty} \\
&= \lambda \left[ \lim_{x \to \infty} \left( - \frac{1}{\lambda} x - \frac{1}{\lambda^2} \right) \exp(-\lambda x) - \left( - \frac{1}{\lambda} \cdot 0 - \frac{1}{\lambda^2} \right) \exp(-\lambda \cdot 0) \right] \\
&= \lambda \left[ 0 + \frac{1}{\lambda^2} \right] \\
&= \frac{1}{\lambda} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Koch, Karl-Rudolf (2007): "Expected Value"; in: \textit{Introduction to Bayesian Statistics}, Springer, Berlin/Heidelberg, 2007, p. 39, eq. 2.142a; URL: \url{https://www.springer.com/de/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Median}]{Median} \label{sec:exp-med}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation} \label{eq:exp-med-exp}
X \sim \mathrm{Exp}(\lambda) \; .
\end{equation}

Then, the median ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:med}) of $X$ is

\begin{equation} \label{eq:exp-med-exp-med}
\mathrm{median}(X) = \frac{\ln 2}{\lambda} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The median ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:med}) is the value at which the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) is $1/2$:

\begin{equation} \label{eq:exp-med-median}
F_X(\mathrm{median}(X)) = \frac{1}{2} \; .
\end{equation}

The cumulative distribution function of the exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp-cdf}) is

\begin{equation} \label{eq:exp-med-exp-cdf}
F_X(x) = 1 - \exp[-\lambda x], \quad x \geq 0 \; .
\end{equation}

Thus, the inverse CDF is

\begin{equation} \label{eq:exp-med-exp-cdf-inv}
x = -\frac{\ln(1-p)}{\lambda}
\end{equation}

and setting $p = 1/2$, we obtain:

\begin{equation} \label{eq:exp-med-exp-med-qed}
\mathrm{median}(X) = -\frac{\ln(1-\frac{1}{2})}{\lambda} = \frac{\ln 2}{\lambda} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mode}]{Mode} \label{sec:exp-mode}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation} \label{eq:exp-mode-exp}
X \sim \mathrm{Exp}(\lambda) \; .
\end{equation}

Then, the mode ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mode}) of $X$ is

\begin{equation} \label{eq:exp-mode-exp-mode}
\mathrm{mode}(X) = 0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:}  The mode ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mode}) is the value which maximizes the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}):

\begin{equation} \label{eq:exp-mode-mode}
\mathrm{mode}(X) = \operatorname*{arg\,max}_x f_X(x) \; .
\end{equation}

The probability density function of the exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp-pdf}) is:

\begin{equation} \label{eq:exp-mode-exp-pdf}
f_X(x) = \left\{
\begin{array}{rl}
0 \; , & \text{if} \; x < 0 \\
\lambda e^{-\lambda x} \; , & \text{if} \; x \geq 0 \; .
\end{array}
\right.
\end{equation}

Since

\begin{equation} \label{eq:exp-mode-exp-pdf-eq0}
f_X(0) = \lambda
\end{equation}

and

\begin{equation} \label{eq:exp-mode-exp-pdf-neq0}
0 < e^{-\lambda x} < 1 \quad \text{for any} \quad x > 0 \; ,
\end{equation}

it follows that

\begin{equation} \label{eq:exp-mode-exp-mode-qed}
\mathrm{mode}(X) = 0 \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Variance}]{Variance} \label{sec:exp-var}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation}
X \sim \mathrm{Exp}(\lambda) .
\end{equation}

Then, the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is

\begin{equation} \label{eq:exp-var-exp-var}
\mathrm{Var}(X) = \frac{1}{\lambda^2} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of a random variable is defined as

\begin{equation} \label{eq:exp-var-var}
\mathrm{Var}(X) = \mathrm{E}\left[ (X-\mathrm{E}(X))^2 \right] 
\end{equation}

which, partitioned into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-mean}), reads:

\begin{equation} \label{eq:exp-var-var2}
\mathrm{Var}(X) = \mathrm{E}\left[ X^2 \right] - \mathrm{E}\left[ X \right]^2 \; .
\end{equation}

The expected value of the exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp-mean}) is: 

\begin{equation} \label{eq:exp-var-exp-mean-ref}
\mathrm{E}[X] = \frac{1}{\lambda}
\end{equation}

The second moment $\mathrm{E}[X^2]$ can be derived as follows:

\begin{equation} \label{eq:exp-var-second-moment-s1}
\begin{split}
\mathrm{E} [X^2] &= \int_{- \infty}^{+\infty} x^2 \cdot f_\mathrm{X}(x) \, \mathrm{d}x \\ 
&= \int_{0}^{+\infty} x^2 \cdot \lambda \exp(-\lambda x) \, \mathrm{d}x \\ 
&= \lambda \int_{0}^{+\infty} x^2 \cdot \exp(-\lambda x) \, \mathrm{d}x \\
\end{split}
\end{equation}

Using the following anti-derivative

\begin{equation} \label{eq:exp-var-second-moment-s2}
\begin{split}
\int x^2 \cdot \exp(-\lambda x) \, \mathrm{d}x &= \left[ - \frac{1}{\lambda} x^2 \cdot \mathrm{exp}(-\lambda x) \right]_{0}^{+\infty} - \int 2x \left( - \frac{1}{\lambda} x \cdot \mathrm{exp}(-\lambda x) \right) \mathrm{d}x \\
&= \left[ - \frac{1}{\lambda} x^2 \cdot \mathrm{exp}(-\lambda x) \right]_{0}^{+\infty} - \left( \left[ \frac{1}{\lambda^2} 2x \cdot \mathrm{exp}(-\lambda x) \right]_{0}^{+\infty} - \int 2 \left( \frac{1}{\lambda^2} \cdot \mathrm{exp}(-\lambda x) \right) \mathrm{d}x \right) \\
&= \left[ - \frac{x^2}{\lambda} \cdot \mathrm{exp}(-\lambda x) \right]_{0}^{+\infty} - \left( \left[ \frac{2x}{\lambda^2} \cdot \mathrm{exp}(-\lambda x) \right]_{0}^{+\infty} - \left[ - \frac{2}{\lambda^3} \cdot \mathrm{exp}(-\lambda x) \right]_{0}^{+\infty} \right) \\
&= \left[ \left( - \frac{x^2}{\lambda} - \frac{2x}{\lambda^2} - \frac{2}{\lambda^3} \right) \exp(-\lambda x) \right]_{0}^{+\infty} \; ,
\end{split}
\end{equation}

the second moment becomes

\begin{equation} \label{eq:exp-var-second-moment-s3}
\begin{split}
\mathrm{E} [X^2] &= \lambda \left[ \left( - \frac{x^2}{\lambda} - \frac{2x}{\lambda^2} - \frac{2}{\lambda^3} \right) \exp(-\lambda x) \right]_{0}^{+\infty} \\
&= \lambda \left[ \lim_{x \to \infty} \left( - \frac{x^2}{\lambda} - \frac{2x}{\lambda^2} - \frac{2}{\lambda^3} \right) \exp(-\lambda x) - \left( 0 - 0 - \frac{2}{\lambda^3} \right) \exp(-\lambda \cdot 0) \right] \\
&= \lambda \left[ 0 + \frac{2}{\lambda^3} \right] \\
&= \frac{2}{\lambda^2} \; .
\end{split}
\end{equation}

Plugging \eqref{eq:exp-var-second-moment-s3} and \eqref{eq:exp-var-exp-mean-ref} into \eqref{eq:exp-var-var2}, we have:

\begin{equation} \label{eq:exp-var-exp-var-2}
\begin{split}
\mathrm{Var}(X) &= \mathrm{E}\left[ X^2 \right] - \mathrm{E}\left[ X \right]^2  \\
&= \frac{2}{\lambda^2} - \left ( \frac{1}{\lambda} \right)^2 \\
&= \frac{2}{\lambda^2} - \frac{1}{\lambda^2} \\
&= \frac{1}{\lambda^2} \; .
\end{split}
\end{equation}

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2023): "Exponential distribution"; in: \textit{Lectures on probability theory and mathematical statistics}, retrieved on 2023-01-23; URL: \url{https://www.statlect.com/probability-distributions/exponential-distribution}.
\item Wikipedia (2023): "Variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2023-01-23; URL: \url{https://en.wikipedia.org/wiki/Variance#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Skewness}]{Skewness} \label{sec:exp-skew}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}):

\begin{equation} \label{eq:exp-skew-exp}
X \sim \mathrm{Exp}(\lambda) \; .
\end{equation}

Then the skewness ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:skew}) of $X$ is

\begin{equation} \label{eq:exp-skew-exp-skew}
\mathrm{Skew}(X) = 2 \; .
\end{equation}

\vspace{1em}
\textbf{Proof:} 

To compute the skewness of $X$, we partition the skewness into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:skew-mean}):

\begin{equation} \label{eq:exp-skew-skew-mean}
\mathrm{Skew}(X) = \frac{\mathrm{E}(X^3)-3\mu\sigma^2-\mu^3}{\sigma^3} \; ,
\end{equation}

where $\mu$ and $\sigma$ are the mean and standard deviation of $X$, respectively. Since $X$ follows an exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}), the mean ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp-mean}) of $X$ is given by 

\begin{equation} \label{eq:exp-skew-exp-mean}
\mu = \mathrm{E}(X) = \frac{1}{\lambda}
\end{equation}

and the standard deviation ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp-var}) of $X$ is given by

\begin{equation} \label{eq:exp-skew-exp-var}
\sigma = \sqrt{\mathrm{Var}(X)} = \sqrt{\frac{1}{\lambda^2}} = \frac{1}{\lambda} \; .
\end{equation}

Substituting \eqref{eq:exp-skew-exp-mean} and \eqref{eq:exp-skew-exp-var} into \eqref{eq:exp-skew-skew-mean} gives:

\begin{equation} \label{eq:exp-skew-skew-mean-alt}
\begin{split}
\mathrm{Skew}(X) &= \frac{\mathrm{E}(X^3)-3\mu\sigma^2-\mu^3}{\sigma^3}\\
&= \frac{\mathrm{E}(X^3)}{\sigma^3}-\frac{3\mu\sigma^2+\mu^3}{\sigma^3}\\
&= \frac{\mathrm{E}(X^3)}{\left(\frac{1}{\lambda}\right)^3}-\frac{3\left(\frac{1}{\lambda}\right)\left(\frac{1}{\lambda}\right)^2 + \left(\frac{1}{\lambda}\right)^3}{\left(\frac{1}{\lambda}\right)^3}\\
&= \lambda^3\cdot\mathrm{E}(X^3)-\frac{\frac{3}{\lambda^3}+\frac{1}{\lambda^3}}{\frac{1}{\lambda^3}}\\
&= \lambda^3\cdot\mathrm{E}(X^3)-4 \; .
\end{split}
\end{equation}

Thus, the remaining work is to compute $\mathrm{E}(X^3)$. To do this, we use the moment-generating function of the exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp-mgf}) to calculate

\begin{equation} \label{eq:exp-skew-exp-moment}
\mathrm{E}(X^3) = M_X'''(0)
\end{equation}

based on the relationship between raw moment and moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-mgf}).

First, we differentiate the moment-generating function of the exponential distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp-mgf})

\begin{equation} \label{eq:exp-skew-exp-mgf}
M_X(t) = \frac{\lambda}{\lambda-t} = \lambda(\lambda-t)^{-1}
\end{equation}

with respect to $t$. Using the chain rule gives:

\begin{equation} \label{eq:exp-skew-exp-skew-s1}
\begin{split}
M_X'(t) &= -1\cdot \lambda(\lambda-t)^{-2}\cdot (-1) \\
&= \lambda(\lambda-t)^{-2} \; .
\end{split}
\end{equation}

We continue using the chain rule to obtain the second derivative:

\begin{equation} \label{eq:exp-skew-exp-skew-s2}
\begin{split}
M_X''(t) &= -2\cdot \lambda(\lambda-t)^{-3}\cdot (-1) \\
&= 2\lambda(\lambda-t)^{-3} \; .
\end{split}
\end{equation}

Finally, one more application of the chain rule gives us the third derivative:

\begin{equation} \label{eq:exp-skew-exp-skew-s3}
\begin{split}
M_X'''(t) &= -3\cdot 2\lambda(\lambda-t)^{-4}\cdot (-1)\\
&= 6\lambda(\lambda-t)^{-4} \\
&= \frac{6\lambda}{(\lambda-t)^4} \; .
\end{split}
\end{equation}

Applying \eqref{eq:exp-skew-exp-moment}, together with \eqref{eq:exp-skew-exp-skew-s3}, yields

\begin{equation} \label{eq:exp-skew-exp-skew-s4}
\begin{split}
\mathrm{E}(X^3) &= M_X'''(0)\\
&= \frac{6\lambda}{(\lambda-0)^4}\\
&= \frac{6\lambda}{\lambda^4}\\
&= \frac{6}{\lambda^3} \; .
\end{split}
\end{equation}

We now substitute \eqref{eq:exp-skew-exp-skew-s4} into \eqref{eq:exp-skew-skew-mean-alt}, giving

\begin{equation} \label{eq:exp-skew-exp-skew-s5}
\begin{split}
\mathrm{Skew}(X) &= \lambda^3\cdot\mathrm{E}(X^3)-4 \\
&= \lambda^3\cdot \left(\frac{6}{\lambda^3}\right)-4\\
&= 6-4\\
&= 2 \; .
\end{split}
\end{equation}

This completes the proof of \eqref{eq:exp-skew-exp-skew}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{Log-normal distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:lognorm}
\setcounter{equation}{0}

\textbf{Definition:} Let $\ln X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with mean $\mu$ and variance $\sigma^2$ (or, standard deviation $\sigma$):

\begin{equation} \label{eq:lognorm-norm}
Y = \ln (X) \sim \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the exponential function of $Y$ is said to have a log-normal distribution with location parameter $\mu$ and scale parameter $\sigma$

\begin{equation} \label{eq:lognorm-lognorm}
\begin{split} 
X = \mathrm{exp}(Y) \sim \ln \mathcal{N}(\mu, \sigma^2)
\end{split}
\end{equation}

where $\mu \in \mathbb{R}$ and $\sigma^2 > 0$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Log-normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-02-07; URL: \url{https://en.wikipedia.org/wiki/Log-normal_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:lognorm-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a log-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm}):

\begin{equation} \label{eq:lognorm-pdf-lognorm}
X \sim \ln \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ is given by:

\begin{equation} \label{eq:lognorm-pdf-lognorm-pdf}
f_X(x) = \frac{1}{x \sigma \sqrt{2 \pi}} \cdot \mathrm{exp} \left[ -\frac{\left( \ln x -\mu \right)^2}{2 \sigma^2} \right] \; .
\end{equation}

\vspace{1em}
\textbf{Proof:} A log-normally distributed random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm}) is defined as the exponential function of a normal random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}):

\begin{equation} \label{eq:lognorm-pdf-lognorm-def}
Y \sim \mathcal{N}(\mu,\sigma^2) \;  \quad \Rightarrow \quad X = \mathrm{exp} (Y) \sim \ln \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

The probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) is

\begin{equation} \label{eq:lognorm-pdf-norm-pdf}
f_X(x) = \frac{1}{\sigma \sqrt{2 \pi}} \cdot \mathrm{exp} \left[ -\frac{\left( x -\mu \right)^2}{2 \sigma^2} \right] \; .
\end{equation}

Writing $X$ as a function of $Y$, we have

\begin{equation} \label{eq:lognorm-pdf-X-Y}
X = g(Y) = \mathrm{exp} (Y) \;
\end{equation}

with the inverse function

\begin{equation} \label{eq:lognorm-pdf-Y-X}
Y = g^{-1}(X) = \ln (X) \; .
\end{equation}

Because the derivative of $\exp (Y)$ is always positive, $g(Y)$ is strictly increasing and we can calculate the probability density function of a strictly increasing function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-sifct}) as

\begin{equation} \label{eq:lognorm-pdf-pdf-sifct}
f_X(x) = \left\{
\begin{array}{rl}
f_Y(g^{-1}(x)) \, \frac{\mathrm{d}g^{-1}(x)}{\mathrm{d}x} \; , & \text{if} \; x \in \mathcal{X} \\
0 \; , & \text{if} \; x \notin \mathcal{X}
\end{array}
\right.
\end{equation}

where $\mathcal{X} = \left\lbrace x = g(y): y \in \mathcal{Y} \right\rbrace$. With the probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}), we have

\begin{equation} \label{eq:lognorm-pdf-pdf-X}
\begin{split}
f_X(x) &= f_Y(g^{-1}(x))\cdot \frac{dg^{-1}(x)}{dx} \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{g^{-1}(x)-\mu}{\sigma} \right)^2 \right] \cdot \frac{\mathrm{d}g^{-1}(x)}{\mathrm{d}x} \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{(\ln x)-\mu}{\sigma} \right)^2 \right] \cdot \frac{\mathrm{d}(\ln x)}{\mathrm{d}x} \\
&= \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{ \ln x -\mu}{\sigma} \right)^2 \right] \cdot \frac{1}{x} \\
&= \frac{1}{x \sigma \sqrt{2 \pi}} \cdot \exp \left[ - \frac{\left( \ln x -\mu\right)^2}{2 \sigma^2} \right]
\end{split}
\end{equation}

which is the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of the log-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm}).

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2021): "Log-normal distribution"; in: \textit{Lectures on probability and statistics}, retrieved on 2022-02-13; URL: \url{https://www.statlect.com/probability-distributions/log-normal-distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Cumulative distribution function}]{Cumulative distribution function} \label{sec:lognorm-cdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a log-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm}):

\begin{equation} \label{eq:lognorm-cdf-norm}
X \sim \ln \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$ is

\begin{equation} \label{eq:lognorm-cdf-lognorm-cdf}
F_X(x) = \frac{1}{2} \left[ 1 + \mathrm{erf}\left( \frac{\ln x-\mu}{\sqrt{2} \sigma} \right) \right]
\end{equation}

where $\mathrm{erf}(x)$ is the error function defined as

\begin{equation} \label{eq:lognorm-cdf-erf}
\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} \exp(-t^2) \, \mathrm{d}t \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density function of the log-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm-pdf}) is:

\begin{equation} \label{eq:lognorm-cdf-lognorm-pdf}
f_X(x) = \frac{1}{x \sigma \sqrt{2 \pi}} \cdot \exp \left[ - \left( \frac{\ln x-\mu}{\sqrt{2} \sigma} \right)^2 \right] \; .
\end{equation}

Thus, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) is:

\begin{equation} \label{eq:lognorm-cdf-lognorm-cdf-s1}
\begin{split}
F_X(x) &= \int_{-\infty}^{x} \mathcal{\ln N}(z; \mu, \sigma^2) \, \mathrm{d}z \\
&= \int_{-\infty}^{x} \frac{1}{z\sigma \sqrt{2 \pi}} \cdot \exp \left[ -\left( \frac{\ln z-\mu}{\sqrt{2} \sigma} \right)^2 \right] \, \mathrm{d}z \\
&= \frac{1}{\sigma \sqrt{2 \pi}} \int_{-\infty}^{x} \frac{1}{z} \cdot \exp \left[ -\left( \frac{\ln z-\mu}{\sqrt{2} \sigma} \right)^2 \right] \, \mathrm{d}z \; .
\end{split}
\end{equation}

From this point forward, the proof is similar to the derivation of the cumulative distribution function for the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-cdf}). Substituting $t = (\ln z-\mu)/(\sqrt{2} \sigma)$, i.e. $\ln z = \sqrt{2} \sigma t + \mu$, $z = \exp( \sqrt{2} \sigma t + \mu) $ this becomes:

\begin{equation} \label{eq:lognorm-cdf-lognorm-cdf-s2}
\begin{split}
F_X(x) &= \frac{1}{\sigma \sqrt{2 \pi}} \int_{(-\infty-\mu)/(\sqrt{2} \sigma)}^{(\ln x-\mu)/(\sqrt{2} \sigma)} \frac{1}{\exp( \sqrt{2} \sigma t + \mu)} \cdot \exp \left(-t^2 \right) \, \mathrm{d} \left[ \exp \left( \sqrt{2} \sigma t + \mu \right) \right] \\
&=\frac{\sqrt{2} \sigma}{\sigma \sqrt{2 \pi}} \int_{-\infty}^{\frac{\ln x-\mu}{\sqrt{2} \sigma}} \frac{1}{\exp( \sqrt{2} \sigma t + \mu)} \cdot
\exp(-t^2) \cdot \exp \left( \sqrt{2} \sigma t + \mu \right) \, \mathrm{d}t  \\
&= \frac{1}{\sqrt{\pi}} \int_{-\infty}^{\frac{\ln x-\mu}{\sqrt{2} \sigma}} \exp(-t^2) \, \mathrm{d}t \\
&= \frac{1}{\sqrt{\pi}} \int_{-\infty}^{0} \exp(-t^2) \, \mathrm{d}t + \frac{1}{\sqrt{\pi}} \int_{0}^{\frac{\ln x-\mu}{\sqrt{2} \sigma}} \exp(-t^2) \, \mathrm{d}t \\
&= \frac{1}{\sqrt{\pi}} \int_{0}^{\infty} \exp(-t^2) \, \mathrm{d}t + \frac{1}{\sqrt{\pi}} \int_{0}^{\frac{\ln x-\mu}{\sqrt{2} \sigma}} \exp(-t^2) \, \mathrm{d}t \; .
\end{split}
\end{equation}

Applying \eqref{eq:lognorm-cdf-erf} to \eqref{eq:lognorm-cdf-lognorm-cdf-s2}, we have:

\begin{equation} \label{eq:lognorm-cdf-lognorm-cdf-s3}
\begin{split}
F_X(x) &= \frac{1}{2} \lim_{x \to \infty} \mathrm{erf}(x) + \frac{1}{2} \, \mathrm{erf}\left( \frac{\ln x-\mu}{\sqrt{2} \sigma} \right) \\
&= \frac{1}{2} + \frac{1}{2} \, \mathrm{erf}\left( \frac{\ln x-\mu}{\sqrt{2} \sigma} \right) \\
&= \frac{1}{2} \left[ 1 + \mathrm{erf}\left( \frac{\ln x-\mu}{\sqrt{2} \sigma} \right) \right] \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item skdhfgeq2134 (2015): "How to derive the cdf of a lognormal distribution from its pdf"; in: \textit{StackExchange}, retrieved on 2022-06-29; URL: \url{https://stats.stackexchange.com/questions/151398/how-to-derive-the-cdf-of-a-lognormal-distribution-from-its-pdf/151404#151404}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Quantile function}]{Quantile function} \label{sec:lognorm-qf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a log-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm}):

\begin{equation} \label{eq:lognorm-qf-lognorm}
X \sim \ln \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the quantile function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf}) of $X$ is

\begin{equation} \label{eq:lognorm-qf-lognorm-qf}
Q_X(p) = \exp( \mu + \sqrt{2} \sigma \cdot \mathrm{erf}^{-1}(2p-1) )
\end{equation}

where $\mathrm{erf}^{-1}(x)$ is the inverse error function.


\vspace{1em}
\textbf{Proof:} The cumulative distribution function of the log-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm-cdf}) is:

\begin{equation} \label{eq:lognorm-qf-lognorm-cdf}
F_X(x) = \frac{1}{2} \left[ 1 + \mathrm{erf}\left( \frac{\ln x-\mu}{\sqrt{2} \sigma} \right) \right] \; .
\end{equation}

From this point forward, the proof is similar to the derivation of the quantile function for the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-qf}). Because the cumulative distribution function (CDF) is strictly monotonically increasing, the quantile function is equal to the inverse of the CDF ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:qf-cdf}): 

\begin{equation} \label{eq:lognorm-qf-lognorm-qf-s1}
Q_X(p) = F_X^{-1}(x) \; .
\end{equation}

This can be derived by rearranging equation \eqref{eq:lognorm-qf-lognorm-cdf}:

\begin{equation} \label{eq:lognorm-qf-lognorm-qf-s2}
\begin{split}
p &= \frac{1}{2} \left[ 1 + \mathrm{erf}\left( \frac{\ln x-\mu}{\sqrt{2} \sigma} \right) \right] \\
2 p - 1 &= \mathrm{erf}\left( \frac{\ln x-\mu}{\sqrt{2} \sigma} \right) \\
\mathrm{erf}^{-1}(2p-1) &= \frac{\ln x-\mu}{\sqrt{2} \sigma} \\
x &= \exp(\mu + \sqrt{2}\sigma \cdot \mathrm{erf}^{-1}(2p-1) ) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Log-normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-07-08; URL: \url{https://en.wikipedia.org/wiki/Log-normal_distribution#Mode,_median,_quantiles}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:lognorm-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a log-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm}):

\begin{equation} \label{eq:lognorm-mean-lognorm}
X \sim \ln  \mathcal{N}(\mu, \sigma^2) 
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation}
\mathrm{E}(X) = \exp \left( \mu + \frac{1}{2} \sigma^2 \right) 
\end{equation}


\vspace{1em}
\textbf{Proof:} The expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is the probability-weighted average over all possible values:

\begin{equation} \label{eq:lognorm-mean-mean}
\mathrm{E}(X) = \int_{\mathcal{X}} x \cdot f_X(x) \, \mathrm{d}x 
\end{equation}

With the probability density function of the log-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm-pdf}), this is:

\begin{equation} \label{eq:lognorm-mean-lognorm-mean-s1}
\begin{split}
\mathrm{E}(X) &= \int_{0}^{+\infty} x \cdot \frac{1}{x\sqrt{2 \pi \sigma^2} } \cdot \exp \left[ -\frac{1}{2}  \frac{\left(\ln x-\mu\right)^2}{\sigma^2} \right]  \mathrm{d}x \\
&= \frac{1}{\sqrt{2 \pi \sigma^2} } \int_{0}^{+\infty} \exp \left[ -\frac{1}{2}  \frac{\left(\ln x-\mu\right)^2}{\sigma^2} \right] \mathrm{d}x
\end{split}
\end{equation}

Substituting $z = \frac{\ln x -\mu}{\sigma}$, i.e. $x = \exp \left( \mu + \sigma z \right )$, we have:

\begin{equation} \label{eq:lognorm-mean-lognorm-mean-s2}
\begin{split}
\mathrm{E}(X) &= \frac{1}{\sqrt{2 \pi \sigma^2} } \int_{(-\infty -\mu )/ (\sigma)}^{(\ln x -\mu )/ (\sigma)} \exp \left( -\frac{1}{2}  z^2 \right) \mathrm{d} \left[ \exp \left( \mu +\sigma z \right) \right] \\
&= \frac{1}{\sqrt{2 \pi \sigma^2} } \int_{-\infty}^{+\infty} \exp \left( -\frac{1}{2}  z^2 \right) \sigma \exp \left( \mu +\sigma z \right) \mathrm{d}z \\
&= \frac{1}{\sqrt{2 \pi} } \int_{-\infty}^{+\infty} \exp \left( -\frac{1}{2}  z^2 + \sigma z + \mu \right)  \mathrm{d}z \\
&= \frac{1}{\sqrt{2 \pi} } \int_{-\infty}^{+\infty} \exp \left[  -\frac{1}{2} \left(  z^2  - 2 \sigma z - 2 \mu \right) \right]  \mathrm{d}z
\end{split}
\end{equation}

Now multiplying $\exp \left( \frac{1}{2} \sigma^2 \right)$ and $\exp \left( -\frac{1}{2} \sigma^2 \right)$, we have:

\begin{equation} \label{eq:lognorm-mean-lognorm-mean-s3}
\begin{split}
\mathrm{E}(X) &= \frac{1}{\sqrt{2 \pi} } \int_{-\infty}^{+\infty} \exp \left[  -\frac{1}{2} \left(  z^2  - 2 \sigma z + \sigma^2 - 2 \mu - \sigma^2 \right) \right]  \mathrm{d}z \\
&= \frac{1}{\sqrt{2 \pi} } \int_{-\infty}^{+\infty} \exp \left[ -\frac{1}{2} \left( z^2 - 2\sigma z + \sigma^2 \right) \right] \exp \left( \mu + \frac{1}{2} \sigma^2  \right) \mathrm{d}z \\
&= \exp \left( \mu + \frac{1}{2} \sigma^2  \right) \int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi} } \exp \left[ -\frac{1}{2} \left( z - \sigma \right)^2 \right] \mathrm{d}z
\end{split}
\end{equation}

The probability density function of a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) is given by

\begin{equation} \label{eq:lognorm-mean-norm-pdf}
f_X(x) = \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right]
\end{equation}

and, with unit variance $\sigma^2 = 1$, this reads:

\begin{equation}
f_X(x) = \frac{1}{\sqrt{2 \pi}} \cdot \exp \left[ -\frac{1}{2} \left({x-\mu} \right)^2 \right]
\end{equation}

Using the definition of the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}), we get

\begin{equation} \label{eq:lognorm-mean-def-pdf}
\int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi}} \cdot \exp \left[ -\frac{1}{2} \left({x-\mu} \right)^2 \right]  \mathrm{d}x  = 1
\end{equation}

and applying \eqref{eq:lognorm-mean-def-pdf} to \eqref{eq:lognorm-mean-lognorm-mean-s3}, we have:

\begin{equation} \label{eq:lognorm-mean-lognorm-mean}
\mathrm{E}(X) = \exp \left( \mu + \frac{1}{2} \sigma^2  \right) .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2022): "Log-normal distribution"; in: \textit{Lectures on probability theory and mathematical statistics}, retrieved on 2022-10-01; URL: \url{https://www.statlect.com/probability-distributions/log-normal-distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Median}]{Median} \label{sec:lognorm-med}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a log-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm}):

\begin{equation} \label{eq:lognorm-med-lognorm}
X \sim \ln \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the median ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:med}) of $X$ is

\begin{equation} \label{eq:lognorm-med-lognorm-med}
\mathrm{median}(X) = e^\mu \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The median ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:med}) is the value at which the cumulative distribution function is $1/2$:

\begin{equation} \label{eq:lognorm-med-median}
F_X(\mathrm{median}(X)) = \frac{1}{2} \; .
\end{equation}

The cumulative distribution function of the lognormal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm-cdf}) is

\begin{equation} \label{eq:lognorm-med-lognorm-cdf}
F_X(x) = \frac{1}{2} \left[ 1 + \mathrm{erf} \left( \frac{\ln (x)-\mu}{\sigma \sqrt{2}} \right) \right]
\end{equation}

where $\mathrm{erf}(x)$ is the error function defined as

\begin{equation} \label{eq:lognorm-med-erf}
\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} \exp(-t^2) \, \mathrm{d}t \; .
\end{equation}

Thus, the inverse CDF is

\begin{equation} \label{eq:lognorm-med-lognorm-cdf-inv}
\begin{split}
\ln(x) &= \sigma \sqrt{2} \cdot \mathrm{erf}^{-1}(2p-1) + \mu \\
x &= \mathrm{exp} \left[ \sigma \sqrt{2} \cdot \mathrm{erf}^{-1}(2p-1) + \mu \right]
\end{split}
\end{equation}

where $\mathrm{erf}^{-1}(x)$ is the inverse error function. Setting $p = 1/2$, we obtain:

\begin{equation} \label{eq:lognorm-med-lognorm-med-qed}
\begin{split}
\ln \left[ \mathrm{median}(X) \right] &= \sigma \sqrt{2} \cdot \mathrm{erf}^{-1}(0) + \mu \\
\mathrm{median}(X) &= e^\mu \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mode}]{Mode} \label{sec:lognorm-mode}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a log-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm}):

\begin{equation} \label{eq:lognorm-mode-lognorm}
X \sim \ln \mathcal{N}(\mu, \sigma^2) \; .
\end{equation}

Then, the mode ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mode}) of $X$ is

\begin{equation} \label{eq:lognorm-mode-lognorm-mode}
\mathrm{mode}(X) = e^{\left( \mu -\sigma^2 \right)} \; .
\end{equation}

\vspace{1em}
\textbf{Proof:} The mode ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mode}) is the value which maximizes the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}):

\begin{equation} \label{eq:lognorm-mode-mode}
\mathrm{mode}(X) = \operatorname*{arg\,max}_x f_X(x) \; .
\end{equation}

The probability density function of the log-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm-pdf}) is:

\begin{equation} \label{eq:lognorm-mode-lognorm-pdf}
f_X(x) = \frac{1}{x \sigma \sqrt{2 \pi}} \cdot \mathrm{exp} \left[ -\frac{\left( \ln x -\mu \right)^2}{2 \sigma^2} \right] \; .
\end{equation}

The first two derivatives of this function are:

\begin{equation} \label{eq:lognorm-mode-lognorm-pdf-der1}
f'_X(x) = -\frac{1}{x^2 \sigma \sqrt{2 \pi}} \cdot \mathrm{exp} \left[ -\frac{\left( \ln x -\mu \right)^2}{2 \sigma^2} \right] \cdot \left(1 + \frac{\ln x -\mu}{\sigma^2} \right)
\end{equation}

\begin{equation} \label{eq:lognorm-mode-lognorm-pdf-der2}
\begin{split}
f''_X(x) &= \frac{1}{\sqrt{2\pi}\sigma^2x^3} \mathrm{exp} \left[ -\frac{\left( \ln x -\mu \right)^2}{2 \sigma^2} \right] \cdot \left( \ln x -\mu \right) \cdot \left(1 + \frac{ \ln x -\mu}{\sigma^2}\right) \\
&+ \frac{\sqrt{2}}{\sqrt{\pi}x^3}\mathrm{exp} \left[ -\frac{\left( \ln x -\mu \right)^2}{2 \sigma^2} \right] \cdot \left(1 + \frac{ \ln x -\mu}{\sigma^2}\right) \\
&- \frac{1}{\sqrt{2\pi}\sigma^2x^3} \mathrm{exp} \left[ -\frac{\left( \ln x -\mu \right)^2}{2 \sigma^2} \right] \; .
\end{split}
\end{equation}

We now calculate the root of the first derivative \eqref{eq:lognorm-mode-lognorm-pdf-der1}:

\begin{equation} \label{eq:lognorm-mode-lognorm-mode-s1}
\begin{split}
f'_X(x) = 0 &= -\frac{1}{x^2 \sigma \sqrt{2 \pi}} \cdot \mathrm{exp} \left[ -\frac{\left( \ln x -\mu \right)^2}{2 \sigma^2} \right] \cdot \left(1 + \frac{\ln x -\mu}{\sigma^2} \right) \\
-1 &= \frac{\ln x -\mu}{\sigma^2} \\
x &= e^{\left( \mu -\sigma^2 \right)} \; .
\end{split}
\end{equation}

By plugging this value into the second derivative \eqref{eq:lognorm-mode-lognorm-pdf-der2},

\begin{equation} \label{eq:lognorm-mode-lognorm-mode-s2}
\begin{split}
f''_X(e^{(\mu-\sigma^2)}) &= \frac{1}{\sqrt{2\pi}\sigma^2(e^{(\mu-\sigma^2)})^3} \mathrm{exp} \left[ -\frac{\sigma^2}{2} \right] \cdot \left( \sigma^2 \right) \cdot \left(0\right) \\
&+ \frac{\sqrt{2}}{\sqrt{\pi}(e^{(\mu-\sigma^2)})^3} \mathrm{exp} \left[ -\frac{\sigma^2}{2} \right] \cdot \left(0 \right) \\
&- \frac{1}{\sqrt{2\pi}\sigma^2(e^{(\mu-\sigma^2)})^3} \mathrm{exp} \left[ -\frac{\sigma^2}{2} \right] \\
&= - \frac{1}{\sqrt{2\pi}\sigma^2(e^{(\mu-\sigma^2)})^3} \mathrm{exp} \left[ -\frac{\sigma^2}{2} \right] < 0 \; ,
\end{split}
\end{equation}

we confirm that it is a maximum, showing that

\begin{equation} \label{eq:lognorm-mode-lognorm-mode-qed}
\mathrm{mode}(X) = e^{\left( \mu -\sigma^2 \right)} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Log-normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-02-12; URL: \url{https://en.wikipedia.org/wiki/Log-normal_distribution#Mode}.
\item Mdoc (2015): "Mode of lognormal distribution"; in: \textit{Mathematics Stack Exchange}, retrieved on 2022-02-12; URL: \url{https://math.stackexchange.com/questions/1321221/mode-of-lognormal-distribution/1321626}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Variance}]{Variance} \label{sec:lognorm-var}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a log-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm}):

\begin{equation}
X \sim \ln \mathcal{N}(\mu, \sigma^2) .
\end{equation}

Then, the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is

\begin{equation} \label{eq:lognorm-var-lognorm-var}
\mathrm{Var}(X) = \exp \left(2\mu +2\sigma^2\right) - \exp \left(2\mu + \sigma^2\right) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of a random variable is defined as

\begin{equation} \label{eq:lognorm-var-var}
\mathrm{Var}(X) = \mathrm{E}\left[ (X-\mathrm{E}(X))^2 \right] 
\end{equation}

which, partitioned into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-mean}), reads:

\begin{equation} \label{eq:lognorm-var-var2}
\mathrm{Var}(X) = \mathrm{E}\left[ X^2 \right] - \mathrm{E}\left[ X \right]^2 \; .
\end{equation}

The expected value of the log-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:lognorm-mean}) is: 

\begin{equation} \label{eq:lognorm-var-lognorm-mean-ref}
\mathrm{E}[X] = \exp \left( \mu + \frac{1}{2} \sigma^2  \right)
\end{equation}

The second moment $\mathrm{E}[X^2]$ can be derived as follows:

\begin{equation} \label{eq:lognorm-var-second-moment}
\begin{split}
\mathrm{E} [X^2] &= \int_{- \infty}^{+\infty} x^2 \cdot f_\mathrm{X}(x) \, \mathrm{d}x \\
&= \int_{0}^{+\infty} x^2 \cdot \frac{1}{x\sqrt{2 \pi \sigma^2} } \cdot \exp \left[ -\frac{1}{2}  \frac{\left(\ln x-\mu\right)^2}{\sigma^2} \right]  \mathrm{d}x \\
&= \frac{1}{\sqrt{2 \pi \sigma^2} } \int_{0}^{+\infty} x \cdot \exp \left[ -\frac{1}{2}  \frac{\left(\ln x-\mu\right)^2}{\sigma^2} \right]\mathrm{d}x
\end{split}
\end{equation}

Substituting $z = \frac{\ln x -\mu}{\sigma}$, i.e. $x = \exp \left( \mu + \sigma z \right )$, we have:

\begin{equation} \label{eq:lognorm-var-second-moment-2}
\begin{split}
\mathrm{E} [X^2] &= \frac{1}{\sqrt{2 \pi \sigma^2} } \int_{(-\infty -\mu )/ (\sigma)}^{(\ln x -\mu )/ (\sigma)} \exp \left( \mu +\sigma z \right) \exp \left( -\frac{1}{2}  z^2 \right) \mathrm{d} \left[ \exp \left( \mu +\sigma z \right) \right] \\
&= \frac{1}{\sqrt{2 \pi \sigma^2} } \int_{-\infty}^{+\infty} \exp \left( -\frac{1}{2}  z^2 \right) \sigma \exp \left( 2\mu + 2 \sigma z \right) \mathrm{d}z \\
&= \frac{1}{\sqrt{2 \pi} } \int_{-\infty}^{+\infty} \exp \left[ -\frac{1}{2} \left(   z^2 - 4 \sigma z - 4 \mu  \right) \right] \mathrm{d}z
\end{split}
\end{equation}

Now multiplying by $\exp \left( 2 \sigma^2 \right)$ and $\exp \left(- 2 \sigma^2 \right)$, this becomes:

\begin{equation} \label{eq:lognorm-var-second-moment-3}
\begin{split}
\mathrm{E} [X^2] &= \frac{1}{\sqrt{2 \pi} } \int_{-\infty}^{+\infty} \exp \left[ -\frac{1}{2} \left(   z^2 - 4 \sigma z + 4 \sigma^2 -4 \sigma^2 - 4 \mu  \right) \right] \mathrm{d}z \\
&= \frac{1}{\sqrt{2 \pi} } \int_{-\infty}^{+\infty} \exp \left[ -\frac{1}{2} \left( z^2 - 4\sigma z + 4\sigma^2 \right) \right] \exp \left( 2 \sigma^2 +2 \mu  \right) \mathrm{d}z \\
&= \exp \left( 2 \sigma^2 +2 \mu   \right) \int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi} } \exp \left[ -\frac{1}{2} \left( z - 2 \sigma \right)^2 \right] \mathrm{d}z
\end{split}
\end{equation}

The probability density function of a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) is given by

\begin{equation} 
f_X(x) = \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right]
\end{equation}

and, with $\mu = 2 \sigma$ and unit variance, this reads:

\begin{equation} 
f_X(x) = \frac{1}{\sqrt{2 \pi}} \cdot \exp \left[ -\frac{1}{2} \left({x - 2 \sigma} \right)^2 \right] \; .
\end{equation}

Using the definition of the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}), we get

\begin{equation} \label{eq:lognorm-var-def-pdf}
\int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi}} \cdot \exp \left[ -\frac{1}{2} \left({x - 2 \sigma} \right)^2 \right]  \mathrm{d}x  = 1 
\end{equation}

and applying \eqref{eq:lognorm-var-def-pdf} to \eqref{eq:lognorm-var-second-moment-3}, we have:

\begin{equation} \label{eq:lognorm-var-second-moment-4}
\mathrm{E}[X]^2 = \exp \left( 2 \sigma^2 +2 \mu   \right) \; .
\end{equation}

Finally, plugging \eqref{eq:lognorm-var-second-moment-4} and \eqref{eq:lognorm-var-lognorm-mean-ref} into \eqref{eq:lognorm-var-var2}, we have:

\begin{equation} \label{eq:lognorm-var-lognorm-var-2}
\begin{split}
\mathrm{Var}(X) &= \mathrm{E}\left[ X^2 \right] - \mathrm{E}\left[ X \right]^2  \\
&= \exp \left(2\sigma^2 + 2\mu \right) - \left[ \exp \left(\mu + \frac{1}{2} \sigma^2 \right) \right]^2 \\
&= \exp \left(2\sigma^2 + 2\mu \right) - \exp \left(2\mu + \sigma^2\right) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2022): "Log-normal distribution"; in: \textit{Lectures on probability theory and mathematical statistics}, retrieved on 2022-10-01; URL: \url{https://www.statlect.com/probability-distributions/log-normal-distribution}.
\item Wikipedia (2022): "Variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-10-01; URL: \url{https://en.wikipedia.org/wiki/Variance#Definition}.
\end{itemize}
\vspace{1em}



\subsection{Chi-squared distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:chi2}
\setcounter{equation}{0}

\textbf{Definition:} Let $X_{1}, ..., X_{k}$ be independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) where each of them is following a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}):

\begin{equation} \label{eq:chi2-snorm}
X_{i} \sim \mathcal{N}(0,1) \quad \text{for} \quad i = 1, \ldots, n \; .
\end{equation}

Then, the sum of their squares follows a chi-squared distribution with $k$ degrees of freedom:

\begin{equation}\label{eq:chi2-chi2}
Y = \sum_{i=1}^{k} X_{i}^{2} \sim \chi^{2}(k) \quad \text{where} \quad k > 0 \; .
\end{equation}

The probability density function of the chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2-pdf}) with $k$ degrees of freedom is

\begin{equation} \label{eq:chi2-chi2-pdf}
\chi^{2}(x; k) = \frac{1}{2^{k/2}\Gamma (k/2)} \, x^{k/2-1} \, e^{-x/2}
\end{equation}

where $k > 0$ and the density is zero if $x \leq 0$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Chi-square distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-10-12; URL: \url{https://en.wikipedia.org/wiki/Chi-square_distribution#Definitions}.
\item Robert V. Hogg, Joseph W. McKean, Allen T. Craig (2018): "The Chi-Squared-Distribution"; in: \textit{Introduction to Mathematical Statistics}, Pearson, Boston, 2019, p. 178, eq. 3.3.7; URL: \url{https://www.pearson.com/store/p/introduction-to-mathematical-statistics/P100000843744}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Special case of gamma distribution}]{Special case of gamma distribution} \label{sec:chi2-gam}
\setcounter{equation}{0}

\textbf{Theorem:} The chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) with $k$ degrees of freedom is a special case of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) with shape $\frac{k}{2}$ and rate $\frac{1}{2}$:

\begin{equation} \label{eq:chi2-gam-chi2-gam}
X \sim \mathrm{Gam}\left( \frac{k}{2}, \frac{1}{2} \right) \Rightarrow X \sim \chi^{2}(k) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}) for $x > 0$, where $\alpha$ is the shape parameter and $\beta$ is the rate parameter, is as follows:

\begin{equation} \label{eq:chi2-gam-gam-pdf}
\mathrm{Gam}(x; \alpha, \beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} \, x^{\alpha-1} \, e^{-\beta x}
\end{equation}

If we let $\alpha = k/2$ and $\beta = 1/2$, we obtain

\begin{equation} \label{eq:chi2-gam-gam-pdf-chi2}
\mathrm{Gam}\left(x; \frac{k}{2}, \frac{1}{2}\right) = \frac{x^{k/2-1} \, e^{-x/2}}{\Gamma(k/2) 2^{k/2}} = \frac{1}{2^{k/2} \Gamma(k/2)} \, x^{k/2-1} \, e^{-x/2}
\end{equation}

which is equivalent to the probability density function of the chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2-pdf}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:chi2-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $Y$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}):

\begin{equation} \label{eq:chi2-pdf-chi2}
Y \sim \chi^{2}(k) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $Y$ is

\begin{equation} \label{eq:chi2-pdf-chi2-pdf}
f_Y(y) = \frac{1}{2^{k/2} \, \Gamma (k/2)} \, y^{k/2-1} \, e^{-y/2} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} A chi-square-distributed random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) with $k$ degrees of freedom is defined as the sum of $k$ squared standard normal random variables ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}):

\begin{equation} \label{eq:chi2-pdf-chi2-def}
X_1, \ldots, X_k \sim \mathcal{N}(0,1) \quad \Rightarrow \quad Y = \sum_{i=1}^{k} X_i^2 \sim \chi^{2}(k) \; .
\end{equation}

Let $x_1, \ldots, x_k$ be values of $X_1, \ldots, X_k$ and consider $x = \left( x_1, \ldots, x_k \right)$ to be a point in $k$-dimensional space. Define

\begin{equation} \label{eq:chi2-pdf-y-x}
y = \sum_{i=1}^{k} x_i^2
\end{equation}

and let $f_Y(y)$ and $F_Y(y)$ be the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) and cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $Y$. Because the PDF is the first derivative of the CDF ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-cdf}), we can write:

\begin{equation} \label{eq:chi2-pdf-y-pdf-s0}
F_Y(y) = \frac{F_Y(y)}{\mathrm{d}y} \, \mathrm{d}y = f_Y(y) \, \mathrm{d}y \; .
\end{equation}

Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $Y$ can be expressed as

\begin{equation} \label{eq:chi2-pdf-y-cdf-s1}
f_Y(y) \, \mathrm{d}y = \int_{V} \prod_{i=1}^{k} \left( \mathcal{N}(x_i; 0, 1) \, \mathrm{d}x_i \right)
\end{equation}

where $\mathcal{N}(x_i; 0, 1)$ is the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of the standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) and $V$ is the elemental shell volume at $y(x)$, which is proportional to the $(k-1)$-dimensional surface in $k$-space for which equation \eqref{eq:chi2-pdf-y-x} is fulfilled. Using the probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}), equation \eqref{eq:chi2-pdf-y-cdf-s1} can be developed as follows:

\begin{equation} \label{eq:chi2-pdf-y-cdf-s2}
\begin{split}
f_Y(y) \, \mathrm{d}y &= \int_{V} \prod_{i=1}^{k} \left( \frac{1}{\sqrt{2 \pi}} \cdot \exp \left[ -\frac{1}{2} x_i^2 \right] \, \mathrm{d}x_i \right) \\
&= \int_{V} \frac{\exp \left[ -\frac{1}{2} \left( x_1^2 + \ldots + x_k^2 \right) \right]}{(2 \pi)^{k/2}} \; \mathrm{d}x_1 \, \ldots \, \mathrm{d}x_k \\
&= \frac{1}{(2 \pi)^{k/2}} \int_{V} \exp \left[ -\frac{y}{2} \right] \; \mathrm{d}x_1 \, \ldots \, \mathrm{d}x_k \; .
\end{split}
\end{equation}

Because $y$ is constant within the set $V$, it can be moved out of the integral:

\begin{equation} \label{eq:chi2-pdf-y-cdf-s3}
f_Y(y) \, \mathrm{d}y = \frac{\exp \left[ -y/2 \right]}{(2 \pi)^{k/2}} \int_{V} \; \mathrm{d}x_1 \, \ldots \, \mathrm{d}x_k \; .
\end{equation}

Now, the integral is simply the surface area of the $(k-1)$-dimensional sphere with radius $r = \sqrt{y}$, which is

\begin{equation} \label{eq:chi2-pdf-A}
A = 2 r^{k-1} \, \frac{\pi^{k/2}}{\Gamma(k/2)} \; ,
\end{equation}

times the infinitesimal thickness of the sphere, which is

\begin{equation} \label{eq:chi2-pdf-dR}
\frac{\mathrm{d}r}{\mathrm{d}y} = \frac{1}{2} y^{-1/2} \quad \Leftrightarrow \quad \mathrm{d}r = \frac{\mathrm{d}y}{2 y^{1/2}} \; .
\end{equation}

Substituting \eqref{eq:chi2-pdf-A} and \eqref{eq:chi2-pdf-dR} into \eqref{eq:chi2-pdf-y-cdf-s3}, we have:

\begin{equation} \label{eq:chi2-pdf-y-cdf-s4}
\begin{split}
f_Y(y) \, \mathrm{d}y &= \frac{\exp \left[ -y/2 \right]}{(2 \pi)^{k/2}} \cdot A \, \mathrm{d}r \\
&= \frac{\exp \left[ -y/2 \right]}{(2 \pi)^{k/2}} \cdot 2 r^{k-1} \, \frac{\pi^{k/2}}{\Gamma(k/2)} \cdot \frac{\mathrm{d}y}{2 y^{1/2}} \\
&= \frac{1}{2^{k/2} \, \Gamma(k/2)} \cdot \frac{2 \sqrt{y}^{k-1}}{2 \sqrt{y}} \cdot \exp \left[ -y/2 \right] \, \mathrm{d}y \\
&= \frac{1}{2^{k/2} \, \Gamma(k/2)} \cdot y^{\frac{k}{2}-1} \cdot \exp \left[ -\frac{y}{2} \right] \, \mathrm{d}y \; .
\end{split}
\end{equation}

From this, we get the final result in \eqref{eq:chi2-pdf-chi2-pdf}:

\begin{equation} \label{eq:chi2-pdf-y-cdf-s5}
f_Y(y) = \frac{1}{2^{k/2} \, \Gamma (k/2)} \, y^{k/2-1} \, e^{-y/2} \; .
\end{equation}

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Proofs related to chi-squared distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-25; URL: \url{https://en.wikipedia.org/wiki/Proofs_related_to_chi-squared_distribution#Derivation_of_the_pdf_for_k_degrees_of_freedom}.
\item Wikipedia (2020): "n-sphere"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-25; URL: \url{https://en.wikipedia.org/wiki/N-sphere#Volume_and_surface_area}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Moments}]{Moments} \label{sec:chi2-mom}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) with $k$ degrees of freedom:

\begin{equation} \label{eq:chi2-mom-chi2}
X \sim \chi^{2}(k) \; .
\end{equation}

Then, if $m > -k/2$, the moment $\mathrm{E}(X^{m})$ exists and is equal to:

\begin{equation} \label{eq:chi2-mom-chi2-mom}
\mathrm{E}(X^{m}) = 2^m \frac{\Gamma\left( \frac{k}{2}+m \right)}{\Gamma\left( \frac{k}{2} \right)} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Combining the definition of the raw moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-raw}) with the probability density function of the chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2-pdf}), we have:

\begin{equation} \label{eq:chi2-mom-chi2-mom-int}
\begin{split}
\mathrm{E}(X^{m}) &= \int_{0}^{\infty} x^m \frac{1}{2^{k/2} \Gamma\left( \frac{k}{2} \right)} \, x^{k/2-1} \, e^{-x/2} \, \mathrm{d}x \\
&= \frac{1}{2^{k/2} \Gamma\left( \frac{k}{2} \right)} \int_{0}^{\infty} x^{(k/2)+m-1} \, e^{-x/2} \, \mathrm{d}x \; .
\end{split}
\end{equation}

Now, we substitute $u = x/2$, such that $x = 2u$. As a result, we obtain:

\begin{equation} \label{eq:chi2-mom-chi2-mom-int-u}
\begin{split}
\mathrm{E}(X^{m}) &= \frac{1}{2^{k/2} \Gamma\left( \frac{k}{2} \right)} \int_{0}^{\infty} 2^{(k/2)+m-1} \, u^{(k/2)+m-1} \, e^{-u} \, \mathrm{d}(2u) \\
&= \frac{2^{(k/2)+m}}{2^{k/2} \Gamma\left( \frac{k}{2} \right)} \int_{0}^{\infty} u^{(k/2)+m-1} \, e^{-u} \, \mathrm{d}u \\
&= \frac{2^m}{\Gamma\left( \frac{k}{2} \right)} \int_{0}^{\infty} u^{(k/2)+m-1} \, e^{-u} \, \mathrm{d}u \; .
\end{split}
\end{equation}

With the definition of the gamma function as

\begin{equation} \label{eq:chi2-mom-gam-fct}
\Gamma(x) = \int_{0}^{\infty} t^{x-1} \, e^{-t} \, \mathrm{d}t, \; z > 0 \; ,
\end{equation}

this leads to the desired result when $m > -k/2$. Observe that, if $m$ is a nonnegative integer, then $m > -k/2$ is always true. Therefore, all moments ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom}) of a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) exist and the $m$-th raw moment is given by the equation above.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Robert V. Hogg, Joseph W. McKean, Allen T. Craig (2018): "The Ï‡2-Distribution"; in: \textit{Introduction to Mathematical Statistics}, Pearson, Boston, 2019, p. 179, eq. 3.3.8; URL: \url{https://www.pearson.com/store/p/introduction-to-mathematical-statistics/P100000843744}.
\end{itemize}
\vspace{1em}



\subsection{F-distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:f}
\setcounter{equation}{0}

\textbf{Definition:} Let $X_1$ and $X_2$ be independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) with $d_1$ and $d_2$ degrees of freedom, respectively:

\begin{equation} \label{eq:f-chi2}
\begin{split}
X_1 &\sim \chi^{2}(d_1) \\
X_2 &\sim \chi^{2}(d_2) \; .
\end{split}
\end{equation}

Then, the ratio of $X_1$ to $X_2$, divided by their respective degrees of freedom, is said to be $F$-distributed with numerator degrees of freedom $d_1$ and denominator degrees of freedom $d_2$:

\begin{equation}\label{eq:f-F}
Y = \frac{X_1 / d_1}{X_2 / d_2} \sim F(d_1,d_2) \quad \text{where} \quad d_1, d_2 > 0 \; .
\end{equation}

The $F$-distribution is also called "Snedecor's $F$-distribution" or "Fisherâ€“Snedecor distribution", after Ronald A. Fisher and George W. Snedecor.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "F-distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-04-21; URL: \url{https://en.wikipedia.org/wiki/F-distribution#Characterization}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:f-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $F$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}):

\begin{equation} \label{eq:f-pdf-f}
F \sim \mathrm{F}(u,v) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $F$ is

\begin{equation} \label{eq:f-pdf-f-pdf}
f_F(f) = \frac{\Gamma\left( \frac{u+v}{2} \right)}{\Gamma\left( \frac{u}{2} \right) \cdot \Gamma\left( \frac{v}{2} \right)} \cdot \left( \frac{u}{v} \right)^{\frac{u}{2}} \cdot f^{\frac{u}{2}-1} \cdot \left( \frac{u}{v}f+1 \right)^{-\frac{u+v}{2}} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} An F-distributed random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}) is defined as the ratio of two chi-squared random variables ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}), divided by their degrees of freedom

\begin{equation} \label{eq:f-pdf-f-def}
X \sim \chi^2(u), \; Y \sim \chi^2(v) \quad \Rightarrow \quad F = \frac{X/u}{Y/v} \sim \mathrm{F}(u,v)
\end{equation}

where $X$ and $Y$ are independent of each other ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}).

The probability density function of the chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2-pdf}) is

\begin{equation} \label{eq:f-pdf-chi2-pdf}
f_X(x) = \frac{1}{\Gamma\left( \frac{u}{2} \right) \cdot 2^{u/2}} \cdot x^{\frac{u}{2}-1} \cdot e^{-\frac{x}{2}} \; .
\end{equation}

Define the random variables $F$ and $W$ as functions of $X$ and $Y$

\begin{equation} \label{eq:f-pdf-FW-XY}
\begin{split}
F &= \frac{X/u}{Y/v} \\
W &= Y \; ,
\end{split}
\end{equation}

such that the inverse functions $X$ and $Y$ in terms of $F$ and $W$ are

\begin{equation} \label{eq:f-pdf-XY-FW}
\begin{split}
X &= \frac{u}{v} F W \\
Y &= W \; .
\end{split}
\end{equation}

This implies the following Jacobian matrix and determinant:

\begin{equation} \label{eq:f-pdf-XY-FW-jac}
\begin{split}
J &= \left[ \begin{matrix}
\frac{\mathrm{d}X}{\mathrm{d}F} & \frac{\mathrm{d}X}{\mathrm{d}W} \\
\frac{\mathrm{d}Y}{\mathrm{d}F} & \frac{\mathrm{d}Y}{\mathrm{d}W}
\end{matrix} \right]
= \left[ \begin{matrix}
\frac{u}{v} W & \frac{u}{v} F \\
0 & 1
\end{matrix} \right] \\
\lvert J \rvert  &= \frac{u}{v} W \; .
\end{split}
\end{equation}

Because $X$ and $Y$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the joint density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) of $X$ and $Y$ is equal to the product ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ind}) of the marginal densities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}):

\begin{equation} \label{eq:f-pdf-f-XY}
f_{X,Y}(x,y) = f_X(x) \cdot f_Y(y) \; .
\end{equation}

With the probability density function of an invertible function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-invfct}), the joint density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) of $F$ and $W$ can be derived as:

\begin{equation} \label{eq:f-pdf-f-FW-s1}
f_{F,W}(f,w) = f_{X,Y}(x,y) \cdot \lvert J \rvert \; .
\end{equation}

Substituting \eqref{eq:f-pdf-XY-FW} into \eqref{eq:f-pdf-chi2-pdf}, and then with \eqref{eq:f-pdf-XY-FW-jac} into \eqref{eq:f-pdf-f-FW-s1}, we get:

\begin{equation} \label{eq:f-pdf-f-FW-s2}
\begin{split}
f_{F,W}(f,w) &= f_X\left( \frac{u}{v} f w \right) \cdot f_Y(w) \cdot \lvert J \rvert \\
&= \frac{1}{\Gamma\left( \frac{u}{2} \right) \cdot 2^{u/2}} \cdot \left( \frac{u}{v} f w \right)^{\frac{u}{2}-1} \cdot e^{-\frac{1}{2} \left( \frac{u}{v} f w \right)} \cdot \frac{1}{\Gamma\left( \frac{v}{2} \right) \cdot 2^{v/2}} \cdot w^{\frac{v}{2}-1} \cdot e^{-\frac{w}{2}} \cdot \frac{u}{v} w \\
&= \frac{\left( \frac{u}{v} \right)^{\frac{u}{2}} \cdot f^{\frac{u}{2}-1}}{\Gamma\left( \frac{u}{2} \right) \cdot \Gamma\left( \frac{v}{2} \right) \cdot 2^{(u+v)/2}} \cdot w^{\frac{u+v}{2}-1} \cdot e^{-\frac{w}{2} \left( \frac{u}{v} f + 1 \right)} \; .
\end{split}
\end{equation}

The marginal density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of $F$ can now be obtained by integrating out ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}) $W$:

\begin{equation} \label{eq:f-pdf-f-F-s1}
\begin{split}
f_F(f) &= \int_{0}^{\infty} f_{F,W}(f,w) \, \mathrm{d}w \\
&= \frac{\left( \frac{u}{v} \right)^{\frac{u}{2}} \cdot f^{\frac{u}{2}-1}}{\Gamma\left( \frac{u}{2} \right) \cdot \Gamma\left( \frac{v}{2} \right) \cdot 2^{(u+v)/2}} \cdot \int_{0}^{\infty} w^{\frac{u+v}{2}-1} \cdot \mathrm{exp}\left[ -\frac{1}{2} \left( \frac{u}{v} f + 1 \right) w \right] \, \mathrm{d}w \\
&= \frac{\left( \frac{u}{v} \right)^{\frac{u}{2}} \cdot f^{\frac{u}{2}-1}}{\Gamma\left( \frac{u}{2} \right) \cdot \Gamma\left( \frac{v}{2} \right) \cdot 2^{(u+v)/2}} \cdot \frac{\Gamma\left( \frac{u+v}{2} \right)}{\left[ \frac{1}{2}\left( \frac{u}{v} f + 1 \right) \right]^{(u+v)/2}} \cdot \int_{0}^{\infty} \frac{\left[ \frac{1}{2}\left( \frac{u}{v} f + 1 \right) \right]^{(u+v)/2}}{\Gamma\left( \frac{u+v}{2} \right)} \cdot w^{\frac{u+v}{2}-1} \cdot \mathrm{exp}\left[ -\frac{1}{2} \left( \frac{u}{v} f + 1 \right) w \right] \, \mathrm{d}w \; .
\end{split}
\end{equation}

At this point, we can recognize that the integrand is equal to the probability density function of a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}) with

\begin{equation} \label{eq:f-pdf-f-W-gam-ab}
a = \frac{u+v}{2} \quad \text{and} \quad b = \frac{1}{2}\left( \frac{u}{v} f + 1 \right) \; ,
\end{equation}

and because a probability density function integrates to one ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}), we finally have:

\begin{equation} \label{eq:f-pdf-f-F-s2}
\begin{split}
f_F(f) &= \frac{\left( \frac{u}{v} \right)^{\frac{u}{2}} \cdot f^{\frac{u}{2}-1}}{\Gamma\left( \frac{u}{2} \right) \cdot \Gamma\left( \frac{v}{2} \right) \cdot 2^{(u+v)/2}} \cdot \frac{\Gamma\left( \frac{u+v}{2} \right)}{\left[ \frac{1}{2}\left( \frac{u}{v} f + 1 \right) \right]^{(u+v)/2}} \\
&= \frac{\Gamma\left( \frac{u+v}{2} \right)}{\Gamma\left( \frac{u}{2} \right) \cdot \Gamma\left( \frac{v}{2} \right)} \cdot \left( \frac{u}{v} \right)^{\frac{u}{2}} \cdot f^{\frac{u}{2}-1} \cdot \left( \frac{u}{v}f+1 \right)^{-\frac{u+v}{2}} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item statisticsmatt (2018): "Statistical Distributions: Derive the F Distribution"; in: \textit{YouTube}, retrieved on 2021-10-11; URL: \url{https://www.youtube.com/watch?v=AmHiOKYmHkI}.
\end{itemize}
\vspace{1em}



\subsection{Beta distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:beta}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, $X$ is said to follow a beta distribution with shape parameters $\alpha$ and $\beta$

\begin{equation} \label{eq:beta-beta}
X \sim \mathrm{Bet}(\alpha, \beta) \; ,
\end{equation}

if and only if its probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) is given by

\begin{equation} \label{eq:beta-beta-pdf}
\mathrm{Bet}(x; \alpha, \beta) = \frac{1}{\mathrm{B}(\alpha, \beta)} \, x^{\alpha-1} \, (1-x)^{\beta-1}
\end{equation}

where $\alpha > 0$ and $\beta > 0$, and the density is zero, if $x \notin [0,1]$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Beta distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-10; URL: \url{https://en.wikipedia.org/wiki/Beta_distribution#Definitions}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relationship to chi-squared distribution}]{Relationship to chi-squared distribution} \label{sec:beta-chi2}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ be independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following chi-squared distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}):

\begin{equation} \label{eq:beta-chi2-chi2}
X \sim \chi^2(m) \quad \text{and} \quad Y \sim \chi^2(n) \; .
\end{equation}

Then, the quantity $X/(X+Y)$ follows a beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta}):

\begin{equation} \label{eq:beta-chi2-beta-chi2}
\frac{X}{X+Y} \sim \mathrm{Bet}\left( \frac{m}{2}, \frac{n}{2} \right) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density function of the chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2-pdf}) is

\begin{equation} \label{eq:beta-chi2-chi2-pdf}
X \sim \chi^2(u) \quad \Rightarrow \quad f_X(x) = \frac{1}{\Gamma\left( \frac{u}{2} \right) \cdot 2^{u/2}} \cdot x^{\frac{u}{2}-1} \cdot e^{-\frac{x}{2}} \; .
\end{equation}

Define the random variables $Z$ and $W$ as functions of $X$ and $Y$

\begin{equation} \label{eq:beta-chi2-ZW-XY}
\begin{split}
Z &= \frac{X}{X+Y} \\
W &= Y \; ,
\end{split}
\end{equation}

such that the inverse functions $X$ and $Y$ in terms of $Z$ and $W$ are

\begin{equation} \label{eq:beta-chi2-XY-ZW}
\begin{split}
X &= \frac{ZW}{1-Z} \\
Y &= W \; .
\end{split}
\end{equation}

This implies the following Jacobian matrix and determinant:

\begin{equation} \label{eq:beta-chi2-XY-ZW-jac}
\begin{split}
J &= \left[ \begin{matrix}
\frac{\mathrm{d}X}{\mathrm{d}Z} & \frac{\mathrm{d}X}{\mathrm{d}W} \\
\frac{\mathrm{d}Y}{\mathrm{d}Z} & \frac{\mathrm{d}Y}{\mathrm{d}W}
\end{matrix} \right]
= \left[ \begin{matrix}
\frac{W}{(1-Z)^2} & \frac{Z}{1-Z} \\
0 & 1
\end{matrix} \right] \\
\lvert J \rvert  &= \frac{W}{(1-Z)^2} \; .
\end{split}
\end{equation}

Because $X$ and $Y$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the joint density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) of $X$ and $Y$ is equal to the product ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-ind}) of the marginal densities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}):

\begin{equation} \label{eq:beta-chi2-f-XY}
f_{X,Y}(x,y) = f_X(x) \cdot f_Y(y) \; .
\end{equation}

With the probability density function of an invertible function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-invfct}), the joint density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) of $Z$ and $W$ can be derived as:

\begin{equation} \label{eq:beta-chi2-f-ZW-s1}
f_{Z,W}(z,w) = f_{X,Y}(x,y) \cdot \lvert J \rvert \; .
\end{equation}

Substituting \eqref{eq:beta-chi2-XY-ZW} into \eqref{eq:beta-chi2-chi2-pdf}, and then with \eqref{eq:beta-chi2-XY-ZW-jac} into \eqref{eq:beta-chi2-f-ZW-s1}, we get:

\begin{equation} \label{eq:beta-chi2-f-ZW-s2}
\begin{split}
f_{Z,W}(z,w) &= f_X\left( \frac{zw}{1-z} \right) \cdot f_Y(w) \cdot \lvert J \rvert \\
&= \frac{1}{\Gamma\left( \frac{m}{2} \right) \cdot 2^{m/2}} \cdot \left( \frac{zw}{1-z} \right)^{\frac{m}{2}-1} \cdot e^{-\frac{1}{2} \left( \frac{zw}{1-z} \right)} \cdot \frac{1}{\Gamma\left( \frac{n}{2} \right) \cdot 2^{n/2}} \cdot w^{\frac{n}{2}-1} \cdot e^{-\frac{w}{2}} \cdot \frac{w}{(1-z)^2} \\
&= \frac{1}{\Gamma\left( \frac{m}{2} \right) \Gamma\left( \frac{n}{2} \right) \cdot 2^{m/2} 2^{n/2}} \cdot \left( \frac{z}{1-z} \right)^{\frac{m}{2}-1} \left( \frac{1}{(1-z)} \right)^2 \cdot w^{\frac{m}{2}+\frac{n}{2}-1} e^{-\frac{1}{2} \left( \frac{zw}{1-z} + \frac{w(1-z)}{1-z} \right)} \\
&= \frac{1}{\Gamma\left( \frac{m}{2} \right) \Gamma\left( \frac{n}{2} \right) \cdot 2^{(m+n)/2}} \cdot z^{\frac{m}{2}-1} \cdot (1-z)^{-\frac{m}{2}-1} \cdot w^{\frac{m+n}{2}-1} \cdot e^{-\frac{1}{2} \left( \frac{w}{1-z} \right)} \; .
\end{split}
\end{equation}

The marginal density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of $Z$ can now be obtained by integrating out ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}) $W$:

\begin{equation} \label{eq:beta-chi2-f-Z-s1}
\begin{split}
f_Z(z) &= \int_{0}^{\infty} f_{Z,W}(z,w) \, \mathrm{d}w \\
&= \frac{1}{\Gamma\left( \frac{m}{2} \right) \Gamma\left( \frac{n}{2} \right) \cdot 2^{(m+n)/2}} \cdot z^{\frac{m}{2}-1} \cdot (1-z)^{-\frac{m}{2}-1} \cdot \int_{0}^{\infty} w^{\frac{m+n}{2}-1} \cdot e^{-\frac{1}{2} \left( \frac{w}{1-z} \right)} \, \mathrm{d}w \\
&= \frac{1}{\Gamma\left( \frac{m}{2} \right) \Gamma\left( \frac{n}{2} \right) \cdot 2^{(m+n)/2}} \cdot z^{\frac{m}{2}-1} \cdot (1-z)^{-\frac{m}{2}-1} \cdot \frac{\Gamma\left( \frac{m+n}{2} \right)}{\left( \frac{1}{2(1-z)} \right)^{\frac{m+n}{2}}} \cdot \\
&\hphantom{=} \int_{0}^{\infty} \frac{\left( \frac{1}{2(1-z)} \right)^{\frac{m+n}{2}}}{\Gamma\left( \frac{m+n}{2} \right)} \cdot w^{\frac{m+n}{2}-1} \cdot e^{-\frac{1}{2(1-z)} \, w} \, \mathrm{d}w \; .
\end{split}
\end{equation}

At this point, we can recognize that the integrand is equal to the probability density function of a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}) with

\begin{equation} \label{eq:beta-chi2-f-W-gam-ab}
a = \frac{m+n}{2} \quad \text{and} \quad b = \frac{1}{2(1-z)} \; ,
\end{equation}

and because a probability density function integrates to one ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}), we have:

\begin{equation} \label{eq:beta-chi2-f-Z-s2}
\begin{split}
f_Z(z) &= \frac{1}{\Gamma\left( \frac{m}{2} \right) \Gamma\left( \frac{n}{2} \right) \cdot 2^{(m+n)/2}} \cdot z^{\frac{m}{2}-1} \cdot (1-z)^{-\frac{m}{2}-1} \cdot \frac{\Gamma\left( \frac{m+n}{2} \right)}{\left( \frac{1}{2(1-z)} \right)^{\frac{m+n}{2}}} \\
&= \frac{\Gamma\left( \frac{m+n}{2} \right) \cdot 2^{(m+n)/2}}{\Gamma\left( \frac{m}{2} \right) \Gamma\left( \frac{n}{2} \right) \cdot 2^{(m+n)/2}} \cdot z^{\frac{m}{2}-1} \cdot (1-z)^{-\frac{m}{2}+\frac{m+n}{2}-1} \\
&= \frac{\Gamma\left( \frac{m+n}{2} \right)}{\Gamma\left( \frac{m}{2} \right) \Gamma\left( \frac{n}{2} \right)} \cdot z^{\frac{m}{2}-1} \cdot (1-z)^{\frac{n}{2}-1} \; .
\end{split}
\end{equation}

With the definition of the beta function ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-mean}), this becomes

\begin{equation} \label{eq:beta-chi2-f-Z-s3}
f_Z(z) = \frac{1}{\mathrm{B}\left( \frac{m}{2}, \frac{n}{2} \right)} \cdot z^{\frac{m}{2}-1} \cdot (1-z)^{\frac{n}{2}-1}
\end{equation}

which is the probability density function of the beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-pdf}) with parameters

\begin{equation} \label{eq:beta-chi2-beta-chi2-para}
\alpha = \frac{m}{2} \quad \mathrm{and} \quad \beta = \frac{n}{2} \; ,
\end{equation}

such that

\begin{equation} \label{eq:beta-chi2-beta-chi2-qed}
Z \sim \mathrm{Bet}\left( \frac{m}{2}, \frac{n}{2} \right) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Probability Fact (2021): "If X ~ chisq(m) and Y ~ chisq(n) are independent"; in: \textit{Twitter}, retrieved on 2022-10-17; URL: \url{https://twitter.com/ProbFact/status/1450492787854647300}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:beta-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta}):

\begin{equation} \label{eq:beta-pdf-beta}
X \sim \mathrm{Bet}(\alpha, \beta) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ is

\begin{equation} \label{eq:beta-pdf-beta-pdf}
f_X(x) = \frac{1}{\mathrm{B}(\alpha, \beta)} \, x^{\alpha-1} \, (1-x)^{\beta-1} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Moment-generating function}]{Moment-generating function} \label{sec:beta-mgf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a positive random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:beta-mgf-beta}
X \sim \mathrm{Bet}(\alpha, \beta) \; .
\end{equation}

Then, the moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) of $X$ is

\begin{equation} \label{eq:beta-mgf-beta-mgf}
M_X(t) = 1 + \sum_{n=1}^{\infty} \left( \prod_{m=0}^{n-1} \frac{\alpha + m}{\alpha + \beta + m} \right) \frac{t^n}{n!} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density function of the beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-pdf}) is

\begin{equation} \label{eq:beta-mgf-beta-pdf}
f_X(x) = \frac{1}{\mathrm{B}(\alpha, \beta)} \, x^{\alpha-1} \, (1-x)^{\beta-1}
\end{equation}

and the moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) is defined as

\begin{equation} \label{eq:beta-mgf-mgf-var}
M_X(t) = \mathrm{E} \left[ e^{tX} \right] \; .
\end{equation}

Using the expected value for continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), the moment-generating function of $X$ therefore is

\begin{equation} \label{eq:beta-mgf-beta-mgf-s1}
\begin{split}
M_X(t) &= \int_{0}^{1} \exp[tx] \cdot \frac{1}{\mathrm{B}(\alpha, \beta)} \, x^{\alpha-1} \, (1-x)^{\beta-1} \, \mathrm{d}x \\
&= \frac{1}{\mathrm{B}(\alpha, \beta)} \int_{0}^{1} e^{tx} \, x^{\alpha-1} \, (1-x)^{\beta-1} \, \mathrm{d}x \; .
\end{split}
\end{equation}

With the relationship between beta function and gamma function

\begin{equation} \label{eq:beta-mgf-beta-gam-fct}
\mathrm{B}(\alpha, \beta) = \frac{\Gamma(\alpha) \, \Gamma(\beta)}{\Gamma(\alpha+\beta)}
\end{equation}

and the integral representation of the confluent hypergeometric function (Kummer's function of the first kind)

\begin{equation} \label{eq:beta-mgf-con-hyp-geo-fct-int}
{}_1 F_1(a,b,z) = \frac{\Gamma(b)}{\Gamma(a) \, \Gamma(b-a)} \int_{0}^{1} e^{zu} \, u^{a-1} \, (1-u)^{(b-a)-1} \, \mathrm{d}u \; ,
\end{equation}

the moment-generating function can be written as

\begin{equation} \label{eq:beta-mgf-beta-mgf-s2}
M_X(t) = {}_1 F_1(\alpha,\alpha+\beta,t) \; .
\end{equation}

Note that the series equation for the confluent hypergeometric function (Kummer's function of the first kind) is

\begin{equation} \label{eq:beta-mgf-con-hyp-geo-fct-ser}
{}_1 F_1(a,b,z) = \sum_{n=0}^{\infty} \frac{a^{\overline{n}}}{b^{\overline{n}}} \, \frac{z^n}{n!}
\end{equation}

where $m^{\overline{n}}$ is the rising factorial

\begin{equation} \label{eq:beta-mgf-fact-rise}
m^{\overline{n}} = \prod_{i=0}^{n-1} (m+i) \; ,
\end{equation}

so that the moment-generating function can be written as

\begin{equation} \label{eq:beta-mgf-beta-mgf-s3}
M_X(t) = \sum_{n=0}^{\infty} \frac{\alpha^{\overline{n}}}{(\alpha+\beta)^{\overline{n}}} \, \frac{t^n}{n!} \; .
\end{equation}

Applying the rising factorial equation \eqref{eq:beta-mgf-fact-rise} and using $m^{\overline{0}} = x^0 = 0! = 1$, we finally have:

\begin{equation} \label{eq:beta-mgf-beta-mgf-s4}
M_X(t) = 1 + \sum_{n=1}^{\infty} \left( \prod_{m=0}^{n-1} \frac{\alpha + m}{\alpha + \beta + m} \right) \frac{t^n}{n!} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Beta distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-25; URL: \url{https://en.wikipedia.org/wiki/Beta_distribution#Moment_generating_function}.
\item Wikipedia (2020): "Confluent hypergeometric function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-25; URL: \url{https://en.wikipedia.org/wiki/Confluent_hypergeometric_function#Kummer's_equation}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Cumulative distribution function}]{Cumulative distribution function} \label{sec:beta-cdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a positive random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:beta-cdf-beta}
X \sim \mathrm{Bet}(\alpha, \beta) \; .
\end{equation}

Then, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of $X$ is

\begin{equation} \label{eq:beta-cdf-beta-cdf}
F_X(x) = \frac{B(x; \alpha, \beta)}{B(\alpha, \beta)}
\end{equation}

where $B(a,b)$ is the beta function and $B(x;a,b)$ is the incomplete gamma function.


\vspace{1em}
\textbf{Proof:} The probability density function of the beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-pdf}) is:

\begin{equation} \label{eq:beta-cdf-beta-pdf}
f_X(x) = \frac{1}{\mathrm{B}(\alpha, \beta)} \, x^{\alpha-1} \, (1-x)^{\beta-1} \; .
\end{equation}

Thus, the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) is:

\begin{equation} \label{eq:beta-cdf-beta-cdf-app}
\begin{split}
F_X(x) &= \int_{0}^{x} \mathrm{Bet}(z; \alpha, \beta) \, \mathrm{d}z \\
&= \int_{0}^{x} \frac{1}{\mathrm{B}(\alpha, \beta)} \, z^{\alpha-1} \, (1-z)^{\beta-1} \, \mathrm{d}z \\
&= \frac{1}{\mathrm{B}(\alpha, \beta)} \int_{0}^{x} z^{\alpha-1} \, (1-z)^{\beta-1} \, \mathrm{d}z \; .
\end{split}
\end{equation}

With the definition of the incomplete beta function

\begin{equation} \label{eq:beta-cdf-inc-beta-fct}
B(x;a,b) = \int_{0}^{x} t^{a-1} \, (1-t)^{b-1} \, \mathrm{d}t \; ,
\end{equation}

we arrive at the final result given by equation \eqref{eq:beta-cdf-beta-cdf}:

\begin{equation} \label{eq:beta-cdf-beta-cdf-qed}
F_X(x) = \frac{B(x; \alpha, \beta)}{B(\alpha, \beta)} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Beta distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-19; URL: \url{https://en.wikipedia.org/wiki/Beta_distribution#Cumulative_distribution_function}.
\item Wikipedia (2020): "Beta function"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-11-19; URL: \url{https://en.wikipedia.org/wiki/Beta_function#Incomplete_beta_function}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:beta-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta}):

\begin{equation} \label{eq:beta-mean-beta}
X \sim \mathrm{Bet}(\alpha, \beta) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation} \label{eq:beta-mean-beta-mean}
\mathrm{E}(X) = \frac{\alpha}{\alpha + \beta} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) is the probability-weighted average over all possible values:

\begin{equation} \label{eq:beta-mean-mean}
\mathrm{E}(X) = \int_{\mathcal{X}} x \cdot f_X(x) \, \mathrm{d}x \; .
\end{equation}

The probability density function of the beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-pdf}) is

\begin{equation} \label{eq:beta-mean-beta-pdf}
f_X(x) = \frac{1}{\mathrm{B}(\alpha, \beta)} \, x^{\alpha-1} \, (1-x)^{\beta-1}, \quad 0 \leq x \leq 1
\end{equation}

where the beta function is given by a ratio gamma functions:

\begin{equation} \label{eq:beta-mean-beta-fct}
\mathrm{B}(\alpha, \beta) = \frac{\Gamma(\alpha) \cdot \Gamma(\beta)}{\Gamma(\alpha+\beta)} \; .
\end{equation}

Combining \eqref{eq:beta-mean-mean}, \eqref{eq:beta-mean-beta-pdf} and \eqref{eq:beta-mean-beta-fct}, we have:

\begin{equation} \label{eq:beta-mean-beta-mean-s1}
\begin{split}
\mathrm{E}(X) &= \int_{0}^{1} x \cdot \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \cdot \Gamma(\beta)} \, x^{\alpha-1} \, (1-x)^{\beta-1} \, \mathrm{d}x \\
&= \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)} \cdot \frac{\Gamma(\alpha+1)}{\Gamma(\alpha+1+\beta)} \int_{0}^{1} \frac{\Gamma(\alpha+1+\beta)}{\Gamma(\alpha+1) \cdot \Gamma(\beta)} \, x^{(\alpha+1)-1} \, (1-x)^{\beta-1} \, \mathrm{d}x \; .
\end{split}
\end{equation}

Employing the relation $\Gamma(x+1) = \Gamma(x) \cdot x$, we have

\begin{equation} \label{eq:beta-mean-beta-mean-s2}
\begin{split}
\mathrm{E}(X) &= \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)} \cdot \frac{\alpha \cdot \Gamma(\alpha)}{(\alpha+\beta) \cdot \Gamma(\alpha+\beta)} \int_{0}^{1} \frac{\Gamma(\alpha+1+\beta)}{\Gamma(\alpha+1) \cdot \Gamma(\beta)} \, x^{(\alpha+1)-1} \, (1-x)^{\beta-1} \, \mathrm{d}x \\
&= \frac{\alpha}{\alpha+\beta} \int_{0}^{1} \frac{\Gamma(\alpha+1+\beta)}{\Gamma(\alpha+1) \cdot \Gamma(\beta)} \, x^{(\alpha+1)-1} \, (1-x)^{\beta-1} \, \mathrm{d}x
\end{split}
\end{equation}

and again using the density of the beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-pdf}), we get

\begin{equation} \label{eq:beta-mean-beta-mean-s3}
\begin{split}
\mathrm{E}(X) &= \frac{\alpha}{\alpha+\beta} \int_{0}^{1} \mathrm{Bet}(x; \alpha+1, \beta) \, \mathrm{d}x \\
&= \frac{\alpha}{\alpha+\beta} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Boer Commander (2020): "Beta Distribution Mean and Variance Proof"; in: \textit{YouTube}, retrieved on 2021-04-29; URL: \url{https://www.youtube.com/watch?v=3OgCcnpZtZ8}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Variance}]{Variance} \label{sec:beta-var}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta}):

\begin{equation} \label{eq:beta-var-beta}
X \sim \mathrm{Bet}(\alpha, \beta) \; .
\end{equation}

Then, the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is

\begin{equation} \label{eq:beta-var-beta-var}
\mathrm{Var}(X) = \frac{\alpha \beta}{(\alpha + \beta + 1) \cdot (\alpha + \beta)^2} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) can be expressed in terms of expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-mean}) as

\begin{equation} \label{eq:beta-var-var-mean}
\mathrm{Var}(X) = \mathrm{E}(X^2) - \mathrm{E}(X)^2 \; .
\end{equation}

The expected value of a beta random variable ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-mean}) is

\begin{equation} \label{eq:beta-var-beta-mean}
\mathrm{E}(X) = \frac{\alpha}{\alpha+\beta} \; .
\end{equation}

The probability density function of the beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-pdf}) is

\begin{equation} \label{eq:beta-var-beta-pdf}
f_X(x) = \frac{1}{\mathrm{B}(\alpha, \beta)} \, x^{\alpha-1} \, (1-x)^{\beta-1}, \quad 0 \leq x \leq 1
\end{equation}

where the beta function is given by a ratio gamma functions:

\begin{equation} \label{eq:beta-var-beta-fct}
\mathrm{B}(\alpha, \beta) = \frac{\Gamma(\alpha) \cdot \Gamma(\beta)}{\Gamma(\alpha+\beta)} \; .
\end{equation}

Therefore, the expected value of a squared beta random variable becomes

\begin{equation} \label{eq:beta-var-beta-sqr-mean-s1}
\begin{split}
\mathrm{E}(X^2) &= \int_{0}^{1} x^2 \cdot \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \cdot \Gamma(\beta)} \, x^{\alpha-1} \, (1-x)^{\beta-1} \, \mathrm{d}x \\
&= \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)} \cdot \frac{\Gamma(\alpha+2)}{\Gamma(\alpha+2+\beta)} \int_{0}^{1} \frac{\Gamma(\alpha+2+\beta)}{\Gamma(\alpha+2) \cdot \Gamma(\beta)} \, x^{(\alpha+2)-1} \, (1-x)^{\beta-1} \, \mathrm{d}x \; .
\end{split}
\end{equation}

Twice-applying the relation $\Gamma(x+1) = \Gamma(x) \cdot x$, we have

\begin{equation} \label{eq:beta-var-beta-sqr-mean-s2}
\begin{split}
\mathrm{E}(X^2) &= \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)} \cdot \frac{(\alpha+1) \cdot \alpha \cdot \Gamma(\alpha)}{(\alpha+\beta+1) \cdot (\alpha+\beta) \cdot \Gamma(\alpha+\beta)} \int_{0}^{1} \frac{\Gamma(\alpha+2+\beta)}{\Gamma(\alpha+2) \cdot \Gamma(\beta)} \, x^{(\alpha+2)-1} \, (1-x)^{\beta-1} \, \mathrm{d}x \\
&= \frac{(\alpha+1) \cdot \alpha}{(\alpha+\beta+1) \cdot (\alpha+\beta)} \int_{0}^{1} \frac{\Gamma(\alpha+2+\beta)}{\Gamma(\alpha+2) \cdot \Gamma(\beta)} \, x^{(\alpha+2)-1} \, (1-x)^{\beta-1} \, \mathrm{d}x
\end{split}
\end{equation}

and again using the density of the beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-pdf}), we get

\begin{equation} \label{eq:beta-var-beta-sqr-mean-s3}
\begin{split}
\mathrm{E}(X^2) &= \frac{(\alpha+1) \cdot \alpha}{(\alpha+\beta+1) \cdot (\alpha+\beta)} \int_{0}^{1} \mathrm{Bet}(x; \alpha+2, \beta) \, \mathrm{d}x \\
&= \frac{(\alpha+1) \cdot \alpha}{(\alpha+\beta+1) \cdot (\alpha+\beta)} \; .
\end{split}
\end{equation}

Plugging \eqref{eq:beta-var-beta-sqr-mean-s3} and \eqref{eq:beta-var-beta-mean} into \eqref{eq:beta-var-var-mean}, the variance of a beta random variable finally becomes

\begin{equation} \label{eq:beta-var-beta-var-qed}
\begin{split}
\mathrm{Var}(X) &= \frac{(\alpha+1) \cdot \alpha}{(\alpha+\beta+1) \cdot (\alpha+\beta)} - \left( \frac{\alpha}{\alpha+\beta} \right)^2 \\
&= \frac{(\alpha^2+\alpha) \cdot (\alpha + \beta)}{(\alpha + \beta + 1) \cdot (\alpha + \beta)^2} - \frac{\alpha^2 \cdot (\alpha + \beta + 1)}{(\alpha + \beta + 1) \cdot (\alpha + \beta)^2} \\
&= \frac{(\alpha^3 + \alpha^2 \beta + \alpha^2 + \alpha \beta) - (\alpha^3 + \alpha^2 \beta + \alpha^2)}{(\alpha + \beta + 1) \cdot (\alpha + \beta)^2} \\
&= \frac{\alpha \beta}{(\alpha + \beta + 1) \cdot (\alpha + \beta)^2} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Boer Commander (2020): "Beta Distribution Mean and Variance Proof"; in: \textit{YouTube}, retrieved on 2021-04-29; URL: \url{https://www.youtube.com/watch?v=3OgCcnpZtZ8}.
\end{itemize}
\vspace{1em}



\subsection{Wald distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:wald}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, $X$ is said to follow a Wald distribution with drift rate $\gamma$ and threshold $\alpha$

\begin{equation} \label{eq:wald-wald}
X \sim \mathrm{Wald}(\gamma, \alpha) \; ,
\end{equation}

if and only if its probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) is given by

\begin{equation} \label{eq:wald-wald-pdf}
\mathrm{Wald}(x; \gamma, \alpha) = \frac{\alpha}{\sqrt{2\pi x^3}}\exp\left(-\frac{(\alpha-\gamma x)^2}{2x}\right)
\end{equation}

where $\gamma > 0$, $\alpha > 0$, and the density is zero if $x \leq 0$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Anders, R., Alario, F.-X., and van Maanen, L. (2016): "The Shifted Wald Distribution for Response Time Data Analysis"; in: \textit{Psychological Methods}, vol. 21, no. 3, pp. 309-327; URL: \url{https://dx.doi.org/10.1037/met0000066}; DOI: 10.1037/met0000066.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:wald-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a positive random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a Wald distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald}):

\begin{equation} \label{eq:wald-pdf-wald}
X \sim \mathrm{Wald}(\gamma, \alpha) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ is

\begin{equation} \label{eq:wald-pdf-wald-pdf}
f_X(x) = \frac{\alpha}{\sqrt{2\pi x^3}}\exp\left(-\frac{(\alpha-\gamma x)^2}{2x}\right) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the Wald distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Moment-generating function}]{Moment-generating function} \label{sec:wald-mgf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a positive random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a Wald distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald}):

\begin{equation} \label{eq:wald-mgf-wald}
X \sim \mathrm{Wald}(\gamma, \alpha) \; .
\end{equation}

Then, the moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) of $X$ is

\begin{equation} \label{eq:wald-mgf-wald-mgf}
M_X(t) = \exp \left[ \alpha\gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density function of the Wald distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald-pdf}) is

\begin{equation} \label{eq:wald-mgf-wald-pdf}
f_X(x) = \frac{\alpha}{\sqrt{2\pi x^3}}\exp\left(-\frac{(\alpha-\gamma x)^2}{2x}\right)
\end{equation}

and the moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) is defined as

\begin{equation} \label{eq:wald-mgf-mgf-var}
M_X(t) = \mathrm{E} \left[ e^{tX} \right] \; .
\end{equation}

Using the definition of expected value for continuous random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), the moment-generating function of $X$ therefore is

\begin{equation} \label{eq:wald-mgf-wald-mgf-s1}
\begin{split}
M_X(t) &= \int_0^{\infty} e^{tx} \cdot \frac{\alpha}{\sqrt{2\pi x^3}}\cdot \exp\left[-\frac{(\alpha-\gamma x)^2}{2x}\right]dx \\
&= \frac{\alpha}{\sqrt{2\pi}}\int_0^{\infty} x^{-3/2}\cdot \exp\left[tx - \frac{(\alpha-\gamma x)^2}{2x}\right]dx \; .
\end{split}
\end{equation}

To evaluate this integral, we will need two identities about modified Bessel functions of the second kind\footnote{\url{https://dlmf.nist.gov/10.25}}, denoted $K_{p}$. The function $K_{p}$ (for $p\in \mathbb{R}$) is one of the two linearly independent solutions of the differential equation

\begin{equation} \label{eq:wald-mgf-bessel-de}
x^2\frac{d^2y}{dx^2} + x\frac{dy}{dx}-(x^2+p^2)y=0 \; .
\end{equation}

The first of these identities\footnote{\url{https://dlmf.nist.gov/10.39.2}} gives an explicit solution for $K_{-1/2}$:

\begin{equation} \label{eq:wald-mgf-bessel-fact1}
K_{-1/2}(x) = \sqrt{\frac{\pi}{2x}} e^{-x} \; .
\end{equation}

The second of these identities\footnote{\url{https://dlmf.nist.gov/10.32.10}} gives an integral representation of $K_p$:

\begin{equation} \label{eq:wald-mgf-bessel-fact2}
K_p(\sqrt{ab}) = \frac{1}{2}\left(\frac{a}{b}\right)^{p/2} \int_0^{\infty}x^{p-1}\cdot \exp\left[-\frac{1}{2}\left(ax + \frac{b}{x}\right)\right]dx \; .
\end{equation}

Starting from \eqref{eq:wald-mgf-wald-mgf-s1}, we can expand the binomial term and rearrange the moment generating function into the following form:

\begin{equation} \label{eq:wald-mgf-wald-mgf-s2}
\begin{split}
M_X(t) &= \frac{\alpha}{\sqrt{2\pi}} \int_0^{\infty} x^{-3/2}\cdot \exp\left[ tx - \frac{\alpha^2}{2x} + \alpha\gamma - \frac{\gamma^2x}{2}\right]dx \\
       &= \frac{\alpha}{\sqrt{2\pi}}\cdot e^{\alpha \gamma} \int_0^{\infty} x^{-3/2}\cdot \exp\left[\left(t-\frac{\gamma^2}{2}\right)x - \frac{\alpha^2}{2x}\right]dx \\
       &= \frac{\alpha}{\sqrt{2\pi}}\cdot e^{\alpha \gamma} \int_0^{\infty} x^{-3/2}\cdot \exp \left[-\frac{1}{2}\left(\gamma^2-2t\right)x - \frac{1}{2}\cdot \frac{\alpha^2}{x}\right]dx \; .
\end{split}
\end{equation}

The integral now has the form of the integral in \eqref{eq:wald-mgf-bessel-fact2} with $p=-1/2$, $a=\gamma^2-2t$, and $b=\alpha^2$. This allows us to write the moment-generating function in terms of the modified Bessel function $K_{-1/2}$:

\begin{equation} \label{eq:wald-mgf-wald-mgf-s3}
M_X(t) = \frac{\alpha}{\sqrt{2\pi}}\cdot e^{\alpha \gamma}\cdot 2\left(\frac{\gamma^2-2t}{\alpha^2}\right)^{1/4}\cdot K_{-1/2}\left(\sqrt{\alpha^2(\gamma^2-2t)}\right).
\end{equation}

Combining with \eqref{eq:wald-mgf-bessel-fact1} and simplifying gives

\begin{equation} \label{eq:wald-mgf-wald-mgf-s4}
\begin{split}
M_X(t) &= \frac{\alpha}{\sqrt{2\pi}}\cdot e^{\alpha \gamma}\cdot 2\left(\frac{\gamma^2-2t}{\alpha^2}\right)^{1/4} \cdot \sqrt{\frac{\pi}{2\sqrt{\alpha^2(\gamma^2-2t)}}}\cdot \exp\left[-\sqrt{\alpha^2(\gamma^2-2t)}\right] \\
       &= \frac{\alpha}{\sqrt{2}\cdot \sqrt{\pi}}\cdot e^{\alpha \gamma}\cdot 2 \cdot \frac{(\gamma^2-2t)^{1/4}}{\sqrt{\alpha}}\cdot \frac{\sqrt{\pi}}{\sqrt{2}\cdot \sqrt{\alpha}\cdot (\gamma^2-2t)^{1/4}}\cdot \exp\left[-\sqrt{\alpha^2(\gamma^2-2t)}\right] \\
       &= e^{\alpha \gamma} \cdot \exp\left[-\sqrt{\alpha^2(\gamma^2-2t)}\right] \\
       &= \exp\left[\alpha \gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right] \; .
\end{split}
\end{equation}

This finishes the proof of \eqref{eq:wald-mgf-wald-mgf}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Siegrist, K. (2020): "The Wald Distribution"; in: \textit{Random: Probability, Mathematical Statistics, Stochastic Processes}, retrieved on 2020-09-13; URL: \url{https://www.randomservices.org/random/special/Wald.html}.
\item National Institute of Standards and Technology (2020): "NIST Digital Library of Mathematical Functions", retrieved on 2020-09-13; URL: \url{https://dlmf.nist.gov}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:wald-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a positive random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a Wald distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald}):

\begin{equation} \label{eq:wald-mean-wald}
X \sim \mathrm{Wald}(\gamma, \alpha) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation} \label{eq:wald-mean-wald-mean}
\mathrm{E}(X) = \frac{\alpha}{\gamma} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The mean or expected value $\mathrm{E}(X)$ is the first moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom}) of $X$, so we can use ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-mgf}) the moment-generating function of the Wald distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald-mgf}) to calculate

\begin{equation} \label{eq:wald-mean-wald-moment}
\mathrm{E}(X) = M_X'(0) \; .
\end{equation}

First we differentiate

\begin{equation} \label{eq:wald-mean-wald-mgf}
M_X(t) = \exp\left[\alpha \gamma - \sqrt{\alpha^2(\gamma^2-2t)}\right]
\end{equation}

with respect to $t$. Using the chain rule gives

\begin{equation} \label{eq:wald-mean-wald-mean-s1}
\begin{split}
  M_X'(t) &= \exp\left[\alpha \gamma - \sqrt{\alpha^2(\gamma^2-2t)}\right] \cdot -\frac{1}{2}\left(\alpha^2(\gamma^2-2t)\right)^{-1/2}\cdot -2\alpha^2 \\
  &= \exp\left[\alpha \gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right] \cdot \frac{\alpha^2}{\sqrt{\alpha^2(\gamma^2-2t)}} \; .
\end{split}
\end{equation}

Evaluating \eqref{eq:wald-mean-wald-mean-s1} at $t=0$ gives the desired result:

\begin{equation} \label{eq:wald-mean-wald-mean-s2}
\begin{split}
  M_X'(0) &= \exp\left[\alpha \gamma-\sqrt{\alpha^2(\gamma^2-2(0))}\right] \cdot \frac{\alpha^2}{\sqrt{\alpha^2(\gamma^2-2(0))}} \\
          &= \exp\left[\alpha \gamma - \sqrt{\alpha^2 \cdot \gamma^2}\right]\cdot \frac{\alpha^2}{\sqrt{\alpha^2\cdot \gamma^2}} \\
          &= \exp[0] \cdot \frac{\alpha^2}{\alpha \gamma} \\
          &= \frac{\alpha}{\gamma} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Variance}]{Variance} \label{sec:wald-var}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a positive random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a Wald distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald}):

\begin{equation} \label{eq:wald-var-wald}
X \sim \mathrm{Wald}(\gamma, \alpha) \; .
\end{equation}

Then, the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is

\begin{equation} \label{eq:wald-var-wald-var}
\mathrm{Var}(X) = \frac{\alpha}{\gamma^3} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} To compute the variance of $X$, we partition the variance into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-mean}):

\begin{equation} \label{eq:wald-var-var-mean}
\mathrm{Var}(X) = \mathrm{E}(X^2)-\mathrm{E}(X)^2.
\end{equation}

We then use the moment-generating function of the Wald distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald-mgf}) to calculate

\begin{equation} \label{eq:wald-var-wald-moment}
\mathrm{E}(X^2) = M_X''(0) \; .
\end{equation}

First we differentiate

\begin{equation} \label{eq:wald-var-wald-mgf}
M_X(t) = \exp\left[\alpha \gamma - \sqrt{\alpha^2(\gamma^2-2t)}\right]
\end{equation}

with respect to $t$. Using the chain rule gives

\begin{equation} \label{eq:wald-var-wald-var-s1}
\begin{split}
  M_X'(t) &= \exp\left[\alpha \gamma - \sqrt{\alpha^2(\gamma^2-2t)}\right] \cdot -\frac{1}{2}\left(\alpha^2(\gamma^2-2t)\right)^{-1/2}\cdot -2\alpha^2 \\
          &= \exp\left[\alpha \gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right] \cdot \frac{\alpha^2}{\sqrt{\alpha^2(\gamma^2-2t)}} \\
          &= \alpha \cdot \exp\left[\alpha \gamma -\sqrt{\alpha^2(\gamma^2-2t)}\right] \cdot (\gamma^2-2t)^{-1/2} \; .
\end{split}
\end{equation}

Now we use the product rule to obtain the second derivative:

\begin{equation} \label{eq:wald-var-wald-var-s2}
\begin{split}
  M_X''(t) &= \alpha \cdot \exp\left[\alpha \gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right]\cdot (\gamma^2-2t)^{-1/2}\cdot -\frac{1}{2}\left(\alpha^2(\gamma^2-2t)\right)^{-1/2}\cdot -2\alpha^2 \\
           &+ \alpha \cdot \exp\left[\alpha \gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right]\cdot -\frac{1}{2}(\gamma^2-2t)^{-3/2}\cdot -2 \\
           &= \alpha^2\cdot \exp\left[\alpha \gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right]\cdot (\gamma^2-2t)^{-1} \\
           &+ \alpha\cdot \exp\left[\alpha \gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right]\cdot (\gamma^2-2t)^{-3/2} \\
           &= \alpha \cdot \exp\left[\alpha \gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right]\left[\frac{\alpha}{\gamma^2-2t}+\frac{1}{\sqrt{(\gamma^2-2t)^3}}\right] \; .
\end{split}
\end{equation}

Applying \eqref{eq:wald-var-wald-moment} yields

\begin{equation} \label{eq:wald-var-wald-var-s3}
\begin{split}
  \mathrm{E}(X^2) &= M_X''(0) \\
                  &= \alpha \cdot \exp\left[\alpha \gamma-\sqrt{\alpha^2(\gamma^2-2(0))}\right]\left[\frac{\alpha}{\gamma^2-2(0)}+\frac{1}{\sqrt{(\gamma^2-2(0))^3}}\right] \\
                  &= \alpha \cdot \exp\left[\alpha \gamma - \alpha \gamma\right] \cdot \left[\frac{\alpha}{\gamma^2} + \frac{1}{\gamma^3}\right] \\
                  &= \frac{\alpha^2}{\gamma^2} + \frac{\alpha}{\gamma^3} \; .
\end{split}
\end{equation}

Since the mean of a Wald distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald-mean}) is given by $\mathrm{E}(X)=\alpha/\gamma$, we can apply \eqref{eq:wald-var-var-mean} to show

\begin{equation} \label{eq:wald-var-wald-var-s4}
\begin{split}
  \mathrm{Var}(X) &= \mathrm{E}(X^2) - \mathrm{E}(X)^2 \\
                  &= \frac{\alpha^2}{\gamma^2} + \frac{\alpha}{\gamma^3} - \left(\frac{\alpha}{\gamma}\right)^2 \\
                  &= \frac{\alpha}{\gamma^3}
\end{split}
\end{equation}

which completes the proof of \eqref{eq:wald-var-wald-var}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Skewness}]{Skewness} \label{sec:wald-skew}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a Wald distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald}):

\begin{equation} \label{eq:wald-skew-wald}
X \sim \mathrm{Wald}(\gamma,\alpha) \; .
\end{equation}

Then the skewness ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:skew}) of $X$ is

\begin{equation} \label{eq:wald-skew-wald-skew}
\mathrm{Skew}(X) = \frac{3}{\sqrt{\alpha\gamma}} \; .
\end{equation}

\vspace{1em}
\textbf{Proof:} 

To compute the skewness of $X$, we partition the skewness into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:skew-mean}):

\begin{equation} \label{eq:wald-skew-skew-mean}
\mathrm{Skew}(X) = \frac{\mathrm{E}(X^3)-3\mu\sigma^2-\mu^3}{\sigma^3} \; ,
\end{equation}

where $\mu$ and $\sigma$ are the mean and standard deviation of $X$, respectively. Since $X$ follows an Wald distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald}), the mean ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald-mean}) of $X$ is given by 

\begin{equation} \label{eq:wald-skew-wald-mean}
\mu = \mathrm{E}(X) = \frac{\alpha}{\gamma}
\end{equation}

and the standard deviation ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald-var}) of $X$ is given by

\begin{equation} \label{eq:wald-skew-wald-var}
\sigma = \sqrt{\mathrm{Var}(X)} = \sqrt{\frac{\alpha}{\gamma^3}}\; .
\end{equation}

Substituting \eqref{eq:wald-skew-wald-mean} and \eqref{eq:wald-skew-wald-var} into \eqref{eq:wald-skew-skew-mean} gives:

\begin{equation} \label{eq:wald-skew-skew-mean-alt}
\begin{split}
\mathrm{Skew}(X) &= \frac{\mathrm{E}(X^3)-3\mu\sigma^2-\mu^3}{\sigma^3}\\
&= \frac{\mathrm{E}(X^3) - 3\left(\frac{\alpha}{\gamma}\right)\left(\frac{\alpha}{\gamma^3}\right)-\left(\frac{\alpha}{\gamma}\right)^3}{\left(\sqrt{\frac{\alpha}{\gamma^3}}\right)^3}\\
&= \frac{\gamma^{9/2}}{\alpha^{3/2}}\left[\mathrm{E}(X^3) - \frac{3\alpha^2}{\gamma^4}-\frac{\alpha^3}{\gamma^3}\right] \; .
\end{split}
\end{equation}

Thus, the remaining work is to compute $\mathrm{E}(X^3)$. To do this, we use the moment-generating function of the Wald distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald-mgf}) to calculate

\begin{equation} \label{eq:wald-skew-wald-moment}
\mathrm{E}(X^3) = M_X'''(0)
\end{equation}

based on the relationship between raw moment and moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-mgf}). First, we differentiate the moment-generating function

\begin{equation} \label{eq:wald-skew-wald-mgf}
M_X(t) = \exp\left[\alpha \gamma - \sqrt{\alpha^2(\gamma^2-2t)}\right]
\end{equation}

with respect to $t$. Using the chain rule, we have:

\begin{equation} \label{eq:wald-skew-wald-skew-s1}
\begin{split}
  M_X'(t) &= \exp\left[\alpha \gamma - \sqrt{\alpha^2(\gamma^2-2t)}\right] \cdot -\frac{1}{2}\left(\alpha^2(\gamma^2-2t)\right)^{-1/2}\cdot -2\alpha^2 \\
          &= \exp\left[\alpha \gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right] \cdot \frac{\alpha^2}{\sqrt{\alpha^2(\gamma^2-2t)}} \\
          &= \alpha \cdot \exp\left[\alpha \gamma -\sqrt{\alpha^2(\gamma^2-2t)}\right] \cdot (\gamma^2-2t)^{-1/2} \; .
\end{split}
\end{equation}

Now we use the product rule to obtain the second derivative:

\begin{equation} \label{eq:wald-skew-wald-skew-s2}
\begin{split}
  M_X''(t) &= \alpha \cdot \exp\left[\alpha \gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right]\cdot (\gamma^2-2t)^{-1/2}\cdot -\frac{1}{2}\left(\alpha^2(\gamma^2-2t)\right)^{-1/2}\cdot -2\alpha^2 \\
           &+ \alpha \cdot \exp\left[\alpha \gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right]\cdot -\frac{1}{2}(\gamma^2-2t)^{-3/2}\cdot -2 \\
           &= \alpha^2\cdot \exp\left[\alpha \gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right]\cdot (\gamma^2-2t)^{-1} \\
           &+ \alpha\cdot \exp\left[\alpha\gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right]\cdot (\gamma^2-2t)^{-3/2} \\
           & = \frac{\alpha^2}{\gamma^2-2t}\exp\left[\alpha\gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right] + \frac{\alpha}{(\gamma^2-2t)^{3/2}}\exp\left[\alpha\gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right] \; .
\end{split}
\end{equation}

Finally, one more application of the chain rule will give us the third derivative. To start, we will decompose the second derivative obtained in \eqref{eq:wald-skew-wald-skew-s2} as

\begin{equation} \label{eq:wald-skew-wald-skew-s3}
M''(t) = f(t) + g(t)
\end{equation}

where

\begin{equation} \label{eq:wald-skew-wald-skew-split1}
f(t) = \frac{\alpha^2}{\gamma^2-2t}\exp\left[\alpha\gamma-\sqrt{\alpha^2(\gamma^2-t2)}\right]
\end{equation}

and

\begin{equation} \label{eq:wald-skew-wald-skew-split2}
g(t) = \frac{\alpha}{(\gamma^2-2t)^{3/2}}\exp\left[\alpha\gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right] \; .
\end{equation}

With this decomposition, $M_X'''(t) = f'(t) + g'(t)$. Applying the product rule to $f$ gives:

\begin{equation} \label{eq:wald-skew-wald-skew-f}
\begin{split}
  f'(t) &= 2\alpha^2(\gamma^2-2t)^{-2}\exp\left[\alpha\gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right] \\
  &+ \alpha^2(\gamma^2-2t)^{-1}\exp\left[\alpha\gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right]\cdot -\frac{1}{2}\left[\alpha^2(\gamma^2-2t)\right]^{-1/2}\cdot -2\alpha^2\\
  &= \frac{2\alpha^2}{(\gamma^2-2t)^2}\exp\left[\alpha\gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right] \\
  &+ \frac{\alpha^3}{(\gamma^2-2t)^{3/2}}\exp\left[\alpha\gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right] \; .
\end{split}
\end{equation}

Similarly, applying the product rule to $g$ gives:

\begin{equation} \label{eq:wald-skew-wald-skew-g}
\begin{split}
  g'(t) &= -\frac{3}{2}\alpha(\gamma^2-2t)^{-5/2}(-2)\exp\left[\alpha\gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right] \\
  &+ \alpha(\gamma^2-2t)^{-3/2}\exp\left[\alpha\gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right]\cdot -\frac{1}{2}\left[\alpha^2(\gamma^2-2t)\right]^{-1/2}\cdot -2\alpha^2\\
&= \frac{3\alpha}{(\gamma^2-2t)^{5/2}}\exp\left[\alpha\gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right] \\
&+ \frac{\alpha^2}{(\gamma^2-2t)^2}\exp\left[\alpha\gamma-\sqrt{\alpha^2(\gamma^2-2t)}\right] \; .
\end{split}
\end{equation}

Applying \eqref{eq:wald-skew-wald-moment}, together with \eqref{eq:wald-skew-wald-skew-f} and \eqref{eq:wald-skew-wald-skew-g}, yields

\begin{equation} \label{eq:wald-skew-wald-skew-s4}
\begin{split}
\mathrm{E}(X^3) &= M_X'''(0)\\
 &= f'(0) + g'(0)\\
 &= \left[\frac{2\alpha^2}{\gamma^4}+\frac{\alpha^3}{\gamma^3}\right] + \left[\frac{3\alpha}{\gamma^5}+\frac{\alpha^2}{\gamma^4}\right]\\
 &= \frac{3\alpha^2}{\gamma^4} + \frac{\alpha^3}{\gamma^3} + \frac{3\alpha}{\gamma^5} \; .
\end{split}
\end{equation}

We now substitute \eqref{eq:wald-skew-wald-skew-s4} into \eqref{eq:wald-skew-skew-mean-alt}, giving

\begin{equation} \label{eq:wald-skew-wald-skew-s5}
\begin{split}
\mathrm{Skew}(X) &= \frac{\gamma^{9/2}}{\alpha^{3/2}}\left[\mathrm{E}(X^3) - \frac{3\alpha^2}{\gamma^4}-\frac{\alpha^3}{\gamma^3}\right] \\
&= \frac{\gamma^{9/2}}{\alpha^{3/2}}\left[\frac{3\alpha^2}{\gamma^4} + \frac{\alpha^3}{\gamma^3} + \frac{3\alpha}{\gamma^5} - \frac{3\alpha^2}{\gamma^4}-\frac{\alpha^3}{\gamma^3}\right] \\
&= \frac{\gamma^{9/2}}{\alpha^{3/2}} \cdot \frac{3\alpha}{\gamma^5}\\
&= \frac{3}{\alpha^{1/2}\cdot \gamma^{1/2}}\\
&= \frac{3}{\sqrt{\alpha\gamma}} \; .
\end{split}
\end{equation}

This completes the proof of \eqref{eq:wald-skew-wald-skew}.

\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Method of moments}]{Method of moments} \label{sec:wald-mome}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ be a set of observed data independent and identically distributed according to a Wald distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald}) with drift rate $\gamma$ and threshold $\alpha$:

\begin{equation} \label{eq:wald-mome-wald}
y_i \sim \mathrm{Wald}(\gamma,\alpha), \quad i = 1, \ldots, n \; .
\end{equation}

Then, the method-of-moments estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mome}) for the parameters $\gamma$ and $\alpha$ are given by

\begin{equation} \label{eq:wald-mome-wald-MoM}
\begin{split}
\hat{\gamma} &= \sqrt{\frac{\bar{y}}{\bar{v}}} \\
\hat{\alpha} &= \sqrt{\frac{\bar{y}^3}{\bar{v}}}
\end{split}
\end{equation}

where $\bar{y}$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) and $\bar{v}$ is the unbiased sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}):

\begin{equation} \label{eq:wald-mome-y-mean-var}
\begin{split}
\bar{y} &= \frac{1}{n} \sum_{i=1}^n y_i \\
\bar{v} &= \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2 \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} The mean ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald-mean}) and variance ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald-var}) of the Wald distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wald}) in terms of the parameters $\gamma$ and $\alpha$ are given by

\begin{equation} \label{eq:wald-mome-wald-E-Var}
\begin{split}
\mathrm{E}(X) &= \frac{\alpha}{\gamma} \\
\mathrm{Var}(X) &= \frac{\alpha}{\gamma^3} \; .
\end{split}
\end{equation}

Thus, matching the moments ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mome}) requires us to solve the following system of equations for $\gamma$ and $\alpha$:

\begin{equation} \label{eq:wald-mome-wald-mean-var}
\begin{split}
\bar{y} &= \frac{\alpha}{\gamma} \\
\bar{v} &= \frac{\alpha}{\gamma^3} \; .
\end{split}
\end{equation}

To this end, our first step is to express the second equation of \eqref{eq:wald-mome-wald-mean-var} as follows:

\begin{equation} \label{eq:wald-mome-gamma-s1}
\begin{split}
\bar{v} &= \frac{\alpha}{\gamma^3} \\
& = \frac{\alpha}{\gamma} \cdot \gamma^{-2}\\
& = \bar{y} \cdot \gamma^{-2} \; .
\end{split}
\end{equation}

Rearranging \eqref{eq:wald-mome-gamma-s1} gives

\begin{equation} \label{eq:wald-mome-gamma-s2}
\gamma^2 = \frac{\bar{y}}{\bar{v}} \; ,
\end{equation}

or equivalently,

\begin{equation} \label{eq:wald-mome-gamma-s3}
\gamma = \sqrt{\frac{\bar{y}}{\bar{v}}} \; .
\end{equation}

Our final step is to solve the first equation of \eqref{eq:wald-mome-wald-mean-var} for $\alpha$ and substitute \eqref{eq:wald-mome-gamma-s3} for $\gamma$:

\begin{equation} \label{eq:wald-mome-alpha-s1}
\begin{split}
\alpha & = \bar{y} \cdot \gamma \\
& = \bar{y} \cdot \sqrt{\frac{\bar{y}}{\bar{v}}}\\
&= \sqrt{\bar{y}^2} \cdot \sqrt{\frac{\bar{y}}{\bar{v}}}\\
&= \sqrt{\frac{\bar{y}^3}{\bar{v}}} \; .
\end{split}
\end{equation}

Together, \eqref{eq:wald-mome-gamma-s3} and \eqref{eq:wald-mome-alpha-s1} constitute the method-of-moment estimates of $\gamma$ and $\alpha$.

\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{ex-Gaussian distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:exg}
\setcounter{equation}{0}

\textbf{Definition:} Let $A$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) that is normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with mean $\mu$ and variance $\sigma^2$, and let $B$ be a random variable that is exponentially distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}) with rate $\lambda$. Suppose further that $A$ and $B$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}). Then the sum $X=A+B$ is said to have an exponentially-modified Gaussian (i.e., ex-Gaussian) distribution, with parameters $\mu$, $\sigma$, and $\lambda$; that is,

\begin{equation} \label{eq:exg-exg}
X \sim \text{ex-Gaussian}(\mu, \sigma, \lambda) \; ,
\end{equation}

where $\mu \in \mathbb{R}$, $\sigma>0$, and $\lambda > 0$.



\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Luce, R. D. (1986): "Response Times: Their Role in Inferring Elementary Mental Organization", 35-36; URL: \url{https://global.oup.com/academic/product/response-times-9780195036428}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:exg-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg}):

\begin{equation} \label{eq:exg-pdf-exg}
X \sim \text{ex-Gaussian}(\mu, \sigma, \lambda) \; .
\end{equation}

Then the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ is 

\begin{equation} \label{eq:exg-pdf-exg-pdf}
f_X(t) = \frac{\lambda}{\sqrt{2\pi}} \exp\left[\frac{\lambda^2\sigma^2}{2} - \lambda(t-\mu)\right] \cdot \int_{-\infty}^{\frac{t-\mu}{\sigma}-\lambda\sigma} \exp\left[-\frac{1}{2}y^2\right] dy \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Suppose $X$ follows an ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg}). Then $X=A+B$, where $A$ and $B$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), $A$ is normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with mean ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-mean}) $\mu$ and variance ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-var}) $\sigma^2$, and $B$ is exponentially distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}) with rate $\lambda$. Then, the probability density function ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) for $A$ is given by

\begin{equation} \label{eq:exg-pdf-norm-pdf}
f_A(t) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left[-\frac{1}{2}\left(\frac{t-\mu}{\sigma}\right)^2\right] \; ,
\end{equation}

and the probability density function ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp-pdf}) for $B$ is given by

\begin{equation} \label{eq:exg-pdf-exp-pdf}
f_B(t) = \left\{
\begin{array}{rl}
\lambda\exp[-\lambda t] \;, & \text{if} \; t\geq 0\\
0 \;, & \text{if} \; t <0 \; .
\end{array}
\right.
\end{equation}

Thus, the probability density function for the sum ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf-sumind}) $X=A+B$ is given by taking the convolution of $f_A$ and $f_B$:

\begin{equation} \label{eq:exg-pdf-convolution}
\begin{split}
f_X(t) &= \int_{-\infty}^{\infty} f_A(x)f_B(t-x)dx\\
&= \int_{-\infty}^{t} f_A(x)f_B(t-x)dx + \int_{t}^{\infty} f_A(x)f_B(t-x)dx \\
&= \int_{-\infty}^{t} f_A(x)f_B(t-x)dx \; ,
\end{split}
\end{equation}

which follows from the fact that $f_B(t-x) = 0$ for $x>t$. From here, we substitute the expressions \eqref{eq:exg-pdf-norm-pdf} and \eqref{eq:exg-pdf-exp-pdf} for the probability density functions $f_A$ and $f_B$ in \eqref{eq:exg-pdf-convolution}:

\begin{equation} \label{eq:exg-pdf-exg-pdf-s1}
\begin{split}
f_X(t) &= \int_{-\infty}^t \frac{1}{\sigma\sqrt{2\pi}}\exp\left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right]\cdot \lambda\exp[-\lambda(t-x)]dx\\
&= \frac{\lambda}{\sigma\sqrt{2\pi}}\int_{-\infty}^t \exp\left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right]\cdot \exp[-\lambda t+\lambda x]dx\\
&= \frac{\lambda}{\sigma\sqrt{2\pi}}\int_{-\infty}^t \exp\left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right]\cdot \exp[-\lambda t]\cdot \exp[\lambda x]dx\\
&= \frac{\lambda\exp[-\lambda t]}{\sigma\sqrt{2\pi}}\int_{-\infty}^t \exp\left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2+\lambda x\right]dx \; .
\end{split}
\end{equation}

We can further simplify the integrand with a substitution; to this end, let

\begin{equation} \label{eq:exg-pdf-substitution}
y = g(x) = \frac{x-\mu}{\sigma} - \lambda\sigma
\end{equation}

This gives the following three identities:

\begin{equation} \label{eq:exg-pdf-identity1}
\frac{dy}{dx} = \frac{1}{\sigma} \; , \quad \text{or equivalently,} \quad dx = \sigma dy \; ,
\end{equation}

\begin{equation} \label{eq:exg-pdf-identity2}
\frac{x-\mu}{\sigma} = y+\lambda\sigma \; , \quad \text{and}
\end{equation}

\begin{equation} \label{eq:exg-pdf-identity3}
x = y\sigma + \lambda\sigma^2 + \mu \; .
\end{equation}

Substituting these identities into \eqref{eq:exg-pdf-exg-pdf-s1} gives

\begin{equation} \label{eq:exg-pdf-exg-pdf-s2}
\begin{split}
f_X(t) &= \frac{\lambda\exp[-\lambda t]}{\sigma\sqrt{2\pi}}\int_{-\infty}^{g(t)} \exp\left[-\frac{1}{2}(y+\lambda\sigma)^2+\lambda(y\sigma + \lambda\sigma^2+\mu)\right]\sigma dy\\
&= \frac{\lambda\sigma\exp[-\lambda t]}{\sigma\sqrt{2\pi}}\int_{-\infty}^{\frac{x-\mu}{\sigma}+\lambda\sigma} \exp\left[-\frac{1}{2}(y^2+2y\lambda\sigma + \lambda^2\sigma^2)+\lambda y\sigma + \lambda^2\sigma^2 + \lambda\mu\right] dy\\
&= \frac{\lambda\exp[-\lambda t]}{\sqrt{2\pi}}\int_{-\infty}^{\frac{x-\mu}{\sigma}+\lambda\sigma} \exp\left[-\frac{1}{2}y^2-y\lambda\sigma - \frac{\lambda^2\sigma^2}{2}+\lambda y\sigma + \lambda^2\sigma^2 + \lambda\mu\right] dy\\
&= \frac{\lambda\exp[-\lambda t]}{\sqrt{2\pi}}\int_{-\infty}^{\frac{x-\mu}{\sigma}+\lambda\sigma} \exp\left[-\frac{1}{2}y^2\right] \cdot \exp\left[\frac{\lambda^2\sigma^2}{2} + \lambda\mu\right] dy\\
&= \frac{\lambda\exp[-\lambda t]}{\sqrt{2\pi}}\cdot \exp\left[\frac{\lambda^2\sigma^2}{2} + \lambda\mu\right] \int_{-\infty}^{\frac{x-\mu}{\sigma}+\lambda\sigma} \exp\left[-\frac{1}{2}y^2\right] \cdot  dy\\
&= \frac{\lambda}{\sqrt{2\pi}}\cdot \exp\left[-\lambda t + \frac{\lambda^2\sigma^2}{2} + \lambda\mu\right] \int_{-\infty}^{\frac{x-\mu}{\sigma}+\lambda\sigma} \exp\left[-\frac{1}{2}y^2\right] \cdot  dy\\
&= \frac{\lambda}{\sqrt{2\pi}}\cdot \exp\left[\frac{\lambda^2\sigma^2}{2} - \lambda(t-\mu)\right] \int_{-\infty}^{\frac{x-\mu}{\sigma}+\lambda\sigma} \exp\left[-\frac{1}{2}y^2\right] \cdot  dy \; .
\end{split}
\end{equation}

This finishes the proof of \eqref{eq:exg-pdf-exg-pdf}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Moment-generating function}]{Moment-generating function} \label{sec:exg-mgf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg}):

\begin{equation} \label{eq:exg-mgf-exg}
X \sim \text{ex-Gaussian}(\mu, \sigma, \lambda) \; .
\end{equation}

Then, the moment generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) of $X$ is 

\begin{equation} \label{eq:exg-mgf-exg-mgf}
M_X(t) = \left( \frac{\lambda}{\lambda-t} \right) \exp \left[ \mu t + \frac{1}{2}\sigma^2t^2 \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Suppose $X$ follows an ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg}). Then, $X=A+B$ where $A$ and $B$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), $A$ is normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with mean ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-mean}) $\mu$ and variance ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-var}) $\sigma^2$, and $B$ is exponentially distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp}) with rate $\lambda$. Then the moment generating function ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-mgf}) for $A$ is given by

\begin{equation} \label{eq:exg-mgf-norm-mgf}
M_A(t) = \exp \left[ \mu t + \frac{1}{2}\sigma^2t^2 \right]
\end{equation} 

and the moment generating function ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exp-mgf}) for $B$ is given by

\begin{equation} \label{eq:exg-mgf-exp-mgf}
M_B(t) = \frac{\lambda}{\lambda - t} \; .
\end{equation}

By definition, $X$ is a linear combination of independent random variables $A$ and $B$, so the moment generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf-lincomb}) of $X$ is the product of $M_A(t)$ and $M_B(t)$. That is,

\begin{equation} \label{eq:exg-mgf-exg-mgf-s1}
\begin{split}
M_X(t) &= M_A(t)\cdot M_B(t)\\
&= \exp\left[ \mu t + \frac{1}{2}\sigma^2t^2 \right] \cdot \left( \frac{\lambda}{\lambda-t} \right)\\
&= \left( \frac{\lambda}{\lambda-t} \right) \exp\left[ \mu t + \frac{1}{2}\sigma^2t^2 \right] \; .
\end{split}
\end{equation}

This finishes the proof of \eqref{eq:exg-mgf-exg-mgf}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:exg-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg}):

\begin{equation} \label{eq:exg-mean-exg}
X \sim \text{ex-Gaussian}(\mu, \sigma, \lambda) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is 

\begin{equation} \label{eq:exg-mean-exg-mean}
\mathrm{E}(X) = \mu + \frac{1}{\lambda} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The mean or expected value $\mathrm{E}(X)$ is the first raw moment ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom}) of $X$, so we can use ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-mgf}) the moment-generating function of the ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg-mgf}) to calculate

\begin{equation} \label{eq:exg-mean-mean-from-mgf}
\mathrm{E}(X) = M_X'(0) \; .
\end{equation}

First, we differentiate

\begin{equation} \label{eq:exg-mean-exg-mgf}
M_X(t) = \left( \frac{\lambda}{\lambda-t} \right) \exp \left[ \mu t + \frac{1}{2}\sigma^2t^2 \right]
\end{equation}

with respect to $t$. Using the product rule and chain rule gives:

\begin{equation} \label{eq:exg-mean-exg-mean-s1}
\begin{split}
M'_X(t) &= \frac{\lambda}{(\lambda-t)^2}\exp \left[ \mu t + \frac{1}{2}\sigma^2t^2\right] + \left(\frac{\lambda}{\lambda-t}\right)\exp\left[\mu t + \frac{1}{2}\sigma^2t^2\right] (\mu + \sigma^2t)\\
&= \left(\frac{\lambda}{\lambda-t}\right) \cdot \exp\left[\mu t + \frac{1}{2}\sigma^2t^2\right] \cdot \left[ \frac{1}{\lambda-t} +\mu +\sigma^2t \right] \; .
\end{split}
\end{equation}

Evaluating \eqref{eq:exg-mean-exg-mean-s1} at $t=0$ gives the desired result:

\begin{equation} \label{eq:exg-mean-exg-mean-s2}
\begin{split}
M'_X(0) &= \left(\frac{\lambda}{\lambda-0}\right) \cdot \exp\left[\mu\cdot 0 + \frac{1}{2}\sigma^2\cdot 0^2\right] \cdot \left[ \frac{1}{\lambda-0} + \mu + \sigma^2\cdot 0 \right] \\
&= 1\cdot 1 \cdot \left[ \frac{1}{\lambda} + \mu \right]\\
&= \mu + \frac{1}{\lambda} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Variance}]{Variance} \label{sec:exg-var}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg}):

\begin{equation} \label{eq:exg-var-exg}
X \sim \text{ex-Gaussian}(\mu, \sigma, \lambda) \; .
\end{equation}

Then, the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $X$ is 

\begin{equation} \label{eq:exg-var-exg-var}
\mathrm{Var}(X) = \sigma^2 + \frac{1}{\lambda^2} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} To compute the variance of $X$, we partition the variance into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-mean}):

\begin{equation} \label{eq:exg-var-var-mean}
\mathrm{Var}(X) = \mathrm{E}(X^2)-\mathrm{E}(X)^2 \; .
\end{equation}

We then use the moment-generating function of the ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg-mgf}) to calculate

\begin{equation} \label{eq:exg-var-exg-moment}
\mathrm{E}(X^2) = M_X''(0)
\end{equation}

based on the relationship between raw moment and moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-mgf}).

First, we differentiate

\begin{equation} \label{eq:exg-var-exg-mgf}
M_X(t) = \left( \frac{\lambda}{\lambda-t} \right) \exp \left[ \mu t + \frac{1}{2}\sigma^2t^2 \right]
\end{equation}

with respect to $t$. Using the product rule and chain rule gives:

\begin{equation} \label{eq:exg-var-exg-var-s1}
\begin{split}
M'_X(t) &= \frac{\lambda}{(\lambda-t)^2}\exp \left[ \mu t + \frac{1}{2}\sigma^2t^2\right] + \left(\frac{\lambda}{\lambda-t}\right)\exp\left[\mu t + \frac{1}{2}\sigma^2t^2\right] (\mu + \sigma^2t)\\
&= \left(\frac{\lambda}{\lambda-t}\right) \cdot \exp\left[\mu t + \frac{1}{2}\sigma^2t^2\right] \cdot \left[ \frac{1}{\lambda-t} +\mu +\sigma^2t \right] \\
&= M_X(t)\cdot \left[ \frac{1}{\lambda-t} + \mu + \sigma^2t\right] \; .
\end{split}
\end{equation}

We now use the product rule to obtain the second derivative:

\begin{equation} \label{eq:exg-var-exg-var-s2}
\begin{split}
M_X''(t) &= M_X'(t)\cdot \left[ \frac{1}{\lambda-t} + \mu + \sigma^2t\right] + M_X(t)\cdot \left[ \frac{1}{(\lambda-t)^2}+\sigma^2\right] \\
&\overset{\eqref{eq:exg-var-exg-var-s1}}{=} M_X(t)\cdot \left[ \frac{1}{\lambda-t} + \mu + \sigma^2t\right]^2 + M_X(t)\cdot \left[ \frac{1}{(\lambda-t)^2}+\sigma^2\right]\\
&= M_X(t) \cdot \left[ \left( \frac{1}{\lambda-t} + \mu + \sigma^2t\right)^2 + \frac{1}{(\lambda-t)^2} + \sigma^2\right] \\
&= \left(\frac{\lambda}{\lambda-t}\right)\cdot \exp\left[\mu t + \frac{1}{2}\sigma^2t^2\right]\cdot \left[ \left( \frac{1}{\lambda-t} + \mu + \sigma^2t\right)^2 + \frac{1}{(\lambda-t)^2} + \sigma^2\right]
\end{split}
\end{equation}

Applying \eqref{eq:exg-var-exg-moment} yields

\begin{equation} \label{eq:exg-var-exg-var-s3}
\begin{split}
\mathrm{E}(X^2) &= M_X''(0) \\
&= \left(\frac{\lambda}{\lambda-0}\right) \cdot \exp\left[\mu\cdot 0 + \frac{1}{2}\sigma^2\cdot 0^2\right] \cdot \left[\left(\frac{1}{\lambda-0}+\mu + \sigma^2\cdot 0\right)^2 + \frac{1}{(\lambda-0)^2}+\sigma^2\right]\\
&= 1\cdot 1 \cdot \left[\left(\frac{1}{\lambda} + \mu\right)^2 + \frac{1}{\lambda^2}+\sigma^2 \right]\\
&= \frac{1}{\lambda^2} + \frac{2\mu}{\lambda} + \mu^2 + \frac{1}{\lambda^2} + \sigma^2\\
&= \frac{2}{\lambda^2} + \frac{2\mu}{\lambda} + \mu^2 + \sigma^2 \; .
\end{split}
\end{equation}

Since the mean of an ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg-mean}) is given by

\begin{equation} \label{exg-mean}
\mathrm{E}(X) = \mu + \frac{1}{\lambda} \; ,
\end{equation}

we can apply \eqref{eq:exg-var-var-mean} to show

\begin{equation} \label{eq:exg-var-exg-var-s4}
\begin{split}
\mathrm{Var}(X) &= \mathrm{E}(X^2) - \mathrm{E}(X)^2 \\
&= \left[\frac{2}{\lambda^2} + \frac{2\mu}{\lambda} + \mu^2+\sigma^2\right] - \left(\mu+\frac{1}{\lambda}\right)^2\\
&= \frac{2}{\lambda^2} + \frac{2\mu}{\lambda} + \mu^2+\sigma^2 - \mu^2 - \frac{2\mu}{\lambda} - \frac{1}{\lambda^2}\\
&= \sigma^2+\frac{1}{\lambda^2} \; .
\end{split}
\end{equation}

This completes the proof of \eqref{eq:exg-var-exg-var}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Skewness}]{Skewness} \label{sec:exg-skew}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following an ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg}):

\begin{equation} \label{eq:exg-skew-exg}
X \sim \text{ex-Gaussian}(\mu, \sigma, \lambda) \; .
\end{equation}

Then the skewness ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:skew}) of $X$ is

\begin{equation} \label{eq:exg-skew-exg-skew}
\mathrm{Skew}(X) = \frac{2}{\lambda^3\left( \sigma^2 + \frac{1}{\lambda^2}\right)^{\frac{3}{2}}} \; .
\end{equation}

\vspace{1em}
\textbf{Proof:} 

To compute the skewness of $X$, we partition the skewness into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:skew-mean}):

\begin{equation} \label{eq:exg-skew-skew-mean}
\mathrm{Skew}(X) = \frac{\mathrm{E}(X^3)-3\mu\sigma^2-\mu^3}{\sigma^3} \; ,
\end{equation}

where $\mu$ and $\sigma$ are the mean and standard deviation of $X$, respectively. To prevent confusion between the labels used for the ex-Gaussian parameters in \eqref{eq:exg-skew-exg} and the mean and standard deviation of $X$, we rewrite \eqref{eq:exg-skew-skew-mean} as

\begin{equation} \label{eq:exg-skew-skew-mean-alt}
\mathrm{Skew}(X) = \frac{\mathrm{E}(X^3) - 3\cdot \mathrm{E}(X)\cdot \mathrm{Var}(X) - \mathrm{E}(X)^3}{\mathrm{Var}(X)^{\frac{3}{2}}} \; .
\end{equation}

Since $X$ follows an ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg}), the mean ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg-mean}) of $X$ is given by 

\begin{equation} \label{eq:exg-skew-exg-mean}
\mathrm{E}(X) = \mu + \frac{1}{\lambda}
\end{equation}

and the variance ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg-var}) of $X$ is given by

\begin{equation} \label{eq:exg-skew-exg-var}
\mathrm{Var}(X) = \sigma^2 + \frac{1}{\lambda^2} \; .
\end{equation}

Thus, the primary work is to compute $\mathrm{E}(X^3)$. To do this, we use the moment-generating function of the ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg-mgf}) to calculate

\begin{equation} \label{eq:exg-skew-exg-moment}
\mathrm{E}(X^3) = M_X'''(0)
\end{equation}

based on the relationship between raw moment and moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-mgf}).

First, we differentiate the moment-generating function of the ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg-mgf})

\begin{equation} \label{eq:exg-skew-exg-mgf}
M_X(t) = \left( \frac{\lambda}{\lambda-t} \right) \exp \left[ \mu t + \frac{1}{2}\sigma^2t^2 \right]
\end{equation}

with respect to $t$. Using the product rule and chain rule, we have:

\begin{equation} \label{eq:exg-skew-exg-skew-s1}
\begin{split}
M'_X(t) &= \frac{\lambda}{(\lambda-t)^2}\exp \left[ \mu t + \frac{1}{2}\sigma^2t^2\right] + \left(\frac{\lambda}{\lambda-t}\right)\exp\left[\mu t + \frac{1}{2}\sigma^2t^2\right] (\mu + \sigma^2t)\\
&= \left(\frac{\lambda}{\lambda-t}\right) \cdot \exp\left[\mu t + \frac{1}{2}\sigma^2t^2\right] \cdot \left[ \frac{1}{\lambda-t} +\mu +\sigma^2t \right] \\
&= M_X(t)\cdot \left[ \frac{1}{\lambda-t} + \mu + \sigma^2t\right] \; .
\end{split}
\end{equation}

We then use the product rule to obtain the second derivative:

\begin{equation} \label{eq:exg-skew-exg-skew-s2}
\begin{split}
M_X''(t) &= M_X'(t)\cdot \left[ \frac{1}{\lambda-t} + \mu + \sigma^2t\right] + M_X(t)\cdot \left[ \frac{1}{(\lambda-t)^2}+\sigma^2\right] \\
&= M_X(t)\cdot \left[ \frac{1}{\lambda-t} + \mu + \sigma^2t\right]^2 + M_X(t)\cdot \left[ \frac{1}{(\lambda-t)^2}+\sigma^2\right]\\
&= M_X(t) \cdot \left[ \left( \frac{1}{\lambda-t} + \mu + \sigma^2t\right)^2 + \frac{1}{(\lambda-t)^2} + \sigma^2\right] \; .
\end{split}
\end{equation}

Finally, we use the product rule and chain rule to obtain the third derivative:

\begin{equation} \label{eq:exg-skew-exg-skew-s3}
M_X'''(t) = M_X'(t)\left[\left(\frac{1}{\lambda-t}+\mu+\sigma^2t\right)^2 + \frac{1}{(\lambda-t)^2}+\sigma^2\right] + M_X(t)\left[2\left(\frac{1}{\lambda-t}+\mu+\sigma^2t\right)\left(\frac{1}{(\lambda-t)^2} + \sigma^2\right) + \frac{2}{(\lambda-t)^3}\right] \; .
\end{equation}

Applying \eqref{eq:exg-skew-exg-moment}, together with \eqref{eq:exg-skew-exg-skew-s3}, yields:

\begin{equation} \label{eq:exg-skew-exg-skew-s4}
\begin{split}
\mathrm{E}(X^3) &= M_X'''(0)\\
&= M_X'(0)\left[\left(\frac{1}{\lambda}+\mu\right)^2 + \frac{1}{\lambda^2} + \sigma^2\right] + M_X(0)\left[2\left(\frac{1}{\lambda}+\mu\right)\left(\frac{1}{\lambda^2}+\sigma^2\right)+\frac{2}{\lambda^3}\right]\\
&= \left(\mu + \frac{1}{\lambda}\right)\left(\frac{1}{\lambda^2}+\frac{2\mu}{\lambda} + \mu^2 + \frac{1}{\lambda^2}+\sigma^2\right) + \left(\frac{2}{\lambda^3}+\frac{2\sigma^2}{\lambda} + \frac{2\mu}{\lambda^2}+2\mu\sigma^2 + \frac{2}{\lambda^3}\right)\\
&= \left(\mu+\frac{1}{\lambda}\right)\left(\frac{2}{\lambda^2}+\frac{2\mu}{\lambda} + \mu^2+\sigma^2\right) + \left(\frac{4}{\lambda^3}+\frac{2\sigma^2}{\lambda} + \frac{2\mu}{\lambda^2} + 2\mu\sigma^2\right)\\
&= \frac{2\mu}{\lambda^2} + \frac{2\mu^2}{\lambda} + \mu^3 + \mu\sigma^2 + \frac{2}{\lambda^3} + \frac{2\mu}{\lambda^2} + \frac{\mu^2}{\lambda} + \frac{\sigma^2}{\lambda} + \frac{4}{\lambda^3} + \frac{2\sigma^2}{\lambda} + \frac{2\mu}{\lambda^2} + 2\mu\sigma^2\\
&= \frac{6\mu}{\lambda^2} + \frac{6}{\lambda^3} + \frac{3\mu^2+ 3\sigma^2}{\lambda} + 3\mu\sigma^2 + \mu^3 \; .
\end{split}
\end{equation}

We now substitute \eqref{eq:exg-skew-exg-skew-s4}, \eqref{eq:exg-skew-exg-mean}, and \eqref{eq:exg-skew-exg-var} into the numerator of \eqref{eq:exg-skew-skew-mean-alt}, giving

\begin{equation} \label{eq:exg-skew-exg-skew-s5}
\begin{split}
\mathrm{E}(X^3) - 3\cdot \mathrm{E}(X)\cdot \mathrm{Var}(X) - \mathrm{E}(X)^3 &= \left(\frac{6\mu}{\lambda^2} + \frac{6}{\lambda^3} + \frac{3\mu^2+ 3\sigma^2}{\lambda} + 3\mu\sigma^2 + \mu^3\right) - 3\left(\mu+\frac{1}{\lambda}\right)\left(\sigma^2+\frac{1}{\lambda^2}\right)- \left(\mu+\frac{1}{\lambda}\right)^3\\
&= \frac{6\mu}{\lambda^2} + \frac{6}{\lambda^3} + \frac{3\mu^2+ 3\sigma^2}{\lambda} + 3\mu\sigma^2 + \mu^3 - 3\mu\sigma^2 - \frac{3\mu}{\lambda^2} - \frac{3\sigma^2}{\lambda} - \frac{3}{\lambda^3} - \mu^3 - \frac{3\mu^2}{\lambda} - \frac{3\mu}{\lambda^2}-\frac{1}{\lambda^3}\\
&= \frac{2}{\lambda^3} \; .
\end{split}
\end{equation}

Thus, we have:

\begin{equation} \label{eq:exg-skew-exg-skew-s6}
\begin{split}
\mathrm{Skew}(X) &= \frac{\mathrm{E}(X^3) - 3\cdot \mathrm{E}(X)\cdot \mathrm{Var}(X) - \mathrm{E}(X)^3}{\mathrm{Var}(X)^{\frac{3}{2}}}\\
&= \frac{\frac{2}{\lambda^3}}{\left(\sigma^2 + \frac{1}{\lambda^2}\right)^{\frac{3}{2}}}\\
&= \frac{2}{\lambda^3\left(\sigma^2+\frac{1}{\lambda^2}\right)^{\frac{3}{2}}} \; .
\end{split}
\end{equation}

This completes the proof of \eqref{eq:exg-skew-exg-skew}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Method of moments}]{Method of moments} \label{sec:exg-mome}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ be a set of observed data independent and identically distributed according to an ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg}) with parameters $\mu$, $\sigma$, and $\lambda$:

\begin{equation} \label{eq:exg-mome-exq}
y_i \sim \mathrm{ex-Gaussian}(\mu,\sigma,\lambda), \quad i = 1, \ldots, n \; .
\end{equation}

Then, the method-of-moments estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mome}) for the parameters $\mu$, $\sigma$, and $\lambda$ are given by

\begin{equation} \label{eq:exg-mome-exg-MoM}
\begin{split}
\hat{\mu} &= \bar{y} - \sqrt[3]{\frac{\bar{s}\cdot \bar{v}^{3/2}}{2}}\\
\hat{\sigma} &= \sqrt{\bar{v}\cdot\left(1 - \sqrt[3]{\frac{\bar{s}^2}{4}}\right)}\\
\hat{\lambda} &= \sqrt[3]{\frac{2}{\bar{s}\cdot \bar{v}^{3/2}}} \; ,
\end{split}
\end{equation}

where $\bar{y}$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}), $\bar{v}$ is the sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) and $\bar{s}$ is the sample skewness ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:skew-samp})

\begin{equation} \label{eq:exg-mome-y-mean-var-skew}
\begin{split}
\bar{y} &= \frac{1}{n} \sum_{i=1}^n y_i \\
\bar{v} &= \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2 \\
\bar{s} &= \frac{\frac{1}{n}\sum_{i=1}^n (y_i-\bar{y})^3}{\left[\frac{1}{n}\sum_{i=1}^n(y_i-\bar{y})^2\right]^{3/2}} \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} The mean ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg-mean}), variance ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg-var}), and skewness ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg-skew}) of the ex-Gaussian distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:exg}) in terms of the parameters $\mu$, $\sigma$, and $\lambda$ are given by

\begin{equation} \label{eq:exg-mome-exg-E-Var-Skew}
\begin{split}
\mathrm{E}(X) &= \mu + \frac{1}{\lambda} \\
\mathrm{Var}(X) &= \sigma^2 + \frac{1}{\lambda^2}\\
\mathrm{Skew}(X) &= \frac{2}{\lambda^3\left(\sigma^2+\frac{1}{\lambda^2}\right)^{3/2}} \; .
\end{split}
\end{equation}

Thus, matching the moments ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mome}) requires us to solve the following system of equations for $\mu$, $\sigma$, and $\lambda$:

\begin{equation} \label{eq:exg-mome-exg-mean-var-skew}
\begin{split}
\bar{y} &= \mu + \frac{1}{\lambda} \\
\bar{v} &= \sigma^2 + \frac{1}{\lambda^2}\\
\bar{s} &= \frac{2}{\lambda^3\left(\sigma^2+\frac{1}{\lambda^2}\right)^{3/2}} \; .
\end{split}
\end{equation}

To this end, our first step is to substitute the second equation of \eqref{eq:exg-mome-exg-mean-var-skew} into the third equation:

\begin{equation} \label{eq:exg-mome-lambda-s1}
\begin{split}
\bar{s} &= \frac{2}{\lambda^3\left(\sigma^2+\frac{1}{\lambda^2}\right)^{3/2}} \\
&= \frac{2}{\lambda^3\cdot \bar{v}^{3/2}} \; .
\end{split}
\end{equation}

Re-expressing \eqref{eq:exg-mome-lambda-s1} in terms of $\lambda^3$ and taking the cube root gives:

\begin{equation} \label{eq:exg-mome-lambda-s2}
\lambda = \sqrt[3]{\frac{2}{\bar{s}\cdot \bar{v}^{3/2}}} \; .
\end{equation}

Next, we solve the first equation of \eqref{eq:exg-mome-exg-mean-var-skew} for $\mu$ and substitute \eqref{eq:exg-mome-lambda-s2}:

\begin{equation} \label{eq:exg-mome-mu-s1}
\begin{split}
\mu &= \bar{y} - \frac{1}{\lambda}\\
&= \bar{y} - \sqrt[3]{\frac{\bar{s}\cdot\bar{v}^{3/2}}{2}} \; .
\end{split}
\end{equation}

Finally, we solve the second equation of \eqref{eq:exg-mome-exg-mean-var-skew} for $\sigma$:

\begin{equation} \label{eq:exg-mome-sigma-s1}
\sigma^2 = \bar{v} - \frac{1}{\lambda^2} \; .
\end{equation}

Taking the square root gives and substituting \eqref{eq:exg-mome-lambda-s2} gives:

\begin{equation} \label{eq:exg-mome-sigma-s2}
\begin{split}
\sigma &= \sqrt{\bar{v}-\frac{1}{\lambda^2}} \\
&= \sqrt{\bar{v} - \left(\sqrt[3]{\frac{\bar{s}\cdot \bar{v}^{3/2}}{2}}\right)^2} \\
&= \sqrt{\bar{v} - \bar{v}\cdot\sqrt[3]{\frac{\bar{s}^2}{4}}} \\
&= \sqrt{\bar{v}\cdot \left(1- \sqrt[3]{\frac{\bar{s}^2}{4}}\right)} \; .
\end{split}
\end{equation}

Together, \eqref{eq:exg-mome-mu-s1}, \eqref{eq:exg-mome-sigma-s2}, and \eqref{eq:exg-mome-lambda-s2} constitute the method-of-moment estimates of $\mu$, $\sigma$, and $\lambda$.

\begin{flushright} $\blacksquare$ \end{flushright}



\pagebreak
\section{Multivariate continuous distributions}

\subsection{Multivariate normal distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:mvn}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, $X$ is said to be multivariate normally distributed with mean $\mu$ and covariance $\Sigma$

\begin{equation} \label{eq:mvn-mvn}
X \sim \mathcal{N}(\mu, \Sigma) \; ,
\end{equation}

if and only if its probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) is given by

\begin{equation} \label{eq:mvn-mvn-pdf}
\mathcal{N}(x; \mu, \Sigma) = \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right]
\end{equation}

where $\mu$ is an $n \times 1$ real vector and $\Sigma$ is an $n \times n$ positive definite matrix.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Koch KR (2007): "Multivariate Normal Distribution"; in: \textit{Introduction to Bayesian Statistics}, ch. 2.5.1, pp. 51-53, eq. 2.195; URL: \url{https://www.springer.com/gp/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Special case of matrix-normal distribution}]{Special case of matrix-normal distribution} \label{sec:mvn-matn}
\setcounter{equation}{0}

\textbf{Theorem:} The multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) is a special case of the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) with number of variables $p = 1$, i.e. random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $X = x \in \mathbb{R}^{n \times 1}$, mean $M = \mu \in \mathbb{R}^{n \times 1}$, covariance across rows $U = \Sigma$ and covariance across columns $V = 1$.


\vspace{1em}
\textbf{Proof:} The probability density function of the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-pdf}) is

\begin{equation} \label{eq:mvn-matn-matn-pdf}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( V^{-1} (X-M)^\mathrm{T} \, U^{-1} (X-M) \right) \right] \; .
\end{equation}

Setting $p = 1$, $X = x$, $M = \mu$, $U = \Sigma$ and $V = 1$, we obtain

\begin{equation} \label{eq:mvn-matn-mvn-pdf}
\begin{split}
\mathcal{MN}(x; \mu, \Sigma, 1) &= \frac{1}{\sqrt{(2\pi)^{n} |1|^n |\Sigma|^1}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( 1^{-1} (x-\mu)^\mathrm{T} \, \Sigma^{-1} (x-\mu) \right) \right] \\
&= \frac{1}{\sqrt{(2\pi)^{n} |\Sigma|}} \cdot \exp\left[-\frac{1}{2} (x-\mu)^\mathrm{T} \, \Sigma^{-1} (x-\mu) \right]
\end{split}
\end{equation}

which is equivalent to the probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}).
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Matrix normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-07-31; URL: \url{https://en.wikipedia.org/wiki/Matrix_normal_distribution#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relationship to chi-squared distribution}]{Relationship to chi-squared distribution} \label{sec:mvn-chi2}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) with zero mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-rvec}) and arbitrary covariance matrix ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-cov}) $\Sigma$:

\begin{equation} \label{eq:mvn-chi2-mvn}
x \sim \mathcal{N}(0, \Sigma) \; .
\end{equation}

Then, the quadratic form of $x$, weighted by $\Sigma$, follows a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) with $n$ degrees of freedom:

\begin{equation} \label{eq:mvn-chi2-mvn-chi2}
y = x^\mathrm{T} \Sigma^{-1} x \sim \chi^2(n) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Define a new random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $z$ as

\begin{equation} \label{eq:mvn-chi2-z}
z = \Sigma^{-1/2} x \; .
\end{equation}

where $\Sigma^{-1/2}$ is the matrix square root of $\Sigma$. This matrix must exist, because $\Sigma$ is a covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) and thus positive semi-definite ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat-psd}). Due to the linear transformation theorem ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}), $z$ is distributed as

\begin{equation} \label{eq:mvn-chi2-z-dist}
\begin{split}
z &\sim \mathcal{N}\left( \Sigma^{-1/2} 0, \, \Sigma^{-1/2} \Sigma \, {\Sigma^{-1/2}}^\mathrm{T} \right) \\
&\sim \mathcal{N}\left( \Sigma^{-1/2} 0, \, \Sigma^{-1/2} \Sigma^{1/2} \Sigma^{1/2} \Sigma^{-1/2} \right) \\
&\sim \mathcal{N}(0, I_n) \; ,
\end{split}
\end{equation}

i.e. each entry of this vector follows ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-marg}) a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}):

\begin{equation} \label{eq:mvn-chi2-zi-dist}
z_i \sim \mathcal{N}(0, 1) \quad \text{for all} \quad i = 1, \ldots, n \; .
\end{equation}

We further observe that $y$ can be represented in terms of $z$

\begin{equation} \label{eq:mvn-chi2-y-z}
y = x^\mathrm{T} \Sigma^{-1} x = \left( x^\mathrm{T} \Sigma^{-1/2} \right) \left( \Sigma^{-1/2} x \right) = z^\mathrm{T} z \; ,
\end{equation}

thus $z$ is a sum of $n$ squared standard normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar})

\begin{equation} \label{eq:mvn-chi2-y-z-sum}
y = \sum_{i=1}^{n} z_i^2 \quad \text{where all} \quad z_i \sim \mathcal{N}(0, 1)
\end{equation}

which, by definition, is chi-squared distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) with $n$ degrees of freedom:

\begin{equation} \label{eq:mvn-chi2-mvn-chi2-qed}
y \sim \chi^2(n) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Koch KR (2007): "Chi-Squared Distribution"; in: \textit{Introduction to Bayesian Statistics}, ch. 2.4.5, pp. 48-49, eq. 2.180; URL: \url{https://www.springer.com/gp/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Bivariate normal distribution}]{Bivariate normal distribution} \label{sec:bvn}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be an $2 \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, $X$ is said to have a bivariate normal distribution, if $X$ follows a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn})

\begin{equation} \label{eq:bvn-mvn}
X \sim \mathcal{N}(\mu, \Sigma)
\end{equation}

with means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $x_1$ and $x_2$, variances ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) $\sigma_1^2$ and $\sigma_2^2$ and covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) $\sigma_{12}$:

\begin{equation} \label{eq:bvn-bvn}
\mu = \left[ \begin{matrix} x_1 \\ x_2 \end{matrix} \right] \quad \text{and} \quad \Sigma = \left[ \begin{matrix} \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2 \end{matrix} \right] \; .
\end{equation}


\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2023): "Multivariate normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2023-09-22; URL: \url{https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Bivariate_case}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function of the bivariate normal distribution}]{Probability density function of the bivariate normal distribution} \label{sec:bvn-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X = \left[ \begin{matrix} X_1 \\\\ X_2 \end{matrix} \right]$ follow a bivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bvn}):

\begin{equation} \label{eq:bvn-pdf-bvn}
X \sim \mathcal{N}\left( \mu = \left[ \begin{matrix} \mu_1 \\ \mu_2 \end{matrix} \right], \Sigma = \left[ \begin{matrix} \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2 \end{matrix} \right] \right) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ is:

\begin{equation} \label{eq:bvn-pdf-bvn-pdf}
f_X(x) = \frac{1}{2 \pi \sqrt{\sigma_1^2 \sigma_2^2 - \sigma_{12}^2}} \cdot \exp \left[ -\frac{1}{2} \frac{\sigma_2^2 (x_1-\mu_1)^2 - 2 \sigma_{12} (x_1-\mu_1)(x_2-\mu_2) + \sigma_1^2 (x_2-\mu_2)^2}{\sigma_1^2 \sigma_2^2 - \sigma_{12}^2} \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}) for an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $x$ is:

\begin{equation} \label{eq:bvn-pdf-mvn-pdf}
f_X(x) = \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right] \; .
\end{equation}

Plugging in $n = 2$, $\mu = \left[ \begin{matrix} \mu_1 \\\\ \mu_2 \end{matrix} \right]$ and $\Sigma = \left[ \begin{matrix} \sigma_1^2 & \sigma_{12} \\\\ \sigma_{12} & \sigma_2^2 \end{matrix} \right]$, we obtain:

\begin{equation} \label{eq:bvn-pdf-bvn-pdf-s1}
\begin{split}
f_X(x) &= \frac{1}{\sqrt{(2 \pi)^2 \left| \left[ \begin{matrix} \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2 \end{matrix} \right] \right|}} \cdot \exp \left[ -\frac{1}{2} \left( \left[ \begin{matrix} x_1 \\ x_2 \end{matrix} \right] - \left[ \begin{matrix} \mu_1 \\ \mu_2 \end{matrix} \right] \right)^\mathrm{T} \left[ \begin{matrix} \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2 \end{matrix} \right]^{-1} \left( \left[ \begin{matrix} x_1 \\ x_2 \end{matrix} \right] - \left[ \begin{matrix} \mu_1 \\ \mu_2 \end{matrix} \right] \right) \right] \\
&= \frac{1}{2 \pi \left| \left[ \begin{matrix} \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2 \end{matrix} \right] \right|^\frac{1}{2}} \cdot \exp \left[ -\frac{1}{2} \left[ \begin{matrix} (x_1-\mu_1) & (x_2-\mu_2) \end{matrix} \right] \left[ \begin{matrix} \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2 \end{matrix} \right]^{-1} \left[ \begin{matrix} (x_1-\mu_1) \\ (x_2-\mu_2) \end{matrix} \right] \right] \; .
\end{split}
\end{equation}

Using the determinant of a $2 \times 2$ matrix

\begin{equation} \label{eq:bvn-pdf-det-2x2}
\left| \left[ \begin{matrix} a & b \\ c & d \end{matrix} \right] \right| = a d - b c
\end{equation}

and the inverse of of a $2 \times 2$ matrix

\begin{equation} \label{eq:bvn-pdf-inv-2x2}
\left[ \begin{matrix} a & b \\ c & d \end{matrix} \right]^{-1} = \frac{1}{a d - b c} \left[ \begin{matrix} d & -b \\ -c & a \end{matrix} \right] \; ,
\end{equation}

the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) becomes:

\begin{equation} \label{eq:bvn-pdf-bvn-pdf-s2}
\begin{split}
f_X(x) &= \frac{1}{2 \pi \sqrt{\sigma_1^2 \sigma_2^2 - \sigma_{12}^2}} \cdot \exp \left[ -\frac{1}{2 (\sigma_1^2 \sigma_2^2 - \sigma_{12}^2)} \left[ \begin{matrix} (x_1-\mu_1) & (x_2-\mu_2) \end{matrix} \right] \left[ \begin{matrix} \sigma_2^2 & -\sigma_{12} \\ -\sigma_{12} & \sigma_1^2 \end{matrix} \right] \left[ \begin{matrix} (x_1-\mu_1) \\ (x_2-\mu_2) \end{matrix} \right] \right] \\
&= \frac{1}{2 \pi \sqrt{\sigma_1^2 \sigma_2^2 - \sigma_{12}^2}} \cdot \exp \left[ -\frac{1}{2 (\sigma_1^2 \sigma_2^2 - \sigma_{12}^2)} \left[ \begin{matrix} \sigma_2^2 (x_1-\mu_1) - \sigma_{12} (x_2-\mu_2) & \sigma_1^2 (x_2-\mu_2) - \sigma_{12} (x_1-\mu_1) \end{matrix} \right] \left[ \begin{matrix} (x_1-\mu_1) \\ (x_2-\mu_2) \end{matrix} \right] \right] \\
&= \frac{1}{2 \pi \sqrt{\sigma_1^2 \sigma_2^2 - \sigma_{12}^2}} \cdot \exp \left[ -\frac{1}{2 (\sigma_1^2 \sigma_2^2 - \sigma_{12}^2)} (\sigma_2^2 (x_1-\mu_1)^2 - \sigma_{12} (x_1-\mu_1)(x_2-\mu_2) + \sigma_1^2 (x_2-\mu_2)^2 - \sigma_{12} (x_1-\mu_1)(x_2-\mu_2)) \right] \\
&= \frac{1}{2 \pi \sqrt{\sigma_1^2 \sigma_2^2 - \sigma_{12}^2}} \cdot \exp \left[ -\frac{1}{2} \frac{\sigma_2^2 (x_1-\mu_1)^2 - 2 \sigma_{12} (x_1-\mu_1)(x_2-\mu_2) + \sigma_1^2 (x_2-\mu_2)^2}{\sigma_1^2 \sigma_2^2 - \sigma_{12}^2} \right] \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Probability density function in terms of correlation coefficient}]{Probability density function in terms of correlation coefficient} \label{sec:bvn-pdfcorr}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X = \left[ \begin{matrix} X_1 \\\\ X_2 \end{matrix} \right]$ follow a bivariate normal distribution:

\begin{equation} \label{eq:bvn-pdfcorr-bvn}
X \sim \mathcal{N}\left( \mu = \left[ \begin{matrix} \mu_1 \\ \mu_2 \end{matrix} \right], \Sigma = \left[ \begin{matrix} \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2 \end{matrix} \right] \right) \; .
\end{equation}

Then, the probability density function of $X$ is

\begin{equation} \label{eq:bvn-pdfcorr-bvn-pdf}
f_X(x) = \frac{1}{2 \pi \, \sigma_1 \sigma_2 \sqrt{1 - \rho^2}} \cdot \exp \left[ -\frac{1}{2 (1 - \rho^2)} \left( \left( \frac{x_1-\mu_1}{\sigma_1} \right)^2 - 2 \rho \frac{(x_1-\mu_1) (x_2-\mu_2)}{\sigma_1 \sigma_2} + \left( \frac{x_2-\mu_2}{\sigma_2} \right)^2 \right) \right]
\end{equation}

where $\rho$ is the correlation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr}) between $X_1$ and $X_2$.


\vspace{1em}
\textbf{Proof:} Since $X$ follows a special case of the multivariate normal distribution, its covariance matrix is ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-cov})

\begin{equation} \label{eq:bvn-pdfcorr-cov-X}
\mathrm{Cov}(X) = \Sigma = \left[ \begin{matrix} \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2 \end{matrix} \right]
\end{equation}

and the covariance matrix can be decomposed into correlation matrix and standard deviations ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat-corrmat}):

\begin{equation} \label{eq:bvn-pdfcorr-Sigma}
\begin{split}
\Sigma &= \left[ \begin{matrix} \sigma_1^2 & \rho \, \sigma_1 \sigma_2 \\ \rho \, \sigma_1 \sigma_2 & \sigma_2^2 \end{matrix} \right] \\
&= \left[ \begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2 \end{matrix} \right] \left[ \begin{matrix} 1 & \rho \\ \rho & 1 \end{matrix} \right] \left[ \begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2 \end{matrix} \right] \; .
\end{split}
\end{equation}

The determinant of this matrix is

\begin{equation} \label{eq:bvn-pdfcorr-Sigma-det}
\begin{split}
\left| \Sigma \right| &= \left| \left[ \begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2 \end{matrix} \right] \left[ \begin{matrix} 1 & \rho \\ \rho & 1 \end{matrix} \right] \left[ \begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2 \end{matrix} \right] \right| \\
&= \left| \left[ \begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2 \end{matrix} \right] \right| \cdot \left| \left[ \begin{matrix} 1 & \rho \\ \rho & 1 \end{matrix} \right] \right| \cdot \left| \left[ \begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2 \end{matrix} \right] \right| \\
&= (\sigma_1 \sigma_2) (1 - \rho^2) (\sigma_1 \sigma_2) \\
&= \sigma_1^2 \sigma_2^2 (1 - \rho^2)
\end{split}
\end{equation}

and the inverse of this matrix is

\begin{equation} \label{eq:bvn-pdfcorr-Sigma-inv}
\begin{split}
\Sigma^{-1} &= \left( \left[ \begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2 \end{matrix} \right] \left[ \begin{matrix} 1 & \rho \\ \rho & 1 \end{matrix} \right] \left[ \begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2 \end{matrix} \right] \right)^{-1} \\
&= \left[ \begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2 \end{matrix} \right]^{-1} \left[ \begin{matrix} 1 & \rho \\ \rho & 1 \end{matrix} \right]^{-1} \left[ \begin{matrix} \sigma_1 & 0 \\ 0 & \sigma_2 \end{matrix} \right]^{-1} \\
&= \frac{1}{1 - \rho^2} \left[ \begin{matrix} 1/\sigma_1 & 0 \\ 0 & 1/\sigma_2 \end{matrix} \right] \left[ \begin{matrix} 1 & -\rho \\ -\rho & 1 \end{matrix} \right] \left[ \begin{matrix} 1/\sigma_1 & 0 \\ 0 & 1/\sigma_2 \end{matrix} \right] \; .
\end{split}
\end{equation}

The probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}) for an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $x$ is:

\begin{equation} \label{eq:bvn-pdfcorr-mvn-pdf}
f_X(x) = \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right] \; .
\end{equation}

Plugging in $n = 2$, $\mu$ from \eqref{eq:bvn-pdfcorr-bvn} and $\Sigma$ from \eqref{eq:bvn-pdfcorr-Sigma-det} and \eqref{eq:bvn-pdfcorr-Sigma-inv}, the probability density function becomes:

\begin{equation} \label{eq:bvn-pdfcorr-bvn-pdf-corr}
\begin{split}
f_X(x) &= \frac{1}{\sqrt{(2 \pi)^2 \sigma_1^2 \sigma_2^2 (1 - \rho^2)}} \cdot \exp \left[ -\frac{1}{2} \left( \left[ \begin{matrix} x_1 \\ x_2 \end{matrix} \right] - \left[ \begin{matrix} \mu_1 \\ \mu_2 \end{matrix} \right] \right)^\mathrm{T} \frac{1}{1 - \rho^2} \left[ \begin{matrix} 1/\sigma_1 & 0 \\ 0 & 1/\sigma_2 \end{matrix} \right] \left[ \begin{matrix} 1 & -\rho \\ -\rho & 1 \end{matrix} \right] \left[ \begin{matrix} 1/\sigma_1 & 0 \\ 0 & 1/\sigma_2 \end{matrix} \right] \left( \left[ \begin{matrix} x_1 \\ x_2 \end{matrix} \right] - \left[ \begin{matrix} \mu_1 \\ \mu_2 \end{matrix} \right] \right) \right] \\
&= \frac{1}{2 \pi \, \sigma_1 \sigma_2 \sqrt{1 - \rho^2}} \cdot \exp \left[ -\frac{1}{2 (1 - \rho^2)} \left[ \begin{matrix} \frac{x_1-\mu_1}{\sigma_1} & \frac{x_2-\mu_2}{\sigma_2} \end{matrix} \right] \left[ \begin{matrix} 1 & -\rho \\ -\rho & 1 \end{matrix} \right] \left[ \begin{matrix} \frac{x_1-\mu_1}{\sigma_1} \\ \frac{x_2-\mu_2}{\sigma_2} \end{matrix} \right]  \right] \\
&= \frac{1}{2 \pi \, \sigma_1 \sigma_2 \sqrt{1 - \rho^2}} \cdot \exp \left[ -\frac{1}{2 (1 - \rho^2)} \left[ \begin{matrix} \left( \frac{x_1-\mu_1}{\sigma_1} - \rho \frac{x_2-\mu_2}{\sigma_2} \right) & \left( \frac{x_2-\mu_2}{\sigma_2} - \rho \frac{x_1-\mu_1}{\sigma_1} \right) \end{matrix} \right] \left[ \begin{matrix} \frac{x_1-\mu_1}{\sigma_1} \\ \frac{x_2-\mu_2}{\sigma_2} \end{matrix} \right]  \right] \\
&= \frac{1}{2 \pi \, \sigma_1 \sigma_2 \sqrt{1 - \rho^2}} \cdot \exp \left[ -\frac{1}{2 (1 - \rho^2)} \left( \left( \frac{x_1-\mu_1}{\sigma_1} \right)^2 - 2 \rho \frac{(x_1-\mu_1) (x_2-\mu_2)}{\sigma_1 \sigma_2} + \left( \frac{x_2-\mu_2}{\sigma_2} \right)^2 \right) \right] \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2023): "Multivariate normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2023-09-29; URL: \url{https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Bivariate_case}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:mvn-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}):

\begin{equation} \label{eq:mvn-pdf-mvn}
X \sim \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ is

\begin{equation} \label{eq:mvn-pdf-mvn-pdf}
f_X(x) = \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Moment-generating function}]{Moment-generating function} \label{sec:mvn-mgf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ follow a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}):

\begin{equation} \label{eq:mvn-mgf-mvn}
x \sim \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Then, the moment-generating function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) of $x$ is

\begin{equation} \label{eq:mvn-mgf-mvn-mgf}
M_x(t) = \exp \left[ t^\mathrm{T} \mu + \frac{1}{2} t^\mathrm{T} \Sigma t \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The moment-generating function of a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) $X$ is defined as:

\begin{equation} \label{eq:mvn-mgf-mgf}
M_X(t) = \mathrm{E} \left[ e^{t^\mathrm{T}X} \right], \quad t \in \mathbb{R}^n \; .
\end{equation}

Applying the law of the unconscious statistician ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lotus}), we have:

\begin{equation} \label{eq:mvn-mgf-mvn-mgf-s1}
M_x(t) = \int_{\mathcal{X}} e^{t^\mathrm{T}x} \cdot f_X(x) \, \mathrm{d}x \; .
\end{equation}

With the probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}), we have:

\begin{equation} \label{eq:mvn-mgf-mvn-mgf-s2}
M_x(t) = \int_{\mathbb{R}^n} \exp \left[ t^\mathrm{T}x \right] \cdot \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right] \, \mathrm{d}x \; .
\end{equation}

Now we summarize the two exponential functions inside the integral:

\begin{equation} \label{eq:mvn-mgf-mvn-mgf-s3}
\begin{split}
M_x(t) &= \int_{\mathbb{R}^n} \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) + t^\mathrm{T}x \right] \, \mathrm{d}x \\
&= \int_{\mathbb{R}^n} \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} \left( x^\mathrm{T} \Sigma^{-1} x - 2 \mu^\mathrm{T} \Sigma^{-1} x + \mu^\mathrm{T} \Sigma^{-1} \mu - 2 t^\mathrm{T}x \right) \right] \, \mathrm{d}x \\
&= \int_{\mathbb{R}^n} \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} \left( x^\mathrm{T} \Sigma^{-1} x - 2 (\mu + \Sigma t)^\mathrm{T} \Sigma^{-1} x + \mu^\mathrm{T} \Sigma^{-1} \mu \right) \right] \, \mathrm{d}x \\
&= \int_{\mathbb{R}^n} \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} \left( (x - [\mu + \Sigma t])^\mathrm{T} \Sigma^{-1} (x - [\mu + \Sigma t]) - 2 t^\mathrm{T} \mu - t^\mathrm{T} \Sigma t \right) \right] \, \mathrm{d}x \\
&= \exp \left[ t^\mathrm{T} \mu + t^\mathrm{T} \Sigma t \right] \int_{\mathbb{R}^n} \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x - [\mu + \Sigma t])^\mathrm{T} \Sigma^{-1} (x - [\mu + \Sigma t]) \right] \, \mathrm{d}x \; .
\end{split}
\end{equation}

The integrand is equal to the probability density function of a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}):

\begin{equation} \label{eq:mvn-mgf-mvn-mgf-s4}
M_x(t) = \exp \left[ t^\mathrm{T} \mu + t^\mathrm{T} \Sigma t \right] \int_{\mathbb{R}^n} \mathcal{N}(x; \mu + \Sigma t, \Sigma) \, \mathrm{d}x \; .
\end{equation}

Because the entire probability density integrates to one ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}), we finally have:

\begin{equation} \label{eq:mvn-mgf-mvn-mgf-s5}
M_x(t) = \exp \left[ t^\mathrm{T} \mu + t^\mathrm{T} \Sigma t \right] \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:mvn-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ follow a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}):

\begin{equation} \label{eq:mvn-mean-mvn}
x \sim \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $x$ is

\begin{equation} \label{eq:mvn-mean-mvn-mean}
\mathrm{E}(x) = \mu \; .
\end{equation}


\vspace{1em}
\textbf{Proof:}

1) First, consider a set of independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) and standard normally ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) distributed random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}):

\begin{equation} \label{eq:mvn-mean-zi}
z_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0,1), \quad i = 1,\ldots,n \; .
\end{equation}

Then, these variables together form a multivariate normally ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ind}) distributed random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}):

\begin{equation} \label{eq:mvn-mean-z}
z \sim \mathcal{N}(0_n, I_n) \; .
\end{equation}

By definition, the expected value of a random vector is equal to the vector of all expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-rvec}):

\begin{equation} \label{eq:mvn-mean-mean-rvec}
\mathrm{E}(z) = \mathrm{E}\left( \left[ \begin{array}{c} z_1 \\ \vdots \\ z_n \end{array} \right] \right) = \left[ \begin{array}{c} \mathrm{E}(z_1) \\ \vdots \\ \mathrm{E}(z_n) \end{array} \right] \; .
\end{equation}

Because the expected value of all its entries is zero ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-mean}), the expected value of the random vector is

\begin{equation} \label{eq:mvn-mean-z-mean}
\mathrm{E}(z) = \left[ \begin{array}{c} \mathrm{E}(z_1) \\ \vdots \\ \mathrm{E}(z_n) \end{array} \right] = \left[ \begin{array}{c} 0 \\ \vdots \\ 0 \end{array} \right] = 0_n \; .
\end{equation}

2) Next, consider an $n \times n$ matrix $A$ solving the equation $A A^\mathrm{T} = \Sigma$. Such a matrix exists, because $\Sigma$ is defined to be positive definite ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}). Then, $x$ can be represented as a linear transformation of ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}) $z$:

\begin{equation} \label{eq:mvn-mean-x-z}
x = Az + \mu \sim \mathcal{N}(A 0_n + \mu, A I_n A^\mathrm{T}) = \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Thus, the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $x$ can be written as:

\begin{equation} \label{eq:mvn-mean-x-mean}
\mathrm{E}(x) = \mathrm{E}( Az + \mu ) \; .
\end{equation}

With the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}), this becomes:

\begin{equation} \label{eq:mvn-mean-mvn-mean-qed}
\begin{split}
\mathrm{E}(x) &= \mathrm{E}( Az + \mu ) \\
&= \mathrm{E}(Az) + \mathrm{E}(\mu) \\
&= A \, \mathrm{E}(z) + \mu \\
&\overset{\eqref{eq:mvn-mean-z-mean}}{=} A \, 0_n + \mu \\
&= \mu \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2021): "Multivariate normal distribution"; in: \textit{Lectures on probability theory and mathematical statistics}, retrieved on 2022-09-15; URL: \url{https://www.statlect.com/probability-distributions/multivariate-normal-distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Covariance}]{Covariance} \label{sec:mvn-cov}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ follow a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}):

\begin{equation} \label{eq:mvn-cov-mvn}
x \sim \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Then, the covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of $x$ is

\begin{equation} \label{eq:mvn-cov-mvn-cov}
\mathrm{Cov}(x) = \Sigma \; .
\end{equation}


\vspace{1em}
\textbf{Proof:}

1) First, consider a set of independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) and standard normally ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) distributed random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}):

\begin{equation} \label{eq:mvn-cov-zi}
z_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0,1), \quad i = 1,\ldots,n \; .
\end{equation}

Then, these variables together form a multivariate normally ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ind}) distributed random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}):

\begin{equation} \label{eq:mvn-cov-z}
z \sim \mathcal{N}(0_n, I_n) \; .
\end{equation}

Because the covariance is zero for independent random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-ind}), we have

\begin{equation} \label{eq:mvn-cov-zij-cov}
\mathrm{Cov}(z_i,z_j) = 0 \quad \text{for all} \quad i \neq j \; .
\end{equation}

Moreover, as the variance of all entries of the vector is one ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-var}), we have

\begin{equation} \label{eq:mvn-cov-zi-var}
\mathrm{Var}(z_i) = 1 \quad \text{for all} \quad i = 1, \ldots, n \; .
\end{equation}

Taking \eqref{eq:mvn-cov-zij-cov} and \eqref{eq:mvn-cov-zi-var} together, the covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of $z$ is

\begin{equation} \label{eq:mvn-cov-z-cov}
\mathrm{Cov}(z) = \left[ \begin{array}{ccc} 1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 1 \end{array} \right] = I_n \; .
\end{equation}

2) Next, consider an $n \times n$ matrix $A$ solving the equation $A A^\mathrm{T} = \Sigma$. Such a matrix exists, because $\Sigma$ is defined to be positive definite ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}). Then, $x$ can be represented as a linear transformation of ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}) $z$:

\begin{equation} \label{eq:mvn-cov-x-z}
x = Az + \mu \sim \mathcal{N}(A 0_n + \mu, A I_n A^\mathrm{T}) = \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Thus, the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of $x$ can be written as:

\begin{equation} \label{eq:mvn-cov-x-mean}
\mathrm{Cov}(x) = \mathrm{Cov}( Az + \mu ) \; .
\end{equation}

With the invariance of the covariance matrix under addition ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat-inv})

\begin{equation} \label{eq:mvn-cov-cov-inv}
\mathrm{Cov}(x + a) = \mathrm{Cov}(x)
\end{equation}

and the scaling of the covariance matrix upon multiplication ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat-scal})

\begin{equation} \label{eq:mvn-cov-cov-scal}
\mathrm{Cov}(Ax) = A \mathrm{Cov}(x) A^\mathrm{T} \; ,
\end{equation}

this becomes:

\begin{equation} \label{eq:mvn-cov-mvn-cov-qed}
\begin{split}
\mathrm{Cov}(x) &= \mathrm{Cov}( Az + \mu ) \\
&\overset{\eqref{eq:mvn-cov-cov-inv}}{=} \mathrm{Cov}(Az) \\
&\overset{\eqref{eq:mvn-cov-cov-scal}}{=} A \, \mathrm{Cov}(z) A^\mathrm{T} \\
&\overset{\eqref{eq:mvn-cov-z-cov}}{=} A I_n A^\mathrm{T} \\
&= A A^\mathrm{T} \\
&= \Sigma \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Rosenfeld, Meni (2016): "Deriving the Covariance of Multivariate Gaussian"; in: \textit{StackExchange Mathematics}, retrieved on 2022-09-15; URL: \url{https://math.stackexchange.com/questions/1905977/deriving-the-covariance-of-multivariate-gaussian}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Differential entropy}]{Differential entropy} \label{sec:mvn-dent}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ follow a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn})

\begin{equation} \label{eq:mvn-dent-mvn}
x \sim \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Then, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $x$ in nats is

\begin{equation} \label{eq:mvn-dent-mvn-dent}
\mathrm{h}(x) = \frac{n}{2} \ln(2\pi) + \frac{1}{2} \ln|\Sigma| + \frac{1}{2} n \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of a random variable is defined as

\begin{equation} \label{eq:mvn-dent-dent}
\mathrm{h}(X) = - \int_{\mathcal{X}} p(x) \, \log_b p(x) \, \mathrm{d}x \; .
\end{equation}

To measure $h(X)$ in nats, we set $b = e$, such that ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean})

\begin{equation} \label{eq:mvn-dent-dent-nats}
\mathrm{h}(X) = - \mathrm{E}\left[ \ln p(x) \right] \; .
\end{equation}

With the probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}), the differential entropy of $x$ is:

\begin{equation} \label{eq:mvn-dent-mvn-dent-s1}
\begin{split}
\mathrm{h}(x) &= - \mathrm{E}\left[ \ln \left( \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right] \right) \right] \\
&= - \mathrm{E}\left[ - \frac{n}{2} \ln(2\pi) - \frac{1}{2} \ln|\Sigma| - \frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right] \\
&= \frac{n}{2} \ln(2\pi) + \frac{1}{2} \ln|\Sigma| + \frac{1}{2} \, \mathrm{E}\left[ (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right] \; .
\end{split}
\end{equation}

The last term can be evaluted as

\begin{equation} \label{eq:mvn-dent-mvn-dent-t3}
\begin{split}
\mathrm{E}\left[ (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right] &= \mathrm{E}\left[ \mathrm{tr}\left( (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right) \right] \\
&= \mathrm{E}\left[ \mathrm{tr}\left( \Sigma^{-1} (x-\mu) (x-\mu)^\mathrm{T} \right) \right] \\
&= \mathrm{tr}\left( \Sigma^{-1} \mathrm{E}\left[ (x-\mu) (x-\mu)^\mathrm{T} \right] \right) \\
&= \mathrm{tr}\left( \Sigma^{-1} \Sigma \right) \\
&= \mathrm{tr}\left( I_n \right) \\
&= n \; , \\
\end{split}
\end{equation}

such that the differential entropy is

\begin{equation} \label{eq:mvn-dent-mvn-dent-qed}
\mathrm{h}(x) = \frac{n}{2} \ln(2\pi) + \frac{1}{2} \ln|\Sigma| + \frac{1}{2} \, n \; .
\end{equation}

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Kiuhnm (2018): "Entropy of the multivariate Gaussian"; in: \textit{StackExchange Mathematics}, retrieved on 2020-05-14; URL: \url{https://math.stackexchange.com/questions/2029707/entropy-of-the-multivariate-gaussian}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Kullback-Leibler divergence}]{Kullback-Leibler divergence} \label{sec:mvn-kl}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Assume two multivariate normal distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) $P$ and $Q$ specifying the probability distribution of $x$ as

\begin{equation} \label{eq:mvn-kl-mvns}
\begin{split}
P: \; x &\sim \mathcal{N}(\mu_1, \Sigma_1) \\
Q: \; x &\sim \mathcal{N}(\mu_2, \Sigma_2) \; .
\end{split}
\end{equation}

Then, the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ is given by

\begin{equation} \label{eq:mvn-kl-mvn-KL}
\mathrm{KL}[P\,||\,Q] = \frac{1}{2} \left[ (\mu_2 - \mu_1)^\mathrm{T} \Sigma_2^{-1} (\mu_2 - \mu_1) + \mathrm{tr}(\Sigma_2^{-1} \Sigma_1) - \ln \frac{|\Sigma_1|}{|\Sigma_2|} - n \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The KL divergence for a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is given by 

\begin{equation} \label{eq:mvn-kl-KL-cont}
\mathrm{KL}[P\,||\,Q] = \int_{\mathcal{X}} p(x) \, \ln \frac{p(x)}{q(x)} \, \mathrm{d}x
\end{equation}

which, applied to the multivariate normal distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) in \eqref{eq:mvn-kl-mvns}, yields

\begin{equation} \label{eq:mvn-kl-mvn-KL-s1}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \int_{\mathbb{R}^n} \mathcal{N}(x; \mu_1, \Sigma_1) \, \ln \frac{\mathcal{N}(x; \mu_1, \Sigma_1)}{\mathcal{N}(x; \mu_2, \Sigma_2)} \, \mathrm{d}x \\
&= \left\langle \ln \frac{\mathcal{N}(x; \mu_1, \Sigma_1)}{\mathcal{N}(x; \mu_2, \Sigma_2)} \right\rangle_{p(x)} \; .
\end{split}
\end{equation}

Using the probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}), this becomes:

\begin{equation} \label{eq:mvn-kl-mvn-KL-s2}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \left\langle \ln \frac{ \frac{1}{\sqrt{(2 \pi)^n |\Sigma_1|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu_1)^\mathrm{T} \Sigma_1^{-1} (x-\mu_1) \right] }{ \frac{1}{\sqrt{(2 \pi)^n |\Sigma_2|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu_2)^\mathrm{T} \Sigma_2^{-1} (x-\mu_2) \right] } \right\rangle_{p(x)} \\
&= \left\langle \frac{1}{2} \ln \frac{|\Sigma_2|}{|\Sigma_1|} - \frac{1}{2} (x-\mu_1)^\mathrm{T} \Sigma_1^{-1} (x-\mu_1) + \frac{1}{2} (x-\mu_2)^\mathrm{T} \Sigma_2^{-1} (x-\mu_2) \right\rangle_{p(x)} \\
&= \frac{1}{2} \left\langle \ln \frac{|\Sigma_2|}{|\Sigma_1|} - (x-\mu_1)^\mathrm{T} \Sigma_1^{-1} (x-\mu_1) + (x-\mu_2)^\mathrm{T} \Sigma_2^{-1} (x-\mu_2) \right\rangle_{p(x)} \; .
\end{split}
\end{equation}

Now, using the fact that $x = \mathrm{tr}(x)$, if $a$ is scalar, and the trace property $\mathrm{tr}(ABC) = \mathrm{tr}(BCA)$, we have:

\begin{equation} \label{eq:mvn-kl-mvn-KL-s3}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \frac{1}{2} \left\langle \ln \frac{|\Sigma_2|}{|\Sigma_1|} - \mathrm{tr}\left[ \Sigma_1^{-1} (x-\mu_1) (x-\mu_1)^\mathrm{T} \right] + \mathrm{tr}\left[ \Sigma_2^{-1} (x-\mu_2) (x-\mu_2)^\mathrm{T} \right] \right\rangle_{p(x)} \\
&= \frac{1}{2} \left\langle \ln \frac{|\Sigma_2|}{|\Sigma_1|} - \mathrm{tr}\left[ \Sigma_1^{-1} (x-\mu_1) (x-\mu_1)^\mathrm{T} \right] + \mathrm{tr}\left[ \Sigma_2^{-1} \left( x x^\mathrm{T} - 2 \mu_2 x^\mathrm{T} + \mu_2 \mu_2^\mathrm{T} \right) \right] \right\rangle_{p(x)} \; .
\end{split}
\end{equation}

Because trace function and expected value are both linear operators ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-tr}), the expectation can be moved inside the trace:

\begin{equation} \label{eq:mvn-kl-mvn-KL-s4}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \frac{1}{2} \left( \ln \frac{|\Sigma_2|}{|\Sigma_1|} - \mathrm{tr}\left[ \Sigma_1^{-1} \left\langle (x-\mu_1) (x-\mu_1)^\mathrm{T} \right\rangle_{p(x)} \right] + \mathrm{tr}\left[ \Sigma_2^{-1} \left\langle x x^\mathrm{T} - 2 \mu_2 x^\mathrm{T} + \mu_2 \mu_2^\mathrm{T} \right\rangle_{p(x)} \right] \right) \\
&= \frac{1}{2} \left( \ln \frac{|\Sigma_2|}{|\Sigma_1|} - \mathrm{tr}\left[ \Sigma_1^{-1} \left\langle (x-\mu_1) (x-\mu_1)^\mathrm{T} \right\rangle_{p(x)} \right] + \mathrm{tr}\left[ \Sigma_2^{-1} \left( \left\langle x x^\mathrm{T} \right\rangle_{p(x)} - \left\langle 2 \mu_2 x^\mathrm{T} \right\rangle_{p(x)} + \left\langle \mu_2 \mu_2^\mathrm{T} \right\rangle_{p(x)} \right) \right] \right) \; .
\end{split}
\end{equation}

Using the expectation of a linear form for the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt})

\begin{equation} \label{eq:mvn-kl-mvn-lfmean}
x \sim \mathcal{N}(\mu, \Sigma) \quad \Rightarrow \quad \left\langle A x \right\rangle = A \mu
\end{equation}

and the expectation of a quadratic form for the multivariate normal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-qf})

\begin{equation} \label{eq:mvn-kl-mvn-qfmean}
x \sim \mathcal{N}(\mu, \Sigma) \quad \Rightarrow \quad \left\langle x^\mathrm{T} A x \right\rangle = \mu^\mathrm{T} A \mu + \mathrm{tr}(A \Sigma) \; ,
\end{equation}

the Kullback-Leibler divergence from \eqref{eq:mvn-kl-mvn-KL-s4} becomes:

\begin{equation} \label{eq:mvn-kl-mvn-KL-s5}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \frac{1}{2} \left( \ln \frac{|\Sigma_2|}{|\Sigma_1|} - \mathrm{tr}\left[ \Sigma_1^{-1} \Sigma_1 \right] + \mathrm{tr}\left[ \Sigma_2^{-1} \left( \Sigma_1 + \mu_1 \mu_1^\mathrm{T} - 2 \mu_2 \mu_1^\mathrm{T} + \mu_2 \mu_2^\mathrm{T} \right) \right] \right) \\
&= \frac{1}{2} \left( \ln \frac{|\Sigma_2|}{|\Sigma_1|} - \mathrm{tr}\left[ I_n \right] + \mathrm{tr}\left[ \Sigma_2^{-1} \Sigma_1 \right] + \mathrm{tr}\left[ \Sigma_2^{-1} \left( \mu_1 \mu_1^\mathrm{T} - 2 \mu_2 \mu_1^\mathrm{T} + \mu_2 \mu_2^\mathrm{T} \right) \right] \right) \\
&= \frac{1}{2} \left( \ln \frac{|\Sigma_2|}{|\Sigma_1|} - n + \mathrm{tr}\left[ \Sigma_2^{-1} \Sigma_1 \right] + \mathrm{tr}\left[ \mu_1^\mathrm{T} \Sigma_2^{-1} \mu_1  - 2 \mu_1^\mathrm{T} \Sigma_2^{-1} \mu_2  + \mu_2^\mathrm{T} \Sigma_2^{-1} \mu_2 \right] \right) \\
&= \frac{1}{2} \left[ \ln \frac{|\Sigma_2|}{|\Sigma_1|} - n + \mathrm{tr}\left[ \Sigma_2^{-1} \Sigma_1 \right] + (\mu_2 - \mu_1)^\mathrm{T} \Sigma_2^{-1} (\mu_2 - \mu_1) \right] \; .
\end{split}
\end{equation}

Finally, rearranging the terms, we get:

\begin{equation} \label{eq:mvn-kl-mvn-KL-qed}
\mathrm{KL}[P\,||\,Q] = \frac{1}{2} \left[ (\mu_2 - \mu_1)^\mathrm{T} \Sigma_2^{-1} (\mu_2 - \mu_1) + \mathrm{tr}(\Sigma_2^{-1} \Sigma_1) - \ln \frac{|\Sigma_1|}{|\Sigma_2|} - n \right] \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Duchi, John (2014): "Derivations for Linear Algebra and Optimization"; in: \textit{University of California, Berkeley}; URL: \url{http://www.eecs.berkeley.edu/~jduchi/projects/general_notes.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Linear transformation}]{Linear transformation} \label{sec:mvn-ltt}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ follow a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}):

\begin{equation} \label{eq:mvn-ltt-mvn}
x \sim \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Then, any linear transformation of $x$ is also multivariate normally distributed:

\begin{equation} \label{eq:mvn-ltt-mvn-lt}
y = Ax + b \sim \mathcal{N}(A\mu + b, A \Sigma A^\mathrm{T}) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The moment-generating function of a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mgf}) $x$ is

\begin{equation} \label{eq:mvn-ltt-vect-mgf}
M_x(t) = \mathbb{E} \left( \exp \left[ t^\mathrm{T} x \right] \right)
\end{equation}

and therefore the moment-generating function of the random vector $y$ is given by

\begin{equation} \label{eq:mvn-ltt-y-mgf-s1}
\begin{split}
M_y(t) &\overset{\eqref{eq:mvn-ltt-mvn-lt}}{=} \mathbb{E} \left( \exp \left[ t^\mathrm{T} (Ax + b) \right] \right) \\
&= \mathbb{E} \left( \exp \left[ t^\mathrm{T} A x \right] \cdot \exp \left[ t^\mathrm{T} b \right] \right) \\
&= \exp \left[ t^\mathrm{T} b \right] \cdot \mathbb{E} \left( \exp \left[ t^\mathrm{T} A x \right] \right) \\
&\overset{\eqref{eq:mvn-ltt-vect-mgf}}{=} \exp \left[ t^\mathrm{T} b \right] \cdot M_x(A^\mathrm{T} t) \; .
\end{split}
\end{equation}

The moment-generating function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-mgf}) is

\begin{equation} \label{eq:mvn-ltt-mvn-mgf}
M_x(t) = \exp \left[ t^\mathrm{T} \mu + \frac{1}{2} t^\mathrm{T} \Sigma t \right]
\end{equation}

and therefore the moment-generating function of the random vector $y$ becomes

\begin{equation} \label{eq:mvn-ltt-y-mgf-s2}
\begin{split}
M_y(t) &\overset{\eqref{eq:mvn-ltt-y-mgf-s1}}{=} \exp \left[ t^\mathrm{T} b \right] \cdot M_x(A^\mathrm{T} t) \\
&\overset{\eqref{eq:mvn-ltt-mvn-mgf}}{=} \exp \left[ t^\mathrm{T} b \right] \cdot \exp \left[ t^\mathrm{T} A \mu + \frac{1}{2} t^\mathrm{T} A \Sigma A^\mathrm{T} t \right] \\
&= \exp \left[ t^\mathrm{T} \left( A \mu + b \right) + \frac{1}{2} t^\mathrm{T} A \Sigma A^\mathrm{T} t \right] \; .
\end{split}
\end{equation}

Because moment-generating function and probability density function of a random variable are equivalent, this demonstrates that $y$ is following a multivariate normal distribution with mean $A \mu + b$ and covariance $A \Sigma A^\mathrm{T}$.

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Taboga, Marco (2010): "Linear combinations of normal random variables"; in: \textit{Lectures on probability and statistics}, retrieved on 2019-08-27; URL: \url{https://www.statlect.com/probability-distributions/normal-distribution-linear-combinations}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Marginal distributions}]{Marginal distributions} \label{sec:mvn-marg}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ follow a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}):

\begin{equation} \label{eq:mvn-marg-mvn}
x \sim \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Then, the marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of any subset vector $x_s$ is also a multivariate normal distribution

\begin{equation} \label{eq:mvn-marg-mvn-marg}
x_s \sim \mathcal{N}(\mu_s, \Sigma_s)
\end{equation}

where $\mu_s$ drops the irrelevant variables (the ones not in the subset, i.e. marginalized out) from the mean vector $\mu$ and $\Sigma_s$ drops the corresponding rows and columns from the covariance matrix $\Sigma$.


\vspace{1em}
\textbf{Proof:} Define an $m \times n$ subset matrix $S$ such that $s_{ij} = 1$, if the $j$-th element in $x_s$ corresponds to the $i$-th element in $x$, and $s_{ij} = 0$ otherwise. Then,

\begin{equation} \label{eq:mvn-marg-xs}
x_s = S x
\end{equation}

and we can apply the linear transformation theorem ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}) to give

\begin{equation} \label{eq:mvn-marg-mvn-marg-qed}
x_s \sim \mathcal{N}(S \mu, S \Sigma S^\mathrm{T}) \; .
\end{equation}

Finally, we see that $S \mu = \mu_s$ and $S \Sigma S^\mathrm{T} = \Sigma_s$.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Conditional distributions}]{Conditional distributions} \label{sec:mvn-cond}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ follow a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn})

\begin{equation} \label{eq:mvn-cond-mvn}
x \sim \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Then, the conditional distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-cond}) of any subset vector $x_1$, given the complement vector $x_2$, is also a multivariate normal distribution

\begin{equation} \label{eq:mvn-cond-mvn-cond}
x_1|x_2 \sim \mathcal{N}(\mu_{1|2}, \Sigma_{1|2})
\end{equation}

where the conditional mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) and covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) are

\begin{equation} \label{eq:mvn-cond-mvn-cond-hyp}
\begin{split}
\mu_{1|2} &= \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (x_2 - \mu_2) \\
\Sigma_{1|2} &= \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}
\end{split}
\end{equation}

with block-wise mean and covariance defined as

\begin{equation} \label{eq:mvn-cond-mvn-joint-hyp}
\begin{split}
\mu &= \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix} \\
\Sigma &= \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{bmatrix} \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} Without loss of generality, we assume that, in parallel to \eqref{eq:mvn-cond-mvn-joint-hyp},

\begin{equation} \label{eq:mvn-cond-x}
x = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
\end{equation}

where $x_1$ is an $n_1 \times 1$ vector, $x_2$ is an $n_2 \times 1$ vector and $x$ is an $n_1 + n_2 = n \times 1$ vector.

\vspace{1em}
By construction, the joint distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) of $x_1$ and $x_2$ is:

\begin{equation} \label{eq:mvn-cond-mvn-joint}
x_1,x_2 \sim \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Moreover, the marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of $x_2$ follows from ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-marg}) \eqref{eq:mvn-cond-mvn} and \eqref{eq:mvn-cond-mvn-joint-hyp} as

\begin{equation} \label{eq:mvn-cond-mvn-marg}
x_2 \sim \mathcal{N}(\mu_2, \Sigma_{22}) \; .
\end{equation}

According to the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), it holds that

\begin{equation} \label{eq:mvn-cond-mvn-cond-s1}
p(x_1|x_2) = \frac{p(x_1,x_2)}{p(x_2)}
\end{equation}

Applying \eqref{eq:mvn-cond-mvn-joint} and \eqref{eq:mvn-cond-mvn-marg} to \eqref{eq:mvn-cond-mvn-cond-s1}, we have:

\begin{equation} \label{eq:mvn-cond-mvn-cond-s2}
p(x_1|x_2) = \frac{\mathcal{N}(x; \mu, \Sigma)}{\mathcal{N}(x_2; \mu_2, \Sigma_{22})} \; .
\end{equation}

Using the probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}), this becomes:

\begin{equation} \label{eq:mvn-cond-mvn-cond-s3}
\begin{split}
p(x_1|x_2) &= \frac{1/\sqrt{(2 \pi)^n |\Sigma|} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right]}{1/\sqrt{(2 \pi)^{n_2} |\Sigma_{22}|} \cdot \exp \left[ -\frac{1}{2} (x_2-\mu_2)^\mathrm{T} \Sigma_{22}^{-1} (x_2-\mu_2) \right]} \\
&= \frac{1}{\sqrt{(2 \pi)^{n-n_2}}} \cdot \sqrt{\frac{|\Sigma_{22}|}{|\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) + \frac{1}{2} (x_2-\mu_2)^\mathrm{T} \Sigma_{22}^{-1} (x_2-\mu_2) \right] \; .
\end{split}
\end{equation}

Writing the inverse of $\Sigma$ as

\begin{equation} \label{eq:mvn-cond-Sigma-inv-def}
\Sigma^{-1} = \begin{bmatrix} \Sigma^{11} & \Sigma^{12} \\ \Sigma^{21} & \Sigma^{22} \end{bmatrix}
\end{equation}

and applying \eqref{eq:mvn-cond-mvn-joint-hyp} to \eqref{eq:mvn-cond-mvn-cond-s3}, we get:

\begin{equation} \label{eq:mvn-cond-mvn-cond-s4}
\begin{split}
p(x_1|x_2) = &\frac{1}{\sqrt{(2 \pi)^{n-n_2}}} \cdot \sqrt{\frac{|\Sigma_{22}|}{|\Sigma|}} \cdot \\
&\exp \left[ -\frac{1}{2} \left( \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} - \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix} \right)^\mathrm{T} \begin{bmatrix} \Sigma^{11} & \Sigma^{12} \\ \Sigma^{21} & \Sigma^{22} \end{bmatrix} \left( \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} - \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix} \right) \right. \\
&\hphantom{\exp \left[\right.} \left. + \frac{1}{2} \, (x_2-\mu_2)^\mathrm{T} \, \Sigma_{22}^{-1} \, (x_2-\mu_2) \right] \; .
\end{split}
\end{equation}

Multiplying out within the exponent of \eqref{eq:mvn-cond-mvn-cond-s4}, we have

\begin{equation} \label{eq:mvn-cond-mvn-cond-s5}
\begin{split}
p(x_1|x_2) = &\frac{1}{\sqrt{(2 \pi)^{n-n_2}}} \cdot \sqrt{\frac{|\Sigma_{22}|}{|\Sigma|}} \cdot \\
&\exp \left[ -\frac{1}{2} \left( (x_1-\mu_1)^\mathrm{T} \Sigma^{11} (x_1-\mu_1) + 2 (x_1-\mu_1)^\mathrm{T} \Sigma^{12} (x_2-\mu_2) + (x_2-\mu_2)^\mathrm{T} \Sigma^{22} (x_2-\mu_2) \right) \right. \\
&\hphantom{\exp \left[\right.} \left. + \frac{1}{2} (x_2-\mu_2)^\mathrm{T} \Sigma_{22}^{-1} (x_2-\mu_2) \right]
\end{split}
\end{equation}

where we have used the fact that ${\Sigma^{21}}^\mathrm{T} = \Sigma^{12}$, because $\Sigma^{-1}$ is a symmetric matrix.

\vspace{1em}
The inverse of a block matrix is

\begin{equation} \label{eq:mvn-cond-block-inv}
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}^{-1} =
\begin{bmatrix} 
A-BD^{-1}C)^{-1}          & -(A-BD^{-1}C)^{-1}BD^{-1}              \\
-D^{-1}C(A-BD^{-1}C)^{-1} & D^{-1}+D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1}
\end{bmatrix} \; ,
\end{equation}

thus the inverse of $\Sigma$ in \eqref{eq:mvn-cond-Sigma-inv-def} is

\begin{equation} \label{eq:mvn-cond-Sigma-inv}
\begin{bmatrix}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{bmatrix}^{-1} =
\begin{bmatrix}
(\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21})^{-1}                               & -(\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21})^{-1} \Sigma_{12} \Sigma_{22}^{-1}                                                \\
-\Sigma_{22}^{-1} \Sigma_{21} (\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21})^{-1} & \Sigma_{22}^{-1} + \Sigma_{22}^{-1} \Sigma_{21} (\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21})^{-1} \Sigma_{12} \Sigma_{22}^{-1}
\end{bmatrix} \; .
\end{equation}

Plugging this into \eqref{eq:mvn-cond-mvn-cond-s5}, we have:

\begin{equation} \label{eq:mvn-cond-mvn-cond-s6}
\begin{split}
p(x_1|x_2) = &\frac{1}{\sqrt{(2 \pi)^{n-n_2}}} \cdot \sqrt{\frac{|\Sigma_{22}|}{|\Sigma|}} \cdot \\
&\exp \left[ -\frac{1}{2} \left( (x_1-\mu_1)^\mathrm{T} (\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21})^{-1} (x_1-\mu_1) \right. \right. - \\
&\hphantom{\exp \left[ -\frac{1}{2} \left( \right. \right.} 2 (x_1-\mu_1)^\mathrm{T} (\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21})^{-1} \Sigma_{12} \Sigma_{22}^{-1} (x_2-\mu_2) + \\
&\hphantom{\exp \left[ -\frac{1}{2} \left( \right. \right.} \left. (x_2-\mu_2)^\mathrm{T} \left[ \Sigma_{22}^{-1} + \Sigma_{22}^{-1} \Sigma_{21} (\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21})^{-1} \Sigma_{12} \Sigma_{22}^{-1} \right] (x_2-\mu_2) \right) \\
&\hphantom{\exp \left[\right.} \left. + \frac{1}{2} \left( (x_2-\mu_2)^\mathrm{T} \Sigma_{22}^{-1} (x_2-\mu_2) \right) \right] \; .
\end{split}
\end{equation}

Eliminating some terms, we have:

\begin{equation} \label{eq:mvn-cond-mvn-cond-s7}
\begin{split}
p(x_1|x_2) = &\frac{1}{\sqrt{(2 \pi)^{n-n_2}}} \cdot \sqrt{\frac{|\Sigma_{22}|}{|\Sigma|}} \cdot \\
&\exp \left[ -\frac{1}{2} \left( (x_1-\mu_1)^\mathrm{T} (\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21})^{-1} (x_1-\mu_1) \right. \right. - \\
&\hphantom{\exp \left[ -\frac{1}{2} \left( \right. \right.} 2 (x_1-\mu_1)^\mathrm{T} (\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21})^{-1} \Sigma_{12} \Sigma_{22}^{-1} (x_2-\mu_2) + \\
&\hphantom{\exp \left[ -\frac{1}{2} \left( \right. \right.} \left. \left. (x_2-\mu_2)^\mathrm{T} \Sigma_{22}^{-1} \Sigma_{21} (\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21})^{-1} \Sigma_{12} \Sigma_{22}^{-1} (x_2-\mu_2) \right) \right] \; .
\end{split}
\end{equation}

Rearranging the terms, we have

\begin{equation} \label{eq:mvn-cond-mvn-cond-s8}
\begin{split}
p(x_1|x_2) = &\frac{1}{\sqrt{(2 \pi)^{n-n_2}}} \cdot \sqrt{\frac{|\Sigma_{22}|}{|\Sigma|}} \cdot \exp \left[ -\frac{1}{2} \cdot \right. \\
&\! \left. \left[ (x_1-\mu_1) - \Sigma_{12} \Sigma_{22}^{-1} (x_2-\mu_2) \right]^\mathrm{T} (\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21})^{-1} \left[ (x_1-\mu_1) - \Sigma_{12} \Sigma_{22}^{-1} (x_2-\mu_2) \right] \right] \\
= &\frac{1}{\sqrt{(2 \pi)^{n-n_2}}} \cdot \sqrt{\frac{|\Sigma_{22}|}{|\Sigma|}} \cdot \exp \left[ -\frac{1}{2} \cdot \right. \\
&\! \left. \left[ x_1 - \left( \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (x_2-\mu_2) \right) \right]^\mathrm{T} (\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21})^{-1} \left[ x_1 - \left( \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (x_2-\mu_2) \right) \right] \right]
\end{split}
\end{equation}

where we have used the fact that $\Sigma_{21} = \Sigma_{12}^\mathrm{T}$, because $\Sigma$ is a covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}).

\vspace{1em}
The determinant of a block matrix is

\begin{equation} \label{eq:mvn-cond-Block-det}
\begin{vmatrix} A & B \\ C & D \end{vmatrix} = |D| \cdot | A - B D^{-1} C | \; ,
\end{equation}

such that we have for $\Sigma$ that

\begin{equation} \label{eq:mvn-cond-Sigma-det}
\begin{vmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{vmatrix} = |\Sigma_{22}| \cdot | \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} | \; .
\end{equation}

With this and $n - n_2 = n_1$, we finally arrive at

\begin{equation} \label{eq:mvn-cond-mvn-cond-s9}
\begin{split}
p(x_1|x_2) = &\frac{1}{\sqrt{(2 \pi)^{n_1} | \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} |}} \cdot \exp \left[ -\frac{1}{2} \cdot \right. \\
&\! \left. \left[ x_1 - \left( \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (x_2-\mu_2) \right) \right]^\mathrm{T} (\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21})^{-1} \left[ x_1 - \left( \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (x_2-\mu_2) \right) \right] \right]
\end{split}
\end{equation}

which is the probability density function of a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf})

\begin{equation} \label{eq:mvn-cond-mvn-cond-s10}
p(x_1|x_2) = \mathcal{N}(x_1; \mu_{1|2}, \Sigma_{1|2})
\end{equation}

with the mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu_{1 \vert 2}$ and covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) $\Sigma_{1 \vert 2}$ given by \eqref{eq:mvn-cond-mvn-cond-hyp}.

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wang, Ruye (2006): "Marginal and conditional distributions of multivariate normal distribution"; in: \textit{Computer Image Processing and Analysis}; URL: \url{http://fourier.eng.hmc.edu/e161/lectures/gaussianprocess/node7.html}.
\item Wikipedia (2020): "Multivariate normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-20; URL: \url{https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Conditions for independence}]{Conditions for independence} \label{sec:mvn-ind}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}):

\begin{equation} \label{eq:mvn-ind-mvn}
x \sim \mathcal{N}(\mu, \Sigma) \; .
\end{equation}

Then, the components of $x$ are statistically independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), if and only if the covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) is a diagonal matrix:

\begin{equation} \label{eq:mvn-ind-mvn-ind}
p(x) = p(x_1) \cdot \ldots \cdot p(x_n) \quad \Leftrightarrow \quad \Sigma = \mathrm{diag}\left( \left[ \sigma^2_1, \ldots, \sigma^2_n \right] \right) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The marginal distribution of one entry from a multivariate normal random vector is a univariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-marg}) where mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) and variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) are equal to the corresponding entries of the mean vector and covariance matrix:

\begin{equation} \label{eq:mvn-ind-mvn-marg}
x \sim \mathcal{N}(\mu, \Sigma) \quad \Rightarrow \quad x_i \sim \mathcal{N}(\mu_i, \sigma^2_{ii}) \; .
\end{equation}

The probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}) is

\begin{equation} \label{eq:mvn-ind-mvn-pdf}
p(x) = \frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right]
\end{equation}

and the probability density function of the univariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) is

\begin{equation} \label{eq:mvn-ind-norm-pdf}
p(x_i) = \frac{1}{\sqrt{2 \pi \sigma^2_i}} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x_i-\mu_i}{\sigma_i} \right)^2 \right] \; .
\end{equation}

\vspace{1em}
1) Let

\begin{equation} \label{eq:mvn-ind-x-ind}
p(x) = p(x_1) \cdot \ldots \cdot p(x_n) \; .
\end{equation}

Then, we have

\begin{equation} \label{eq:mvn-ind-x-ind-dev}
\begin{split}
\frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right] &\overset{\eqref{eq:mvn-ind-mvn-pdf},\eqref{eq:mvn-ind-norm-pdf}}{=} \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^2_i}} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x_i-\mu_i}{\sigma_i} \right)^2 \right] \\
\frac{1}{\sqrt{(2 \pi)^n |\Sigma|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right] &= \frac{1}{\sqrt{(2 \pi)^n \prod_{i=1}^{n} \sigma^2_i}} \cdot \exp \left[ -\frac{1}{2} \sum_{i=1}^{n} (x_i-\mu_i) \frac{1}{\sigma^2_i} (x_i-\mu_i) \right] \\
- \frac{1}{2} \log |\Sigma| - \frac{1}{2} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) &= - \frac{1}{2} \sum_{i=1}^{n} \log \sigma^2_i - \frac{1}{2} \sum_{i=1}^{n} (x_i-\mu_i) \frac{1}{\sigma^2_i} (x_i-\mu_i)
\end{split}
\end{equation}

which is only fulfilled by a diagonal covariance matrix

\begin{equation} \label{eq:mvn-ind-Sigma-diag-qed}
\Sigma = \mathrm{diag}\left( \left[ \sigma^2_1, \ldots, \sigma^2_n \right] \right) \; ,
\end{equation}

because the determinant of a diagonal matrix is a product

\begin{equation} \label{eq:mvn-ind-diag-det}
| \mathrm{diag}\left( \left[ a_1, \ldots, a_n \right] \right) | = \prod_{i=1}^n a_i \; ,
\end{equation}

the inverse of a diagonal matrix is a diagonal matrix

\begin{equation} \label{eq:mvn-ind-diag-inv}
\mathrm{diag}\left( \left[ a_1, \ldots, a_n \right] \right)^{-1} = \mathrm{diag}\left( \left[ 1/a_1, \ldots, 1/a_n \right] \right)
\end{equation}

and the squared form with a diagonal matrix is

\begin{equation} \label{eq:mvn-ind-diag-sqr}
x^\mathrm{T} \mathrm{diag}\left( \left[ a_1, \ldots, a_n \right] \right) x = \sum_{i=1}^n a_i x_i^2 \; .
\end{equation}


\vspace{1em}
2) Let

\begin{equation} \label{eq:mvn-ind-Sigma-diag}
\Sigma = \mathrm{diag}\left( \left[ \sigma^2_1, \ldots, \sigma^2_n \right] \right) \; .
\end{equation}

Then, we have

\begin{equation} \label{eq:mvn-ind-Sigma-diag-dev}
\begin{split}
p(x) &\overset{\eqref{eq:mvn-ind-mvn-pdf}}{=} \frac{1}{\sqrt{(2 \pi)^n |\mathrm{diag}\left( \left[ \sigma^2_1, \ldots, \sigma^2_n \right] \right)|}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \mathrm{diag}\left( \left[ \sigma^2_1, \ldots, \sigma^2_n \right] \right)^{-1} (x-\mu) \right] \\
&= \frac{1}{\sqrt{(2 \pi)^n \prod_{i=1}^{n} \sigma^2_i}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} \mathrm{diag}\left( \left[ 1/\sigma^2_1, \ldots, 1/\sigma^2_n \right] \right) (x-\mu) \right] \\
&= \frac{1}{\sqrt{(2 \pi)^n \prod_{i=1}^{n} \sigma^2_i}} \cdot \exp \left[ -\frac{1}{2} \sum_{i=1}^{n} \frac{(x_i-\mu_i)^2}{\sigma^2_i} \right] \\
&= \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2_i}} \cdot \exp \left[ -\frac{1}{2} \left( \frac{x_i-\mu_i}{\sigma_i} \right)^2 \right]
\end{split}
\end{equation}

which implies that

\begin{equation} \label{eq:mvn-ind-x-ind-qed}
p(x) = p(x_1) \cdot \ldots \cdot p(x_n) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Independence of products}]{Independence of products} \label{sec:mvn-indprod}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}):

\begin{equation} \label{eq:mvn-indprod-mvn}
X \sim \mathcal{N}(\mu, \Sigma)
\end{equation}

and consider two matrices $A \in \mathbb{R}^{k \times n}$ and $B \in \mathbb{R}^{l \times n}$. Then, $AX$ and $BX$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), if and only if the cross-matrix product, weighted with the covariance matrix ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-cov}) is equal to the zero matrix:

\begin{equation} \label{eq:mvn-indprod-mvn-indprod}
AX \quad \text{and} \quad BX \quad \text{ind.} \quad \Leftrightarrow \quad A \Sigma B^\mathrm{T} = 0_{kl} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Define a new random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $C$ as

\begin{equation} \label{eq:mvn-indprod-C}
C = \left[ \begin{array}{c} A \\ B \end{array} \right] \in \mathbb{R}^{(k+l) \times n} \; .
\end{equation}

Then, due to the linear transformation theorem ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}), we have

\begin{equation} \label{eq:mvn-indprod-CX}
CX = \left[ \begin{array}{c} AX \\ BX \end{array} \right] \sim \mathcal{N}\left( \left[ \begin{array}{c} A\mu \\ B\mu \end{array} \right], C \Sigma C^\mathrm{T} \right)
\end{equation}

with the combined covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat})

\begin{equation} \label{eq:mvn-indprod-CSC}
C \Sigma C^\mathrm{T} = \left[ \begin{array}{cc} A \Sigma A^\mathrm{T} & A \Sigma B^\mathrm{T} \\ B \Sigma A^\mathrm{T} & B \Sigma B^\mathrm{T} \end{array} \right] \; .
\end{equation}

We know that the necessary and sufficient condition for two components of a multivariate normal random vector to be independent is that their entries in the covariance matrix are zero ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ind}). Thus, $AX$ and $BX$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), if and only if

\begin{equation} \label{eq:mvn-indprod-mvn-indprod-qed}
A \Sigma B^\mathrm{T} = (B \Sigma A^\mathrm{T})^\mathrm{T} = 0_{kl}
\end{equation}

where $0_{kl}$ is the $k \times l$ zero matrix. This proves the result in \eqref{eq:mvn-indprod-mvn-indprod}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item jld (2018): "Understanding t-test for linear regression"; in: \textit{StackExchange CrossValidated}, retrieved on 2022-12-13; URL: \url{https://stats.stackexchange.com/a/344008}.
\end{itemize}
\vspace{1em}



\subsection{Multivariate t-distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:mvt}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, $X$ is said to follow a multivariate $t$-distribution with mean $\mu$, scale matrix $\Sigma$ and degrees of freedom $\nu$

\begin{equation} \label{eq:mvt-mvt}
X \sim t(\mu, \Sigma, \nu) \; ,
\end{equation}

if and only if its probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) is given by

\begin{equation} \label{eq:mvt-mvt-pdf}
t(x; \mu, \Sigma, \nu) = \sqrt{\frac{1}{(\nu \pi)^{n} |\Sigma|}} \, \frac{\Gamma([\nu+n]/2)}{\Gamma(\nu/2)} \, \left[ 1 + \frac{1}{\nu} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right]^{-(\nu+n)/2}
\end{equation}

where $\mu$ is an $n \times 1$ real vector, $\Sigma$ is an $n \times n$ positive definite matrix and $\nu > 0$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Koch KR (2007): "Multivariate t-Distribution"; in: \textit{Introduction to Bayesian Statistics}, ch. 2.5.2, pp. 53-55; URL: \url{https://www.springer.com/gp/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:mvt-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt}):

\begin{equation} \label{eq:mvt-pdf-mvt}
X \sim t(\mu, \Sigma, \nu) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ is

\begin{equation} \label{eq:mvt-pdf-mvt-pdf}
f_X(x) = \sqrt{\frac{1}{(\nu \pi)^{n} |\Sigma|}} \, \frac{\Gamma([\nu+n]/2)}{\Gamma(\nu/2)} \, \left[ 1 + \frac{1}{\nu} (x-\mu)^\mathrm{T} \Sigma^{-1} (x-\mu) \right]^{-(\nu+n)/2} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Relationship to F-distribution}]{Relationship to F-distribution} \label{sec:mvt-f}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt}) with mean $\mu$, scale matrix $\Sigma$ and degrees of freedom $\nu$:

\begin{equation} \label{eq:mvt-f-X}
X \sim t(\mu, \Sigma, \nu) \; .
\end{equation}

Then, the centered, weighted and standardized quadratic form of $X$ follows an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}) with degrees of freedom $n$ and $\nu$:

\begin{equation} \label{eq:mvt-f-mvt-f}
(X-\mu)^\mathrm{T} \, \Sigma^{-1} (X-\mu)/n \sim F(n, \nu) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The linear transformation theorem for the multivariate t-distribution states

\begin{equation} \label{eq:mvt-f-mvt-ltt}
x \sim t(\mu, \Sigma, \nu) \quad \Rightarrow \quad y = Ax + b \sim t(A\mu + b, A \Sigma A^\mathrm{T}, \nu)
\end{equation}

where $x$ is an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt}), $A$ is an $m \times n$ matrix and $b$ is an $m \times 1$ vector. Define the following quantities

\begin{equation} \label{eq:mvt-f-YZ}
\begin{split}
Y = \Sigma^{-1/2} (X-\mu) = \Sigma^{-1/2} X - \Sigma^{-1/2} \mu \\
Z = Y^\mathrm{T} Y / n = (X-\mu)^\mathrm{T} \, \Sigma^{-1} (X-\mu)/n
\end{split}
\end{equation}

where $\Sigma^{-1/2}$ is a matrix square root of the inverse of $\Sigma$. Then, applying \eqref{eq:mvt-f-mvt-ltt} to \eqref{eq:mvt-f-YZ} with \eqref{eq:mvt-f-X}, one obtains the distribution of $Y$ as

\begin{equation} \label{eq:mvt-f-Y-dist}
\begin{split}
Y &\sim t(\Sigma^{-1/2} \mu - \Sigma^{-1/2} \mu, \Sigma^{-1/2} \Sigma \, \Sigma^{-1/2}, \nu) \\
&= t(0_n, \Sigma^{-1/2} \Sigma^{1/2} \Sigma^{1/2} \Sigma^{-1/2}, \nu) \\
&= t(0_n, I_n, \nu) \; ,
\end{split}
\end{equation}

i.e. the marginal distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of the individual entries of $Y$ are univariate t-distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) with $\nu$ degrees of freedom:

\begin{equation} \label{eq:mvt-f-yi-dist}
Y_i \sim t(\nu), \; i = 1,\ldots,n \; .
\end{equation}

Note that, when $X$ follows a t-distribution with $n$ degrees of freedom, this is equivalent to ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) an expression of $X$ in terms of a standard normal ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) random variable $Z$ and a chi-squared ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) random variable $V$:

\begin{equation} \label{eq:mvt-f-t}
X \sim t(n) \quad \Leftrightarrow \quad X = \frac{Z}{\sqrt{V/n}} \quad \text{with independent} \quad Z \sim \mathcal{N}(0,1) \quad \text{and} \quad V \sim \chi^2(n) \; .
\end{equation}

With that, $Z$ from \eqref{eq:mvt-f-YZ} can be rewritten as follows:

\begin{equation} \label{eq:mvt-f-Z-eq-s1}
\begin{split}
Z &\overset{\eqref{eq:mvt-f-YZ}}{=} Y^\mathrm{T} Y / n \\
&= \frac{1}{n} \sum_{i=1}^n Y_i^2 \\
&\overset{\eqref{eq:mvt-f-t}}{=} \frac{1}{n} \sum_{i=1}^n \left( \frac{Z_i}{\sqrt{V/\nu}} \right)^2 \\
&= \frac{\left( \sum_{i=1}^n Z_i^2 \right)/n}{V/\nu} \; .
\end{split}
\end{equation}

Because by definition, the sum of squared standard normal random variables follows a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2})

\begin{equation} \label{eq:mvt-f-chi2}
X_i \sim \mathcal{N}(0,1), \; i = 1,\ldots,n \quad \Rightarrow \quad \sum_{i=1}^n X_i^2 \sim \chi^2(n) \; ,
\end{equation}

the quantity $Z$ becomes a ratio of the following form

\begin{equation} \label{eq:mvt-f-Z-eq-s2}
Z = \frac{W/n}{V/\nu} \quad \text{with} \quad W \sim \chi^2(n) \quad \text{and} \quad V \sim \chi^2(\nu) \; ,
\end{equation}

such that $Z$, by definition, follows an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}):

\begin{equation} \label{eq:mvt-f-Z-dist}
Z = \frac{W/n}{V/\nu} \sim F(n, \nu) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Lin, Pi-Erh (1972): "Some Characterizations of the Multivariate t Distribution"; in: \textit{Journal of Multivariate Analysis}, vol. 2, pp. 339-344, Lemma 2; URL: \url{https://core.ac.uk/download/pdf/81139018.pdf}; DOI: 10.1016/0047-259X(72)90021-8.
\item Nadarajah, Saralees; Kotz, Samuel (2005): "Mathematical Properties of the Multivariate t Distribution"; in: \textit{Acta Applicandae Mathematicae}, vol. 89, pp. 53-84, page 56; URL: \url{https://link.springer.com/content/pdf/10.1007/s10440-005-9003-4.pdf}; DOI: 10.1007/s10440-005-9003-4.
\end{itemize}
\vspace{1em}



\subsection{Normal-gamma distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:ng}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) and let $Y$ be a positive random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Then, $X$ and $Y$ are said to follow a normal-gamma distribution

\begin{equation} \label{eq:ng-ng}
X,Y \sim \mathrm{NG}(\mu, \Lambda, a, b) \; ,
\end{equation}

if the distribution of $X$ conditional on $Y$ is a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) with mean vector $\mu$ and covariance matrix $(y \Lambda)^{-1}$ and $Y$ follows a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) with shape parameter $a$ and rate parameter $b$:

\begin{equation} \label{eq:ng-mvn-gam}
\begin{split}
X \vert Y &\sim \mathcal{N}(\mu, (Y \Lambda)^{-1}) \\
Y &\sim \mathrm{Gam}(a, b) \; .
\end{split}
\end{equation}

The $n \times n$ matrix $\Lambda$ is referred to as the precision matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:precmat}) of the normal-gamma distribution.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Koch KR (2007): "Normal-Gamma Distribution"; in: \textit{Introduction to Bayesian Statistics}, ch. 2.5.3, pp. 55-56, eq. 2.212; URL: \url{https://www.springer.com/gp/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Special case of normal-Wishart distribution}]{Special case of normal-Wishart distribution} \label{sec:ng-nw}
\setcounter{equation}{0}

\textbf{Theorem:} The normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}) is a special case of the normal-Wishart distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:nw}) where the number of columns of the random matrices ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}) is $p = 1$.


\vspace{1em}
\textbf{Proof:} Let $X$ be an $n \times p$ real matrix and let $Y$ be a $p \times p$ positive-definite symmetric matrix, such that $X$ and $Y$ jointly follow a normal-Wishart distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:nw}):

\begin{equation} \label{eq:ng-nw-nw}
X,Y \sim \mathrm{NW}(M, U, V, \nu) \; .
\end{equation}

Then, $X$ and $Y$ are described by the probability density function ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:nw-pdf})

\begin{equation} \label{eq:ng-nw-nw-pdf}
\begin{split}
p(X,Y) = \; & \frac{1}{\sqrt{(2\pi)^{np} |U|^p |V|^{\nu}}} \cdot \frac{\sqrt{2^{-\nu p}}}{\Gamma_p \left( \frac{\nu}{2} \right)} \cdot |Y|^{(\nu+n-p-1)/2} \cdot \\
& \exp\left[-\frac{1}{2} \mathrm{tr}\left( Y \left[ (X-M)^\mathrm{T} \, U^{-1} (X-M) + V^{-1} \right] \right) \right]
\end{split}
\end{equation}

where $\lvert A \rvert$ is a matrix determinant, $A^{-1}$ is a matrix inverse and $\Gamma_p(x)$ is the multivariate gamma function of order $p$. If $p = 1$, then $\Gamma_p(x) = \Gamma(x)$ is the ordinary gamma function, $x = X$ is a column vector and $y = Y$ is a real number. Thus, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $x$ and $y$ can be developed as

\begin{equation} \label{eq:ng-nw-ng-pdf-s1}
\begin{split}
p(x,y) = \; & \frac{1}{\sqrt{(2\pi)^n |U| |V|^{\nu}}} \cdot \frac{\sqrt{2^{-\nu}}}{\Gamma \left( \frac{\nu}{2} \right)} \cdot y^{(\nu+n-2)/2} \cdot \\
& \exp\left[-\frac{1}{2} \mathrm{tr}\left( y \left[ (x-M)^\mathrm{T} \, U^{-1} (x-M) + V^{-1} \right] \right) \right] \\
= \; & \sqrt{\frac{|U^{-1}|}{(2\pi)^n}} \cdot \frac{\sqrt{\left(2 |V|\right)^{-\nu}}}{\Gamma \left( \frac{\nu}{2} \right)} \cdot y^{\frac{\nu}{2}+\frac{n}{2}-1} \cdot \\
& \exp\left[-\frac{1}{2} \left( y \left[ (x-M)^\mathrm{T} \, U^{-1} (x-M) + 2 \left(2 V\right)^{-1} \right] \right) \right] \\
= \; & \sqrt{\frac{|U^{-1}|}{(2\pi)^n}} \cdot \frac{\left(\frac{1}{2 |V|}\right)^{\frac{\nu}{2}}}{\Gamma \left( \frac{\nu}{2} \right)} \cdot y^{\frac{\nu}{2}+\frac{n}{2}-1} \cdot \\
& \exp\left[-\frac{y}{2} \left( (x-M)^\mathrm{T} \, U^{-1} (x-M) + 2 \left(\frac{1}{2 V}\right) \right) \right] \\
\end{split}
\end{equation}

In the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}), we have $M \in \mathbb{R}^{n \times p}$, $U \in \mathbb{R}^{n \times n}$, $V \in \mathbb{R}^{p \times p}$ and $\nu \in \mathbb{R}$. Thus, with $p = 1$, $M$ becomes a column vector and $V$ becomes a real number, such that $V = \lvert V \rvert = 1/V^{-1}$. Finally, substituting $\mu = M$, $\Lambda = U^{-1}$, $a = \frac{\nu}{2}$ and $b = \frac{1}{2V}$, we get

\begin{equation} \label{eq:ng-nw-ng-pdf-s2}
p(x,y) = \sqrt{\frac{|\Lambda|}{(2 \pi)^n}} \frac{b^a}{\Gamma(a)} \cdot y^{a+\frac{n}{2}-1} \exp \left[ -\frac{y}{2} \left( (x-\mu)^\mathrm{T} \Lambda (x-\mu) + 2b \right) \right]
\end{equation}

which is the probability density function of the normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-pdf}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:ng-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ and $y$ follow a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}):

\begin{equation} \label{eq:ng-pdf-ng}
x,y \sim \mathrm{NG}(\mu, \Lambda, a, b) \; .
\end{equation}

Then, the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $x$ and $y$ is

\begin{equation} \label{eq:ng-pdf-ng-pdf}
p(x,y) = \sqrt{\frac{|\Lambda|}{(2 \pi)^n}} \frac{b^a}{\Gamma(a)} \cdot y^{a+\frac{n}{2}-1} \exp \left[ -\frac{y}{2} \left( (x-\mu)^\mathrm{T} \Lambda (x-\mu) + 2b \right) \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}) is defined as $X$ conditional on $Y$ following a multivariate distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) and $Y$ following a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:ng-pdf-mvn-gam}
\begin{split}
X \vert Y &\sim \mathcal{N}(\mu, (Y \Lambda)^{-1}) \\
Y &\sim \mathrm{Gam}(a, b) \; .
\end{split}
\end{equation}

Thus, using the probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}) and the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), we have the following probabilities:

\begin{equation} \label{eq:ng-pdf-mvn-gam-pdf}
\begin{split}
p(x \vert y) &= \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \\
&= \sqrt{\frac{|y \Lambda|}{(2 \pi)^n}} \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} (y \Lambda) (x-\mu) \right] \\
p(y) &= \mathrm{Gam}(y; a, b) \\
&= \frac{b^a}{\Gamma(a)} y^{a-1} \exp\left[-by\right] \; .
\end{split}
\end{equation}

The law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}) implies that

\begin{equation} \label{eq:ng-pdf-prob-cond}
p(x,y) = p(x \vert y) \, p(y) \; ,
\end{equation}

such that the normal-gamma density function becomes:

\begin{equation} \label{eq:ng-pdf-ng-pdf-prod}
p(x,y) = \sqrt{\frac{|y \Lambda|}{(2 \pi)^n}} \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} (y \Lambda) (x-\mu) \right] \cdot \frac{b^a}{\Gamma(a)} y^{a-1} \exp\left[-by\right] \; .
\end{equation}

Using the relation $\lvert y A \rvert = y^n \lvert A \rvert$ for an $n \times n$ matrix $A$ and rearranging the terms, we have:

\begin{equation} \label{eq:ng-pdf-ng-pdf-qed}
p(x,y) = \sqrt{\frac{|\Lambda|}{(2 \pi)^n}} \frac{b^a}{\Gamma(a)} \cdot y^{a+\frac{n}{2}-1} \exp \left[ -\frac{y}{2} \left( (x-\mu)^\mathrm{T} \Lambda (x-\mu) + 2b \right) \right] \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Koch KR (2007): "Normal-Gamma Distribution"; in: \textit{Introduction to Bayesian Statistics}, ch. 2.5.3, pp. 55-56, eq. 2.212; URL: \url{https://www.springer.com/gp/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:ng-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x \in \mathbb{R}^n$ and $y > 0$ follow a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}):

\begin{equation} \label{eq:ng-mean-ng}
x,y \sim \mathrm{NG}(\mu, \Lambda, a, b) \; .
\end{equation}

Then, the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $x$ and $y$ is

\begin{equation} \label{eq:ng-mean-ng-mean}
\mathrm{E}[(x,y)] = \left( \mu, \frac{a}{b} \right) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Consider the random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec})

\begin{equation} \label{eq:ng-mean-rvec}
\left[ \begin{array}{c} x \\ y \end{array} \right] = \left[ \begin{array}{c} x_1 \\ \vdots \\ x_n \\ y \end{array} \right] \; .
\end{equation}

According to the expected value of a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-rvec}), its expected value is

\begin{equation} \label{eq:ng-mean-mean-rvec}
\mathrm{E}\left( \left[ \begin{array}{c} x \\ y \end{array} \right] \right) = \left[ \begin{array}{c} \mathrm{E}(x_1) \\ \vdots \\ \mathrm{E}(x_n) \\ \mathrm{E}(y) \end{array} \right] = \left[ \begin{array}{c} \mathrm{E}(x) \\ \mathrm{E}(y) \end{array} \right] \; .
\end{equation}

When $x$ and $y$ are jointly normal-gamma distributed, then ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}) by definition $x$ follows a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) conditional on $y$ and $y$ follows a univariate gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:ng-mean-ng-def}
x,y \sim \mathrm{NG}(\mu, \Lambda, a, b) \quad \Leftrightarrow \quad x \vert y \sim \mathcal{N}(\mu, (y \Lambda)^{-1}) \quad \wedge \quad y \sim \mathrm{Gam}(a,b) \; .
\end{equation}

Thus, with the expected value of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-mean}) and the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), $\mathrm{E}(x)$ becomes

\begin{equation} \label{eq:ng-mean-mean-x}
\begin{split}
\mathrm{E}(x) &= \iint x \cdot p(x,y) \, \mathrm{d}x \, \mathrm{d}y \\
&= \iint x \cdot p(x|y) \cdot p(y) \, \mathrm{d}x \, \mathrm{d}y \\
&= \int p(y) \int x \cdot p(x|y) \, \mathrm{d}x \, \mathrm{d}y \\
&= \int p(y) \left\langle x \right\rangle_{\mathcal{N}(\mu, (y \Lambda)^{-1})} \, \mathrm{d}y \\
&= \int p(y) \cdot \mu \, \mathrm{d}y \\
&= \mu \int p(y) \, \mathrm{d}y \\
&= \mu \; ,
\end{split}
\end{equation}

and with the expected value of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-mean}), $\mathrm{E}(y)$ becomes

\begin{equation} \label{eq:ng-mean-mean-y}
\begin{split}
\mathrm{E}(y) &= \int y \cdot p(y) \, \mathrm{d}y \\
&= \left\langle y \right\rangle_{\mathrm{Gam}(a,b)} \\
&= \frac{a}{b} \; .
\end{split}
\end{equation}

Thus, the expectation of the random vector in equations \eqref{eq:ng-mean-rvec} and \eqref{eq:ng-mean-mean-rvec} is

\begin{equation} \label{eq:ng-mean-ng-mean-qed}
\mathrm{E}\left( \left[ \begin{array}{c} x \\ y \end{array} \right] \right) = \left[ \begin{array}{c} \mu \\ a/b \end{array} \right] \; ,
\end{equation}

as indicated by equation \eqref{eq:ng-mean-ng-mean}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Covariance}]{Covariance} \label{sec:ng-cov}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x \in \mathbb{R}^n$ and $y > 0$ follow a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}):

\begin{equation} \label{eq:ng-cov-ng}
x,y \sim \mathrm{NG}(\mu, \Lambda, a, b) \; .
\end{equation}

Then,

1) the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of $x$, conditional ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-cond}) on $y$ is

\begin{equation} \label{eq:ng-cov-ng-cov-cond}
\mathrm{Cov}(x \vert y) = \frac{1}{y} \Lambda^{-1} \; ;
\end{equation}

2) the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of $x$, unconditional ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) on $y$ is

\begin{equation} \label{eq:ng-cov-ng-cov-x}
\mathrm{Cov}(x) = \frac{b}{a-1} \Lambda^{-1} \; ;
\end{equation}

3) the variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of $y$ is

\begin{equation} \label{eq:ng-cov-ng-var-y}
\mathrm{Var}(y) = \frac{a}{b^2} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} 

1) According to the definition of the normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}), the distribution of $x$ given $y$ is a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}):

\begin{equation} \label{eq:ng-cov-ng-mvn}
x \vert y \sim \mathcal{N}(\mu, (y \Lambda)^{-1}) \; .
\end{equation}

The covariance of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-cov}) is

\begin{equation} \label{eq:ng-cov-mvn-cov}
x \sim \mathcal{N}(\mu, \Sigma) \quad \Rightarrow \quad \mathrm{Cov}(x) = \Sigma \; ,
\end{equation}

such that we have:

\begin{equation} \label{eq:ng-cov-ng-cov-cond-qed}
\mathrm{Cov}(x \vert y) = (y \Lambda)^{-1} = \frac{1}{y} \Lambda^{-1} \; .
\end{equation}

2) The marginal distribution of the normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-marg}) with respect to $x$ is a multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt}):

\begin{equation} \label{eq:ng-cov-ng-marg-x}
x \sim t\left( \mu, \left(\frac{a}{b} \Lambda \right)^{-1}, 2a \right) \; .
\end{equation}

The covariance of the multivariate t-distribution is

\begin{equation} \label{eq:ng-cov-mvt-cov}
x \sim t(\mu, \Sigma, \nu) \quad \Rightarrow \quad \mathrm{Cov}(x) = \frac{\nu}{\nu-2} \Sigma \; ,
\end{equation}

such that we have:

\begin{equation} \label{eq:ng-cov-ng-cov-x-qed}
\mathrm{Cov}(x) = \frac{2a}{2a-2} \left(\frac{a}{b} \Lambda \right)^{-1} = \frac{a}{a-1} \, \frac{b}{a} \, \Lambda^{-1} = \frac{b}{a-1} \Lambda^{-1} \; .
\end{equation}

3) The marginal distribution of the normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-marg}) with respect to $y$ is a univariate gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:ng-cov-ng-marg-y}
y \sim \mathrm{Gam}(a, b) \; .
\end{equation}

The variance of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-var}) is

\begin{equation} \label{eq:ng-cov-gam-var}
x \sim \mathrm{Gam}(a, b) \quad \Rightarrow \quad \mathrm{Var}(x) = \frac{a}{b^2} \; ,
\end{equation}

such that we have:

\begin{equation} \label{eq:ng-cov-ng-var-y-qed}
\mathrm{Var}(y) = \frac{a}{b^2} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Differential entropy}]{Differential entropy} \label{sec:ng-dent}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) and let $y$ be a positive random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Assume that $x$ and $y$ are jointly normal-gamma distributed:

\begin{equation} \label{eq:ng-dent-NG}
(x,y) \sim \mathrm{NG}(\mu, \Lambda^{-1}, a, b)
\end{equation}

Then, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $x$ in nats is

\begin{equation} \label{eq:ng-dent-NG-dent}
\begin{split}
\mathrm{h}(x,y) &= \frac{n}{2} \ln(2\pi) - \frac{1}{2} \ln|\Lambda| + \frac{1}{2} n \\
&+ a + \ln \Gamma(a) - \frac{n-2+2a}{2} \psi(a) + \frac{n-2}{2} \ln b \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} The probabibility density function of the normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-pdf}) is

\begin{equation} \label{eq:ng-dent-NG-pdf}
p(x,y) = p(x|y) \cdot p(y) = \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \cdot \mathrm{Gam}(y; a, b) \; .
\end{equation}

The differential entropy of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-dent}) is

\begin{equation} \label{eq:ng-dent-mvn-dent}
\mathrm{h}(x) = \frac{n}{2} \ln(2\pi) + \frac{1}{2} \ln|\Sigma| + \frac{1}{2} n
\end{equation}

and the differential entropy of the univariate gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-dent}) is

\begin{equation} \label{eq:ng-dent-gam-dent}
\mathrm{h}(y) = a + \ln \Gamma(a) + (1-a) \cdot \psi(a) - \ln b
\end{equation}

where $\Gamma(x)$ is the gamma function and $\psi(x)$ is the digamma function.

\vspace{1em}
The differential entropy of a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) in nats is given by

\begin{equation} \label{eq:ng-dent-dent}
\mathrm{h}(Z) = - \int_{\mathcal{Z}} p(z) \ln p(z) \, \mathrm{d}z
\end{equation}

which, applied to the normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}) over $x$ and $y$, yields

\begin{equation} \label{eq:ng-dent-NG-dent0}
\mathrm{h}(x,y) = - \int_{0}^{\infty} \int_{\mathbb{R}^n} p(x,y) \, \ln p(x,y) \, \mathrm{d}x \, \mathrm{d}y \; .
\end{equation}

Using the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), this can be evaluated as follows:

\begin{equation} \label{eq:ng-dent-NG-dent1}
\begin{split}
\mathrm{h}(x,y) &= - \int_{0}^{\infty} \int_{\mathbb{R}^n} p(x|y) \, p(y) \, \ln p(x|y) \, p(y) \, \mathrm{d}x \, \mathrm{d}y \\
&= - \int_{0}^{\infty} \int_{\mathbb{R}^n} p(x|y)\, p(y) \, \ln p(x|y) \, \mathrm{d}x \, \mathrm{d}y - \int_{0}^{\infty} \int_{\mathbb{R}^n} p(x|y)\, p(y) \, \ln p(y) \, \mathrm{d}x \, \mathrm{d}y \\
&= \int_{0}^{\infty} p(y) \int_{\mathbb{R}^n} p(x|y) \, \ln p(x|y) \, \mathrm{d}x \, \mathrm{d}y + \int_{0}^{\infty} p(y) \, \ln p(y) \int_{\mathbb{R}^n} p(x|y) \, \mathrm{d}x \, \mathrm{d}y \\
&= \left\langle \mathrm{h}(x|y) \right\rangle_{p(y)} + \mathrm{h}(y) \; .
\end{split}
\end{equation}

In other words, the differential entropy of the normal-gamma distribution over $x$ and $y$ is equal to the sum of a multivariate normal entropy regarding $x$ conditional on $y$, expected over $y$, and a univariate gamma entropy regarding $y$.

\vspace{1em}
From equations \eqref{eq:ng-dent-NG-pdf} and \eqref{eq:ng-dent-mvn-dent}, the first term becomes

\begin{equation} \label{eq:ng-dent-exp-mvn-dent-s1}
\begin{split}
\left\langle \mathrm{h}(x|y) \right\rangle_{p(y)} &= \left\langle \frac{n}{2} \ln(2\pi) + \frac{1}{2} \ln|(y \Lambda)^{-1}| + \frac{1}{2} n \right\rangle_{p(y)} \\
&= \left\langle \frac{n}{2} \ln(2\pi) - \frac{1}{2} \ln|(y \Lambda)| + \frac{1}{2} n \right\rangle_{p(y)} \\
&= \left\langle \frac{n}{2} \ln(2\pi) - \frac{1}{2} \ln(y^n |\Lambda|) + \frac{1}{2} n \right\rangle_{p(y)} \\
&= \frac{n}{2} \ln(2\pi) - \frac{1}{2} \ln|\Lambda| + \frac{1}{2} n - \left\langle \frac{n}{2} \ln y \right\rangle_{p(y)} \\
\end{split}
\end{equation}

and using the relation ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-logmean}) $y \sim \mathrm{Gam}(a,b) \Rightarrow \left\langle \ln y \right\rangle = \psi(a) - \ln(b)$, we have

\begin{equation} \label{eq:ng-dent-exp-mvn-dent-s2}
\begin{split}
\left\langle \mathrm{h}(x|y) \right\rangle_{p(y)} = \frac{n}{2} \ln(2\pi) - \frac{1}{2} \ln|\Lambda| + \frac{1}{2} n - \frac{n}{2} \psi(a) + \frac{n}{2} \ln b \; .
\end{split}
\end{equation}

By plugging \eqref{eq:ng-dent-exp-mvn-dent-s2} and \eqref{eq:ng-dent-gam-dent} into \eqref{eq:ng-dent-NG-dent1}, one arrives at the differential entropy given by \eqref{eq:ng-dent-NG-dent}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Kullback-Leibler divergence}]{Kullback-Leibler divergence} \label{sec:ng-kl}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ be an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) and let $y$ be a positive random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}). Assume two normal-gamma distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}) $P$ and $Q$ specifying the joint distribution of $x$ and $y$ as

\begin{equation} \label{eq:ng-kl-NGs}
\begin{split}
P: \; (x,y) &\sim \mathrm{NG}(\mu_1, \Lambda_1^{-1}, a_1, b_1) \\
Q: \; (x,y) &\sim \mathrm{NG}(\mu_2, \Lambda_2^{-1}, a_2, b_2) \; .
\end{split}
\end{equation}

Then, the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ is given by

\begin{equation} \label{eq:ng-kl-NG-KL}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \frac{1}{2} \frac{a_1}{b_1} \left[ (\mu_2 - \mu_1)^\mathrm{T} \Lambda_2 (\mu_2 - \mu_1) \right] + \frac{1}{2} \, \mathrm{tr}(\Lambda_2 \Lambda_1^{-1}) - \frac{1}{2} \ln \frac{|\Lambda_2|}{|\Lambda_1|} - \frac{n}{2} \\
&+ a_2 \, \ln \frac{b_1}{b_2} - \ln \frac{\Gamma(a_1)}{\Gamma(a_2)} + (a_1 - a_2) \, \psi(a_1) - (b_1 - b_2) \, \frac{a_1}{b_1} \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} The probabibility density function of the normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-pdf}) is

\begin{equation} \label{eq:ng-kl-NG-pdf}
p(x,y) = p(x|y) \cdot p(y) = \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \cdot \mathrm{Gam}(y; a, b) \; .
\end{equation}

The Kullback-Leibler divergence of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-kl}) is

\begin{equation} \label{eq:ng-kl-mvn-KL}
\mathrm{KL}[P\,||\,Q] = \frac{1}{2} \left[ (\mu_2 - \mu_1)^\mathrm{T} \Sigma_2^{-1} (\mu_2 - \mu_1) + \mathrm{tr}(\Sigma_2^{-1} \Sigma_1) - \ln \frac{|\Sigma_1|}{|\Sigma_2|} - n \right]
\end{equation}

and the Kullback-Leibler divergence of the univariate gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-kl}) is

\begin{equation} \label{eq:ng-kl-gam-KL}
\mathrm{KL}[P\,||\,Q] = a_2 \, \ln \frac{b_1}{b_2} - \ln \frac{\Gamma(a_1)}{\Gamma(a_2)} + (a_1 - a_2) \, \psi(a_1) - (b_1 - b_2) \, \frac{a_1}{b_1}
\end{equation}

where $\Gamma(x)$ is the gamma function and $\psi(x)$ is the digamma function.

\vspace{1em}
The KL divergence for a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is given by 

\begin{equation} \label{eq:ng-kl-KL-cont}
\mathrm{KL}[P\,||\,Q] = \int_{\mathcal{Z}} p(z) \, \ln \frac{p(z)}{q(z)} \, \mathrm{d}z
\end{equation}

which, applied to the normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}) over $x$ and $y$, yields

\begin{equation} \label{eq:ng-kl-NG-KL0}
\mathrm{KL}[P\,||\,Q] = \int_{0}^{\infty} \int_{\mathbb{R}^n} p(x,y) \, \ln \frac{p(x,y)}{q(x,y)} \, \mathrm{d}x \, \mathrm{d}y \; .
\end{equation}

Using the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), this can be evaluated as follows:

\begin{equation} \label{eq:ng-kl-NG-KL1}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \int_{0}^{\infty} \int_{\mathbb{R}^n} p(x|y) \, p(y) \, \ln \frac{p(x|y) \, p(y)}{q(x|y) \, q(y)} \, \mathrm{d}x \, \mathrm{d}y \\
&= \int_{0}^{\infty} \int_{\mathbb{R}^n} p(x|y)\, p(y) \, \ln \frac{p(x|y)}{q(x|y)} \, \mathrm{d}x \, \mathrm{d}y + \int_{0}^{\infty} \int_{\mathbb{R}^n} p(x|y)\, p(y) \, \ln \frac{p(y)}{q(y)} \, \mathrm{d}x \, \mathrm{d}y \\
&= \int_{0}^{\infty} p(y) \int_{\mathbb{R}^n} p(x|y) \, \ln \frac{p(x|y)}{q(x|y)} \, \mathrm{d}x \, \mathrm{d}y + \int_{0}^{\infty} p(y) \, \ln \frac{p(y)}{q(y)} \int_{\mathbb{R}^n} p(x|y) \, \mathrm{d}x \, \mathrm{d}y \\
&= \left\langle \mathrm{KL}[p(x|y)\,||\,q(x|y)] \right\rangle_{p(y)} + \mathrm{KL}[p(y)\,||\,q(y)] \; .
\end{split}
\end{equation}

In other words, the KL divergence between two normal-gamma distributions over $x$ and $y$ is equal to the sum of a multivariate normal KL divergence regarding $x$ conditional on $y$, expected over $y$, and a univariate gamma KL divergence regarding $y$.

\vspace{1em}
From equations \eqref{eq:ng-kl-NG-pdf} and \eqref{eq:ng-kl-mvn-KL}, the first term becomes

\begin{equation} \label{eq:ng-kl-exp-mvn-KL-s1}
\begin{split}
&\left\langle \mathrm{KL}[p(x|y)\,||\,q(x|y)] \right\rangle_{p(y)} \\
&= \left\langle \frac{1}{2} \left[ (\mu_2 - \mu_1)^\mathrm{T} (y \Lambda_2) (\mu_2 - \mu_1) + \mathrm{tr}\left( (y \Lambda_2) (y \Lambda_1)^{-1} \right) - \ln \frac{|(y \Lambda_1)^{-1}|}{|(y \Lambda_2)^{-1}|} - n \right] \right\rangle_{p(y)} \\
&= \left\langle \frac{y}{2} (\mu_2 - \mu_1)^\mathrm{T} \Lambda_2 (\mu_2 - \mu_1) + \frac{1}{2} \, \mathrm{tr}(\Lambda_2 \Lambda_1^{-1}) - \frac{1}{2} \ln \frac{|\Lambda_2|}{|\Lambda_1|} - \frac{n}{2} \right\rangle_{p(y)} \\
\end{split}
\end{equation}

and using the relation ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-mean}) $y \sim \mathrm{Gam}(a,b) \Rightarrow \left\langle y \right\rangle = a/b$, we have

\begin{equation} \label{eq:ng-kl-exp-mvn-KL-s2}
\begin{split}
\left\langle \mathrm{KL}[p(x|y)\,||\,q(x|y)] \right\rangle_{p(y)} = \frac{1}{2} \frac{a_1}{b_1} (\mu_2 - \mu_1)^\mathrm{T} \Lambda_2 (\mu_2 - \mu_1) + \frac{1}{2} \, \mathrm{tr}(\Lambda_2 \Lambda_1^{-1}) - \frac{1}{2} \ln \frac{|\Lambda_2|}{|\Lambda_1|} - \frac{n}{2} \; .
\end{split}
\end{equation}

By plugging \eqref{eq:ng-kl-exp-mvn-KL-s2} and \eqref{eq:ng-kl-gam-KL} into \eqref{eq:ng-kl-NG-KL1}, one arrives at the KL divergence given by \eqref{eq:ng-kl-NG-KL}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Soch J, Allefeld A (2016): "Kullback-Leibler Divergence for the Normal-Gamma Distribution"; in: \textit{arXiv math.ST}, 1611.01437; URL: \url{https://arxiv.org/abs/1611.01437}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Marginal distributions}]{Marginal distributions} \label{sec:ng-marg}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ and $y$ follow a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}):

\begin{equation} \label{eq:ng-marg-ng}
x,y \sim \mathrm{NG}(\mu, \Lambda, a, b) \; .
\end{equation}

Then, the marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of $y$ is a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam})

\begin{equation} \label{eq:ng-marg-ng-marg-y}
y \sim \mathrm{Gam}(a, b)
\end{equation}

and the marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of $x$ is a multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt})

\begin{equation} \label{eq:ng-marg-ng-marg-x}
x \sim t\left( \mu, \left(\frac{a}{b} \Lambda \right)^{-1}, 2a \right) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density function of the normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-pdf}) is given by

\begin{equation} \label{eq:ng-marg-ng-pdf}
\begin{split}
p(x,y) &= p(x|y) \cdot p(y) \\
p(x|y) &= \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \\
p(y) &= \mathrm{Gam}(y; a, b) \; .
\end{split}
\end{equation}

\vspace{1em}
Using the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), the marginal distribution of $y$ can be derived as

\begin{equation} \label{eq:ng-marg-ng-marg-y-qed}
\begin{split}
p(y) &= \int p(x,y) \, \mathrm{d}x \\
&= \int \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \, \mathrm{Gam}(y; a, b) \, \mathrm{d}x \\
&= \mathrm{Gam}(y; a, b) \int \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \, \mathrm{d}x \\
&= \mathrm{Gam}(y; a, b)
\end{split}
\end{equation}

which is the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}) with shape parameter $a$ and rate parameter $b$.

\vspace{1em}
Using the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), the marginal distribution of $x$ can be derived as

\begin{equation} \label{eq:ng-marg-ng-marg-x-qed}
\begin{split}
p(x) &= \int p(x,y) \, \mathrm{d}y \\
&= \int \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \, \mathrm{Gam}(y; a, b) \, \mathrm{d}y \\
&= \int \sqrt{\frac{|y \Lambda|}{(2 \pi)^n}} \, \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} (y \Lambda) (x-\mu) \right] \cdot \frac{b^a}{\Gamma(a)} \, y^{a-1} \exp[-b y] \, \mathrm{d}y \\
&= \int \sqrt{\frac{y^n |\Lambda|}{(2 \pi)^n}} \, \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} (y \Lambda) (x-\mu) \right] \cdot \frac{b^a}{\Gamma(a)} \, y^{a-1} \exp[-b y] \, \mathrm{d}y \\
&= \int \sqrt{\frac{|\Lambda|}{(2 \pi)^n}} \cdot \frac{b^a}{\Gamma(a)} \cdot y^{a+\frac{n}{2}-1} \cdot \exp \left[ -\left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right) y \right] \mathrm{d}y \\
&= \int \sqrt{\frac{|\Lambda|}{(2 \pi)^n}} \cdot \frac{b^a}{\Gamma(a)} \cdot \frac{\Gamma\left( a+\frac{n}{2} \right)}{\left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{a+\frac{n}{2}}} \cdot \mathrm{Gam}\left( y; a+\frac{n}{2}, b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right) \mathrm{d}y \\
&= \sqrt{\frac{|\Lambda|}{(2 \pi)^n}} \cdot \frac{b^a}{\Gamma(a)} \cdot \frac{\Gamma\left( a+\frac{n}{2} \right)}{\left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{a+\frac{n}{2}}} \int \mathrm{Gam}\left( y; a+\frac{n}{2}, b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right) \mathrm{d}y \\
&= \sqrt{\frac{|\Lambda|}{(2 \pi)^n}} \cdot \frac{b^a}{\Gamma(a)} \cdot \frac{\Gamma\left( a+\frac{n}{2} \right)}{\left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{a+\frac{n}{2}}} \\
&= \frac{\sqrt{|\Lambda|}}{(2 \pi)^\frac{n}{2}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot b^a \cdot \left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{-\left( a+\frac{n}{2} \right)} \\
&= \frac{\sqrt{|\Lambda|}}{\pi^\frac{n}{2}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot \left( \frac{1}{b} \right)^{-a} \cdot \left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{-a} \cdot 2^{-\frac{n}{2}} \cdot \left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{-\frac{n}{2}} \\
&= \frac{\sqrt{|\Lambda|}}{\pi^\frac{n}{2}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot \left( 1 + \frac{1}{2b} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{-a} \cdot \left( 2b + (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{-\frac{n}{2}} \\
&= \frac{\sqrt{|\Lambda|}}{\pi^\frac{n}{2}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot \left( \frac{1}{2a} \right)^{-a} \cdot \left( 2a + (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-a} \cdot \left( \frac{b}{a} \right)^{-\frac{n}{2}} \cdot \left( 2a + (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-\frac{n}{2}} \\
&= \frac{\sqrt{\left( \frac{a}{b} \right)^n |\Lambda|}}{(2a)^{-a}\,\pi^\frac{n}{2}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot \left( 2a + (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-a} \cdot \left( 2a + (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-\frac{n}{2}} \\
&= \frac{\sqrt{\left( \frac{a}{b} \right)^n |\Lambda|}}{(2a)^{-a}\,\pi^\frac{n}{2}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot (2a)^{-a} \cdot \left( 1 + \frac{1}{2a} (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-a} \cdot (2a)^{-\frac{n}{2}} \cdot \left( 1 + \frac{1}{2a} (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-\frac{n}{2}} \\
&= \frac{\sqrt{\left( \frac{a}{b} \right)^n |\Lambda|}}{(2a)^\frac{n}{2}\,\pi^\frac{n}{2}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot \left( 1 + \frac{1}{2a} (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-\frac{2a+n}{2}} \\
&= \sqrt{\frac{\left| \frac{a}{b}\,\Lambda \right|}{(2a\,\pi)^n}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot \left( 1 + \frac{1}{2a} (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-\frac{2a+n}{2}} \\
\end{split}
\end{equation}

which is the probability density function of a multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt-pdf}) with mean vector $\mu$, shape matrix $\left( \frac{a}{b}\Lambda \right)^{-1}$ and $2a$ degrees of freedom.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Conditional distributions}]{Conditional distributions} \label{sec:ng-cond}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ and $y$ follow a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}):

\begin{equation} \label{eq:ng-cond-ng}
x,y \sim \mathrm{NG}(\mu, \Lambda, a, b) \; .
\end{equation}

Then,

1) the conditional distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-cond}) of $x$ given $y$ is a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn})

\begin{equation} \label{eq:ng-cond-ng-cond-x-y}
x|y \sim \mathcal{N}(\mu, (y \Lambda)^{-1}) \; ;
\end{equation}

2) the conditional distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-cond}) of a subset vector $x_1$, given the complement vector $x_2$ and $y$, is also a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn})

\begin{equation} \label{eq:ng-cond-ng-cond-x1-x2-y}
x_1|x_2,y \sim \mathcal{N}(\mu_{1|2}(y), \Sigma_{1|2}(y))
\end{equation}

with the conditional mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) and covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov})

\begin{equation} \label{eq:ng-cond-ng-cond-x1-x2-y-hyp}
\begin{split}
\mu_{1|2}(y) &= \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (x_2 - \mu_2) \\
\Sigma_{1|2}(y) &= \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{12}
\end{split}
\end{equation}

where $\mu_1$, $\mu_2$ and $\Sigma_{11}$, $\Sigma_{12}$, $\Sigma_{22}$, $\Sigma_{21}$ are block-wise components ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-cond}) of $\mu$ and $\Sigma(y) = (y \Lambda)^{-1}$;

3) the conditional distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-cond}) of $y$ given $x$ is a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam})

\begin{equation} \label{eq:ng-cond-ng-cond-y-x}
y|x \sim \mathrm{Gam}\left( a + \frac{n}{2}, b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)
\end{equation}

where $n$ is the dimensionality of $x$.


\vspace{1em}
\textbf{Proof:}

1) This follows from the definition of the normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}):

\begin{equation} \label{eq:ng-cond-ng-pdf}
\begin{split}
p(x,y) &= p(x|y) \cdot p(y) \\
&= \mathcal{N}(x; \mu, (y \Lambda)^{-1}) \cdot \mathrm{Gam}(y; a, b) \; .
\end{split}
\end{equation}

2) This follows from \eqref{eq:ng-cond-ng-cond-x-y} and the conditional distributions of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-cond}):

\begin{equation} \label{eq:ng-cond-mvn-cond}
\begin{split}
x &\sim \mathcal{N}(\mu, \Sigma) \\
\Rightarrow x_1|x_2 &\sim \mathcal{N}(\mu_{1|2}, \Sigma_{1|2}) \\
\mu_{1|2} &= \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (x_2 - \mu_2) \\
\Sigma_{1|2} &= \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} \; .
\end{split}
\end{equation}

3) The conditional density of $y$ given $x$ follows from Bayes' theorem ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:bayes-th}) as

\begin{equation} \label{eq:ng-cond-ng-cond-y-x-s1}
p(y|x) = \frac{p(x|y) \cdot p(y)}{p(x)} \; .
\end{equation}

The conditional distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-cond}) of $x$ given $y$ is a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-pdf})

\begin{equation} \label{eq:ng-cond-ng-x-y-pdf}
p(x|y) = \mathcal{N}(x; \mu, (y \Lambda)^{-1}) = \sqrt{\frac{|y \Lambda|}{(2 \pi)^n}} \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} (y \Lambda) (x-\mu) \right] \; ,
\end{equation}

the marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of $y$ is a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-marg})

\begin{equation} \label{eq:ng-cond-ng-y-pdf}
p(y) = \mathrm{Gam}(y; a, b) = \frac{b^a}{\Gamma(a)} y^{a-1} \exp\left[ -by \right]
\end{equation}

and the marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of $x$ is a multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-marg})

\begin{equation} \label{eq:ng-cond-ng-x-pdf}
\begin{split}
p(x) &= t\left( x; \mu, \left(\frac{a}{b} \Lambda \right)^{-1}, 2a \right) \\
&= \sqrt{\frac{\left| \frac{a}{b}\,\Lambda \right|}{(2a\,\pi)^n}} \cdot \frac{\Gamma\left( \frac{2a+n}{2} \right)}{\Gamma\left( \frac{2a}{2} \right)} \cdot \left( 1 + \frac{1}{2a} (x-\mu)^\mathrm{T} \left( \frac{a}{b}\Lambda \right) (x-\mu) \right)^{-\frac{2a+n}{2}} \\
&= \sqrt{\frac{|\Lambda|}{(2 \pi)^n}} \cdot \frac{\Gamma\left( a+\frac{n}{2} \right)}{\Gamma(a)} \cdot b^a \cdot \left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{-\left( a+\frac{n}{2} \right)} \; .
\end{split}
\end{equation}

Plugging \eqref{eq:ng-cond-ng-x-y-pdf}, \eqref{eq:ng-cond-ng-y-pdf} and \eqref{eq:ng-cond-ng-x-pdf} into \eqref{eq:ng-cond-ng-cond-y-x-s1}, we obtain

\begin{equation} \label{eq:ng-cond-ng-cond-y-x-s2}
\begin{split}
p(y|x) &= \frac{\sqrt{\frac{|y \Lambda|}{(2 \pi)^n}} \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} (y \Lambda) (x-\mu) \right] \cdot \frac{b^a}{\Gamma(a)} y^{a-1} \exp\left[ -by \right]}{\sqrt{\frac{|\Lambda|}{(2 \pi)^n}} \cdot \frac{\Gamma\left( a+\frac{n}{2} \right)}{\Gamma(a)} \cdot b^a \cdot \left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{-\left( a+\frac{n}{2} \right)}} \\
&= y^{\frac{n}{2}} \cdot \exp \left[ -\frac{1}{2} (x-\mu)^\mathrm{T} (y \Lambda) (x-\mu) \right] \cdot y^{a-1} \cdot \exp\left[ -by \right] \cdot \frac{1}{\Gamma\left( a+\frac{n}{2} \right)} \cdot \left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{a+\frac{n}{2}} \\
&= \frac{\left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right)^{a+\frac{n}{2}}}{\Gamma\left( a+\frac{n}{2} \right)} \cdot y^{a+\frac{n}{2}-1} \cdot \exp \left[ -\left( b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right) \right]
\end{split}
\end{equation}

which is the probability density function of a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}) with shape and rate parameters

\begin{equation} \label{eq:ng-cond-ng-cond-y-x-hyp}
a + \frac{n}{2} \quad \text{and} \quad b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \; ,
\end{equation}

such that

\begin{equation} \label{eq:ng-cond-ng-cond-y-x-qed}
p(y|x) = \mathrm{Gam}\left( y; a + \frac{n}{2}, b + \frac{1}{2} (x-\mu)^\mathrm{T} \Lambda (x-\mu) \right) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Drawing samples}]{Drawing samples} \label{sec:ng-samp}
\setcounter{equation}{0}

\textbf{Theorem:} Let $Z_1 \in \mathbb{R}^n$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) with all entries independently following a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) and let $Z_2 \in \mathbb{R}$ be a random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) following a standard gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:sgam}) with shape $a$. Moreover, let $A \in \mathbb{R}^{n \times n}$ be a matrix, such that $A A^\mathrm{T} = \Lambda^{-1}$.

Then, $X = \mu + A Z_1 / \sqrt{Z_2/b}$ and $Y = Z_2/b$ jointly follow a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}) with mean vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-rvec}) $\mu$, precision matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:precmat}) $\Lambda$, shape parameter $a$ and rate parameter $b$:

\begin{equation} \label{eq:ng-samp-ng-samp}
\left( X = \mu + A Z_1 / \sqrt{Z_2/b}, \; Y = Z_2/b \right) \sim \mathrm{NG}(\mu, \Lambda, a, b) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} If all entries of $Z_1$ are independent and standard normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm})

\begin{equation} \label{eq:ng-samp-zi-dist}
z_{1i} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, 1) \quad \text{for all} \quad i = 1,\ldots,n \; ,
\end{equation}

this implies a multivariate normal distribution with diagonal covariance matrix ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ind}):

\begin{equation} \label{eq:ng-samp-Z1-dist}
Z_1 \sim \mathcal{N}\left(0_n, I_n \right)
\end{equation}

where $0_n$ is an $n \times 1$ matrix of zeros and $I_n$ is the $n \times n$ identity matrix.

If the distribution of $Z_2$ is a standard gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:sgam})

\begin{equation} \label{eq:ng-samp-Z2-dist}
Z_2 \sim \mathrm{Gam}(a, 1) \; ,
\end{equation}

then due to the relationship between gamma and standard gamma distribution distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-sgam}), we have:

\begin{equation} \label{eq:ng-samp-Y-dist}
Y = \frac{Z_2}{b} \sim \mathrm{Gam}(a,b) \; .
\end{equation}

Moreover, using the linear transformation theorem for the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}), it follows that:

\begin{equation} \label{eq:ng-samp-X-dist}
\begin{split}
Z_1 &\sim \mathcal{N}\left(0_n, I_n \right) \\
X = \mu + \frac{1}{\sqrt{Z_2/b}} A Z_1 &\sim \mathcal{N}\left(\mu + \frac{1}{\sqrt{Z_2/b}} A \, 0_n, \left( \frac{1}{\sqrt{Z_2/b}} A \right) I_n \left( \frac{1}{\sqrt{Z_2/b}} A \right)^\mathrm{T} \right) \\
X &\sim \mathcal{N}\left(\mu + 0_n, \left( \frac{1}{\sqrt{Y}} \right)^2 A A^\mathrm{T} \right) \\
X &\sim \mathcal{N}\left(\mu, \left( Y \Lambda \right)^{-1} \right) \; .
\end{split}
\end{equation}

Thus, $Y$ follows a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) and the distribution of $X$ conditional on $Y$ is a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}):

\begin{equation} \label{eq:ng-samp-mvn-gam}
\begin{split}
X \vert Y &\sim \mathcal{N}(\mu, (Y \Lambda)^{-1}) \\
Y &\sim \mathrm{Gam}(a, b) \; .
\end{split}
\end{equation}

This means that, by definition ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}), $X$ and $Y$ jointly follow a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}):

\begin{equation} \label{eq:ng-samp-ng-samp-qed}
X,Y \sim \mathrm{NG}(\mu, \Lambda, a, b) \; ,
\end{equation}

Thus, given $Z_1$ defined by \eqref{eq:ng-samp-zi-dist} and $Z_2$ defined by \eqref{eq:ng-samp-Z2-dist}, $X$ and $Y$ defined by \eqref{eq:ng-samp-ng-samp} are a sample from $\mathrm{NG}(\mu, \Lambda, a, b)$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Normal-gamma distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-09-22; URL: \url{https://en.wikipedia.org/wiki/Normal-gamma_distribution#Generating_normal-gamma_random_variates}.
\end{itemize}
\vspace{1em}



\subsection{Dirichlet distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:dir}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be a $k \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Then, $X$ is said to follow a Dirichlet distribution with concentration parameters $\alpha = \left[ \alpha_1, \ldots, \alpha_k \right]$

\begin{equation} \label{eq:dir-Dir}
X \sim \mathrm{Dir}(\alpha) \; ,
\end{equation}

if and only if its probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) is given by

\begin{equation} \label{eq:dir-beta-pdf}
\mathrm{Dir}(x; \alpha) = \frac{\Gamma\left( \sum_{i=1}^k \alpha_i \right)}{\prod_{i=1}^k \Gamma(\alpha_i)} \, \prod_{i=1}^k {x_i}^{\alpha_i-1}
\end{equation}

where $\alpha_i > 0$ for all $i = 1, \ldots, k$, and the density is zero, if $x_i \notin [0,1]$ for any $i = 1, \ldots, k$ or $\sum_{i=1}^k x_i \neq 1$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Dirichlet distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-05-10; URL: \url{https://en.wikipedia.org/wiki/Dirichlet_distribution#Probability_density_function}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:dir-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a Dirichlet distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir}):

\begin{equation} \label{eq:dir-pdf-Dir}
X \sim \mathrm{Dir}(\alpha) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ is

\begin{equation} \label{eq:dir-pdf-Dir-pdf}
f_X(x) = \frac{\Gamma\left( \sum_{i=1}^k \alpha_i \right)}{\prod_{i=1}^k \Gamma(\alpha_i)} \, \prod_{i=1}^k {x_i}^{\alpha_i-1} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the Dirichlet distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Kullback-Leibler divergence}]{Kullback-Leibler divergence} \label{sec:dir-kl}
\setcounter{equation}{0}

\textbf{Theorem:} Let $x$ be an $k \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}). Assume two Dirichlet distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir}) $P$ and $Q$ specifying the probability distribution of $x$ as

\begin{equation} \label{eq:dir-kl-dirs}
\begin{split}
P: \; x &\sim \mathrm{Dir}(\alpha_1) \\
Q: \; x &\sim \mathrm{Dir}(\alpha_2) \; .
\end{split}
\end{equation}

Then, the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ is given by

\begin{equation} \label{eq:dir-kl-dir-KL}
\mathrm{KL}[P\,||\,Q] = \ln \frac{\Gamma\left(\sum_{i=1}^{k} \alpha_{1i}\right)}{\Gamma\left(\sum_{i=1}^{k} \alpha_{2i}\right)} + \sum_{i=1}^{k} \ln \frac{\Gamma(\alpha_{2i})}{\Gamma(\alpha_{1i})} + \sum_{i=1}^{k} \left( \alpha_{1i} - \alpha_{2i} \right) \left[ \psi(\alpha_{1i}) - \psi\left(\sum_{i=1}^{k} \alpha_{1i}\right) \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The KL divergence for a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is given by 

\begin{equation} \label{eq:dir-kl-KL-cont}
\mathrm{KL}[P\,||\,Q] = \int_{\mathcal{X}} p(x) \, \ln \frac{p(x)}{q(x)} \, \mathrm{d}x
\end{equation}

which, applied to the Dirichlet distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) in \eqref{eq:dir-kl-dirs}, yields

\begin{equation} \label{eq:dir-kl-dir-KL-s1}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \int_{\mathcal{X}^k} \mathrm{Dir}(x; \alpha_1) \, \ln \frac{\mathrm{Dir}(x; \alpha_1)}{\mathrm{Dir}(x; \alpha_2)} \, \mathrm{d}x \\
&= \left\langle \ln \frac{\mathrm{Dir}(x; \alpha_1)}{\mathrm{Dir}(x; \alpha_2)} \right\rangle_{p(x)}
\end{split}
\end{equation}

where $\mathcal{X}^k$ is the set $\left\lbrace x \in \mathbb{R}^k \; \vert \; \sum_{i=1}^{k} x_i = 1, \; 0 \leq x_i \leq 1, \; i = 1,\ldots,k \right\rbrace$.

Using the probability density function of the Dirichlet distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir-pdf}), this becomes:

\begin{equation} \label{eq:dir-kl-dir-KL-s2}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \left\langle \ln \frac{ \frac{\Gamma\left( \sum_{i=1}^k \alpha_{1i} \right)}{\prod_{i=1}^k \Gamma(\alpha_{1i})} \, \prod_{i=1}^k {x_i}^{\alpha_{1i}-1} }{ \frac{\Gamma\left( \sum_{i=1}^k \alpha_{2i} \right)}{\prod_{i=1}^k \Gamma(\alpha_{2i})} \, \prod_{i=1}^k {x_i}^{\alpha_{2i}-1} } \right\rangle_{p(x)} \\
&= \left\langle \ln \left( \frac{\Gamma\left( \sum_{i=1}^k \alpha_{1i} \right)}{\Gamma\left( \sum_{i=1}^k \alpha_{2i} \right)} \cdot \frac{\prod_{i=1}^k \Gamma(\alpha_{2i})}{\prod_{i=1}^k \Gamma(\alpha_{1i})} \cdot \prod_{i=1}^k {x_i}^{\alpha_{1i}-\alpha_{2i}} \right) \right\rangle_{p(x)} \\
&= \left\langle \ln \frac{\Gamma\left( \sum_{i=1}^k \alpha_{1i} \right)}{\Gamma\left( \sum_{i=1}^k \alpha_{2i} \right)} + \sum_{i=1}^k \ln \frac{\Gamma(\alpha_{2i})}{\Gamma(\alpha_{1i})} + \sum_{i=1}^k (\alpha_{1i}-\alpha_{2i}) \cdot \ln (x_i) \right\rangle_{p(x)} \\
&= \ln \frac{\Gamma\left( \sum_{i=1}^k \alpha_{1i} \right)}{\Gamma\left( \sum_{i=1}^k \alpha_{2i} \right)} + \sum_{i=1}^k \ln \frac{\Gamma(\alpha_{2i})}{\Gamma(\alpha_{1i})} + \sum_{i=1}^k (\alpha_{1i}-\alpha_{2i}) \cdot \left\langle \ln x_i \right\rangle_{p(x)} \; .
\end{split}
\end{equation}

Using the expected value of a logarithmized Dirichlet variate

\begin{equation} \label{eq:dir-kl-dir-logmean}
x \sim \mathrm{Dir}(\alpha) \quad \Rightarrow \quad \left\langle \ln x_i \right\rangle = \psi(\alpha_i) - \psi\left(\sum_{i=1}^{k} \alpha_i\right) \; ,
\end{equation}

the Kullback-Leibler divergence from \eqref{eq:dir-kl-dir-KL-s2} becomes:

\begin{equation} \label{eq:dir-kl-dir-KL-s3}
\mathrm{KL}[P\,||\,Q] = \ln \frac{\Gamma\left( \sum_{i=1}^k \alpha_{1i} \right)}{\Gamma\left( \sum_{i=1}^k \alpha_{2i} \right)} + \sum_{i=1}^k \ln \frac{\Gamma(\alpha_{2i})}{\Gamma(\alpha_{1i})} + \sum_{i=1}^k (\alpha_{1i}-\alpha_{2i}) \cdot \left[ \psi(\alpha_{1i}) - \psi\left(\sum_{i=1}^{k} \alpha_{1i}\right) \right]
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Penny, William D. (2001): "KL-Divergences of Normal, Gamma, Dirichlet and Wishart densities"; in: \textit{University College, London}, p. 2, eqs. 8-9; URL: \url{https://www.fil.ion.ucl.ac.uk/~wpenny/publications/densities.ps}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Exceedance probabilities}]{Exceedance probabilities} \label{sec:dir-ep}
\setcounter{equation}{0}

\textbf{Theorem:} Let $r = [r_1, \ldots, r_k]$ be a random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) following a Dirichlet distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir}) with concentration parameters $\alpha = [\alpha_1, \ldots, \alpha_k]$:

\begin{equation} \label{eq:dir-ep-r-Dir}
r \sim \mathrm{Dir}(\alpha) \; .
\end{equation}

\vspace{1em}
1) If $k = 2$, then the exceedance probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-exc}) for $r_1$ is

\begin{equation} \label{eq:dir-ep-Dir2-EP}
\varphi_1 = 1 - \frac{\mathrm{B}\left( \frac{1}{2};\alpha_1,\alpha_2 \right)}{\mathrm{B}(\alpha_1,\alpha_2)}
\end{equation}

where $\mathrm{B}(x,y)$ is the beta function and $\mathrm{B}(x;a,b)$ is the incomplete beta function.

\vspace{1em}
2) If $k > 2$, then the exceedance probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-exc}) for $r_i$ is

\begin{equation} \label{eq:dir-ep-Dir-EP}
\varphi_i = \int_0^\infty \prod_{j \neq i} \left( \frac{\gamma(\alpha_j,q_i)}{\Gamma(\alpha_j)} \right) \, \frac{q_i^{\alpha_i-1} \exp[-q_i]}{\Gamma(\alpha_i)} \, \mathrm{d}q_i \; .
\end{equation}

where $\Gamma(x)$ is the gamma function and $\gamma(s,x)$ is the lowerr incomplete gamma function.


\vspace{1em}
\textbf{Proof:} In the context of the Dirichlet distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir}), the exceedance probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-exc}) for a particular $r_i$ is defined as:

\begin{equation} \label{eq:dir-ep-Dir-EP-def}
\begin{split}
\varphi_i &= p \Bigl( \forall j \in \left\lbrace 1, \ldots, k \Bigm| j \neq i \right\rbrace: \, r_i > r_j |\alpha \bigr) \\
&= p \Bigl( \bigwedge_{j \neq i} r_i > r_j \Bigm| \alpha \Bigr) \; .
\end{split}
\end{equation}

The probability density function of the Dirichlet distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir-pdf}) is given by:

\begin{equation} \label{eq:dir-ep-Dir-pdf}
\mathrm{Dir}(r; \alpha) = \frac{\Gamma\left( \sum_{i=1}^k \alpha_i \right)}{\prod_{i=1}^k \Gamma(\alpha_i)} \, \prod_{i=1}^k {r_i}^{\alpha_i-1} \; .
\end{equation}

Note that the probability density function is only calculated, if

\begin{equation} \label{eq:dir-ep-Dir-req}
r_i \in [0,1] \quad \text{for} \quad i = 1,\ldots,k \quad \text{and} \quad \sum_{i=1}^k r_i = 1 \; ,
\end{equation}

and defined to be zero otherwise ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir}).

\vspace{1em}
1) If $k = 2$, the probability density function of the Dirichlet distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir-pdf}) reduces to

\begin{equation} \label{eq:dir-ep-Dir2-pdf}
p(r) = \frac{\Gamma(\alpha_1 + \alpha_2)}{\Gamma(\alpha_1) \, \Gamma(\alpha_2)} \, r_1^{\alpha_1-1} \, r_2^{\alpha_2-1}
\end{equation}

which is equivalent to the probability density function of the beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-pdf})

\begin{equation} \label{eq:dir-ep-Beta-pdf}
p(r_1) = \frac{r_1^{\alpha_1-1} \, (1-r_1)^{\alpha_2-1}}{\mathrm{B}(\alpha_1,\alpha_2)}
\end{equation}

with the beta function given by

\begin{equation} \label{eq:dir-ep-beta-fct}
\mathrm{B}(x,y) = \frac{\Gamma(x) \, \Gamma(y)}{\Gamma(x + y)} \; .
\end{equation}

With \eqref{eq:dir-ep-Dir-req}, the exceedance probability for this bivariate case simplifies to

\begin{equation} \label{eq:dir-ep-Dir2-EP-def}
\varphi_1 = p(r_1 > r_2) = p(r_1 > 1 - r_1) = p(r_1 > 1/2) = \int_{\frac{1}{2}}^1 p(r_1) \, \mathrm{d}r_1 \; .
\end{equation}

Using the cumulative distribution function of the beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-cdf}), it evaluates to

\begin{equation} \label{eq:dir-ep-Dir2-EP-qed}
\varphi_1 = 1 - \int_0^{\frac{1}{2}} p(r_1) \, \mathrm{d}r_1 = 1 - \frac{\mathrm{B}\left( \frac{1}{2};\alpha_1,\alpha_2 \right)}{\mathrm{B}(\alpha_1,\alpha_2)}
\end{equation}

with the incomplete beta function

\begin{equation} \label{eq:dir-ep-inc-beta-fct}
\mathrm{B}(x; a, b) = \int_0^x x^{a-1} \, (1-x)^{b-1} \, \mathrm{d}x \; .
\end{equation}

\vspace{1em}
2) If $k > 2$, there is no similarly simple expression, because in general

\begin{equation} \label{eq:dir-ep-Dir-EP-ineq}
\varphi_i = p(r_i = \mathrm{max}(r)) > p(r_i > 1/2) \quad \text{for} \quad i = 1, \ldots, k \; ,
\end{equation}

i.e. exceedance probabilities cannot be evaluated using a simple threshold on $r_i$, because $r_i$ might be the maximal element in $r$ without being larger than $1/2$. Instead, we make use of the relationship between the Dirichlet and the gamma distribution which states that

\begin{equation} \label{eq:dir-ep-Gam-Dir}
\begin{split}
& Y_1 \sim \mathrm{Gam}(\alpha_1,\beta), \, \ldots, \, Y_k \sim \mathrm{Gam}(\alpha_k,\beta), \, Y_s = \sum_{i=1}^k Y_j \\
\Rightarrow \; & X = (X_1, \ldots, X_k) = \left( \frac{Y_1}{Y_s}, \ldots, \frac{Y_k}{Y_s} \right) \sim \mathrm{Dir}(\alpha_1, \ldots, \alpha_k) \; .
\end{split}
\end{equation}

The probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}) is given by

\begin{equation} \label{eq:dir-ep-Gam-pdf}
\mathrm{Gam}(x; a, b) = \frac{ {b}^{a} }{\Gamma(a)} \, x^{a-1} \, \exp[-b x] \quad \text{for} \quad x > 0 \; .
\end{equation}

Consider the gamma random variables ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam})

\begin{equation} \label{eq:dir-ep-Gam-Dir-A}
q_1 \sim \mathrm{Gam}(\alpha_1,1), \, \ldots, \, q_k \sim \mathrm{Gam}(\alpha_k,1), \, q_s = \sum_{j=1}^k q_j
\end{equation}

and the Dirichlet random vector ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir})

\begin{equation} \label{eq:dir-ep-Gam-Dir-B}
r = (r_1, \ldots, r_k) = \left( \frac{q_1}{q_s}, \ldots, \frac{q_k}{q_s} \right) \sim \mathrm{Dir}(\alpha_1, \ldots, \alpha_k) \; .
\end{equation}

Obviously, it holds that

\begin{equation} \label{eq:dir-ep-Gam-Dir-eq}
r_i > r_j \; \Leftrightarrow \; q_i > q_j \quad \text{for} \quad i,j = 1, \ldots, k \quad \text{with} \quad j \neq i \; .
\end{equation}

Therefore, consider the probability that $q_i$ is larger than $q_j$, given $q_i$ is known. This probability is equal to the probability that $q_j$ is smaller than $q_i$, given $q_i$ is known

\begin{equation} \label{eq:dir-ep-Gam-EP0}
p(q_i > q_j|q_i) = p(q_j < q_i|q_i)
\end{equation}

which can be expressed in terms of the cumulative distribution function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-cdf}) as

\begin{equation} \label{eq:dir-ep-Gam-EP1}
p(q_j < q_i|q_i) = \int_0^{q_i} \mathrm{Gam}(q_j;\alpha_j,1) \, \mathrm{d}q_j = \frac{\gamma(\alpha_j,q_i)}{\Gamma(\alpha_j)}
\end{equation}

where $\Gamma(x)$ is the gamma function and $\gamma(s,x)$ is the lower incomplete gamma function. Since the gamma variates are independent of each other, these probabilties factorize:

\begin{equation} \label{eq:dir-ep-Gam-EP2}
p(\forall_{j \neq i} \left[ q_i > q_j \right]|q_i) = \prod_{j \neq i} p(q_i > q_j|q_i) = \prod_{j \neq i} \frac{\gamma(\alpha_j,q_i)}{\Gamma(\alpha_j)} \; .
\end{equation}

In order to obtain the exceedance probability $\varphi_i$, the dependency on $q_i$ in this probability still has to be removed. From equations \eqref{eq:dir-ep-Dir-EP-def} and \eqref{eq:dir-ep-Gam-Dir-eq}, it follows that

\begin{equation} \label{eq:dir-ep-Dir-EP2a}
\varphi_i = p(\forall_{j \neq i} \left[ r_i > r_j \right]) = p(\forall_{j \neq i} \left[ q_i > q_j \right]) \; .
\end{equation}

Using the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), we have

\begin{equation} \label{eq:dir-ep-Dir-EP2b}
\varphi_i = \int_0^\infty p(\forall_{j \neq i} \left[ q_i > q_j \right]|q_i) \, p(q_i) \, \mathrm{d}q_i \; .
\end{equation}

With \eqref{eq:dir-ep-Gam-EP2} and \eqref{eq:dir-ep-Gam-Dir-A}, this becomes

\begin{equation} \label{eq:dir-ep-Dir-EP2c}
\varphi_i = \int_0^\infty \prod_{j \neq i} \left( p(q_i > q_j|q_i) \right) \cdot \mathrm{Gam}(q_i;\alpha_i,1) \, \mathrm{d}q_i \; .
\end{equation}

And with \eqref{eq:dir-ep-Gam-EP1} and \eqref{eq:dir-ep-Gam-pdf}, it becomes

\begin{equation} \label{eq:dir-ep-Dir-EP-qed}
\varphi_i = \int_0^\infty \prod_{j \neq i} \left( \frac{\gamma(\alpha_j,q_i)}{\Gamma(\alpha_j)} \right) \cdot \frac{q_i^{\alpha_i-1} \exp[-q_i]}{\Gamma(\alpha_i)} \, \mathrm{d}q_i \; .
\end{equation}

In other words, the exceedance probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-exc}) for one element from a Dirichlet-distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir}) random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) is an integral from zero to infinity where the first term in the integrand conforms to a product of gamma ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) cumulative distribution functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) and the second term is a gamma ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}).

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Soch J, Allefeld C (2016): "Exceedance Probabilities for the Dirichlet Distribution"; in: \textit{arXiv stat.AP}, 1611.01439; URL: \url{https://arxiv.org/abs/1611.01439}.
\end{itemize}
\vspace{1em}



\pagebreak
\section{Matrix-variate continuous distributions}

\subsection{Matrix-normal distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:matn}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be an $n \times p$ random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}). Then, $X$ is said to be matrix-normally distributed with mean $M$, covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) across rows $U$ and covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) across columns $V$

\begin{equation} \label{eq:matn-matn}
X \sim \mathcal{MN}(M, U, V) \; ,
\end{equation}

if and only if its probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) is given by

\begin{equation} \label{eq:matn-matn-pdf}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( V^{-1} (X-M)^\mathrm{T} \, U^{-1} (X-M) \right) \right]
\end{equation}

where $M$ is an $n \times p$ real matrix, $U$ is an $n \times n$ positive definite matrix and $V$ is a $p \times p$ positive definite matrix.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Matrix normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-27; URL: \url{https://en.wikipedia.org/wiki/Matrix_normal_distribution#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Equivalence to multivariate normal distribution}]{Equivalence to multivariate normal distribution} \label{sec:matn-mvn}
\setcounter{equation}{0}

\textbf{Theorem:} The matrix $X$ is matrix-normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn})

\begin{equation} \label{eq:matn-mvn-matn}
X \sim \mathcal{MN}(M, U, V) \; ,
\end{equation}

if and only if $\mathrm{vec}(X)$ is multivariate normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn})

\begin{equation} \label{eq:matn-mvn-mvn}
\mathrm{vec}(X) \sim \mathcal{N}(\mathrm{vec}(M), V \otimes U)
\end{equation}

where $\mathrm{vec}(X)$ is the vectorization operator and $\otimes$ is the Kronecker product.


\vspace{1em}
\textbf{Proof:} The probability density function of the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-pdf}) with $n \times p$ mean $M$, $n \times n$ covariance across rows $U$ and $p \times p$ covariance across columns $V$ is

\begin{equation} \label{eq:matn-mvn-matn-pdf}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( V^{-1} (X-M)^\mathrm{T} \, U^{-1} (X-M) \right) \right] \; .
\end{equation}

Using the trace property $\mathrm{tr}(ABC) = \mathrm{tr}(BCA)$, we have:

\begin{equation} \label{eq:matn-mvn-matn-mvn-s1}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( (X-M)^\mathrm{T} \, U^{-1} (X-M) \, V^{-1} \right) \right] \; .
\end{equation}

Using the trace-vectorization relation $\mathrm{tr}(A^\mathrm{T} B) = \mathrm{vec}(A)^\mathrm{T} \, \mathrm{vec}(B)$, we have:

\begin{equation} \label{eq:matn-mvn-matn-mvn-s2}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{vec}(X-M)^\mathrm{T} \, \mathrm{vec}\left( U^{-1} (X-M) \, V^{-1} \right) \right] \; .
\end{equation}

Using the vectorization-Kronecker relation $\mathrm{vec}(ABC) = \left( C^\mathrm{T} \otimes A \right) \mathrm{vec}(B)$, we have:

\begin{equation} \label{eq:matn-mvn-matn-mvn-s3}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{vec}(X-M)^\mathrm{T} \, \left( V^{-1} \otimes U^{-1} \right) \mathrm{vec}(X-M) \right] \; .
\end{equation}

Using the Kronecker product property $\left( A^{-1} \otimes B^{-1} \right) = \left( A \otimes B \right)^{-1}$, we have:

\begin{equation} \label{eq:matn-mvn-matn-mvn-s4}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{vec}(X-M)^\mathrm{T} \, \left( V \otimes U \right)^{-1} \mathrm{vec}(X-M) \right] \; .
\end{equation}

Using the vectorization property $\mathrm{vec}(A+B) = \mathrm{vec}(A) + \mathrm{vec}(B)$, we have:

\begin{equation} \label{eq:matn-mvn-matn-mvn-s5}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \left[ \mathrm{vec}(X) - \mathrm{vec}(M) \right]^\mathrm{T} \, \left( V \otimes U \right)^{-1} \left[ \mathrm{vec}(X) - \mathrm{vec}(M) \right] \right] \; .
\end{equation}

Using the Kronecker-determinant relation $\lvert A \otimes B \rvert = \lvert A \rvert^m \lvert B \rvert^n$, we have:

\begin{equation} \label{eq:matn-mvn-matn-mvn-s6}
\mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V \otimes U|}} \cdot \exp\left[-\frac{1}{2} \left[ \mathrm{vec}(X) - \mathrm{vec}(M) \right]^\mathrm{T} \, \left( V \otimes U \right)^{-1} \left[ \mathrm{vec}(X) - \mathrm{vec}(M) \right] \right] \; .
\end{equation}

This is the probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}) with the $np \times 1$ mean vector $\mathrm{vec}(M)$ and the $np \times np$ covariance matrix $V \otimes U$:

\begin{equation} \label{eq:matn-mvn-matn-mvn}
\mathcal{MN}(X; M, U, V) = \mathcal{N}(\mathrm{vec}(X); \mathrm{vec}(M), V \otimes U) \; .
\end{equation}

By showing that the probability density functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) are identical, it is proven that the associated probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) are equivalent.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Matrix normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-20; URL: \url{https://en.wikipedia.org/wiki/Matrix_normal_distribution#Proof}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:matn-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}) following a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}):

\begin{equation} \label{eq:matn-pdf-matn}
X \sim \mathcal{MN}(M, U, V) \; .
\end{equation}

Then, the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ is

\begin{equation} \label{eq:matn-pdf-matn-pdf}
f(X) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( V^{-1} (X-M)^\mathrm{T} \, U^{-1} (X-M) \right) \right] \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This follows directly from the definition of the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:matn-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}) following a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}):

\begin{equation} \label{eq:matn-mean-matn}
X \sim \mathcal{MN}(M, U, V) \; .
\end{equation}

Then, the mean or expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ is

\begin{equation} \label{eq:matn-mean-matn-mean}
\mathrm{E}(X) = M \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} When $X$ follows a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}), its vectorized version follows a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-mvn})

\begin{equation} \label{eq:matn-mean-matn-mvn}
\mathrm{vec}(X) \sim \mathcal{N}(\mathrm{vec}(M), V \otimes U)
\end{equation}

and the expected value of this multivariate normal distribution is ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-mean})

\begin{equation} \label{eq:matn-mean-mvn-mean}
\mathrm{E}[\mathrm{vec}(X)] = \mathrm{vec}(M) \; .
\end{equation}

Since the expected value of a random matrix is calculated element-wise ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-rmat}), we can invert the vectorization operator to get:

\begin{equation} \label{eq:matn-mean-matn-mean-qed}
\mathrm{E}[X] = M \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Matrix normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-09-15; URL: \url{https://en.wikipedia.org/wiki/Matrix_normal_distribution#Expected_values}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Covariance}]{Covariance} \label{sec:matn-cov}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be an $n \times p$ random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}) following a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}):

\begin{equation} \label{eq:matn-cov-matn}
X \sim \mathcal{MN}(M, U, V) \; .
\end{equation}

Then,

1) the covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of each row of $X$ is a scalar multiple of $V$

\begin{equation} \label{eq:matn-cov-matn-cov-row}
\mathrm{Cov}(x_{i,\bullet}^\mathrm{T}) \propto V \quad \text{for all} \quad i = 1,\ldots,n \; ;
\end{equation}

2) the covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of each column of $X$ is a scalar multiple of $U$

\begin{equation} \label{eq:matn-cov-matn-cov-col}
\mathrm{Cov}(x_{\bullet,j}) \propto U \quad \text{for all} \quad i = 1,\ldots,p \; .
\end{equation}


\vspace{1em}
\textbf{Proof:}

1) The marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of a given row of $X$ is a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-marg})

\begin{equation} \label{eq:matn-cov-matn-marg-row}
x_{i,\bullet}^\mathrm{T} \sim \mathcal{N}(m_{i,\bullet}^\mathrm{T}, u_{ii} V) \; ,
\end{equation}

and the covariance of this multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-cov}) is

\begin{equation} \label{eq:matn-cov-matn-cov-row-qed}
\mathrm{Cov}(x_{i,\bullet}^\mathrm{T}) = u_{ii} V \propto V \; .
\end{equation}

2) The marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of a given column of $X$ is a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-marg})

\begin{equation} \label{eq:matn-cov-matn-marg-col}
x_{\bullet,j} \sim \mathcal{N}(m_{\bullet,j}, v_{jj} U) \; ,
\end{equation}

and the covariance of this multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-cov}) is

\begin{equation} \label{eq:matn-cov-matn-cov-col-qed}
\mathrm{Cov}(x_{\bullet,j}) = v_{jj} U \propto U \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Matrix normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-09-15; URL: \url{https://en.wikipedia.org/wiki/Matrix_normal_distribution#Expected_values}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Differential entropy}]{Differential entropy} \label{sec:matn-dent}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be an $n \times p$ random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}) following a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn})

\begin{equation} \label{eq:matn-dent-matn}
X \sim \mathcal{MN}(M, U, V) \; .
\end{equation}

Then, the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent}) of $X$ in nats is

\begin{equation} \label{eq:matn-dent-matn-dent}
\mathrm{h}(X) = \frac{np}{2} \ln(2\pi) + \frac{n}{2} \ln|V| + \frac{p}{2} \ln|U| + \frac{np}{2} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The matrix-normal distribution is equivalent to the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-mvn}),

\begin{equation} \label{eq:matn-dent-matn-mvn}
X \sim \mathcal{MN}(M, U, V) \quad \Leftrightarrow \quad \mathrm{vec}(X) \sim \mathcal{N}(\mathrm{vec}(M), V \otimes U) \; ,
\end{equation}

and the differential entropy for the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-dent}) in nats is

\begin{equation} \label{eq:matn-dent-mvn-dent}
X \sim \mathcal{N}(\mu, \Sigma) \quad \Rightarrow \quad \mathrm{h}(X) = \frac{n}{2} \ln(2\pi) + \frac{1}{2} \ln|\Sigma| + \frac{1}{2} n
\end{equation}

where $X$ is an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}).

Thus, we can plug the distribution parameters from \eqref{eq:matn-dent-matn} into the differential entropy in \eqref{eq:matn-dent-mvn-dent} using the relationship given by \eqref{eq:matn-dent-matn-mvn}

\begin{equation} \label{eq:matn-dent-matn-dent-s1}
\mathrm{h}(X) = \frac{np}{2} \ln(2\pi) + \frac{1}{2} \ln|V \otimes U| + \frac{1}{2} np \; .
\end{equation}

Using the Kronecker product property

\begin{equation} \label{eq:matn-dent-kron-det}
|A \otimes B| = |A|^m \, |B|^n \quad \text{where} \quad A \in \mathbb{R}^{n \times n} \quad \text{and} \quad B \in \mathbb{R}^{m \times m} \; ,
\end{equation}

the differential entropy from \eqref{eq:matn-dent-matn-dent-s1} becomes:

\begin{equation} \label{eq:matn-dent-matn-dent-s2}
\begin{split}
\mathrm{h}(X) &= \frac{np}{2} \ln(2\pi) + \frac{1}{2} \ln\left(|V|^n |U|^p\right) + \frac{1}{2} np \\
&= \frac{np}{2} \ln(2\pi) + \frac{n}{2} \ln|V| + \frac{p}{2} \ln|U| + \frac{np}{2} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Kullback-Leibler divergence}]{Kullback-Leibler divergence} \label{sec:matn-kl}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be an $n \times p$ random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}). Assume two matrix-normal distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) $P$ and $Q$ specifying the probability distribution of $X$ as

\begin{equation} \label{eq:matn-kl-matns}
\begin{split}
P: \; X &\sim \mathcal{MN}(M_1, U_1, V_1) \\
Q: \; X &\sim \mathcal{MN}(M_2, U_2, V_2) \; .
\end{split}
\end{equation}

Then, the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ is given by

\begin{equation} \label{eq:matn-kl-matn-KL}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \frac{1}{2} \left[ \mathrm{vec}(M_2 - M_1)^\mathrm{T} \mathrm{vec}\left(U_2^{-1} (M_2 - M_1) V_2^{-1}\right) \right. \\
&+ \left. \mathrm{tr}\left( (V_2^{-1}V_1) \otimes (U_2^{-1}U_1) \right) - n \ln \frac{|V_1|}{|V_2|} - p \ln \frac{|U_1|}{|U_2|} - n p \right] \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} The matrix-normal distribution is equivalent to the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-mvn}),

\begin{equation} \label{eq:matn-kl-matn-mvn}
X \sim \mathcal{MN}(M, U, V) \quad \Leftrightarrow \quad \mathrm{vec}(X) \sim \mathcal{N}(\mathrm{vec}(M), V \otimes U) \; ,
\end{equation}

and the Kullback-Leibler divergence for the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-kl}) is

\begin{equation} \label{eq:matn-kl-mvn-KL}
\mathrm{KL}[P\,||\,Q] = \frac{1}{2} \left[ (\mu_2 - \mu_1)^T \Sigma_2^{-1} (\mu_2 - \mu_1) + \mathrm{tr}(\Sigma_2^{-1} \Sigma_1) - \ln \frac{|\Sigma_1|}{|\Sigma_2|} - n \right]
\end{equation}

where $X$ is an $n \times 1$ random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}).

Thus, we can plug the distribution parameters from \eqref{eq:matn-kl-matns} into the KL divergence in \eqref{eq:matn-kl-mvn-KL} using the relationship given by \eqref{eq:matn-kl-matn-mvn}

\begin{equation} \label{eq:matn-kl-matn-KL-s1}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \frac{1}{2} \left[ (\mathrm{vec}(M_2) - \mathrm{vec}(M_1))^T (V_2 \otimes U_2)^{-1} (\mathrm{vec}(M_2) - \mathrm{vec}(M_1)) \right. \\
&+ \left. \mathrm{tr}\left( (V_2 \otimes U_2)^{-1} (V_1 \otimes U_1) \right) - \ln \frac{|V_1 \otimes U_1|}{|V_2 \otimes U_2|} - n p \right] \; .
\end{split}
\end{equation}

Using the vectorization operator and Kronecker product properties

\begin{equation} \label{eq:matn-kl-vec-add}
\mathrm{vec}(A) + \mathrm{vec}(B) = \mathrm{vec}(A+B)
\end{equation}

\begin{equation} \label{eq:matn-kl-kron-inv}
(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}
\end{equation}

\begin{equation} \label{eq:matn-kl-kron-prod}
(A \otimes B) (C \otimes D) = (AC) \otimes (BD)
\end{equation}

\begin{equation} \label{eq:matn-kl-kron-det}
|A \otimes B| = |A|^m \, |B|^n \quad \text{where} \quad A \in \mathbb{R}^{n \times n} \quad \text{and} \quad B \in \mathbb{R}^{m \times m} \; ,
\end{equation}

the Kullback-Leibler divergence from \eqref{eq:matn-kl-matn-KL-s1} becomes:

\begin{equation} \label{eq:matn-kl-matn-KL-s2}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \frac{1}{2} \left[ \mathrm{vec}(M_2 - M_1)^\mathrm{T} \, (V_2^{-1} \otimes U_2^{-1}) \, \mathrm{vec}(M_2 - M_1) \right. \\
&+ \left. \mathrm{tr}\left( (V_2^{-1}V_1) \otimes (U_2^{-1}U_1) \right) - n \ln \frac{|V_1|}{|V_2|} - p \ln \frac{|U_1|}{|U_2|} - n p \right] \; .
\end{split}
\end{equation}

Using the relationship between Kronecker product and vectorization operator

\begin{equation} \label{eq:matn-kl-kron-vec}
(C^\mathrm{T} \otimes A) \, \mathrm{vec}(B) = \mathrm{vec}(ABC) \; ,
\end{equation}

we finally have:

\begin{equation} \label{eq:matn-kl-matn-KL-s3}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \frac{1}{2} \left[ \mathrm{vec}(M_2 - M_1)^\mathrm{T} \mathrm{vec}\left(U_2^{-1} (M_2 - M_1) V_2^{-1}\right) \right. \\
&+ \left. \mathrm{tr}\left( (V_2^{-1}V_1) \otimes (U_2^{-1}U_1) \right) - n \ln \frac{|V_1|}{|V_2|} - p \ln \frac{|U_1|}{|U_2|} - n p \right] \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Transposition}]{Transposition} \label{sec:matn-trans}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be a random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}) following a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}):

\begin{equation} \label{eq:matn-trans-matn}
X \sim \mathcal{MN}(M, U, V) \; .
\end{equation}

Then, the transpose of $X$ also has a matrix-normal distribution:

\begin{equation} \label{eq:matn-trans-matn-trans}
X^\mathrm{T} \sim \mathcal{MN}(M^\mathrm{T}, V, U) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The probability density function of the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-pdf}) is:

\begin{equation} \label{eq:matn-trans-matn-pdf-X}
f(X) = \mathcal{MN}(X; M, U, V) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( V^{-1} (X-M)^\mathrm{T} \, U^{-1} (X-M) \right) \right] \; .
\end{equation}

Define $Y = X^\mathrm{T}$. Then, $X = Y^\mathrm{T}$ and we can substitute:

\begin{equation} \label{eq:matn-trans-matn-pdf-Y-s1}
f(Y) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( V^{-1} (Y^\mathrm{T}-M)^\mathrm{T} \, U^{-1} (Y^\mathrm{T}-M) \right) \right] \; .
\end{equation}

Using $(A+B)^\mathrm{T} = (A^\mathrm{T} + B^\mathrm{T})$, we have:

\begin{equation} \label{eq:matn-trans-matn-pdf-Y-s2}
f(Y) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( V^{-1} (Y-M^\mathrm{T}) \, U^{-1} (Y-M^\mathrm{T})^\mathrm{T} \right) \right] \; .
\end{equation}

Using $\mathrm{tr}(ABC) = \mathrm{tr}(CAB)$, we obtain

\begin{equation} \label{eq:matn-trans-matn-pdf-Y-s3}
f(Y) = \frac{1}{\sqrt{(2\pi)^{np} |V|^n |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( U^{-1} (Y-M^\mathrm{T})^\mathrm{T} \, V^{-1} (Y-M^\mathrm{T}) \right) \right]
\end{equation}

which is the probability density function of a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-pdf}) with mean $M^T$, covariance across rows $V$ and covariance across columns $U$.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Linear transformation}]{Linear transformation} \label{sec:matn-ltt}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be an $n \times p$ random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}) following a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}):

\begin{equation} \label{eq:matn-ltt-matn}
X \sim \mathcal{MN}(M, U, V) \; .
\end{equation}

Then, a linear transformation of $X$ is also matrix-normally distributed

\begin{equation} \label{eq:matn-ltt-matn-trans}
Y = AXB + C \sim \mathcal{MN}(AMB+C, AUA^\mathrm{T}, B^\mathrm{T}VB)
\end{equation}

where $A$ us ab $r \times n$ matrix of full rank $r \leq b$ and $B$ is a $p \times s$ matrix of full rank $s \leq p$ and $C$ is an $r \times s$ matrix.


\vspace{1em}
\textbf{Proof:} The matrix-normal distribution is equivalent to the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-mvn}),

\begin{equation} \label{eq:matn-ltt-matn-mvn}
X \sim \mathcal{MN}(M, U, V) \quad \Leftrightarrow \quad \mathrm{vec}(X) \sim \mathcal{N}(\mathrm{vec}(M), V \otimes U) \; ,
\end{equation}

and the linear transformation theorem for the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}) states:

\begin{equation} \label{eq:matn-ltt-mvn-ltt}
x \sim \mathcal{N}(\mu, \Sigma) \quad \Rightarrow \quad y = Ax + b \sim \mathcal{N}(A\mu + b, A \Sigma A^\mathrm{T}) \; .
\end{equation}

The vectorization of $Y = AXB + C$ is

\begin{equation} \label{eq:matn-ltt-vec-Y-s1}
\begin{split}
\mathrm{vec}(Y) &= \mathrm{vec}(AXB + C) \\
&= \mathrm{vec}(AXB) + \mathrm{vec}(C) \\
&= (B^\mathrm{T} \otimes A)\mathrm{vec}(X) + \mathrm{vec}(C)
\end{split}
\end{equation}

and the Kronecker product obeys

\begin{equation} \label{eq:matn-ltt-kron-prod}
(A \otimes B) (C \otimes D) = (AC) \otimes (BD) \; .
\end{equation}

Using \eqref{eq:matn-ltt-matn-mvn} and \eqref{eq:matn-ltt-mvn-ltt}, we have

\begin{equation} \label{eq:matn-ltt-vec-Y-s2}
\begin{split}
\mathrm{vec}(Y) &\sim \mathcal{N}((B^\mathrm{T} \otimes A) \mathrm{vec}(M) + \mathrm{vec}(C), (B^\mathrm{T} \otimes A) (V \otimes U) (B^\mathrm{T} \otimes A)^\mathrm{T}) \\
&= \mathcal{N}(\mathrm{vec}(AMB) + \mathrm{vec}(C), (B^\mathrm{T}V \otimes AU) (B^\mathrm{T} \otimes A)^\mathrm{T}) \\
&= \mathcal{N}(\mathrm{vec}(AMB + C), B^\mathrm{T}VB \otimes AUA^\mathrm{T}) \; .
\end{split}
\end{equation}

Using \eqref{eq:matn-ltt-matn-mvn}, we finally have:

\begin{equation} \label{eq:matn-ltt-matn-ltt-qed}
Y \sim \mathcal{MN}(AMB + C, AUA^\mathrm{T} ,B^\mathrm{T}VB) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Marginal distributions}]{Marginal distributions} \label{sec:matn-marg}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ be an $n \times p$ random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}) following a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}):

\begin{equation} \label{eq:matn-marg-matn}
X \sim \mathcal{MN}(M, U, V) \; .
\end{equation}

Then,

1) the marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of any subset matrix $X_{I,J}$, obtained by dropping some rows and/or columns from $X$, is also a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn})

\begin{equation} \label{eq:matn-marg-matn-marg-subs}
X_{I,J} \sim \mathcal{MN}(M_{I,J}, U_{I,I}, V_{J,J})
\end{equation}

where $I \subseteq \left\lbrace 1, \ldots, n \right\rbrace$ is an (ordered) subset of all row indices and $J \subseteq \left\lbrace 1, \ldots, p \right\rbrace$ is an (ordered) subset of all column indices, such that $M_{I,J}$ is the matrix dropping the irrelevant rows and columns (the ones not in the subset, i.e. marginalized out) from the mean matrix $M$; $U_{I,I}$ is the matrix dropping rows not in $I$ from $U$; and $V_{J,J}$ is the matrix dropping columns not in $J$ from $V$;

2) the marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of each row vector is a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn})

\begin{equation} \label{eq:matn-marg-matn-marg-row}
x_{i,\bullet}^\mathrm{T} \sim \mathcal{N}(m_{i,\bullet}^\mathrm{T}, u_{ii} V)
\end{equation}

where $m_{i,\bullet}$ is the $i$-th row of $M$ and $u_{ii}$ is the $i$-th diagonal entry of $U$;

3) the marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of each column vector is a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn})

\begin{equation} \label{eq:matn-marg-matn-marg-col}
x_{\bullet,j} \sim \mathcal{N}(m_{\bullet,j}, v_{jj} U)
\end{equation}

where $m_{\bullet,j}$ is the $j$-th column of $M$ and $v_{jj}$ is the $j$-th diagonal entry of $V$; and

4) the marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of one element of $X$ is a univariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm})

\begin{equation} \label{eq:matn-marg-matn-marg-elem}
x_{ij} \sim \mathcal{N}(m_{ij}, u_{ii} v_{jj})
\end{equation}

where $m_{ij}$ is the $(i,j)$-th entry of $M$.


\vspace{1em}
\textbf{Proof:}

1) Define a selector matrix $A$, such that $a_{ij} = 1$, if the $i$-th row in the subset matrix should be the $j$-th row from the original matrix (and $a_{ij} = 0$ otherwise)

\begin{equation} \label{eq:matn-marg-A}
A \in \mathbb{R}^{\lvert I \rvert \times n}, \quad \text{s.t.} \quad a_{ij} = \left\{
\begin{array}{rl}
1 \; , & \text{if} \; I_i = j \\
0 \; , & \text{otherwise}
\end{array}
\right.
\end{equation}

and define a selector matrix $B$, such that $b_{ij} = 1$, if the $j$-th column in the subset matrix should be the $i$-th column from the original matrix (and $b_{ij} = 0$ otherwise)

\begin{equation} \label{eq:matn-marg-B}
B \in \mathbb{R}^{p \times \lvert J \rvert}, \quad \text{s.t.} \quad b_{ij} = \left\{
\begin{array}{rl}
1 \; , & \text{if} \; J_j = i \\
0 \; , & \text{otherwise} \; .
\end{array}
\right.
\end{equation}

Then, $X_{I,J}$ can be expressed as

\begin{equation} \label{eq:matn-marg-XIJ}
X_{I,J} = A X B
\end{equation}

and we can apply the linear transformation theorem ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-ltt}) to give

\begin{equation} \label{eq:matn-marg-XIJ-marg}
X_{I,J} \sim \mathcal{MN}(A M B, A U A^\mathrm{T}, B^\mathrm{T} V B) \; .
\end{equation}

Finally, we see that $A M B = M_{I,J}$, $A U A^\mathrm{T} = U_{I,I}$ and $B^\mathrm{T} V B = V_{J,J}$.

2) This is a special case of 1). Setting $A$ to the $i$-th elementary row vector in $n$ dimensions and $B$ to the $p \times p$ identity matrix

\begin{equation} \label{eq:matn-marg-AB-row}
A = e_i, \; B = I_p \; ,
\end{equation}

the $i$-th row of $X$ can be expressed as

\begin{equation} \label{eq:matn-marg-xi-marg}
\begin{split}
x_{i,\bullet} &= AXB = e_i X I_p = e_i X \\
&\overset{\eqref{eq:matn-marg-XIJ-marg}}{\sim} \mathcal{MN}(m_{i,\bullet}, u_{ii}, V) \; .
\end{split}
\end{equation}

Thus, the transpose of the row vector is distributed as ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-trans})

\begin{equation} \label{eq:matn-marg-xi-marg-trans}
x_{i,\bullet}^\mathrm{T} \sim \mathcal{MN}(m_{i,\bullet}^\mathrm{T}, V, u_{ii})
\end{equation}

which is equivalent to a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-mvn}):

\begin{equation} \label{eq:matn-marg-xi-marg-trans-mvn}
x_{i,\bullet}^\mathrm{T} \sim \mathcal{N}(m_{i,\bullet}^\mathrm{T}, u_{ii} V) \; .
\end{equation}

3) This is a special case of 1). Setting $A$ to the $n \times n$ identity matrix and $B$ to the $j$-th elementary row vector in $p$ dimensions

\begin{equation} \label{eq:matn-marg-AB-col}
A = I_n, \; B = e_j^\mathrm{T} \; ,
\end{equation}

the $j$-th column of $X$ can be expressed as

\begin{equation} \label{eq:matn-marg-xj-marg}
\begin{split}
x_{\bullet,j} &= AXB = I_n X e_j^\mathrm{T} = X e_j^\mathrm{T} \\
&\overset{\eqref{eq:matn-marg-XIJ-marg}}{\sim} \mathcal{MN}(m_{\bullet,j}, U, v_{jj})
\end{split}
\end{equation}

which is equivalent to a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-mvn}):

\begin{equation} \label{eq:matn-marg-xj-marg-mvn}
x_{\bullet,j} \sim \mathcal{N}(m_{\bullet,j}, v_{jj} U) \; .
\end{equation}

4) This is a special case of 2) and 3). Setting $A$ to the $i$-th elementary row vector in $n$ dimensions and $B$ to the $j$-th elementary row vector in $p$ dimensions

\begin{equation} \label{eq:matn-marg-AB-elem}
A = e_i, \; B = e_j^\mathrm{T} \; ,
\end{equation}

the $(i,j)$-th entry of $X$ can be expressed as

\begin{equation} \label{eq:matn-marg-xij-marg}
\begin{split}
x_{ij} &= AXB = e_i X e_j^\mathrm{T} \\
&\overset{\eqref{eq:matn-marg-XIJ-marg}}{\sim} \mathcal{MN}(m_{ij}, u_{ii}, v_{jj}) \; .
\end{split}
\end{equation}

As $x_{ij}$ is a scalar, this is equivalent to a univariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) as a special case ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-mvn}) of the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-matn}):

\begin{equation} \label{eq:matn-marg-xij-marg-norm}
x_{ij} \sim \mathcal{N}(m_{ij}, u_{ii} v_{jj}) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Drawing samples}]{Drawing samples} \label{sec:matn-samp}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X \in \mathbb{R}^{n \times p}$ be a random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}) with all entries independently following a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}). Moreover, let $A \in \mathbb{R}^{n \times n}$ and $B \in \mathbb{R}^{p \times p}$, such that $A A^\mathrm{T} = U$ and $B^\mathrm{T} B = V$.

Then, $Y = M + A X B$ follows a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) with mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-rmat}) $M$, covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) across rows $U$ and covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) across columns $V$:

\begin{equation} \label{eq:matn-samp-matn-samp}
Y = M + A X B \sim \mathcal{MN}(M, U, V) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} If all entries of $X$ are independent and standard normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm})

\begin{equation} \label{eq:matn-samp-xij-dist}
x_{ij} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, 1) \quad \text{for all} \quad i = 1,\ldots,n \quad \text{and} \quad j = 1,\ldots,p \; ,
\end{equation}

this implies a multivariate normal distribution with diagonal covariance matrix ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ind}):

\begin{equation} \label{eq:matn-samp-vecX-dist}
\begin{split}
\mathrm{vec}(X) &\sim \mathcal{N}\left(\mathrm{vec}(0_{np}), I_{np} \right) \\
&\sim \mathcal{N}\left(\mathrm{vec}(0_{np}), I_p \otimes I_n \right)
\end{split}
\end{equation}

where $0_{np}$ is an $n \times p$ matrix of zeros and $I_n$ is the $n \times n$ identity matrix.

Due to the relationship between multivariate and matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-mvn}), we have:

\begin{equation} \label{eq:matn-samp-X-dist}
X \sim \mathcal{MN}(0_{np}, I_n, I_p) \; .
\end{equation}

Thus, with the linear transformation theorem for the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-ltt}), it follows that

\begin{equation} \label{eq:matn-samp-matn-samp-qed}
\begin{split}
Y = M + AXB &\sim \mathcal{MN}\left(M + A 0_{np} B, A I_n A^\mathrm{T}, B^\mathrm{T} I_p B \right) \\
&\sim \mathcal{MN}\left(M, A A^\mathrm{T}, B^\mathrm{T} B \right) \\
&\sim \mathcal{MN}\left(M, U, V \right) \; .
\end{split}
\end{equation}

Thus, given $X$ defined by \eqref{eq:matn-samp-xij-dist}, $Y$ defined by \eqref{eq:matn-samp-matn-samp} is a sample from $\mathcal{MN}\left(M, U, V \right)$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Matrix normal distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-12-07; URL: \url{https://en.wikipedia.org/wiki/Matrix_normal_distribution#Drawing_values_from_the_distribution}.
\end{itemize}
\vspace{1em}



\subsection{Wishart distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:wish}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be an $n \times p$ matrix following a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) with mean zero, independence across rows and covariance across columns $V$:

\begin{equation} \label{eq:wish-matn}
X \sim \mathcal{MN}(0, I_n, V) \; .
\end{equation}

Define the scatter matrix $S$ as the product of the transpose of $X$ with itself:

\begin{equation} \label{eq:wish-scat-mat}
S = X^T X = \sum_{i=1}^n x_i^\mathrm{T} x_i \; .
\end{equation}

Then, the matrix $S$ is said to follow a Wishart distribution with scale matrix $V$ and degrees of freedom $n$

\begin{equation} \label{eq:wish-wish}
S \sim \mathcal{W}(V, n)
\end{equation}

where $n > p - 1$ and $V$ is a positive definite symmetric covariance matrix.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Wishart distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-22; URL: \url{https://en.wikipedia.org/wiki/Wishart_distribution#Definition}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Kullback-Leibler divergence}]{Kullback-Leibler divergence} \label{sec:wish-kl}
\setcounter{equation}{0}

\textbf{Theorem:} Let $S$ be a $p \times p$ random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}). Assume two Wishart distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wish}) $P$ and $Q$ specifying the probability distribution of $S$ as

\begin{equation} \label{eq:wish-kl-wishs}
\begin{split}
P: \; S &\sim \mathcal{W}(V_1, n_1) \\
Q: \; S &\sim \mathcal{W}(V_2, n_2) \; .
\end{split}
\end{equation}

Then, the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of $P$ from $Q$ is given by

\begin{equation} \label{eq:wish-kl-wish-KL}
\mathrm{KL}[P\,||\,Q] = \frac{1}{2} \left[ n_2 \left( \ln |V_2| - \ln |V_1| \right) + n_1 \mathrm{tr}(V_2^{-1} V_1) + 2 \ln \frac{\Gamma_p\left(\frac{n_2}{2}\right)}{\Gamma_p\left(\frac{n_1}{2}\right)} + (n_1-n_2) \psi_p\left(\frac{n_1}{2}\right) - n_1 p \right]
\end{equation}

where $\Gamma_p(x)$ is the multivariate gamma function

\begin{equation} \label{eq:wish-kl-mult-gam-fct}
\Gamma_p(x) = \pi^{p(p-1)/4} \, \prod_{j=1}^k \Gamma\left(x - \frac{j-1}{2}\right)
\end{equation}

and $\psi_p(x)$ is the multivariate digamma function

\begin{equation} \label{eq:wish-kl-mult-psi-fct}
\psi_p(x) = \frac{\mathrm{d}\ln \Gamma_p(x)}{\mathrm{d}x} = \sum_{j=1}^k \psi\left(x - \frac{j-1}{2}\right) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The KL divergence for a continuous random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) is given by 

\begin{equation} \label{eq:wish-kl-KL-cont}
\mathrm{KL}[P\,||\,Q] = \int_{\mathcal{X}} p(x) \, \ln \frac{p(x)}{q(x)} \, \mathrm{d}x
\end{equation}

which, applied to the Wishart distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wish}) in \eqref{eq:wish-kl-wishs}, yields

\begin{equation} \label{eq:wish-kl-wish-KL-s1}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \int_{\mathcal{S}^p} \mathcal{W}(S; V_1, n_1) \, \ln \frac{\mathcal{W}(S; V_1, n_1)}{\mathcal{W}(S; V_2, n_2)} \, \mathrm{d}S \\
&= \left\langle \ln \frac{\mathcal{W}(S; \alpha_1)}{\mathcal{W}(S; \alpha_1)} \right\rangle_{p(S)}
\end{split}
\end{equation}

where $\mathcal{S}^p$ is the set of all positive-definite symmetric $p \times p$ matrices.

Using the probability density function of the Wishart distribution, this becomes:

\begin{equation} \label{eq:wish-kl-wish-KL-s2}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \left\langle \ln \frac{\frac{1}{\sqrt{2^{n_1 p} |V_1|^{n_1}} \Gamma_p \left( \frac{n_1}{2} \right)} \cdot |S|^{(n_1-p-1)/2} \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( V_1^{-1} S \right) \right]}{\frac{1}{\sqrt{2^{n_2 p} |V_2|^{n_2}} \Gamma_p \left( \frac{n_2}{2} \right)} \cdot |S|^{(n_2-p-1)/2} \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( V_2^{-1} S \right) \right]} \right\rangle_{p(S)} \\
&= \left\langle \ln \left( \sqrt{2^{(n_2-n_1)p} \cdot \frac{|V_2|^{n_2}}{|V_1|^{n_1}}} \cdot \frac{\Gamma_p\left( \frac{n_2}{2} \right)}{\Gamma_p\left( \frac{n_1}{2} \right)} \cdot |S|^{(n_1-n_2)/2} \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( V_1^{-1} S \right) +\frac{1}{2} \mathrm{tr}\left( V_2^{-1} S \right) \right] \right) \right\rangle_{p(S)} \\
&= \left\langle \frac{(n_2-n_1)p}{2} \ln 2 + \frac{n_2}{2} \ln |V_2| - \frac{n_1}{2} \ln |V_1| + \ln \frac{\Gamma_p\left( \frac{n_2}{2} \right)}{\Gamma_p\left( \frac{n_1}{2} \right)} \right. \\
&+ \left. \quad \frac{n_1-n_2}{2} \ln |S| - \frac{1}{2} \mathrm{tr}\left( V_1^{-1} S \right) + \frac{1}{2} \mathrm{tr}\left( V_2^{-1} S \right) \right\rangle_{p(S)} \\
&= \frac{(n_2-n_1)p}{2} \ln 2 + \frac{n_2}{2} \ln |V_2| - \frac{n_1}{2} \ln |V_1| + \ln \frac{\Gamma_p\left( \frac{n_2}{2} \right)}{\Gamma_p\left( \frac{n_1}{2} \right)} \\
&+ \frac{n_1-n_2}{2} \left\langle \ln |S| \right\rangle_{p(S)} - \frac{1}{2} \left\langle \mathrm{tr}\left( V_1^{-1} S \right) \right\rangle_{p(S)} + \frac{1}{2} \left\langle \mathrm{tr}\left( V_2^{-1} S \right) \right\rangle_{p(S)} \; .
\end{split}
\end{equation}

Using the expected value of a Wishart random matrix

\begin{equation} \label{eq:wish-kl-wish-mean}
S \sim \mathcal{W}(V,n) \quad \Rightarrow \quad \left\langle S \right\rangle = n V \; ,
\end{equation}

such that the expected value of the matrix trace ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-tr}) becomes

\begin{equation} \label{eq:wish-kl-wish-trmean}
\left\langle \mathrm{tr}(AS) \right\rangle = \mathrm{tr}\left( \left\langle AS \right\rangle \right) = \mathrm{tr}\left( A \left\langle S \right\rangle \right) = \mathrm{tr}\left( A \cdot (nV) \right) = n \cdot \mathrm{tr}(AV) \; ,
\end{equation}

and the expected value of a Wishart log-determinant

\begin{equation} \label{eq:wish-kl-wish-logdetmean}
S \sim \mathcal{W}(V,n) \quad \Rightarrow \quad \left\langle \ln |S| \right\rangle = \psi_p\left(\frac{n}{2}\right) + p \cdot \ln 2 + \ln |V| \; ,
\end{equation}

the Kullback-Leibler divergence from \eqref{eq:wish-kl-wish-KL-s2} becomes:

\begin{equation} \label{eq:wish-kl-wish-KL-s3}
\begin{split}
\mathrm{KL}[P\,||\,Q] &= \frac{(n_2-n_1)p}{2} \ln 2 + \frac{n_2}{2} \ln |V_2| - \frac{n_1}{2} \ln |V_1| + \ln \frac{\Gamma_p\left( \frac{n_2}{2} \right)}{\Gamma_p\left( \frac{n_1}{2} \right)} \\
&+ \frac{n_1-n_2}{2} \left[ \psi_p\left(\frac{n_1}{2}\right) + p \cdot \ln 2 + \ln |V_1| \right] - \frac{n_1}{2} \mathrm{tr}\left( V_1^{-1} V_1 \right) + \frac{n_1}{2} \mathrm{tr}\left( V_2^{-1} V_1 \right) \\
&= \frac{n_2}{2} \left( \ln |V_2| - \ln |V_1| \right) + \ln \frac{\Gamma_p\left( \frac{n_2}{2} \right)}{\Gamma_p\left( \frac{n_1}{2} \right)} + \frac{n_1-n_2}{2} \psi_p\left(\frac{n_1}{2}\right) - \frac{n_1}{2} \mathrm{tr}\left( I_p \right) + \frac{n_1}{2} \mathrm{tr}\left( V_2^{-1} V_1 \right) \\
& = \frac{1}{2} \left[ n_2 \left( \ln |V_2| - \ln |V_1| \right) + n_1 \mathrm{tr}(V_2^{-1} V_1) + 2 \ln \frac{\Gamma_p\left(\frac{n_2}{2}\right)}{\Gamma_p\left(\frac{n_1}{2}\right)} + (n_1-n_2) \psi_p\left(\frac{n_1}{2}\right) - n_1 p \right] \; .
\end{split}
\end{equation}

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Penny, William D. (2001): "KL-Divergences of Normal, Gamma, Dirichlet and Wishart densities"; in: \textit{University College, London}, pp. 2-3, eqs. 13/15; URL: \url{https://www.fil.ion.ucl.ac.uk/~wpenny/publications/densities.ps}.
\item Wikipedia (2021): "Wishart distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-12-02; URL: \url{https://en.wikipedia.org/wiki/Wishart_distribution#KL-divergence}.
\end{itemize}
\vspace{1em}



\subsection{Normal-Wishart distribution}

\subsubsection[\textit{Definition}]{Definition} \label{sec:nw}
\setcounter{equation}{0}

\textbf{Definition:} Let $X$ be an $n \times p$ random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat}) and let $Y$ be a $p \times p$ positive-definite symmetric matrix. Then, $X$ and $Y$ are said to follow a normal-Wishart distribution

\begin{equation} \label{eq:nw-nw}
X,Y \sim \mathrm{NW}(M, U, V, \nu) \; ,
\end{equation}

if the distribution of $X$ conditional on $Y$ is a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) with mean $M$, covariance across rows $U$, covariance across columns $Y^{-1}$ and $Y$ follows a Wishart distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wish}) with scale matrix $V$ and degrees of freedom $\nu$:

\begin{equation} \label{eq:nw-matn-wish}
\begin{split}
X \vert Y &\sim \mathcal{MN}(M, U, Y^{-1}) \\
Y &\sim \mathcal{W}(V, \nu) \; .
\end{split}
\end{equation}

The $p \times p$ matrix $Y$ can be seen as the precision matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:precmat}) across the columns of the $n \times p$ matrix $X$.


\subsubsection[\textbf{Probability density function}]{Probability density function} \label{sec:nw-pdf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X$ and $Y$ follow a normal-Wishart distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:nw}):

\begin{equation} \label{eq:nw-pdf-nw}
X,Y \sim \mathrm{NW}(M, U, V, \nu) \; .
\end{equation}

Then, the joint probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}) of $X$ and $Y$ is

\begin{equation} \label{eq:nw-pdf-nw-pdf}
\begin{split}
p(X,Y) = \; & \frac{1}{\sqrt{(2\pi)^{np} |U|^p |V|^{\nu}}} \cdot \frac{\sqrt{2^{-\nu p}}}{\Gamma_p \left( \frac{\nu}{2} \right)} \cdot |Y|^{(\nu+n-p-1)/2} \cdot \\
& \exp\left[-\frac{1}{2} \mathrm{tr}\left( Y \left[ (X-M)^\mathrm{T} \, U^{-1} (X-M) + V^{-1} \right] \right) \right] \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} The normal-Wishart distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:nw}) is defined as $X$ conditional on $Y$ following a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) and $Y$ following a Wishart distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wish}):

\begin{equation} \label{eq:nw-pdf-matn-wish}
\begin{split}
X \vert Y &\sim \mathcal{MN}(M, U, Y^{-1}) \\
Y &\sim \mathcal{W}(V, \nu) \; .
\end{split}
\end{equation}

Thus, using the probability density function of the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-pdf}) and the probability density function of the Wishart distribution, we have the following probabilities:

\begin{equation} \label{eq:nw-pdf-matn-wish-pdf}
\begin{split}
p(X \vert Y) &= \mathcal{MN}(X; M, U, Y^{-1}) \\
&= \sqrt{\frac{|Y|^n}{(2\pi)^{np} |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( Y (X-M)^\mathrm{T} \, U^{-1} (X-M) \right) \right] \\
p(Y) &= \mathcal{W}(Y; V, \nu) \\
&= \frac{1}{\Gamma_p \left( \frac{\nu}{2} \right)} \cdot \frac{1}{\sqrt{2^{\nu p} |V|^{\nu}}} \cdot |Y|^{(\nu-p-1)/2} \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( V^{-1} Y \right) \right] \; .
\end{split}
\end{equation}

The law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}) implies that

\begin{equation} \label{eq:nw-pdf-prob-cond}
p(X,Y) = p(X \vert Y) \, p(Y) \; ,
\end{equation}

such that the normal-Wishart density function becomes:

\begin{equation} \label{eq:nw-pdf-nw-pdf-qed}
\begin{split}
p(X,Y) = \; & \mathcal{MN}(X; M, U, Y^{-1}) \cdot \mathcal{W}(Y; V, \nu) \\
= \; & \sqrt{\frac{|Y|^n}{(2\pi)^{np} |U|^p}} \cdot \exp\left[-\frac{1}{2} \mathrm{tr}\left( Y (X-M)^\mathrm{T} \, U^{-1} (X-M) \right) \right] \cdot \\
& \frac{1}{\Gamma_p \left( \frac{\nu}{2} \right)} \cdot \frac{1}{\sqrt{2^{\nu p} |V|^{\nu}}} \cdot |Y|^{(\nu-p-1)/2} \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( V^{-1} Y \right) \right] \\
= \; & \frac{1}{\sqrt{(2\pi)^{np} |U|^p |V|^{\nu}}} \cdot \frac{\sqrt{2^{-\nu p}}}{\Gamma_p \left( \frac{\nu}{2} \right)} \cdot |Y|^{(\nu+n-p-1)/2} \cdot \\
& \exp\left[-\frac{1}{2} \mathrm{tr}\left( Y \left[ (X-M)^\mathrm{T} \, U^{-1} (X-M) + V^{-1} \right] \right) \right] \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mean}]{Mean} \label{sec:nw-mean}
\setcounter{equation}{0}

\textbf{Theorem:} Let $X \in \mathbb{R}^{n \times p}$ and $Y \in \mathbb{R}^{p \times p}$ follow a normal-Wishart distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:nw}):

\begin{equation} \label{eq:nw-mean-nw}
X,Y \sim \mathrm{NW}(M, U, V, \nu) \; .
\end{equation}

Then, the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $X$ and $Y$ is

\begin{equation} \label{eq:nw-mean-nw-mean}
\mathrm{E}[(X,Y)] = \left( M, \nu V \right) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Consider the random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rmat})

\begin{equation} \label{eq:nw-mean-rmat}
\left[ \begin{array}{c} X \\ Y \end{array} \right] = \left[ \begin{array}{ccc} x_{11} & \ldots & x_{1p} \\ \vdots & \ddots & \vdots \\ x_{n1} & \ldots & x_{np} \\ y_{11} & \ldots & y_{1p} \\ \vdots & \ddots & \vdots \\ y_{p1} & \ldots & y_{pp} \end{array} \right] \; .
\end{equation}

According to the expected value of a random matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-rmat}), its expected value is

\begin{equation} \label{eq:nw-mean-mean-rmat}
\mathrm{E}\left( \left[ \begin{array}{c} X \\ Y \end{array} \right] \right) = \left[ \begin{array}{ccc} \mathrm{E}(x_{11}) & \ldots & \mathrm{E}(x_{1p}) \\ \vdots & \ddots & \vdots \\ \mathrm{E}(x_{n1}) & \ldots & \mathrm{E}(x_{np}) \\ \mathrm{E}(y_{11}) & \ldots & \mathrm{E}(y_{1p}) \\ \vdots & \ddots & \vdots \\ \mathrm{E}(y_{p1}) & \ldots & \mathrm{E}(y_{pp}) \end{array} \right] = \left[ \begin{array}{c} \mathrm{E}(X) \\ \mathrm{E}(Y) \end{array} \right] \; .
\end{equation}

When $X$ and $Y$ are jointly normal-Wishart distributed, then ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:nw}) by definition $X$ follows a matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) conditional on $Y$ and $Y$ follows a Wishart distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wish}):

\begin{equation} \label{eq:nw-mean-nw-def}
X,Y \sim \mathrm{NW}(M, U, V, \nu) \quad \Leftrightarrow \quad X \vert Y \sim \mathcal{MN}(M, U, Y^{-1}) \quad \wedge \quad Y \sim \mathcal{W}(V, \nu) \; .
\end{equation}

Thus, with the expected value of the matrix-variate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-mean}) and the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), $\mathrm{E}(X)$ becomes

\begin{equation} \label{eq:nw-mean-mean-X}
\begin{split}
\mathrm{E}(X) &= \iint X \cdot p(X,Y) \, \mathrm{d}X \, \mathrm{d}Y \\
&= \iint X \cdot p(X|Y) \cdot p(Y) \, \mathrm{d}X \, \mathrm{d}Y \\
&= \int p(Y) \int X \cdot p(X|Y) \, \mathrm{d}X \, \mathrm{d}Y \\
&= \int p(Y) \left\langle X \right\rangle_{\mathcal{MN}(M, U, Y^{-1})} \, \mathrm{d}Y \\
&= \int p(Y) \cdot M \, \mathrm{d}Y \\
&= M \int p(Y) \, \mathrm{d}Y \\
&= M \; ,
\end{split}
\end{equation}

and with the expected value of the Wishart distribution, $\mathrm{E}(Y)$ becomes

\begin{equation} \label{eq:nw-mean-mean-Y}
\begin{split}
\mathrm{E}(Y) &= \int Y \cdot p(Y) \, \mathrm{d}Y \\
&= \left\langle Y \right\rangle_{\mathcal{W}(V,\nu)} \\
&= \nu V \; .
\end{split}
\end{equation}

Thus, the expectation of the random matrix in equations \eqref{eq:nw-mean-rmat} and \eqref{eq:nw-mean-mean-rmat} is

\begin{equation} \label{eq:nw-mean-nw-mean-qed}
\mathrm{E}\left( \left[ \begin{array}{c} X \\ Y \end{array} \right] \right) = \left[ \begin{array}{c} M \\ \nu V \end{array} \right] \; ,
\end{equation}

as indicated by equation \eqref{eq:nw-mean-nw-mean}.
\begin{flushright} $\blacksquare$ \end{flushright}





% Chapter 3 %
\chapter{Statistical Models} \label{sec:Statistical Models} \newpage

\pagebreak
\section{Univariate normal data}

\subsection{Univariate Gaussian}

\subsubsection[\textit{Definition}]{Definition} \label{sec:ug}
\setcounter{equation}{0}

\textbf{Definition:} A univariate Gaussian data set is given by a set of real numbers $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$, independent and identically distributed according to a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with unknown mean $\mu$ and unknown variance $\sigma^2$:

\begin{equation} \label{eq:ug-ug}
y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop, Christopher M. (2006): "Example: The univariate Gaussian"; in: \textit{Pattern Recognition for Machine Learning}, ch. 10.1.3, p. 470, eq. 10.21; URL: \url{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Maximum likelihood estimation}]{Maximum likelihood estimation} \label{sec:ug-mle}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug}) $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$:

\begin{equation} \label{eq:ug-mle-ug}
y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n \; .
\end{equation}

Then, the maximum likelihood estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) for mean $\mu$ and variance $\sigma^2$ are given by

\begin{equation} \label{eq:ug-mle-ug-MLE}
\begin{split}
\hat{\mu} &= \frac{1}{n} \sum_{i=1}^n y_i \\
\hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2 \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} The likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) for each observation is given by the probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf})

\begin{equation} \label{eq:ug-mle-ug-yi}
p(y_i|\mu,\sigma^2) = \mathcal{N}(x; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot \exp \left[ -\frac{1}{2} \left( \frac{y_i-\mu}{\sigma} \right)^2 \right]
\end{equation}

and because observations are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:ug-mle-ug-LF-s1}
p(y|\mu,\sigma^2) = \prod_{i=1}^n p(y_i|\mu) = \sqrt{ \frac{1}{(2 \pi \sigma^2)^n} } \cdot \exp \left[ -\frac{1}{2} \sum_{i=1}^{n} \left( \frac{y_i-\mu}{\sigma} \right)^2 \right] \; .
\end{equation}

This can be developed into

\begin{equation} \label{eq:ug-mle-ug-LF-s2}
\begin{split}
p(y|\mu,\sigma^2) &= \left( \frac{1}{2 \pi \sigma^2} \right)^{n/2} \cdot \exp \left[ -\frac{1}{2} \sum_{i=1}^{n} \left( \frac{y_i^2 - 2 y_i \mu + \mu^2}{\sigma^2} \right) \right] \\
&= \left( \frac{1}{2 \pi \sigma^2} \right)^{n/2} \cdot \exp \left[ -\frac{1}{2 \sigma^2} \left( y^\mathrm{T} y - 2 n \bar{y} \mu + n \mu^2 \right) \right]
\end{split}
\end{equation}

where $\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$ is the mean of data points and $y^\mathrm{T} y = \sum_{i=1}^{n} y_i^2$ is the sum of squared data points.

Thus, the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) is

\begin{equation} \label{eq:ug-mle-ug-LL}
\mathrm{LL}(\mu,\sigma^2) = \log p(y|\mu,\sigma^2) = -\frac{n}{2} \log (2 \pi \sigma^2) - \frac{1}{2 \sigma^2} \left( y^\mathrm{T} y - 2 n \bar{y} \mu + n \mu^2 \right) \; .
\end{equation}

\vspace{1em}
The derivative of the log-likelihood function \eqref{eq:ug-mle-ug-LL} with respect to $\mu$ is

\begin{equation} \label{eq:ug-mle-dLL-dmu}
\frac{\mathrm{d}\mathrm{LL}(\mu,\sigma^2)}{\mathrm{d}\mu} = \frac{n \bar{y}}{\sigma^2} - \frac{n \mu}{\sigma^2} = \frac{n}{\sigma^2} (\bar{y}-\mu)
\end{equation}

and setting this derivative to zero gives the MLE for $\mu$:

\begin{equation} \label{eq:ug-mle-mu-MLE}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{\mu},\sigma^2)}{\mathrm{d}\mu} &= 0 \\
0 &= \frac{n}{\sigma^2} (\bar{y}-\hat{\mu}) \\
0 &= \bar{y}-\hat{\mu} \\
\hat{\mu} &= \bar{y} \\
\hat{\mu} &= \frac{1}{n} \sum_{i=1}^n y_i \; .
\end{split}
\end{equation}

\vspace{1em}
The derivative of the log-likelihood function \eqref{eq:ug-mle-ug-LL} at $\hat{\mu}$ with respect to $\sigma^2$ is

\begin{equation} \label{eq:ug-mle-dLL-ds2}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{\mu},\sigma^2)}{\mathrm{d}\sigma^2} &= -\frac{n}{2} \frac{1}{\sigma^2} + \frac{1}{2 (\sigma^2)^2} \left( y^\mathrm{T} y - 2 n \bar{y} \hat{\mu} + n \hat{\mu}^2 \right) \\
&= -\frac{n}{2 \sigma^2} + \frac{1}{2 (\sigma^2)^2} \sum_{i=1}^n \left( y_i^2 - 2 y_i \hat{\mu} + \hat{\mu}^2 \right) \\
&= -\frac{n}{2 \sigma^2} + \frac{1}{2 (\sigma^2)^2} \sum_{i=1}^n (y_i - \hat{\mu})^2
\end{split}
\end{equation}

and setting this derivative to zero gives the MLE for $\sigma^2$:

\begin{equation} \label{eq:ug-mle-s2-MLE}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{\mu},\hat{\sigma}^2)}{\mathrm{d}\sigma^2} &= 0 \\
0 &= \frac{1}{2 (\hat{\sigma}^2)^2} \sum_{i=1}^n (y_i - \hat{\mu})^2 \\
\frac{n}{2 \hat{\sigma}^2} &= \frac{1}{2 (\hat{\sigma}^2)^2} \sum_{i=1}^n (y_i - \hat{\mu})^2 \\
\frac{2 (\hat{\sigma}^2)^2}{n} \cdot \frac{n}{2 \hat{\sigma}^2} &= \frac{2 (\hat{\sigma}^2)^2}{n} \cdot \frac{1}{2 (\hat{\sigma}^2)^2} \sum_{i=1}^n (y_i - \hat{\mu})^2 \\
\hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^n (y_i - \hat{\mu})^2 \\
\hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2 \\
\end{split}
\end{equation}

\vspace{1em}
Together, \eqref{eq:ug-mle-mu-MLE} and \eqref{eq:ug-mle-s2-MLE} constitute the MLE for the univariate Gaussian.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop CM (2006): "Bayesian inference for the Gaussian"; in: \textit{Pattern Recognition for Machine Learning}, pp. 93-94, eqs. 2.121, 2.122; URL: \url{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{One-sample t-test}]{One-sample t-test} \label{sec:ug-ttest1}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ug-ttest1-ug}
y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug}) with unknown mean $\mu$ and unknown variance $\sigma^2$. Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:ug-ttest1-t}
t = \frac{\bar{y}-\mu_0}{s / \sqrt{n}}
\end{equation}

with sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) $\bar{y}$ and sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) $s^2$ follows a Student's t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) with $n-1$ degrees of freedom

\begin{equation} \label{eq:ug-ttest1-t-dist}
t \sim \mathrm{t}(n-1)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})

\begin{equation} \label{eq:ug-ttest1-ttest1-h0}
H_0: \; \mu = \mu_0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) is given by

\begin{equation} \label{eq:ug-ttest1-mean-samp}
\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i
\end{equation}

and the sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) is given by

\begin{equation} \label{eq:ug-ttest1-var-samp}
s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (y_i - \bar{y})^2 \; .
\end{equation}

Using the linear combination formula for normal random variables ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-lincomb}), the sample mean follows a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with the following parameters:

\begin{equation} \label{eq:ug-ttest1-mean-samp-dist}
\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i \sim \mathcal{N}\left( \frac{1}{n} n \mu, \left(\frac{1}{n}\right)^2 n \sigma^2 \right) = \mathcal{N}\left( \mu, \sigma^2/n \right) \; .
\end{equation}

Again employing the linear combination theorem and applying the null hypothesis from \eqref{eq:ug-ttest1-ttest1-h0}, the distribution of $Z = \sqrt{n}(\bar{y}-\mu_0)/\sigma$ becomes standard normal ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm})

\begin{equation} \label{eq:ug-ttest1-Z-dist}
Z = \frac{\sqrt{n}(\bar{y}-\mu_0)}{\sigma} \sim \mathcal{N}\left( \frac{\sqrt{n}}{\sigma} (\mu - \mu_0), \left(\frac{\sqrt{n}}{\sigma}\right)^2 \frac{\sigma^2}{n} \right) \overset{H_0}{=} \mathcal{N}\left( 0, 1 \right) \; .
\end{equation}

Because sample variances calculated from independent normal random variables follow a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-chi2}), the distribution of $V = (n-1)\,s^2/\sigma^2$ is

\begin{equation} \label{eq:ug-ttest1-V-dist}
V = \frac{(n-1)\,s^2}{\sigma^2} \sim \chi^2\left(n-1\right) \; .
\end{equation}

Finally, since the ratio of a standard normal random variable and the square root of a chi-squared random variable follows a t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}), the distribution of the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat}) is given by

\begin{equation} \label{eq:ug-ttest1-t-dist-qed}
t = \frac{\bar{y}-\mu_0}{s / \sqrt{n}} = \frac{Z}{\sqrt{V / (n-1)}} \sim \mathrm{t}(n-1) \; .
\end{equation}

This means that the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) can be rejected when $t$ is as extreme or more extreme than the critical value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cval}) obtained from the Student's t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) with $n-1$ degrees of freedom using a significance level ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:alpha}) $\alpha$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Student's t-distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-12; URL: \url{https://en.wikipedia.org/wiki/Student%27s_t-distribution#Derivation}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Two-sample t-test}]{Two-sample t-test} \label{sec:ug-ttest2}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ug-ttest2-ug}
\begin{split}
y_{1i} &\sim \mathcal{N}(\mu_1, \sigma^2), \quad i = 1, \ldots, n_1 \\
y_{2i} &\sim \mathcal{N}(\mu_2, \sigma^2), \quad i = 1, \ldots, n_2
\end{split}
\end{equation}

be two univariate Gaussian data sets ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug}) representing two groups of unequal size $n_1$ and $n_2$ with unknown means $\mu_1$ and $\mu_2$ and equal unknown variance $\sigma^2$. Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:ug-ttest2-t}
t = \frac{(\bar{y}_1-\bar{y}_2)-\mu_\Delta}{s_p \cdot \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}
\end{equation}

with sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) $\bar{y}_1$ and $\bar{y}_2$ and pooled standard deviation $s_p$ follows a Student's t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) with $n_1+n_2-2$ degrees of freedom

\begin{equation} \label{eq:ug-ttest2-t-dist}
t \sim \mathrm{t}(n_1+n_2-2)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})

\begin{equation} \label{eq:ug-ttest2-ttest2-h0}
H_0: \; \mu_1-\mu_2 = \mu_\Delta \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) are given by

\begin{equation} \label{eq:ug-ttest2-mean-samp}
\begin{split}
\bar{y}_1 &= \frac{1}{n_1} \sum_{i=1}^{n_1} y_{1i} \\
\bar{y}_2 &= \frac{1}{n_2} \sum_{i=1}^{n_2} y_{2i}
\end{split}
\end{equation}

and the pooled standard deviation is given by

\begin{equation} \label{eq:ug-ttest2-std-pool}
s_p = \sqrt{ \frac{(n_1-1) s^2_1 + (n_2-1) s^2_2}{n_1+n_2-2} }
\end{equation}

with the sample variances ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp})

\begin{equation} \label{eq:ug-ttest2-var-samp}
\begin{split}
s^2_1 &= \frac{1}{n_1-1} \sum_{i=1}^{n_1} (y_{1i} - \bar{y}_1)^2 \\
s^2_2 &= \frac{1}{n_2-1} \sum_{i=1}^{n_2} (y_{2i} - \bar{y}_2)^2 \; .
\end{split}
\end{equation}

Using the linear combination formula for normal random variables ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-lincomb}), the sample means follows normal distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with the following parameters:

\begin{equation} \label{eq:ug-ttest2-mean-samp-dist}
\begin{split}
\bar{y}_1 &= \frac{1}{n_1} \sum_{i=1}^{n_1} y_{1i} \sim \mathcal{N}\left( \frac{1}{n_1} n_1 \mu_1, \left(\frac{1}{n_1}\right)^2 n_1 \sigma^2 \right) = \mathcal{N}\left( \mu_1, \sigma^2/n_1 \right) \\
\bar{y}_2 &= \frac{1}{n_2} \sum_{i=1}^{n_2} y_{2i} \sim \mathcal{N}\left( \frac{1}{n_2} n_2 \mu_2, \left(\frac{1}{n_2}\right)^2 n_2 \sigma^2 \right) = \mathcal{N}\left( \mu_2, \sigma^2/n_2 \right) \; .
\end{split}
\end{equation}

Again employing the linear combination theorem and applying the null hypothesis from \eqref{eq:ug-ttest2-ttest2-h0}, the distribution of $Z = ((\bar{y}\_1-\bar{y}\_2) - \mu\_{\Delta}) / (\sigma \sqrt{1/n\_1+1/n\_2})$ becomes standard normal ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm})

\begin{equation} \label{eq:ug-ttest2-Z-dist}
Z = \frac{(\bar{y}_1-\bar{y}_2)-\mu_\Delta}{\sigma \cdot \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}} \sim \mathcal{N}\left( \frac{(\mu_1-\mu_2)-\mu_\Delta}{\sigma \cdot \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}, \left(\frac{1}{\sigma \cdot \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\right)^2 \left( \frac{\sigma^2}{n_1} + \frac{\sigma^2}{n_2} \right) \right) \overset{H_0}{=} \mathcal{N}\left( 0, 1 \right) \; .
\end{equation}

Because sample variances calculated from independent normal random variables follow a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-chi2}), the distribution of $V = (n_1+n_2-2)\,s_p^2/\sigma^2$ is

\begin{equation} \label{eq:ug-ttest2-V-dist}
V = \frac{(n_1+n_2-2)\,s_p^2}{\sigma^2} \sim \chi^2\left(n_1+n_2-2\right) \; .
\end{equation}

Finally, since the ratio of a standard normal random variable and the square root of a chi-squared random variable follows a t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}), the distribution of the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat}) is given by

\begin{equation} \label{eq:ug-ttest2-t-dist-qed}
t = \frac{(\bar{y}_1-\bar{y}_2)-\mu_\Delta}{s_p \cdot \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}} = \frac{Z}{\sqrt{V / (n_1+n_2-2)}} \sim \mathrm{t}(n_1+n_2-2) \; .
\end{equation}

This means that the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) can be rejected when $t$ is as extreme or more extreme than the critical value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cval}) obtained from the Student's t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) with $n_1+n_2-2$ degrees of freedom using a significance level ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:alpha}) $\alpha$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Student's t-distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-12; URL: \url{https://en.wikipedia.org/wiki/Student%27s_t-distribution#Derivation}.
\item Wikipedia (2021): "Student's t-test"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-12; URL: \url{https://en.wikipedia.org/wiki/Student%27s_t-test#Equal_or_unequal_sample_sizes,_similar_variances_(1/2_%3C_sX1/sX2_%3C_2)}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Paired t-test}]{Paired t-test} \label{sec:ug-ttestp}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y_{i1}$ and $y_{i2}$ with $i = 1, \ldots, n$ be paired observations, such that

\begin{equation} \label{eq:ug-ttestp-ug}
y_{i1} \sim \mathcal{N}(y_{i2} + \mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

is a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug}) with unknown shift $\mu$ and unknown variance $\sigma^2$. Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:ug-ttestp-t}
t = \frac{\bar{d}-\mu_0}{s_d / \sqrt{n}} \quad \text{where} \quad d_i = y_{i1} - y_{i2}
\end{equation}

with sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) $\bar{d}$ and sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) $s^2_d$ follows a Student's t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) with $n-1$ degrees of freedom

\begin{equation} \label{eq:ug-ttestp-t-dist}
t \sim \mathrm{t}(n-1)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})

\begin{equation} \label{eq:ug-ttestp-ttestp-h0}
H_0: \; \mu = \mu_0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Define the pair-wise difference $d_i = y_{i1} - y_{i2}$ which is, according to the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}) and the invariance of the variance under addition ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-inv}), distributed as

\begin{equation} \label{eq:ug-ttestp-d-dist}
d_i = y_{i1} - y_{i2} \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n \; .
\end{equation}

Therefore, $d_1, \ldots, d_n$ satisfy the conditions of the one-sample t-test ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug-ttest1}) which results in the test statistic given by \eqref{eq:ug-ttestp-t}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Student's t-test"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-12; URL: \url{https://en.wikipedia.org/wiki/Student%27s_t-test#Dependent_t-test_for_paired_samples}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{F-test for equality of variances}]{F-test for equality of variances} \label{sec:ug-fev}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ug-fev-ug}
\begin{split}
y_{1i} &\sim \mathcal{N}(\mu_1, \sigma_1^2), \quad i = 1, \ldots, n_1 \\
y_{2i} &\sim \mathcal{N}(\mu_2, \sigma_2^2), \quad i = 1, \ldots, n_2
\end{split}
\end{equation}

be two univariate Gaussian data sets ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug}) representing two groups of unequal size $n_1$ and $n_2$ with unknown means $\mu_1$ and $\mu_2$ and unknown variances $\sigma_1^2$ and $\sigma_2^2$. Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:ug-fev-F}
F
= \frac{s_1^2}{s_1^2}
= \frac{\frac{1}{n_1-1} \sum_{i=1}^{n_1} (y_{1i}-\bar{y}_1)^2}{\frac{1}{n_2-1} \sum_{i=1}^{n_2} (y_{2i}-\bar{y}_2)^2}
\end{equation}

with sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) $\bar{y}_1$ and $\bar{y}_2$ and samle variances ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) $s_1^2$ and $s_2^2$ follows an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}) with numerator degrees of freedom $n_1-1$ and denominator degrees of freedom $n_2-1$

\begin{equation} \label{eq:ug-fev-F-dist}
F \sim \mathrm{F}(n_1-1, n_2-1)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that the two variances ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) are equal:

\begin{equation} \label{eq:ug-fev-fev-h0}
H_0: \; \sigma_1^2 = \sigma_2^2 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} We know that, for a sample of normal random variables, the sample variance is following a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-chi2}):

\begin{equation} \label{eq:ug-fev-norm-chi2}
X_i \sim \mathcal{N}(\mu, \sigma^2), \; i = 1, \ldots, n
\quad \Rightarrow \quad
V = (n-1) \frac{s^2}{\sigma^2} \sim \chi^2(n-1) \; .
\end{equation}

Thus, we have:

\begin{equation} \label{eq:ug-fev-V-dist}
\begin{split}
V_1 &= (n_1-1) \frac{s_1^2}{\sigma_1^2} \sim \chi^2(n_1-1) \quad \text{and} \\
V_2 &= (n_2-1) \frac{s_2^2}{\sigma_2^2} \sim \chi^2(n_2-1) \; .
\end{split}
\end{equation}

Moreover, by definition, the ratio of two chi-squared random variables, divided by their degrees of freedom, is following an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}):

\begin{equation} \label{eq:ug-fev-chi2-f}
X_1 \sim \chi^2(d_1), \; X_2 \sim \chi^2(d_2)
\quad \Rightarrow \quad
Y = \frac{X_1/d_1}{X_2/d_2} \sim F(d_1, d_2) \; .
\end{equation}

Thus, we have:

\begin{equation} \label{eq:ug-fev-F-dist-qed}
\begin{split}
F
&= \frac{V_1/(n_1-1)}{V_2/(n_2-1)} \\
&= \frac{\left. (n_1-1) \frac{s_1^2}{\sigma_1^2} \middle/ (n_1-1) \right.}{\left. (n_2-1) \frac{s_2^2}{\sigma_2^2} \middle/ (n_2-1) \right.} \\
&= \frac{s_1^2 / \sigma_1^2}{s_2^2 / \sigma_2^2} \\
&\overset{H_0}{=} \frac{s_1^2}{s_2^2} \; .
\end{split}
\end{equation}

This means that the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) of equal variances can be rejected when $F$ is as extreme or more extreme than the critical value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cval}) obtained from the F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:F}) with degrees of freedom $n_1-1$ and $n_2-1$ using a significance level ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:alpha}) $\alpha$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2024): "F-test of equality of variances"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2024-07-05; URL: \url{https://en.wikipedia.org/wiki/F-test_of_equality_of_variances#The_test}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Conjugate prior distribution}]{Conjugate prior distribution} \label{sec:ug-prior}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ug-prior-ug}
y = \left\lbrace y_1, \ldots, y_n \right\rbrace, \quad y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug}) with unknown mean $\mu$ and unknown variance $\sigma^2$. Then, the conjugate prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-conj}) for this model is a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng})

\begin{equation} \label{eq:ug-prior-UG-NG-prior}
p(\mu,\tau) = \mathcal{N}(\mu; \mu_0, (\tau \lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0)
\end{equation}

where $\tau = 1/\sigma^2$ is the inverse variance or precision.


\vspace{1em}
\textbf{Proof:} By definition, a conjugate prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-conj}) is a prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) that, when combined with the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}), leads to a posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) that belongs to the same family of probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}). This is fulfilled when the prior density and the likelihood function are proportional to the model model parameters in the same way, i.e. the model parameters appear in the same functional form in both.

Equation \eqref{eq:ug-prior-ug} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:ug-prior-UG-LF-class}
\begin{split}
p(y|\mu,\sigma^2) &= \prod_{i=1}^{n} \mathcal{N}(y_i; \mu, \sigma^2) \\
&= \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp\left[ -\frac{1}{2} \left( \frac{y_i-\mu}{\sigma} \right)^2 \right] \\
&= \frac{1}{(\sqrt{2 \pi \sigma^2})^n} \cdot \exp\left[ -\frac{1}{2 \sigma^2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right]
\end{split}
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:ug-prior-UG-LF-Bayes}
\begin{split}
p(y|\mu,\tau) &= \prod_{i=1}^{n} \mathcal{N}(y_i; \mu, \tau^{-1}) \\
&= \prod_{i=1}^{n} \sqrt{\frac{\tau}{2 \pi}} \cdot \exp\left[ -\frac{\tau}{2} \left( y_i-\mu \right)^2 \right] \\
&= \left( \sqrt{\frac{\tau}{2 \pi}} \right)^n \cdot \exp\left[ -\frac{\tau}{2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right]
\end{split}
\end{equation}

using the inverse variance or precision $\tau = 1/\sigma^2$.

\vspace{1em}
Separating constant and variable terms, we have:

\begin{equation} \label{eq:ug-prior-UG-LF-s1}
p(y|\mu,\tau) = \sqrt{\frac{1}{(2 \pi)^n}} \cdot \tau^{n/2} \cdot \exp\left[ -\frac{\tau}{2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right] \; .
\end{equation}

Expanding the product in the exponent, we have

\begin{equation} \label{eq:ug-prior-UG-LF-s2}
\begin{split}
p(y|\mu,\tau) &= \sqrt{\frac{1}{(2 \pi)^n}} \cdot \tau^{n/2} \cdot \exp\left[ -\frac{\tau}{2} \sum_{i=1}^{n} \left( y_i^2 - 2 \mu y_i + \mu^2 \right) \right] \\
&= \sqrt{\frac{1}{(2 \pi)^n}} \cdot \tau^{n/2} \cdot \exp\left[ -\frac{\tau}{2} \left( \sum_{i=1}^{n} y_i^2 - 2 \mu \sum_{i=1}^{n} y_i + n \mu^2 \right) \right] \\
&= \sqrt{\frac{1}{(2 \pi)^n}} \cdot \tau^{n/2} \cdot \exp\left[ -\frac{\tau}{2} \left( y^\mathrm{T} y - 2 \mu n \bar{y} + n \mu^2 \right) \right] \\
&= \sqrt{\frac{1}{(2 \pi)^n}} \cdot \tau^{n/2} \cdot \exp\left[ -\frac{\tau n}{2} \left( \frac{1}{n} y^\mathrm{T} y - 2 \mu \bar{y} + \mu^2 \right) \right]
\end{split}
\end{equation}

where $\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$ is the mean of data points and $y^\mathrm{T} y = \sum_{i=1}^{n} y_i^2$ is the sum of squared data points.

Completing the square over $\mu$, finally gives

\begin{equation} \label{eq:ug-prior-UG-LF-s3}
p(y|\mu,\tau) = \sqrt{\frac{1}{(2 \pi)^n}} \cdot \tau^{n/2} \cdot \exp\left[ -\frac{\tau n}{2} \left( (\mu-\bar{y})^2 - \bar{y}^2 + \frac{1}{n} y^\mathrm{T} y \right) \right]
\end{equation}

\vspace{1em}
In other words, the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) is proportional to a power of $\tau$ times an exponential of $\tau$ and an exponential of a squared form of $\mu$, weighted by $\tau$:

\begin{equation} \label{eq:ug-prior-UG-LF-s4}
p(y|\mu,\tau) \propto \tau^{n/2} \cdot \exp\left[ -\frac{\tau}{2} \left( y^\mathrm{T} y - n \bar{y}^2 \right) \right] \cdot \exp\left[ -\frac{\tau n}{2} \left( \mu-\bar{y} \right)^2 \right] \; .
\end{equation}

The same is true for a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}) over $\mu$ and $\tau$

\begin{equation} \label{eq:ug-prior-UG-prior-s1}
p(\mu,\tau) = \mathcal{N}(\mu; \mu_0, (\tau \lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0)
\end{equation}

the probability density function of which ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-pdf})

\begin{equation} \label{eq:ug-prior-UG-prior-s2}
p(\mu,\tau) = \sqrt{\frac{\tau \lambda_0}{2 \pi}} \cdot \exp\left[ -\frac{\tau \lambda_0}{2} \left( \mu-\mu_0 \right)^2 \right] \cdot \frac{ {b_0}^{a_0} }{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau]
\end{equation}

exhibits the same proportionality

\begin{equation} \label{eq:ug-prior-UG-prior-s3}
p(\mu,\tau) \propto \tau^{a_0+1/2-1} \cdot \exp[-\tau b_0] \cdot \exp\left[ -\frac{\tau \lambda_0}{2} \left( \mu-\mu_0 \right)^2 \right]
\end{equation}

and is therefore conjugate relative to the likelihood.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop CM (2006): "Bayesian inference for the Gaussian"; in: \textit{Pattern Recognition for Machine Learning}, pp. 97-102, eq. 2.154; URL: \url{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Posterior distribution}]{Posterior distribution} \label{sec:ug-post}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ug-post-ug}
y = \left\lbrace y_1, \ldots, y_n \right\rbrace, \quad y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug}) with unknown mean $\mu$ and unknown variance $\sigma^2$. Moreover, assume a normal-gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug-prior}) over the model parameters $\mu$ and $\tau = 1/\sigma^2$:

\begin{equation} \label{eq:ug-post-UG-NG-prior}
p(\mu,\tau) = \mathcal{N}(\mu; \mu_0, (\tau \lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0) \; .
\end{equation}

Then, the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) is also a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng})

\begin{equation} \label{eq:ug-post-UG-NG-post}
p(\mu,\tau|y) = \mathcal{N}(\mu; \mu_n, (\tau \lambda_n)^{-1}) \cdot \mathrm{Gam}(\tau; a_n, b_n)
\end{equation}

and the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:ug-post-UG-NG-post-par}
\begin{split}
\mu_n &= \frac{\lambda_0 \mu_0 + n \bar{y}}{\lambda_0 + n} \\
\lambda_n &= \lambda_0 + n \\
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} y + \lambda_0 \mu_0^2 - \lambda_n \mu_n^2) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} According to Bayes' theorem ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:bayes-th}), the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) is given by

\begin{equation} \label{eq:ug-post-UG-NG-BT}
p(\mu,\tau|y) = \frac{p(y|\mu,\tau) \, p(\mu,\tau)}{p(y)} \; .
\end{equation}

Since $p(y)$ is just a normalization factor, the posterior is proportional ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl}) to the numerator:

\begin{equation} \label{eq:ug-post-UG-NG-post-JL}
p(\mu,\tau|y) \propto p(y|\mu,\tau) \, p(\mu,\tau) = p(y,\mu,\tau) \; .
\end{equation}

Equation \eqref{eq:ug-post-ug} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:ug-post-UG-LF-class}
\begin{split}
p(y|\mu,\sigma^2) &= \prod_{i=1}^{n} \mathcal{N}(y_i; \mu, \sigma^2) \\
&= \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp\left[ -\frac{1}{2} \left( \frac{y_i-\mu}{\sigma} \right)^2 \right] \\
&= \frac{1}{(\sqrt{2 \pi \sigma^2})^n} \cdot \exp\left[ -\frac{1}{2 \sigma^2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right]
\end{split}
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:ug-post-UG-LF-Bayes}
\begin{split}
p(y|\mu,\tau) &= \prod_{i=1}^{n} \mathcal{N}(y_i; \mu, \tau^{-1}) \\
&= \prod_{i=1}^{n} \sqrt{\frac{\tau}{2 \pi}} \cdot \exp\left[ -\frac{\tau}{2} \left( y_i-\mu \right)^2 \right] \\
&= \left( \sqrt{\frac{\tau}{2 \pi}} \right)^n \cdot \exp\left[ -\frac{\tau}{2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right]
\end{split}
\end{equation}

using the inverse variance or precision $\tau = 1/\sigma^2$.

\vspace{1em}
Combining the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) \eqref{eq:ug-post-UG-LF-Bayes} with the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) \eqref{eq:ug-post-UG-NG-prior}, the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) of the model is given by

\begin{equation} \label{eq:ug-post-UG-NG-JL-s1}
\begin{split}
p(y,\mu,\tau) = \; & p(y|\mu,\tau) \, p(\mu,\tau) \\
= \; & \left( \sqrt{\frac{\tau}{2 \pi}} \right)^n \cdot \exp\left[ -\frac{\tau}{2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right] \cdot \\
& \sqrt{\frac{\tau \lambda_0}{2 \pi}} \cdot \exp\left[ -\frac{\tau \lambda_0}{2} \left( \mu-\mu_0 \right)^2 \right] \cdot \\
& \frac{ {b_0}^{a_0} }{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \; .
\end{split}
\end{equation}

Collecting identical variables gives:

\begin{equation} \label{eq:ug-post-UG-NG-JL-s2}
\begin{split}
p(y,\mu,\tau) = \; & \sqrt{\frac{\tau^{n+1} \lambda_0}{(2 \pi)^{n+1}}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau}{2} \left( \sum_{i=1}^{n} \left( y_i-\mu \right)^2 + \lambda_0 \left( \mu-\mu_0 \right)^2 \right) \right] \; .
\end{split}
\end{equation}

Expanding the products in the exponent ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug-prior}) gives

\begin{equation} \label{eq:ug-post-UG-NG-JL-s3}
\begin{split}
p(y,\mu,\tau) = \; & \sqrt{\frac{\tau^{n+1} \lambda_0}{(2 \pi)^{n+1}}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau}{2} \left( (y^\mathrm{T} y - 2 \mu n \bar{y} + n \mu^2) + \lambda_0 (\mu^2 - 2 \mu \mu_0 + \mu_0^2) \right) \right]
\end{split}
\end{equation}

where $\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$ and $y^\mathrm{T} y = \sum_{i=1}^{n} y_i^2$, such that

\begin{equation} \label{eq:ug-post-UG-NG-JL-s4}
\begin{split}
p(y,\mu,\tau) = \; & \sqrt{\frac{\tau^{n+1} \lambda_0}{(2 \pi)^{n+1}}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau}{2} \left( \mu^2 (\lambda_0 + n) - 2 \mu (\lambda_0 \mu_0 + n \bar{y}) + (y^\mathrm{T} y + \lambda_0 \mu_0^2) \right) \right]
\end{split}
\end{equation}

Completing the square over $\mu$, we finally have

\begin{equation} \label{eq:ug-post-UG-NG-JL-s5}
\begin{split}
p(y,\mu,\tau) = \; & \sqrt{\frac{\tau^{n+1} \lambda_0}{(2 \pi)^{n+1}}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau \lambda_n}{2} \left( \mu - \mu_n \right)^2 -\frac{\tau}{2} \left( y^\mathrm{T} y + \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right) \right]
\end{split}
\end{equation}

with the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post})

\begin{equation} \label{eq:ug-post-UG-NG-post-mu-par}
\begin{split}
\mu_n &= \frac{\lambda_0 \mu_0 + n \bar{y}}{\lambda_0 + n} \\
\lambda_n &= \lambda_0 + n \; .
\end{split}
\end{equation}

Ergo, the joint likelihood is proportional to

\begin{equation} \label{eq:ug-post-UG-NG-JL-s6}
p(y,\mu,\tau) \propto \tau^{1/2} \cdot \exp\left[ -\frac{\tau \lambda_n}{2} \left( \mu - \mu_n \right)^2 \right] \cdot \tau^{a_n-1} \cdot \exp\left[ -b_n \tau \right]
\end{equation}

with the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post})

\begin{equation} \label{eq:ug-post-UG-NG-post-tau-par}
\begin{split}
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} \left( y^\mathrm{T} y + \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right) \; .
\end{split}
\end{equation}

From the term in \eqref{eq:ug-post-UG-NG-JL-s5}, we can isolate the posterior distribution over $\mu$ given $\tau$:

\begin{equation} \label{eq:ug-post-UG-NG-post-mu}
p(\mu|\tau,y) = \mathcal{N}(\mu; \mu_n, (\tau \lambda_n)^{-1}) \; .
\end{equation}

From the remaining term, we can isolate the posterior distribution over $\tau$:

\begin{equation} \label{eq:ug-post-UG-NG-post-tau}
p(\tau|y) = \mathrm{Gam}(\tau; a_n, b_n) \; .
\end{equation}

Together, \eqref{eq:ug-post-UG-NG-post-mu} and \eqref{eq:ug-post-UG-NG-post-tau} constitute the joint ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) of $\mu$ and $\tau$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop CM (2006): "Bayesian inference for the Gaussian"; in: \textit{Pattern Recognition for Machine Learning}, pp. 97-102, eq. 2.154; URL: \url{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Log model evidence}]{Log model evidence} \label{sec:ug-lme}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ug-lme-ug}
m: \; y = \left\lbrace y_1, \ldots, y_n \right\rbrace, \quad y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug}) with unknown mean $\mu$ and unknown variance $\sigma^2$. Moreover, assume a normal-gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug-prior}) over the model parameters $\mu$ and $\tau = 1/\sigma^2$:

\begin{equation} \label{eq:ug-lme-UG-NG-prior}
p(\mu,\tau) = \mathcal{N}(\mu; \mu_0, (\tau \lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0) \; .
\end{equation}

Then, the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) for this model is

\begin{equation} \label{eq:ug-lme-UG-NG-LME}
\log p(y|m) = - \frac{n}{2} \log (2 \pi)  + \frac{1}{2} \log \frac{\lambda_0}{\lambda_n} + \log \Gamma(a_n) - \log \Gamma(a_0) + a_0 \log b_0 - a_n \log b_n
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:ug-lme-UG-NG-post-par}
\begin{split}
\mu_n &= \frac{\lambda_0 \mu_0 + n \bar{y}}{\lambda_0 + n} \\
\lambda_n &= \lambda_0 + n \\
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} y + \lambda_0 \mu_0^2 - \lambda_n \mu_n^2) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} According to the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), the model evidence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml}) for this model is:

\begin{equation} \label{eq:ug-lme-UG-NG-ME-s1}
p(y|m) = \iint p(y|\mu,\tau) \, p(\mu,\tau) \, \mathrm{d}\mu \, \mathrm{d}\tau \; .
\end{equation}

According to the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), the integrand is equivalent to the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}):

\begin{equation} \label{eq:ug-lme-UG-NG-ME-s2}
p(y|m) = \iint p(y,\mu,\tau) \, \mathrm{d}\mu \, \mathrm{d}\tau \; .
\end{equation}

Equation \eqref{eq:ug-lme-ug} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:ug-lme-UG-LF-class}
\begin{split}
p(y|\mu,\sigma^2) &= \prod_{i=1}^{n} \mathcal{N}(y_i; \mu, \sigma^2) \\
&= \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp\left[ -\frac{1}{2} \left( \frac{y_i-\mu}{\sigma} \right)^2 \right] \\
&= \frac{1}{(\sqrt{2 \pi \sigma^2})^n} \cdot \exp\left[ -\frac{1}{2 \sigma^2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right]
\end{split}
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:ug-lme-UG-LF-Bayes}
\begin{split}
p(y|\mu,\tau) &= \prod_{i=1}^{n} \mathcal{N}(y_i; \mu, \tau^{-1}) \\
&= \prod_{i=1}^{n} \sqrt{\frac{\tau}{2 \pi}} \cdot \exp\left[ -\frac{\tau}{2} \left( y_i-\mu \right)^2 \right] \\
&= \left( \sqrt{\frac{\tau}{2 \pi}} \right)^n \cdot \exp\left[ -\frac{\tau}{2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right]
\end{split}
\end{equation}

using the inverse variance or precision $\tau = 1/\sigma^2$.

\vspace{1em}
When deriving the posterior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug-post}) $p(\mu,\tau|y)$, the joint likelihood $p(y,\mu,\tau)$ is obtained as

\begin{equation} \label{eq:ug-lme-UG-NG-LME-s1}
\begin{split}
p(y,\mu,\tau) = \; & \sqrt{\frac{\tau^{n+1} \lambda_0}{(2 \pi)^{n+1}}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau \lambda_n}{2} \left( \mu - \mu_n \right)^2 -\frac{\tau}{2} \left( y^\mathrm{T} y + \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right) \right] \; .
\end{split}
\end{equation}

Using the probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}), we can rewrite this as

\begin{equation} \label{eq:ug-lme-UG-NG-LME-s2}
\begin{split}
p(y,\mu,\tau) = \; & \sqrt{\frac{\tau^{n}}{(2 \pi)^{n}}} \sqrt{\frac{\tau \lambda_0}{2 \pi}} \sqrt{\frac{2 \pi}{\tau \lambda_n}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \mathcal{N}(\mu; \mu_n, (\tau \lambda_n)^{-1}) \, \exp\left[ -\frac{\tau}{2} \left( y^\mathrm{T} y + \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right) \right] \; .
\end{split}
\end{equation}

Now, $\mu$ can be integrated out easily:

\begin{equation} \label{eq:ug-lme-UG-NG-LME-s3}
\begin{split}
\int p(y,\mu,\tau) \, \mathrm{d}\mu = \; & \sqrt{\frac{1}{(2 \pi)^{n}}} \sqrt{\frac{\lambda_0}{\lambda_n}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0+n/2-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau}{2} \left( y^\mathrm{T} y + \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right) \right] \; .
\end{split}
\end{equation}

Using the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), we can rewrite this as

\begin{equation} \label{eq:ug-lme-UG-NG-LME-s4}
\int p(y,\mu,\tau) \, \mathrm{d}\mu = \sqrt{\frac{1}{(2 \pi)^{n}}} \sqrt{\frac{\lambda_0}{\lambda_n}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \frac{\Gamma(a_n)}{ {b_n}^{a_n}} \, \mathrm{Gam}(\tau; a_n, b_n) \; .
\end{equation}

Finally, $\tau$ can also be integrated out:

\begin{equation} \label{eq:ug-lme-UG-NG-LME-s5}
\iint p(y,\mu,\tau) \, \mathrm{d}\mu \, \mathrm{d}\tau = \sqrt{\frac{1}{(2 \pi)^{n}}} \sqrt{\frac{\lambda_0}{\lambda_n}} \, \frac{\Gamma(a_n)}{\Gamma(a_0)} \, \frac{ {b_0}^{a_0}}{ {b_n}^{a_n}} \; .
\end{equation}

Thus, the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) of this model is given by

\begin{equation} \label{eq:ug-lme-UG-NG-LME-s6}
\log p(y|m) = - \frac{n}{2} \log (2 \pi)  + \frac{1}{2} \log \frac{\lambda_0}{\lambda_n} + \log \Gamma(a_n) - \log \Gamma(a_0) + a_0 \log b_0 - a_n \log b_n \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop CM (2006): "Bayesian linear regression"; in: \textit{Pattern Recognition for Machine Learning}, pp. 152-161, ex. 3.23, eq. 3.118; URL: \url{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Accuracy and complexity}]{Accuracy and complexity} \label{sec:ug-anc}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ug-anc-ug}
m: \; y = \left\lbrace y_1, \ldots, y_n \right\rbrace, \quad y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug}) with unknown mean $\mu$ and unknown variance $\sigma^2$. Moreover, assume a normal-gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug-prior}) over the model parameters $\mu$ and $\tau = 1/\sigma^2$:

\begin{equation} \label{eq:ug-anc-UG-NG-prior}
p(\mu,\tau) = \mathcal{N}(\mu; \mu_0, (\tau \lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0) \; .
\end{equation}

Then, accuracy and complexity ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme-anc}) of this model are

\begin{equation} \label{eq:ug-anc-UG-NG-AnC}
\begin{split}
\mathrm{Acc}(m) &= - \frac{1}{2} \frac{a_n}{b_n} \left( y^\mathrm{T} y - 2 n \bar{y} \mu_n + n \mu_n^2 \right) - \frac{1}{2} n \lambda_n^{-1} + \frac{n}{2} \left(\psi(a_n) - \log(b_n)\right) - \frac{n}{2} \log (2 \pi) \\
\mathrm{Com}(m) &= \frac{1}{2} \frac{a_n}{b_n} \left[ \lambda_0 (\mu_0 - \mu_n)^2 - 2 (b_n - b_0) \right] + \frac{1}{2} \frac{\lambda_0}{\lambda_n} - \frac{1}{2} \log \frac{\lambda_0}{\lambda_n} - \frac{1}{2} \\
&+ a_0 \cdot \log \frac{b_n}{b_0} - \log \frac{\Gamma(a_n)}{\Gamma(a_0)} + (a_n - a_0) \cdot \psi(a_n)
\end{split}
\end{equation}

where $\mu_n$ and $\lambda_n$ as well as $a_n$ and $b_n$ are the posterior hyperparameters for the univariate Gaussian ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug-post}) and $\bar{y}$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}).


\vspace{1em}
\textbf{Proof:} Model accuracy and complexity are defined as ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme-anc})

\begin{equation} \label{eq:ug-anc-lme-anc}
\begin{split}
\mathrm{LME}(m) &= \mathrm{Acc}(m) - \mathrm{Com}(m) \\
\mathrm{Acc}(m) &= \left\langle \log p(y|\mu,\tau,m) \right\rangle_{p(\mu,\tau|y,m)} \\
\mathrm{Com}(m) &= \mathrm{KL} \left[ p(\mu,\tau|y,m) \, || \, p(\mu,\tau|m) \right] \; .
\end{split}
\end{equation}

\vspace{1em}
The accuracy term is the expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) $\log p(y|\mu,\tau)$ with respect to the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) $p(\mu,\tau|y)$. With the log-likelihood function for the univariate Gaussian ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug-mle}) and the posterior distribution for the univariate Gaussian ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug-post}), the model accuracy of $m$ evaluates to:

\begin{equation} \label{eq:ug-anc-UG-NG-Acc}
\begin{split}
\mathrm{Acc}(m) &= \left\langle \log p(y|\mu,\tau) \right\rangle_{p(\mu,\tau|y)} \\
&= \left\langle \left\langle \log p(y|\mu,\tau) \right\rangle_{p(\mu|\tau,y)} \right\rangle_{p(\tau|y)} \\
&= \left\langle \left\langle \frac{n}{2} \log (\tau) - \frac{n}{2} \log (2 \pi) - \frac{\tau}{2} \left( y^\mathrm{T} y - 2 n \bar{y} \mu + n \mu^2 \right) \right\rangle_{\mathcal{N}(\mu_n, (\tau \lambda_n)^{-1})} \right\rangle_{\mathrm{Gam}(a_n, b_n)} \\
&= \left\langle \frac{n}{2} \log (\tau) - \frac{n}{2} \log (2 \pi) - \frac{\tau}{2} \left( y^\mathrm{T} y - 2 n \bar{y} \mu_n + n \mu_n^2 \right) - \frac{1}{2} n \lambda_n^{-1} \right\rangle_{\mathrm{Gam}(a_n, b_n)} \\
&= \frac{n}{2} \left(\psi(a_n) - \log(b_n)\right) - \frac{n}{2} \log (2 \pi) - \frac{1}{2} \frac{a_n}{b_n} \left( y^\mathrm{T} y - 2 n \bar{y} \mu_n + n \mu_n^2 \right) - \frac{1}{2} n \lambda_n^{-1} \\
&= - \frac{1}{2} \frac{a_n}{b_n} \left( y^\mathrm{T} y - 2 n \bar{y} \mu_n + n \mu_n^2 \right) - \frac{1}{2} n \lambda_n^{-1} + \frac{n}{2} \left(\psi(a_n) - \log(b_n)\right) - \frac{n}{2} \log (2 \pi) \\
\end{split}
\end{equation}

\vspace{1em}
The complexity penalty is the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) $p(\mu,\tau|y)$ from the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) $p(\mu,\tau)$. With the prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug-prior}) given by \eqref{eq:ug-anc-UG-NG-prior}, the posterior distribution for the univariate Gaussian ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug-post}) and the Kullback-Leibler divergence of the normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-kl}), the model complexity of $m$ evaluates to:

\begin{equation} \label{eq:ug-anc-UG-NG-Com}
\begin{split}
\mathrm{Com}(m) &= \mathrm{KL} \left[ p(\mu,\tau|y) \, || \, p(\mu,\tau) \right] \\
&= \mathrm{KL} \left[ \mathrm{NG}(\mu_n, \lambda_n^{-1}, a_n, b_n) \, || \,  \mathrm{NG}(\mu_0, \lambda_0^{-1}, a_0, b_0) \right] \\
&= \frac{1}{2} \frac{a_n}{b_n} \left[ \lambda_0 (\mu_0 - \mu_n)^2 \right] + \frac{1}{2} \frac{\lambda_0}{\lambda_n} - \frac{1}{2} \log \frac{\lambda_0}{\lambda_n} - \frac{1}{2} \\
&+ a_0 \cdot \log \frac{b_n}{b_0} - \log \frac{\Gamma(a_n)}{\Gamma(a_0)} + (a_n - a_0) \cdot \psi(a_n) - (b_n - b_0) \cdot \frac{a_n}{b_n} \\
&= \frac{1}{2} \frac{a_n}{b_n} \left[ \lambda_0 (\mu_0 - \mu_n)^2 - 2 (b_n - b_0) \right] + \frac{1}{2} \frac{\lambda_0}{\lambda_n} - \frac{1}{2} \log \frac{\lambda_0}{\lambda_n} - \frac{1}{2} \\
&+ a_0 \cdot \log \frac{b_n}{b_0} - \log \frac{\Gamma(a_n)}{\Gamma(a_0)} + (a_n - a_0) \cdot \psi(a_n) \; .
\end{split}
\end{equation}

A control calculation confirms that

\begin{equation} \label{eq:ug-anc-UG-NG-AnC-LME}
\mathrm{Acc}(m) - \mathrm{Com}(m) = \mathrm{LME}(m)
\end{equation}

where $\mathrm{LME}(m)$ is the log model evidence for the univariate Gaussian ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug-lme}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{Univariate Gaussian with known variance}

\subsubsection[\textit{Definition}]{Definition} \label{sec:ugkv}
\setcounter{equation}{0}

\textbf{Definition:} A univariate Gaussian data set with known variance is given by a set of real numbers $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$, independent and identically distributed according to a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with unknown mean $\mu$ and known variance $\sigma^2$:

\begin{equation} \label{eq:ugkv-ug}
y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop, Christopher M. (2006): "Bayesian inference for the Gaussian"; in: \textit{Pattern Recognition for Machine Learning}, ch. 2.3.6, p. 97, eq. 2.137; URL: \url{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Maximum likelihood estimation}]{Maximum likelihood estimation} \label{sec:ugkv-mle}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be univariate Gaussian data with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv}) $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$:

\begin{equation} \label{eq:ugkv-mle-ugkv}
y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n \; .
\end{equation}

Then, the maximum likelihood estimate ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) for the mean $\mu$ is given by

\begin{equation} \label{eq:ugkv-mle-ugkv-MLE}
\hat{\mu} = \bar{y}
\end{equation}

where $\bar{y}$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp})

\begin{equation} \label{eq:ugkv-mle-y-mean}
\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) for each observation is given by the probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf})

\begin{equation} \label{eq:ugkv-mle-ugkv-yi}
p(y_i|\mu) = \mathcal{N}(x; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot \exp \left[ -\frac{1}{2} \left( \frac{y_i-\mu}{\sigma} \right)^2 \right]
\end{equation}

and because observations are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:ugkv-mle-ugkv-LF-s1}
p(y|\mu) = \prod_{i=1}^n p(y_i|\mu) = \sqrt{ \frac{1}{(2 \pi \sigma^2)^n} } \cdot \exp \left[ -\frac{1}{2} \sum_{i=1}^{n} \left( \frac{y_i-\mu}{\sigma} \right)^2 \right] \; .
\end{equation}

This can be developed into

\begin{equation} \label{eq:ugkv-mle-ugkv-LF-s2}
\begin{split}
p(y|\mu) &= \left( \frac{1}{2 \pi \sigma^2} \right)^{n/2} \cdot \exp \left[ -\frac{1}{2} \sum_{i=1}^{n} \left( \frac{y_i^2 - 2 y_i \mu + \mu^2}{\sigma^2} \right) \right] \\
&= \left( \frac{1}{2 \pi \sigma^2} \right)^{n/2} \cdot \exp \left[ -\frac{1}{2 \sigma^2} \left( y^\mathrm{T} y - 2 n \bar{y} \mu + n \mu^2 \right) \right]
\end{split}
\end{equation}

where $\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$ is the mean of data points and $y^\mathrm{T} y = \sum_{i=1}^{n} y_i^2$ is the sum of squared data points.

Thus, the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) is

\begin{equation} \label{eq:ugkv-mle-ugkv-LL}
\mathrm{LL}(\mu) = \log p(y|\mu) = -\frac{n}{2} \log (2 \pi \sigma^2) - \frac{1}{2 \sigma^2} \left( y^\mathrm{T} y - 2 n \bar{y} \mu + n \mu^2 \right) \; .
\end{equation}

The derivatives of the log-likelihood with respect to $\mu$ are

\begin{equation} \label{eq:ugkv-mle-ugkv-dLLdl-d2LLdl2}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\mu)}{\mathrm{d}\mu} &= \frac{n \bar{y}}{\sigma^2} - \frac{n \mu}{\sigma^2} = \frac{n}{\sigma^2} (\bar{y}-\mu) \\
\frac{\mathrm{d}^2\mathrm{LL}(\mu)}{\mathrm{d}\mu^2} &= - \frac{n}{\sigma^2} \; . \\
\end{split}
\end{equation}

Setting the first derivative to zero, we obtain:

\begin{equation} \label{eq:ugkv-mle-ugkv-dLLdl}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{\mu})}{\mathrm{d}\mu} &= 0 \\
0 &= \frac{n}{\sigma^2} (\bar{y}-\hat{\mu}) \\
0 &= \bar{y}-\hat{\mu} \\
\hat{\mu} &= \bar{y} \\
\end{split}
\end{equation}

Plugging this value into the second derivative, we confirm:

\begin{equation} \label{eq:ugkv-mle-ugkv-d2LLdl2}
\frac{\mathrm{d}^2\mathrm{LL}(\hat{\mu})}{\mathrm{d}\mu^2} = -\frac{n}{\sigma^2} < 0 \; .
\end{equation}

This demonstrates that the estimate $\hat{\mu} = \bar{y}$ maximizes the likelihood $p(y \vert \mu)$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop, Christopher M. (2006): "Bayesian inference for the Gaussian"; in: \textit{Pattern Recognition for Machine Learning}, ch. 2.3.6, p. 98, eq. 2.143; URL: \url{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{One-sample z-test}]{One-sample z-test} \label{sec:ugkv-ztest1}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ugkv-ztest1-ugkv}
y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv}) with unknown mean $\mu$ and known variance $\sigma^2$. Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:ugkv-ztest1-z}
z = \sqrt{n} \, \frac{\bar{y}-\mu_0}{\sigma}
\end{equation}

with sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) $\bar{y}$ follows a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm})

\begin{equation} \label{eq:ugkv-ztest1-z-dist}
z \sim \mathcal{N}(0, 1)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})

\begin{equation} \label{eq:ugkv-ztest1-ztest1-h0}
H_0: \; \mu = \mu_0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) is given by

\begin{equation} \label{eq:ugkv-ztest1-mean-samp}
\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i \; .
\end{equation}

Using the linear combination formula for normal random variables ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-lincomb}), the sample mean follows a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with the following parameters:

\begin{equation} \label{eq:ugkv-ztest1-mean-samp-dist}
\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i \sim \mathcal{N}\left( \frac{1}{n} n \mu, \left(\frac{1}{n}\right)^2 n \sigma^2 \right) = \mathcal{N}\left( \mu, \sigma^2/n \right) \; .
\end{equation}

Again employing the linear combination theorem, the distribution of $z = \sqrt{n/\sigma^2} (\bar{y}-\mu_0)$ becomes

\begin{equation} \label{eq:ugkv-ztest1-z-dist-s1}
z = \sqrt{\frac{n}{\sigma^2}} (\bar{y} - \mu_0) \sim \mathcal{N}\left( \sqrt{\frac{n}{\sigma^2}} (\mu - \mu_0), \left(\sqrt{\frac{n}{\sigma^2}}\right)^2 \frac{\sigma^2}{n} \right) = \mathcal{N}\left( \sqrt{n} \, \frac{\mu-\mu_0}{\sigma}, 1 \right) \; ,
\end{equation}

such that, under the null hypothesis in \eqref{eq:ugkv-ztest1-ztest1-h0}, we have:

\begin{equation} \label{eq:ugkv-ztest1-z-dist-s2}
z \sim \mathcal{N}(0, 1), \quad \text{if } \mu = \mu_0 \; .
\end{equation}

This means that the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) can be rejected when $z$ is as extreme or more extreme than the critical value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cval}) obtained from the standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) using a significance level ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:alpha}) $\alpha$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Z-test"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-24; URL: \url{https://en.wikipedia.org/wiki/Z-test#Use_in_location_testing}.
\item Wikipedia (2021): "GauÃŸ-Test"; in: \textit{Wikipedia â€“ Die freie EnzyklopÃ¤die}, retrieved on 2021-03-24; URL: \url{https://de.wikipedia.org/wiki/Gau%C3%9F-Test#Einstichproben-Gau%C3%9F-Test}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Two-sample z-test}]{Two-sample z-test} \label{sec:ugkv-ztest2}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ugkv-ztest2-ugkv}
\begin{split}
y_{1i} &\sim \mathcal{N}(\mu_1, \sigma_1^2), \quad i = 1, \ldots, n_1 \\
y_{2i} &\sim \mathcal{N}(\mu_2, \sigma_2^2), \quad i = 1, \ldots, n_2
\end{split}
\end{equation}

be two univariate Gaussian data sets ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug}) representing two groups of unequal size $n_1$ and $n_2$ with unknown means $\mu_1$ and $\mu_2$ and unknown variances $\sigma_1^2$ and $\sigma_2^2$. Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:ugkv-ztest2-z}
z = \frac{(\bar{y}_1-\bar{y}_2)-\mu_\Delta}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}
\end{equation}

with sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) $\bar{y}_1$ and $\bar{y}_2$ follows a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm})

\begin{equation} \label{eq:ugkv-ztest2-z-dist}
z \sim \mathcal{N}(0, 1)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})

\begin{equation} \label{eq:ugkv-ztest2-ztest2-h0}
H_0: \; \mu_1-\mu_2 = \mu_\Delta \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) are given by

\begin{equation} \label{eq:ugkv-ztest2-mean-samp}
\begin{split}
\bar{y}_1 &= \frac{1}{n_1} \sum_{i=1}^{n_1} y_{1i} \\
\bar{y}_2 &= \frac{1}{n_2} \sum_{i=1}^{n_2} y_{2i} \; .
\end{split}
\end{equation}

Using the linear combination formula for normal random variables ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-lincomb}), the sample means follows normal distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) with the following parameters:

\begin{equation} \label{eq:ugkv-ztest2-mean-samp-dist}
\begin{split}
\bar{y}_1 &= \frac{1}{n_1} \sum_{i=1}^{n_1} y_{1i} \sim \mathcal{N}\left( \frac{1}{n_1} n_1 \mu_1, \left(\frac{1}{n_1}\right)^2 n_1 \sigma^2 \right) = \mathcal{N}\left( \mu_1, \sigma_1^2/n_1 \right) \\
\bar{y}_2 &= \frac{1}{n_2} \sum_{i=1}^{n_2} y_{2i} \sim \mathcal{N}\left( \frac{1}{n_2} n_2 \mu_2, \left(\frac{1}{n_2}\right)^2 n_2 \sigma^2 \right) = \mathcal{N}\left( \mu_2, \sigma_2^2/n_2 \right) \; .
\end{split}
\end{equation}

Again employing the linear combination theorem, the distribution of $z = [(\bar{y}_1-\bar{y}_2)-\mu_\Delta]/\sigma_\Delta$ becomes

\begin{equation} \label{eq:ugkv-ztest2-z-dist-s1}
z = \frac{(\bar{y}_1-\bar{y}_2)-\mu_\Delta}{\sigma_\Delta} \sim \mathcal{N}\left( \frac{(\mu_1-\mu_2)-\mu_\Delta}{\sigma_\Delta}, \left(\frac{1}{\sigma_\Delta}\right)^2 \sigma_\Delta^2 \right) = \mathcal{N}\left( \frac{(\mu_1-\mu_2)-\mu_\Delta}{\sigma_\Delta}, 1 \right)
\end{equation}

where $\sigma_\Delta$ is the pooled standard deviation

\begin{equation} \label{eq:ugkv-ztest2-std-pool}
\sigma_\Delta = \sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}} \; ,
\end{equation}

such that, under the null hypothesis in \eqref{eq:ugkv-ztest2-ztest2-h0}, we have:

\begin{equation} \label{eq:ugkv-ztest2-z-dist-s2}
z \sim \mathcal{N}(0, 1), \quad \text{if } \mu_\Delta = \mu_1-\mu_2 \; .
\end{equation}

This means that the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) can be rejected when $z$ is as extreme or more extreme than the critical value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cval}) obtained from the standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) using a significance level ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:alpha}) $\alpha$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Z-test"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-24; URL: \url{https://en.wikipedia.org/wiki/Z-test#Use_in_location_testing}.
\item Wikipedia (2021): "GauÃŸ-Test"; in: \textit{Wikipedia â€“ Die freie EnzyklopÃ¤die}, retrieved on 2021-03-24; URL: \url{https://de.wikipedia.org/wiki/Gau%C3%9F-Test#Zweistichproben-Gau%C3%9F-Test_f%C3%BCr_unabh%C3%A4ngige_Stichproben}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Paired z-test}]{Paired z-test} \label{sec:ugkv-ztestp}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y_{i1}$ and $y_{i2}$ with $i = 1, \ldots, n$ be paired observations, such that

\begin{equation} \label{eq:ugkv-ztestp-ugkv}
y_{i1} \sim \mathcal{N}(y_{i2} + \mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

is a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv}) with unknown shift $\mu$ and known variance $\sigma^2$. Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:ugkv-ztestp-z}
z = \sqrt{n} \, \frac{\bar{d}-\mu_0}{\sigma} \quad \text{where} \quad d_i = y_{i1} - y_{i2}
\end{equation}

with sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) $\bar{d}$ follows a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm})

\begin{equation} \label{eq:ugkv-ztestp-z-dist}
z \sim \mathcal{N}(0, 1)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})

\begin{equation} \label{eq:ugkv-ztestp-ztestp-h0}
H_0: \; \mu = \mu_0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Define the pair-wise difference $d_i = y_{i1} - y_{i2}$ which is, according to the linearity of the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-lin}) and the invariance of the variance under addition ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-inv}), distributed as

\begin{equation} \label{eq:ugkv-ztestp-d-dist}
d_i = y_{i1} - y_{i2} \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n \; .
\end{equation}

Therefore, $d_1, \ldots, d_n$ satisfy the conditions of the one-sample z-test ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-ztest1}) which results in the test statistic given by \eqref{eq:ugkv-ztestp-z}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Z-test"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-03-24; URL: \url{https://en.wikipedia.org/wiki/Z-test#Use_in_location_testing}.
\item Wikipedia (2021): "GauÃŸ-Test"; in: \textit{Wikipedia â€“ Die freie EnzyklopÃ¤die}, retrieved on 2021-03-24; URL: \url{https://de.wikipedia.org/wiki/Gau%C3%9F-Test#Zweistichproben-Gau%C3%9F-Test_f%C3%BCr_abh%C3%A4ngige_(verbundene)_Stichproben}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Conjugate prior distribution}]{Conjugate prior distribution} \label{sec:ugkv-prior}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ugkv-prior-ugkv}
y = \left\lbrace y_1, \ldots, y_n \right\rbrace, \quad y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv}) with unknown mean $\mu$ and known variance $\sigma^2$. Then, the conjugate prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-conj}) for this model is a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm})

\begin{equation} \label{eq:ugkv-prior-UGkv-prior}
p(\mu) = \mathcal{N}(\mu; \mu_0, \lambda_0^{-1})
\end{equation}

with prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu_0$ and prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) precision ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prec}) $\lambda_0$.


\vspace{1em}
\textbf{Proof:} By definition, a conjugate prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-conj}) is a prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) that, when combined with the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}), leads to a posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) that belongs to the same family of probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}). This is fulfilled when the prior density and the likelihood function are proportional to the model model parameters in the same way, i.e. the model parameters appear in the same functional form in both.

Equation \eqref{eq:ugkv-prior-ugkv} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:ugkv-prior-UGkv-LF-class}
\begin{split}
p(y|\mu) &= \prod_{i=1}^{n} \mathcal{N}(y_i; \mu, \sigma^2) \\
&= \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp\left[ -\frac{1}{2} \left( \frac{y_i-\mu}{\sigma} \right)^2 \right] \\
&= \left( \sqrt{\frac{1}{2 \pi \sigma^2}} \right)^n \cdot \exp\left[ -\frac{1}{2 \sigma^2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right]
\end{split}
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:ugkv-prior-UGkv-LF-Bayes}
\begin{split}
p(y|\mu) &= \prod_{i=1}^{n} \mathcal{N}(y_i; \mu, \tau^{-1}) \\
&= \prod_{i=1}^{n} \sqrt{\frac{\tau}{2 \pi}} \cdot \exp\left[ -\frac{\tau}{2} \left( y_i-\mu \right)^2 \right] \\
&= \left( \sqrt{\frac{\tau}{2 \pi}} \right)^n \cdot \exp\left[ -\frac{\tau}{2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right]
\end{split}
\end{equation}

using the inverse variance or precision $\tau = 1/\sigma^2$.

\vspace{1em}
Expanding the product in the exponent, we have

\begin{equation} \label{eq:ugkv-prior-UGkv-LF-s2}
\begin{split}
p(y|\mu) &= \left( \frac{\tau}{2 \pi} \right)^{n/2} \cdot \exp\left[ -\frac{\tau}{2} \sum_{i=1}^{n} \left( y_i^2 - 2 \mu y_i + \mu^2 \right) \right] \\
&= \left( \frac{\tau}{2 \pi} \right)^{n/2} \cdot \exp\left[ -\frac{\tau}{2} \left( \sum_{i=1}^{n} y_i^2 - 2 \mu \sum_{i=1}^{n} y_i + n \mu^2 \right) \right] \\
&= \left( \frac{\tau}{2 \pi} \right)^{n/2} \cdot \exp\left[ -\frac{\tau}{2} \left( y^\mathrm{T} y - 2 \mu n \bar{y} + n \mu^2 \right) \right] \\
&= \left( \frac{\tau}{2 \pi} \right)^{n/2} \cdot \exp\left[ -\frac{\tau n}{2} \left( \frac{1}{n} y^\mathrm{T} y - 2 \mu \bar{y} + \mu^2 \right) \right]
\end{split}
\end{equation}

where $\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$ is the mean of data points and $y^\mathrm{T} y = \sum_{i=1}^{n} y_i^2$ is the sum of squared data points.

Completing the square over $\mu$, finally gives

\begin{equation} \label{eq:ugkv-prior-UGkv-LF-s3}
p(y|\mu) = \left( \frac{\tau}{2 \pi} \right)^{n/2} \cdot \exp\left[ -\frac{\tau n}{2} \left( (\mu-\bar{y})^2 - \bar{y}^2 + \frac{1}{n} y^\mathrm{T} y \right) \right]
\end{equation}

\vspace{1em}
In other words, the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) is proportional to an exponential of a squared form of $\mu$, weighted by some constant:

\begin{equation} \label{eq:ugkv-prior-UGkv-LF-s4}
p(y|\mu) \propto \exp\left[ -\frac{\tau n}{2} \left( \mu-\bar{y} \right)^2 \right] \; .
\end{equation}

The same is true for a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) over $\mu$

\begin{equation} \label{eq:ugkv-prior-UGkv-prior-s1}
p(\mu) = \mathcal{N}(\mu; \mu_0, \lambda_0^{-1})
\end{equation}

the probability density function of which ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf})

\begin{equation} \label{eq:ugkv-prior-UGkv-prior-s2}
p(\mu) = \sqrt{\frac{\lambda_0}{2 \pi}} \cdot \exp\left[ -\frac{\lambda_0}{2} \left( \mu-\mu_0 \right)^2 \right]
\end{equation}

exhibits the same proportionality

\begin{equation} \label{eq:ugkv-prior-UGkv-prior-s3}
p(\mu) \propto \exp\left[ -\frac{\lambda_0}{2} \left( \mu-\mu_0 \right)^2 \right]
\end{equation}

and is therefore conjugate relative to the likelihood.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop, Christopher M. (2006): "Bayesian inference for the Gaussian"; in: \textit{Pattern Recognition for Machine Learning}, ch. 2.3.6, pp. 97-98, eq. 2.138; URL: \url{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Posterior distribution}]{Posterior distribution} \label{sec:ugkv-post}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ugkv-post-ugkv}
y = \left\lbrace y_1, \ldots, y_n \right\rbrace, \quad y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv}) with unknown mean $\mu$ and known variance $\sigma^2$. Moreover, assume a normal distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-prior}) over the model parameter $\mu$:

\begin{equation} \label{eq:ugkv-post-UGkv-prior}
p(\mu) = \mathcal{N}(\mu; \mu_0, \lambda_0^{-1}) \; .
\end{equation}

Then, the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) is also a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm})

\begin{equation} \label{eq:ugkv-post-UGkv-post}
p(\mu|y) = \mathcal{N}(\mu; \mu_n, \lambda_n^{-1})
\end{equation}

and the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:ugkv-post-UGkv-post-par}
\begin{split}
\mu_n &= \frac{\lambda_0 \mu_0 + \tau n \bar{y}}{\lambda_0 + \tau n} \\
\lambda_n &= \lambda_0 + \tau n
\end{split}
\end{equation}

with the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) $\bar{y}$ and the inverse variance or precision ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prec}) $\tau = 1/\sigma^2$.


\vspace{1em}
\textbf{Proof:} According to Bayes' theorem ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:bayes-th}), the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) is given by

\begin{equation} \label{eq:ugkv-post-UGkv-BT}
p(\mu|y) = \frac{p(y|\mu) \, p(\mu)}{p(y)} \; .
\end{equation}

Since $p(y)$ is just a normalization factor, the posterior is proportional ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl}) to the numerator:

\begin{equation} \label{eq:ugkv-post-UGkv-post-JL}
p(\mu|y) \propto p(y|\mu) \, p(\mu) = p(y,\mu) \; .
\end{equation}

Equation \eqref{eq:ugkv-post-ugkv} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:ugkv-post-UGkv-LF-class}
\begin{split}
p(y|\mu) &= \prod_{i=1}^{n} \mathcal{N}(y_i; \mu, \sigma^2) \\
&= \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp\left[ -\frac{1}{2} \left( \frac{y_i-\mu}{\sigma} \right)^2 \right] \\
&= \left( \sqrt{\frac{1}{2 \pi \sigma^2}} \right)^n \cdot \exp\left[ -\frac{1}{2 \sigma^2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right]
\end{split}
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:ugkv-post-UGkv-LF-Bayes}
\begin{split}
p(y|\mu) &= \prod_{i=1}^{n} \mathcal{N}(y_i; \mu, \tau^{-1}) \\
&= \prod_{i=1}^{n} \sqrt{\frac{\tau}{2 \pi}} \cdot \exp\left[ -\frac{\tau}{2} \left( y_i-\mu \right)^2 \right] \\
&= \left( \sqrt{\frac{\tau}{2 \pi}} \right)^n \cdot \exp\left[ -\frac{\tau}{2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right]
\end{split}
\end{equation}

using the inverse variance or precision $\tau = 1/\sigma^2$.

\vspace{1em}
Combining the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) \eqref{eq:ugkv-post-UGkv-LF-Bayes} with the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) \eqref{eq:ugkv-post-UGkv-prior}, the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) of the model is given by

\begin{equation} \label{eq:ugkv-post-UGkv-JL-s1}
\begin{split}
p(y,\mu) = \; & p(y|\mu) \, p(\mu) \\
= \; & \left( \sqrt{\frac{\tau}{2 \pi}} \right)^n \cdot \exp\left[ -\frac{\tau}{2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right] \cdot \sqrt{\frac{\lambda_0}{2 \pi}} \cdot \exp\left[ -\frac{\lambda_0}{2} \left( \mu-\mu_0 \right)^2 \right] \; .
\end{split}
\end{equation}

Rearranging the terms, we then have:

\begin{equation} \label{eq:ugkv-post-UGkv-JL-s2}
p(y,\mu) = \left( \frac{\tau}{2 \pi} \right)^{n/2} \cdot \sqrt{\frac{\lambda_0}{2 \pi}} \cdot \exp\left[ -\frac{\tau}{2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 - \frac{\lambda_0}{2} \left( \mu-\mu_0 \right)^2 \right] \; .
\end{equation}

Expanding the products in the exponent ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-prior}) gives

\begin{equation} \label{eq:ugkv-post-UGkv-JL-s3}
\begin{split}
p(y,\mu) &= \left( \frac{\tau}{2 \pi} \right)^\frac{n}{2} \cdot \left( \frac{\lambda_0}{2 \pi} \right)^\frac{1}{2} \cdot \exp \left[ -\frac{1}{2} \left( \sum_{i=1}^n \tau (y_i-\mu)^2 + \lambda_0 (\mu-\mu_0)^2 \right) \right] \\
&= \left( \frac{\tau}{2 \pi} \right)^\frac{n}{2} \cdot \left( \frac{\lambda_0}{2 \pi} \right)^\frac{1}{2} \cdot \exp \left[ -\frac{1}{2} \left( \sum_{i=1}^n \tau (y_i^2 - 2 y_i \mu + \mu^2) + \lambda_0 (\mu^2 - 2 \mu \mu_0 + \mu_0^2) \right) \right] \\
&= \left( \frac{\tau}{2 \pi} \right)^\frac{n}{2} \cdot \left( \frac{\lambda_0}{2 \pi} \right)^\frac{1}{2} \cdot \exp \left[ -\frac{1}{2} \left( \tau (y^\mathrm{T} y - 2 n \bar{y} \mu + n \mu^2) + \lambda_0 (\mu^2 - 2 \mu \mu_0 + \mu_0^2) \right) \right] \\
&= \left( \frac{\tau}{2 \pi} \right)^\frac{n}{2} \cdot \left( \frac{\lambda_0}{2 \pi} \right)^\frac{1}{2} \cdot \exp \left[ -\frac{1}{2} \left( \mu^2 (\tau n + \lambda_0) - 2 \mu (\tau n \bar{y} + \lambda_0 \mu_0) + (\tau y^\mathrm{T} y + \lambda_0 \mu_0^2) \right) \right] \\
\end{split}
\end{equation}

where $\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$ and $y^\mathrm{T} y = \sum_{i=1}^{n} y_i^2$. Completing the square in $\mu$ then yields

\begin{equation} \label{eq:ugkv-post-UGkv-JL-s4}
p(y,\mu) = \left( \frac{\tau}{2 \pi} \right)^\frac{n}{2} \cdot \left( \frac{\lambda_0}{2 \pi} \right)^\frac{1}{2} \cdot \exp \left[ -\frac{\lambda_n}{2} (\mu - \mu_n)^2 + f_n \right]
\end{equation}

with the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post})

\begin{equation} \label{eq:ugkv-post-UGkv-post-mu-par}
\begin{split}
\mu_n &= \frac{\lambda_0 \mu_0 + \tau n \bar{y}}{\lambda_0 + \tau n} \\
\lambda_n &= \lambda_0 + \tau n
\end{split}
\end{equation}

and the remaining independent term

\begin{equation} \label{eq:ugkv-post-UGkv-JL-fn}
f_n = -\frac{1}{2} \left( \tau y^\mathrm{T} y + \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right) \; .
\end{equation}

Ergo, the joint likelihood in \eqref{eq:ugkv-post-UGkv-JL-s4} is proportional to

\begin{equation} \label{eq:ugkv-post-UGkv-JL-s5}
p(y,\mu) \propto \exp \left[ -\frac{\lambda_n}{2} (\mu - \mu_n)^2 \right] \; ,
\end{equation}

such that the posterior distribution over $\mu$ is given by

\begin{equation} \label{eq:ugkv-post-UGkv-post-mu}
p(\mu|y) = \mathcal{N}(\mu; \mu_n, \lambda_n^{-1}) \; .
\end{equation}

with the posterior hyperparameters given in \eqref{eq:ugkv-post-UGkv-post-mu-par}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop, Christopher M. (2006): "Bayesian inference for the Gaussian"; in: \textit{Pattern Recognition for Machine Learning}, ch. 2.3.6, p. 98, eqs. 2.139-2.142; URL: \url{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Log model evidence}]{Log model evidence} \label{sec:ugkv-lme}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ugkv-lme-ug}
m: \; y = \left\lbrace y_1, \ldots, y_n \right\rbrace, \quad y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv}) with unknown mean $\mu$ and known variance $\sigma^2$. Moreover, assume a normal distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-prior}) over the model parameter $\mu$:

\begin{equation} \label{eq:ugkv-lme-UGkv-prior}
p(\mu) = \mathcal{N}(\mu; \mu_0, \lambda_0^{-1}) \; .
\end{equation}

Then, the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) for this model is

\begin{equation} \label{eq:ugkv-lme-UGkv-LME}
\log p(y|m) = \frac{n}{2} \log\left( \frac{\tau}{2 \pi} \right) + \frac{1}{2} \log\left( \frac{\lambda_0}{\lambda_n} \right) - \frac{1}{2} \left( \tau y^T y + \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right) \; .
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:ugkv-lme-UGkv-post-par}
\begin{split}
\mu_n &= \frac{\lambda_0 \mu_0 + \tau n \bar{y}}{\lambda_0 + \tau n} \\
\lambda_n &= \lambda_0 + \tau n
\end{split}
\end{equation}

with the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) $\bar{y}$ and the inverse variance or precision ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prec}) $\tau = 1/\sigma^2$.


\vspace{1em}
\textbf{Proof:} According to the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), the model evidence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml}) for this model is:

\begin{equation} \label{eq:ugkv-lme-UGkv-ME-s1}
p(y|m) = \int p(y|\mu) \, p(\mu) \, \mathrm{d}\mu \; .
\end{equation}

According to the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), the integrand is equivalent to the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}):

\begin{equation} \label{eq:ugkv-lme-UGkv-ME-s2}
p(y|m) = \int p(y,\mu) \, \mathrm{d}\mu \; .
\end{equation}

Equation \eqref{eq:ugkv-lme-ug} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:ugkv-lme-UG-LF-class}
\begin{split}
p(y|\mu,\sigma^2) &= \prod_{i=1}^{n} \mathcal{N}(y_i; \mu, \sigma^2) \\
&= \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \cdot \exp\left[ -\frac{1}{2} \left( \frac{y_i-\mu}{\sigma} \right)^2 \right] \\
&= \left( \sqrt{\frac{1}{2 \pi \sigma^2}} \right)^n \cdot \exp\left[ -\frac{1}{2 \sigma^2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right]
\end{split}
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:ugkv-lme-UG-LF-Bayes}
\begin{split}
p(y|\mu,\tau) &= \prod_{i=1}^{n} \mathcal{N}(y_i; \mu, \tau^{-1}) \\
&= \prod_{i=1}^{n} \sqrt{\frac{\tau}{2 \pi}} \cdot \exp\left[ -\frac{\tau}{2} \left( y_i-\mu \right)^2 \right] \\
&= \left( \sqrt{\frac{\tau}{2 \pi}} \right)^n \cdot \exp\left[ -\frac{\tau}{2} \sum_{i=1}^{n} \left( y_i-\mu \right)^2 \right]
\end{split}
\end{equation}

using the inverse variance or precision $\tau = 1/\sigma^2$.

\vspace{1em}
When deriving the posterior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-post}) $p(\mu \vert y)$, the joint likelihood $p(y,\mu)$ is obtained as

\begin{equation} \label{eq:ugkv-lme-UGkv-LME-s1}
p(y,\mu) = \left( \frac{\tau}{2 \pi} \right)^\frac{n}{2} \cdot \sqrt{\frac{\lambda_0}{2 \pi}} \cdot \exp \left[ -\frac{\lambda_n}{2} (\mu - \mu_n)^2 -\frac{1}{2} \left( \tau y^\mathrm{T} y + \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right) \right] \; .
\end{equation}

Using the probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}), we can rewrite this as

\begin{equation} \label{eq:ugkv-lme-UGkv-LME-s2}
p(y,\mu) =  \left( \frac{\tau}{2 \pi} \right)^\frac{n}{2} \cdot \sqrt{\frac{\lambda_0}{2 \pi}} \cdot \sqrt{\frac{2 \pi}{\lambda_n}} \cdot \mathcal{N}(\mu; \lambda_n^{-1}) \cdot \exp \left[ -\frac{1}{2} \left( \tau y^\mathrm{T} y + \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right) \right] \; .
\end{equation}

Now, $\mu$ can be integrated out using the properties of the probability density function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}):

\begin{equation} \label{eq:ugkv-lme-UGkv-LME-s3}
p(y|m) = \int p(y,\mu) \, \mathrm{d}\mu = \left( \frac{\tau}{2 \pi} \right)^\frac{n}{2} \cdot \sqrt{\frac{\lambda_0}{\lambda_n}} \cdot \exp \left[ -\frac{1}{2} \left( \tau y^\mathrm{T} y + \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right) \right] \; .
\end{equation}

Thus, the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) of this model is given by

\begin{equation} \label{eq:ugkv-lme-UGkv-LME-s4}
\log p(y|m) = \frac{n}{2} \log\left( \frac{\tau}{2 \pi} \right) + \frac{1}{2} \log\left( \frac{\lambda_0}{\lambda_n} \right) - \frac{1}{2} \left( \tau y^T y + \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Accuracy and complexity}]{Accuracy and complexity} \label{sec:ugkv-anc}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ugkv-anc-ugkv}
y = \left\lbrace y_1, \ldots, y_n \right\rbrace, \quad y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv}) with unknown mean $\mu$ and known variance $\sigma^2$. Moreover, assume a statistical model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fpm}) imposing a normal distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-prior}) as the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) on the model parameter $\mu$:

\begin{equation} \label{eq:ugkv-anc-m}
m: \; y_i \sim \mathcal{N}(\mu, \sigma^2), \; \mu \sim \mathcal{N}(\mu_0, \lambda_0^{-1}) \; .
\end{equation}

Then, accuracy and complexity ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme-anc}) of this model are

\begin{equation} \label{eq:ugkv-anc-UGkv-anc}
\begin{split}
\mathrm{Acc}(m) &= \frac{n}{2} \log\left( \frac{\tau}{2 \pi} \right) - \frac{1}{2} \left[ \tau y^\mathrm{T} y - 2 \, \tau n \bar{y} \mu_n + \tau n \mu_n^2 + \frac{\tau n}{\lambda_n} \right] \\
\mathrm{Com}(m) &= \frac{1}{2} \left[ \frac{\lambda_0}{\lambda_n} + \lambda_0 (\mu_0 - \mu_n)^2 - 1 + \log\left( \frac{\lambda_0}{\lambda_n} \right) \right]
\end{split}
\end{equation}

where $\mu_n$ and $\lambda_n$ are the posterior hyperparameters for the univariate Gaussian with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-post}), $\tau = 1/\sigma^2$ is the inverse variance or precision ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prec}) and $\bar{y}$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}).


\vspace{1em}
\textbf{Proof:} Model accuracy and complexity are defined as ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme-anc})

\begin{equation} \label{eq:ugkv-anc-lme-anc}
\begin{split}
\mathrm{LME}(m) &= \mathrm{Acc}(m) - \mathrm{Com}(m) \\
\mathrm{Acc}(m) &= \left\langle \log p(y|\mu,m) \right\rangle_{p(\mu|y,m)} \\
\mathrm{Com}(m) &= \mathrm{KL} \left[ p(\mu|y,m) \, || \, p(\mu|m) \right] \; .
\end{split}
\end{equation}

\vspace{1em}
The accuracy term is the expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) $\log p(y|\mu)$ with respect to the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) $p(\mu|y)$. With the log-likelihood function for the univariate Gaussian with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-mle}) and the posterior distribution for the univariate Gaussian with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-post}), the model accuracy of $m$ evaluates to:

\begin{equation} \label{eq:ugkv-anc-UGkv-Acc}
\begin{split}
\mathrm{Acc}(m) &= \left\langle \log p(y|\mu) \right\rangle_{p(\mu|y)} \\
&= \left\langle \frac{n}{2} \log\left( \frac{\tau}{2 \pi} \right) - \frac{\tau}{2} \left( y^\mathrm{T} y - 2 n \bar{y} \mu + n \mu^2 \right) \right\rangle_{\mathcal{N}(\mu_n, \lambda_n^{-1})} \\
&= \frac{n}{2} \log\left( \frac{\tau}{2 \pi} \right) - \frac{1}{2} \left[ \tau y^\mathrm{T} y - 2 \tau n \bar{y} \mu_n + \tau n \mu_n^2 + \frac{\tau n}{\lambda_n} \right] \; .
\end{split}
\end{equation}

\vspace{1em}
The complexity penalty is the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) $p(\mu|y)$ from the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) $p(\mu)$. With the prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-prior}) given by \eqref{eq:ugkv-anc-m}, the posterior distribution for the univariate Gaussian with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-post}) and the Kullback-Leibler divergence of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-kl}), the model complexity of $m$ evaluates to:

\begin{equation} \label{eq:ugkv-anc-UGkv-Com}
\begin{split}
\mathrm{Com}(m) &= \mathrm{KL} \left[ p(\mu|y) \, || \, p(\mu) \right] \\
&= \mathrm{KL} \left[ \mathcal{N}(\mu_n, \lambda_n^{-1}) \, || \, \mathcal{N}(\mu_0, \lambda_0^{-1}) \right] \\
&= \frac{1}{2} \left[ \frac{\lambda_0}{\lambda_n} + \lambda_0 (\mu_0 - \mu_n)^2 - 1 + \log\left( \frac{\lambda_0}{\lambda_n} \right) \right] \; .
\end{split}
\end{equation}

A control calculation confirms that

\begin{equation} \label{eq:ugkv-anc-UGkv-anc-lme}
\mathrm{Acc}(m) - \mathrm{Com}(m) = \mathrm{LME}(m)
\end{equation}

where $\mathrm{LME}(m)$ is the log model evidence for the univariate Gaussian with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-lme}).
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Log Bayes factor}]{Log Bayes factor} \label{sec:ugkv-lbf}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ugkv-lbf-ugkv}
y = \left\lbrace y_1, \ldots, y_n \right\rbrace, \quad y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv}) with unknown mean $\mu$ and known variance $\sigma^2$. Moreover, assume two statistical models ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fpm}), one assuming that $\mu$ is zero (null model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})), the other imposing a normal distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-prior}) as the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) on the model parameter $\mu$ (alternative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1})):

\begin{equation} \label{eq:ugkv-lbf-UGkv-m01}
\begin{split}
m_0&: \; y_i \sim \mathcal{N}(\mu, \sigma^2), \; \mu = 0 \\
m_1&: \; y_i \sim \mathcal{N}(\mu, \sigma^2), \; \mu \sim \mathcal{N}(\mu_0, \lambda_0^{-1}) \; .
\end{split}
\end{equation}

Then, the log Bayes factor ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf}) in favor of $m_1$ against $m_0$ is

\begin{equation} \label{eq:ugkv-lbf-UGkv-LBF}
\mathrm{LBF}_{10} = \frac{1}{2} \log\left( \frac{\lambda_0}{\lambda_n} \right) - \frac{1}{2} \left( \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right)
\end{equation}

where $\mu_n$ and $\lambda_n$ are the posterior hyperparameters for the univariate Gaussian with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-post}) which are functions of the inverse variance or precision ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prec}) $\tau = 1/\sigma^2$ and the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) $\bar{y}$.


\vspace{1em}
\textbf{Proof:} The log Bayes factor is equal to the difference of two log model evidences ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf-lme}):

\begin{equation} \label{eq:ugkv-lbf-LBF-LME}
\mathrm{LBF}_{12} = \mathrm{LME}(m_1) - \mathrm{LME}(m_2) \; .
\end{equation}

The LME of the alternative $m_1$ is equal to the log model evidence for the univariate Gaussian with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-lme}):

\begin{equation} \label{eq:ugkv-lbf-UGkv-LME-m1}
\mathrm{LME}(m_1) = \log p(y|m_1) = \frac{n}{2} \log\left( \frac{\tau}{2 \pi} \right) + \frac{1}{2} \log\left( \frac{\lambda_0}{\lambda_n} \right) - \frac{1}{2} \left( \tau y^\mathrm{T} y + \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right) \; .
\end{equation}

Because the null model $m_0$ has no free parameter, its log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) (logarithmized marginal likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml})) is equal to the log-likelihood function for the univariate Gaussian with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-mle}) at the value $\mu = 0$:

\begin{equation} \label{eq:ugkv-lbf-UGkv-LME-m0}
\mathrm{LME}(m_0) = \log p(y|\mu=0) = \frac{n}{2} \log\left( \frac{\tau}{2 \pi} \right) - \frac{1}{2} \left( \tau y^\mathrm{T} y \right) \; .
\end{equation}

Subtracting the two LMEs from each other, the LBF emerges as

\begin{equation} \label{eq:ugkv-lbf-UGkv-LBF-m10}
\mathrm{LBF}_{10} = \mathrm{LME}(m_1) - \mathrm{LME}(m_0) = \frac{1}{2} \log\left( \frac{\lambda_0}{\lambda_n} \right) - \frac{1}{2} \left( \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right)
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-post})

\begin{equation} \label{eq:ugkv-lbf-UGkv-post-par}
\begin{split}
\mu_n &= \frac{\lambda_0 \mu_0 + \tau n \bar{y}}{\lambda_0 + \tau n} \\
\lambda_n &= \lambda_0 + \tau n
\end{split}
\end{equation}

with the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) $\bar{y}$ and the inverse variance or precision ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prec}) $\tau = 1/\sigma^2$.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Expectation of log Bayes factor}]{Expectation of log Bayes factor} \label{sec:ugkv-lbfmean}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ugkv-lbfmean-ugkv}
y = \left\lbrace y_1, \ldots, y_n \right\rbrace, \quad y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv}) with unknown mean $\mu$ and known variance $\sigma^2$. Moreover, assume two statistical models ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fpm}), one assuming that $\mu$ is zero (null model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})), the other imposing a normal distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-prior}) as the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) on the model parameter $\mu$ (alternative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1})):

\begin{equation} \label{eq:ugkv-lbfmean-UGkv-m01}
\begin{split}
m_0&: \; y_i \sim \mathcal{N}(\mu, \sigma^2), \; \mu = 0 \\
m_1&: \; y_i \sim \mathcal{N}(\mu, \sigma^2), \; \mu \sim \mathcal{N}(\mu_0, \lambda_0^{-1}) \; .
\end{split}
\end{equation}

Then, under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that $m_0$ generated the data, the expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the log Bayes factor ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf}) in favor of $m_1$ with $\mu_0 = 0$ against $m_0$ is

\begin{equation} \label{eq:ugkv-lbfmean-UGkv-LBF}
\left\langle \mathrm{LBF}_{10} \right\rangle = \frac{1}{2} \log\left( \frac{\lambda_0}{\lambda_n} \right) + \frac{1}{2} \left( \frac{\lambda_n - \lambda_0}{\lambda_n} \right)
\end{equation}

where $\lambda_n$ is the posterior precision for the univariate Gaussian with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-post}).


\vspace{1em}
\textbf{Proof:} The log Bayes factor for the univariate Gaussian with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-lbf}) is

\begin{equation} \label{eq:ugkv-lbfmean-UGkv-LBF-m10-s1}
\mathrm{LBF}_{10} = \frac{1}{2} \log\left( \frac{\lambda_0}{\lambda_n} \right) - \frac{1}{2} \left( \lambda_0 \mu_0^2 - \lambda_n \mu_n^2 \right)
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-post})

\begin{equation} \label{eq:ugkv-lbfmean-UGkv-post-par}
\begin{split}
\mu_n &= \frac{\lambda_0 \mu_0 + \tau n \bar{y}}{\lambda_0 + \tau n} \\
\lambda_n &= \lambda_0 + \tau n
\end{split}
\end{equation}

with the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) $\bar{y}$ and the inverse variance or precision ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prec}) $\tau = 1/\sigma^2$. Plugging $\mu_n$ from \eqref{eq:ugkv-lbfmean-UGkv-post-par} into \eqref{eq:ugkv-lbfmean-UGkv-LBF-m10-s1}, we obtain:

\begin{equation} \label{eq:ugkv-lbfmean-UGkv-LBF-m10-s2}
\begin{split}
\mathrm{LBF}_{10} &= \frac{1}{2} \log\left( \frac{\lambda_0}{\lambda_n} \right) - \frac{1}{2} \left( \lambda_0 \mu_0^2 - \lambda_n \, \frac{(\lambda_0 \mu_0 + \tau n \bar{y})^2}{\lambda_n^2} \right) \\
&= \frac{1}{2} \log\left( \frac{\lambda_0}{\lambda_n} \right) - \frac{1}{2} \left( \lambda_0 \mu_0^2 - \frac{1}{\lambda_n} (\lambda_0^2 \mu_0^2 - 2 \tau n \lambda_0 \mu_0 \bar{y} + \tau^2 (n \bar{y})^2) \right)
\end{split}
\end{equation}

Because $m_1$ uses a zero-mean prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) with prior mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu_0 = 0$ per construction, the log Bayes factor simplifies to:

\begin{equation} \label{eq:ugkv-lbfmean-UGkv-LBF-m10-s3}
\mathrm{LBF}_{10} = \frac{1}{2} \log\left( \frac{\lambda_0}{\lambda_n} \right) + \frac{1}{2} \left( \frac{\tau^2 (n \bar{y})^2}{\lambda_n} \right) \; .
\end{equation}

From \eqref{eq:ugkv-lbfmean-ugkv}, we know that the data are distributed as $y_i \sim \mathcal{N}(\mu, \sigma^2)$, such that we can derive the expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $(n \bar{y})^2$ as follows:

\begin{equation} \label{eq:ugkv-lbfmean-UGkv-E(ny2)}
\begin{split}
\left\langle (n \bar{y})^2 \right\rangle = \left\langle \sum_{i=1}^n \sum_{j=1}^n y_i y_j \right\rangle &= \left\langle n y_i^2 + (n^2-n) [y_i y_j]_{i \neq j} \right\rangle \\
&= n (\mu^2 + \sigma^2) + (n^2 - n) \mu^2 \\
&= n^2 \mu^2 + n \sigma^2 \; .
\end{split}
\end{equation}

Applying this expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) to \eqref{eq:ugkv-lbfmean-UGkv-LBF-m10-s3}, the expected LBF emerges as:

\begin{equation} \label{eq:ugkv-lbfmean-UGkv-LBF-m10-s4}
\begin{split}
\left\langle \mathrm{LBF}_{10} \right\rangle &= \frac{1}{2} \log\left( \frac{\lambda_0}{\lambda_n} \right) + \frac{1}{2} \left( \frac{\tau^2 (n^2 \mu^2 + n \sigma^2)}{\lambda_n} \right) \\
&= \frac{1}{2} \log\left( \frac{\lambda_0}{\lambda_n} \right) + \frac{1}{2} \left( \frac{(\tau n \mu)^2 + \tau n}{\lambda_n} \right)
\end{split}
\end{equation}

Under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that $m_0$ generated the data, the unknown mean is $\mu = 0$, such that the log Bayes factor further simplifies to:

\begin{equation} \label{eq:ugkv-lbfmean-UGkv-LBF-m10-s5}
\left\langle \mathrm{LBF}_{10} \right\rangle = \frac{1}{2} \log\left( \frac{\lambda_0}{\lambda_n} \right) + \frac{1}{2} \left( \frac{\tau n}{\lambda_n} \right) \; .
\end{equation}

Finally, plugging $\lambda_n$ from \eqref{eq:ugkv-lbfmean-UGkv-post-par} into \eqref{eq:ugkv-lbfmean-UGkv-LBF-m10-s5}, we obtain:

\begin{equation} \label{eq:ugkv-lbfmean-UGkv-LBF-m10-s6}
\left\langle \mathrm{LBF}_{10} \right\rangle = \frac{1}{2} \log\left( \frac{\lambda_0}{\lambda_n} \right) + \frac{1}{2} \left( \frac{\lambda_n - \lambda_0}{\lambda_n} \right) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Cross-validated log model evidence}]{Cross-validated log model evidence} \label{sec:ugkv-cvlme}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ugkv-cvlme-ugkv}
y = \left\lbrace y_1, \ldots, y_n \right\rbrace, \quad y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv}) with unknown mean $\mu$ and known variance $\sigma^2$. Moreover, assume two statistical models ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fpm}), one assuming that $\mu$ is zero (null model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})), the other imposing a normal distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-prior}) as the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) on the model parameter $\mu$ (alternative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1})):

\begin{equation}\label{eq:ugkv-cvlme-UGkv-m01}
\begin{split}
m_0&: \; y_i \sim \mathcal{N}(\mu, \sigma^2), \; \mu = 0 \\
m_1&: \; y_i \sim \mathcal{N}(\mu, \sigma^2), \; \mu \sim \mathcal{N}(\mu_0, \lambda_0^{-1}) \; .
\end{split}
\end{equation}

Then, the cross-validated log model evidences ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:cvlme}) of $m_0$ and $m_1$ are

\begin{equation} \label{eq:ugkv-cvlme-UGkv-cvLME-m01}
\begin{split}
\mathrm{cvLME}(m_0) &= \frac{n}{2} \log\left( \frac{\tau}{2 \pi} \right) - \frac{1}{2} \left( \tau y^\mathrm{T} y \right) \\
\mathrm{cvLME}(m_1) &= \frac{n}{2} \log \left( \frac{\tau}{2 \pi} \right) + \frac{S}{2} \log \left( \frac{S-1}{S} \right) - \frac{\tau}{2} \left[ y^\mathrm{T} y + \sum_{i=1}^S \left( \frac{\left(n_1 \bar{y}_1^{(i)}\right)^2}{n_1} - \frac{(n \bar{y})^2}{n} \right) \right]
\end{split}
\end{equation}

where $\bar{y}$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}), $\tau = 1/\sigma^2$ is the inverse variance or precision ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prec}), $y_1^{(i)}$ are the training data in the $i$-th cross-validation fold and $S$ is the number of data subsets ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:cvlme}).


\vspace{1em}
\textbf{Proof:} For evaluation of the cross-validated log model evidences ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:cvlme}) (cvLME), we assume that $n$ data points are divided into $S \mid n$ data subsets without remainder. Then, the number of training data points $n_1$ and test data points $n_2$ are given by

\begin{equation} \label{eq:ugkv-cvlme-CV-n12}
\begin{split}
n &= n_1 + n_2 \\
n_1 &= \frac{S-1}{S} n \\
n_2 &= \frac{1}{S} n \; ,
\end{split}
\end{equation}

such that training data $y_1$ and test data $y_2$ in the $i$-th cross-validation fold are

\begin{equation} \label{eq:ugkv-cvlme-CV-y12}
\begin{split}
y &= \left\lbrace y_1, \ldots, y_n \right\rbrace \\
y_1^{(i)} &= \left\lbrace x \in y \mid x \notin y_2^{(i)} \right\rbrace = y \backslash y_2^{(i)} \\
y_2^{(i)} &= \left\lbrace y_{(i-1) \cdot n_2 + 1}, \ldots, y_{i \cdot n_2} \right\rbrace \; .
\end{split}
\end{equation}

\vspace{1em}
First, we consider the null model $m_0$ assuming $\mu = 0$. Because this model has no free parameter, nothing is estimated from the training data and the assumed parameter value is applied to the test data. Consequently, the out-of-sample log model evidence (oosLME) is equal to the log-likelihood function ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-mle}) of the test data at $\mu = 0$:

\begin{equation} \label{eq:ugkv-cvlme-UGkv-m0-oosLME}
\mathrm{oosLME}_i(m_0) = \log p\left( \left. y_2^{(i)} \right| \mu=0 \right) = \frac{n_2}{2} \log \left( \frac{\tau}{2 \pi} \right) - \frac{1}{2} \left[ \tau {y_2^{(i)}}^\mathrm{T} y_2^{(i)} \right] \; .
\end{equation}

By definition, the cross-validated log model evidence is the sum of out-of-sample log model evidences ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:cvlme}) over cross-validation folds, such that the cvLME of $m_0$ is:

\begin{equation} \label{eq:ugkv-cvlme-UGkv-m0-cvLME}
\begin{split}
\mathrm{cvLME}(m_0) &= \sum_{i=1}^S \mathrm{oosLME}_i(m_0) \\
&= \sum_{i=1}^S \left( \frac{n_2}{2} \log \left( \frac{\tau}{2 \pi} \right) - \frac{1}{2} \left[ \tau {y_2^{(i)}}^\mathrm{T} y_2^{(i)} \right] \right) \\
&= \frac{n}{2} \log \left( \frac{\tau}{2 \pi} \right) - \frac{1}{2} \left[ \tau y^\mathrm{T} y \right] \; .
\end{split}
\end{equation}

\vspace{1em}
Next, we have a look at the alternative $m_1$ assuming $\mu \neq 0$. First, the training data $y_1^{(i)}$ are analyzed using a non-informative prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-inf}) and applying the posterior distribution for the univariate Gaussian with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-post}):

\begin{equation} \label{eq:ugkv-cvlme-UGkv-m1-y1}
\begin{split}
\mu_0^{(1)} &= 0 \\
\lambda_0^{(1)} &= 0 \\
\mu_n^{(1)} &= \frac{\tau n_1 \bar{y}_1^{(i)} + \lambda_0^{(1)} \mu_0^{(1)}}{\tau n_1 + \lambda_0^{(1)}} = \bar{y}_1^{(i)} \\
\lambda_n^{(1)} &= \tau n_1 + \lambda_0^{(1)} = \tau n_1 \; .
\end{split}
\end{equation}

This results in a posterior characterized by $\mu_n^{(1)}$ and $\lambda_n^{(1)}$. Then, the test data $y_2^{(i)}$ are analyzed using this posterior as an informative prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-inf}), again applying the posterior distribution for the univariate Gaussian with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-post}):

\begin{equation} \label{eq:ugkv-cvlme-UGkv-m1-y2}
\begin{split}
\mu_0^{(2)} &= \mu_n^{(1)} = \bar{y}_1^{(i)} \\
\lambda_0^{(2)} &= \lambda_n^{(1)} = \tau n_1 \\
\mu_n^{(2)} &= \frac{\tau n_2 \bar{y}_2^{(i)} + \lambda_0^{(2)} \mu_0^{(2)}}{\tau n_2 + \lambda_0^{(2)}} = \bar{y} \\
\lambda_n^{(2)} &= \tau n_2 + \lambda_0^{(2)} = \tau n \; .
\end{split}
\end{equation}

In the test data, we now have a prior characterized by $\mu_0^{(2)}$/$\lambda_0^{(2)}$ and a posterior characterized $\mu_n^{(2)}$/$\lambda_n^{(2)}$. Applying the log model evidence for the univariate Gaussian with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-lme}), the out-of-sample log model evidence (oosLME) therefore follows as

\begin{equation} \label{eq:ugkv-cvlme-UGkv-m1-oosLME}
\begin{split}
\mathrm{oosLME}_i(m_1) &= \frac{n_2}{2} \log \left( \frac{\tau}{2 \pi} \right) + \frac{1}{2} \log \left( \frac{\lambda_0^{(2)}}{\lambda_n^{(2)}} \right) - \frac{1}{2} \left[ \tau {y_2^{(i)}}^\mathrm{T} y_2^{(i)} + \lambda_0^{(2)} {\mu_0^{(2)}}^2 - \lambda_n^{(2)} {\mu_n^{(2)}}^2 \right] \\
&= \frac{n_2}{2} \log \left( \frac{\tau}{2 \pi} \right) + \frac{1}{2} \log \left( \frac{n_1}{n} \right) - \frac{1}{2} \left[ \tau {y_2^{(i)}}^\mathrm{T} y_2^{(i)} + \frac{\tau}{n_1}\left(n_1 \bar{y}_1^{(i)}\right)^2 - \frac{\tau}{n}(n \bar{y})^2 \right] \; .
\end{split}
\end{equation}

Again, because the cross-validated log model evidence is the sum of out-of-sample log model evidences ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:cvlme}) over cross-validation folds, the cvLME of $m_1$ becomes:

\begin{equation} \label{eq:ugkv-cvlme-UGkv-m1-cvLME}
\begin{split}
\mathrm{cvLME}(m_1) &= \sum_{i=1}^S \mathrm{oosLME}_i(m_1) \\
&= \sum_{i=1}^S \left( \frac{n_2}{2} \log \left( \frac{\tau}{2 \pi} \right) + \frac{1}{2} \log \left( \frac{n_1}{n} \right) - \frac{1}{2} \left[ \tau {y_2^{(i)}}^\mathrm{T} y_2^{(i)} + \frac{\tau}{n_1}\left(n_1 \bar{y}_1^{(i)}\right)^2 - \frac{\tau}{n}(n \bar{y})^2 \right] \right) \\
&= \frac{S \cdot n_2}{2} \log \left( \frac{\tau}{2 \pi} \right) + \frac{S}{2} \log \left( \frac{n_1}{n} \right) - \frac{\tau}{2} \sum_{i=1}^S \left[ {y_2^{(i)}}^\mathrm{T} y_2^{(i)} + \frac{\left(n_1 \bar{y}_1^{(i)}\right)^2}{n_1} - \frac{(n \bar{y})^2}{n} \right] \\
&= \frac{n}{2} \log \left( \frac{\tau}{2 \pi} \right) + \frac{S}{2} \log \left( \frac{S-1}{S} \right) - \frac{\tau}{2} \left[ y^\mathrm{T} y + \sum_{i=1}^S \left( \frac{\left(n_1 \bar{y}_1^{(i)}\right)^2}{n_1} - \frac{(n \bar{y})^2}{n} \right) \right] \; .
\end{split}
\end{equation}

Together, \eqref{eq:ugkv-cvlme-UGkv-m0-cvLME} and \eqref{eq:ugkv-cvlme-UGkv-m1-cvLME} conform to the results given in \eqref{eq:ugkv-cvlme-UGkv-cvLME-m01}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Cross-validated log Bayes factor}]{Cross-validated log Bayes factor} \label{sec:ugkv-cvlbf}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ugkv-cvlbf-ugkv}
y = \left\lbrace y_1, \ldots, y_n \right\rbrace, \quad y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv}) with unknown mean $\mu$ and known variance $\sigma^2$. Moreover, assume two statistical models ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fpm}), one assuming that $\mu$ is zero (null model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})), the other imposing a normal distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-prior}) as the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) on the model parameter $\mu$ (alternative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1})):

\begin{equation} \label{eq:ugkv-cvlbf-UGkv-m01}
\begin{split}
m_0&: \; y_i \sim \mathcal{N}(\mu, \sigma^2), \; \mu = 0 \\
m_1&: \; y_i \sim \mathcal{N}(\mu, \sigma^2), \; \mu \sim \mathcal{N}(\mu_0, \lambda_0^{-1}) \; .
\end{split}
\end{equation}

Then, the cross-validated ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:cvlme}) log Bayes factor ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf}) in favor of $m_1$ against $m_0$ is

\begin{equation} \label{eq:ugkv-cvlbf-UGkv-cvLBF}
\mathrm{cvLBF}_{10} = \frac{S}{2} \log \left( \frac{S-1}{S} \right) - \frac{\tau}{2} \sum_{i=1}^S \left( \frac{\left(n_1 \bar{y}_1^{(i)}\right)^2}{n_1} - \frac{(n \bar{y})^2}{n} \right)
\end{equation}

where $\bar{y}$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}), $\tau = 1/\sigma^2$ is the inverse variance or precision ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prec}), $y_1^{(i)}$ are the training data in the $i$-th cross-validation fold and $S$ is the number of data subsets ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:cvlme}).


\vspace{1em}
\textbf{Proof:} The relationship between log Bayes factor and log model evidences ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf-lme}) also holds for cross-validated log bayes factor ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf}) (cvLBF) and cross-validated log model evidences ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:cvlme}) (cvLME):

\begin{equation} \label{eq:ugkv-cvlbf-cvLBF-cvLME}
\mathrm{cvLBF}_{12} = \mathrm{cvLME}(m_1) - \mathrm{cvLME}(m_2) \; .
\end{equation}

The cross-validated log model evidences ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:cvlme}) of $m_0$ and $m_1$ are given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-cvlme})

\begin{equation} \label{eq:ugkv-cvlbf-UGkv-cvLME-m01}
\begin{split}
\mathrm{cvLME}(m_0) &= \frac{n}{2} \log\left( \frac{\tau}{2 \pi} \right) - \frac{1}{2} \left( \tau y^\mathrm{T} y \right) \\
\mathrm{cvLME}(m_1) &= \frac{n}{2} \log \left( \frac{\tau}{2 \pi} \right) + \frac{S}{2} \log \left( \frac{S-1}{S} \right) - \frac{\tau}{2} \left[ y^\mathrm{T} y + \sum_{i=1}^S \left( \frac{\left(n_1 \bar{y}_1^{(i)}\right)^2}{n_1} - \frac{(n \bar{y})^2}{n} \right) \right] \; .
\end{split}
\end{equation}

Subtracting the two cvLMEs from each other, the cvLBF emerges as

\begin{equation} \label{eq:ugkv-cvlbf-UGkv-cvLBF-qed}
\begin{split}
\mathrm{cvLBF}_{10} &= \mathrm{cvLME}(m_1) - \mathrm{LME}(m_0) \\
&= \left( \frac{n}{2} \log \left( \frac{\tau}{2 \pi} \right) + \frac{S}{2} \log \left( \frac{S-1}{S} \right) - \frac{\tau}{2} \left[ y^\mathrm{T} y + \sum_{i=1}^S \left( \frac{\left(n_1 \bar{y}_1^{(i)}\right)^2}{n_1} - \frac{(n \bar{y})^2}{n} \right) \right] \right) \\
&- \left( \frac{n}{2} \log \left( \frac{\tau}{2 \pi} \right) - \frac{1}{2} \left( \tau y^\mathrm{T} y \right) \right) \\
&= \frac{S}{2} \log \left( \frac{S-1}{S} \right) - \frac{\tau}{2} \sum_{i=1}^S \left( \frac{\left(n_1 \bar{y}_1^{(i)}\right)^2}{n_1} - \frac{(n \bar{y})^2}{n} \right) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Expectation of cross-validated log Bayes factor}]{Expectation of cross-validated log Bayes factor} \label{sec:ugkv-cvlbfmean}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:ugkv-cvlbfmean-ugkv}
y = \left\lbrace y_1, \ldots, y_n \right\rbrace, \quad y_i \sim \mathcal{N}(\mu, \sigma^2), \quad i = 1, \ldots, n
\end{equation}

be a univariate Gaussian data set ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv}) with unknown mean $\mu$ and known variance $\sigma^2$. Moreover, assume two statistical models ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fpm}), one assuming that $\mu$ is zero (null model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})), the other imposing a normal distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-prior}) as the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) on the model parameter $\mu$ (alternative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1})):

\begin{equation} \label{eq:ugkv-cvlbfmean-UGkv-m01}
\begin{split}
m_0&: \; y_i \sim \mathcal{N}(\mu, \sigma^2), \; \mu = 0 \\
m_1&: \; y_i \sim \mathcal{N}(\mu, \sigma^2), \; \mu \sim \mathcal{N}(\mu_0, \lambda_0^{-1}) \; .
\end{split}
\end{equation}

Then, the expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the cross-validated ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:cvlme}) log Bayes factor ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf}) (cvLBF) in favor of $m_1$ against $m_0$ is

\begin{equation} \label{eq:ugkv-cvlbfmean-UGkv-cvLBF}
\left\langle \mathrm{cvLBF}_{10} \right\rangle = \frac{S}{2} \log \left( \frac{S-1}{S} \right) + \frac{1}{2} \left[ \tau n \mu^2 \right]
\end{equation}

where $\tau = 1/\sigma^2$ is the inverse variance or precision ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prec}) and $S$ is the number of data subsets ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:cvlme}).


\vspace{1em}
\textbf{Proof:} The cross-validated log Bayes factor for the univariate Gaussian with known variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-cvlbf}) is

\begin{equation} \label{eq:ugkv-cvlbfmean-UGkv-cvLBF-m10-s1}
\mathrm{cvLBF}_{10} = \frac{S}{2} \log \left( \frac{S-1}{S} \right) - \frac{\tau}{2} \sum_{i=1}^S \left( \frac{\left(n_1 \bar{y}_1^{(i)}\right)^2}{n_1} - \frac{(n \bar{y})^2}{n} \right)
\end{equation}

From \eqref{eq:ugkv-cvlbfmean-ugkv}, we know that the data are distributed as $y_i \sim \mathcal{N}(\mu, \sigma^2)$, such that we can derive the expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $(n \bar{y})^2$ and $\left(n_1 \bar{y}_1^{(i)}\right)^2$ as follows:

\begin{equation} \label{eq:ugkv-cvlbfmean-UGkv-E(ny2)}
\begin{split}
\left\langle (n \bar{y})^2 \right\rangle = \left\langle \sum_{i=1}^n \sum_{j=1}^n y_i y_j \right\rangle &= \left\langle n y_i^2 + (n^2-n) [y_i y_j]_{i \neq j} \right\rangle \\
&= n (\mu^2 + \sigma^2) + (n^2 - n) \mu^2 \\
&= n^2 \mu^2 + n \sigma^2 \; .
\end{split}
\end{equation}

Applying this expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) to \eqref{eq:ugkv-cvlbfmean-UGkv-cvLBF-m10-s1}, the expected cvLBF emerges as:

\begin{equation} \label{eq:ugkv-cvlbfmean-UGkv-cvLBF-m10-s2}
\begin{split}
\left\langle \mathrm{cvLBF}_{10} \right\rangle &= \left\langle \frac{S}{2} \log \left( \frac{S-1}{S} \right) - \frac{\tau}{2} \sum_{i=1}^S \left( \frac{\left(n_1 \bar{y}_1^{(i)}\right)^2}{n_1} - \frac{(n \bar{y})^2}{n} \right) \right\rangle \\
&= \frac{S}{2} \log \left( \frac{S-1}{S} \right) - \frac{\tau}{2} \sum_{i=1}^S \left( \frac{\left\langle \left(n_1 \bar{y}_1^{(i)}\right)^2 \right\rangle}{n_1} - \frac{\left\langle (n \bar{y})^2 \right\rangle}{n} \right) \\
&\overset{\eqref{eq:ugkv-cvlbfmean-UGkv-E(ny2)}}{=} \frac{S}{2} \log \left( \frac{S-1}{S} \right) - \frac{\tau}{2} \sum_{i=1}^S \left( \frac{n_1^2 \mu^2 + n_1 \sigma^2}{n_1} - \frac{n^2 \mu^2 + n \sigma^2}{n} \right) \\
&= \frac{S}{2} \log \left( \frac{S-1}{S} \right) - \frac{\tau}{2} \sum_{i=1}^S \left( [n_1 \mu^2 + \sigma^2] - [n \mu^2 + \sigma^2] \right) \\
&= \frac{S}{2} \log \left( \frac{S-1}{S} \right) - \frac{\tau}{2} \sum_{i=1}^S (n_1 - n) \mu^2
\end{split}
\end{equation}

Because it holds that ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ugkv-cvlme}) $n_1 + n_2 = n$ and $n_2 = n/S$, we finally have:

\begin{equation} \label{eq:ugkv-cvlbfmean-UGkv-cvLBF-m10-s3}
\begin{split}
\left\langle \mathrm{cvLBF}_{10} \right\rangle &= \frac{S}{2} \log \left( \frac{S-1}{S} \right) - \frac{\tau}{2} \sum_{i=1}^S (-n_2) \mu^2 \\
&= \frac{S}{2} \log \left( \frac{S-1}{S} \right) + \frac{1}{2} \left[ \tau n \mu^2 \right] \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{Analysis of variance}

\subsubsection[\textit{One-way ANOVA}]{One-way ANOVA} \label{sec:anova1}
\setcounter{equation}{0}

\textbf{Definition:} Consider measurements $y_{ij} \in \mathbb{R}$ from distinct objects $j = 1, \ldots, n_i$ in separate groups $i = 1, \ldots, k$.

Then, in one-way analysis of variance (ANOVA), these measurements are assumed to come from normal distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm})

\begin{equation} \label{eq:anova1-anova1}
y_{ij} \sim \mathcal{N}(\mu_i, \sigma^2) \quad \text{for all} \quad i = 1, \ldots, k \quad \text{and} \quad j = 1, \dots, n_i
\end{equation}

where

\begin{itemize}

\item $\mu_i$ is the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) in group $i$ and

\item $\sigma^2$ is the common variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) across groups.

\end{itemize}

Alternatively, the model may be written as

\begin{equation} \label{eq:anova1-anova1-alt}
\begin{split}
y_{ij} &= \mu_i + \varepsilon_{ij} \\
\varepsilon_{ij} &\overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)
\end{split}
\end{equation}

where $\varepsilon_{ij}$ is the error term ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) belonging to observation $j$ in category $i$ and $\varepsilon_{ij}$ are the independent and identically distributed.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Bortz, JÃ¼rgen (1977): "Einfaktorielle Varianzanalyse"; in: \textit{Lehrbuch der Statistik. FÃ¼r Sozialwissenschaftler}, ch. 12.1, pp. 528ff.; URL: \url{https://books.google.de/books?id=lNCyBgAAQBAJ}.
\item Denziloe (2018): "Derive the distribution of the ANOVA F-statistic under the alternative hypothesis"; in: \textit{StackExchange CrossValidated}, retrieved on 2022-11-06; URL: \url{https://stats.stackexchange.com/questions/355594/derive-the-distribution-of-the-anova-f-statistic-under-the-alternative-hypothesi}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Treatment sum of squares}]{Treatment sum of squares} \label{sec:trss}
\setcounter{equation}{0}

\textbf{Definition:} Let there be an analysis of variance (ANOVA) model with one ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1}), two ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) or multiple factors influencing the measured data $y$ (here, using the reparametrized version ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1-repara}) of one-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1})):

\begin{equation} \label{eq:trss-anova}
y_{ij} = \mu + \delta_i + \varepsilon_{ij}, \; \varepsilon_{ij} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; .
\end{equation}

Then, the treatment sum of squares is defined as the explained sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess}) (ESS) for each main effect, i.e. as the sum of squared deviations of the average for each level of the factor, from the average across all observations:

\begin{equation} \label{eq:trss-trss}
\mathrm{SS}_\mathrm{treat} = \sum_{i=1}^{k} \sum_{j=1}^{n_i} (\bar{y}_i - \bar{y})^2 \; .
\end{equation}

Here, $\bar{y}\_i$ is the mean for the $i$-th level of the factor (out of $k$ levels), computed from $n_i$ values $y\_{ij}$, and $\bar{y}$ is the mean across all values $y\_{ij}$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Analysis of variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-11-15; URL: \url{https://en.wikipedia.org/wiki/Analysis_of_variance#Partitioning_of_the_sum_of_squares}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Ordinary least squares for one-way ANOVA}]{Ordinary least squares for one-way ANOVA} \label{sec:anova1-ols}
\setcounter{equation}{0}

\textbf{Theorem:} Given the one-way analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1}) assumption

\begin{equation} \label{eq:anova1-ols-anova1}
y_{ij} = \mu_i + \varepsilon_{ij}, \; \varepsilon_{ij} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2), \; i = 1, \ldots, k, \; j = 1, \dots, n_i \; ,
\end{equation}

the parameters minimizing the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) are given by

\begin{equation} \label{eq:anova1-ols-anova1-ols}
\hat{\mu}_i = \bar{y}_i
\end{equation}

where $\bar{y}_i$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) of all observations in group ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1}) $i$:

\begin{equation} \label{eq:anova1-ols-mean-samp}
\hat{\mu}_i = \bar{y}_i = \frac{1}{n_i} \sum_{j=1}^{n_i} y_{ij} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) for this model is

\begin{equation} \label{eq:anova1-ols-rss}
\mathrm{RSS}(\mu) = \sum_{i=1}^{k} \sum_{j=1}^{n_i} \varepsilon_{ij}^2 = \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \mu_i)^2
\end{equation}

and the derivatives of $\mathrm{RSS}$ with respect to $\mu_i$ are

\begin{equation} \label{eq:anova1-ols-rss-der}
\begin{split}
\frac{\mathrm{d}\mathrm{RSS}(\mu)}{\mathrm{d}\mu_i} &= \sum_{j=1}^{n_i} \frac{\mathrm{d}}{\mathrm{d}\mu_i} (y_{ij} - \mu_i)^2 \\
&= \sum_{j=1}^{n_i} 2 (y_{ij} - \mu_i) (-1) \\
&= 2 \sum_{j=1}^{n_i} (\mu_i - y_{ij}) \\
&= 2 n_i \mu_i - 2 \sum_{j=1}^{n_i} y_{ij} \quad \text{for} \quad i = 1, \ldots, k \; .
\end{split}
\end{equation}

Setting these derivatives to zero, we obtain the estimates of $\mu_i$:

\begin{equation} \label{eq:anova1-ols-rss-der-zero}
\begin{split}
0 &= 2 n_i \hat{\mu}_i - 2 \sum_{j=1}^{n_i} y_{ij} \\
\hat{\mu}_i &= \frac{1}{n_i} \sum_{j=1}^{n_i} y_{ij} \quad \text{for} \quad i = 1, \ldots, k \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Sums of squares in one-way ANOVA}]{Sums of squares in one-way ANOVA} \label{sec:anova1-pss}
\setcounter{equation}{0}

\textbf{Theorem:} Given one-way analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1}),

\begin{equation} \label{eq:anova1-pss-anova1}
y_{ij} = \mu_i + \varepsilon_{ij}, \; \varepsilon_{ij} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)
\end{equation}

sums of squares can be partitioned as follows

\begin{equation} \label{eq:anova1-pss-anova1-pss}
\mathrm{SS}_\mathrm{tot} = \mathrm{SS}_\mathrm{treat} + \mathrm{SS}_\mathrm{res}
\end{equation}

where $\mathrm{SS} _\mathrm{tot}$ is the total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}), $\mathrm{SS} _\mathrm{treat}$ is the treatment sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:trss}) (equivalent to explained sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess})) and $\mathrm{SS} _\mathrm{res}$ is the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}).


\vspace{1em}
\textbf{Proof:} The total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}) for one-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1}) is given by

\begin{equation} \label{eq:anova1-pss-anova1-tss}
\mathrm{SS}_\mathrm{tot} = \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y})^2
\end{equation}

where $\bar{y}$ is the mean across all values $y_{ij}$. This can be rewritten as

\begin{equation} \label{eq:anova1-pss-anova1-pss-s1}
\begin{split}
\sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y})^2 &= \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left[ (y_{ij} - \bar{y}_i) + (\bar{y}_i - \bar{y}) \right]^2 \\
&= \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left[ (y_{ij} - \bar{y}_i)^2 + (\bar{y}_i - \bar{y})^2 + 2 (y_{ij} - \bar{y}_i) (\bar{y}_i - \bar{y}) \right] \\
&= \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2 + \sum_{i=1}^{k} \sum_{j=1}^{n_i} (\bar{y}_i - \bar{y})^2 + 2 \sum_{i=1}^{k} (\bar{y}_i - \bar{y}) \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i) \; .
\end{split}
\end{equation}

Note that the following sum is zero

\begin{equation} \label{eq:anova1-pss-anova1-pss-s2}
\sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i) = \sum_{j=1}^{n_i} y_{ij} - n_i \cdot \bar{y}_i = \sum_{j=1}^{n_i} y_{ij} - n_i \cdot \frac{1}{n_i} \sum_{j=1}^{n_i} y_{ij} \; ,
\end{equation}

so that the sum in \eqref{eq:anova1-pss-anova1-pss-s1} reduces to

\begin{equation} \label{eq:anova1-pss-anova1-pss-s3}
\sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y})^2 = \sum_{i=1}^{k} \sum_{j=1}^{n_i} (\bar{y}_i - \bar{y})^2 + \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2 \; .
\end{equation}

With the treatment sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:trss}) for one-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1})

\begin{equation} \label{eq:anova1-pss-anova1-trss}
\mathrm{SS}_\mathrm{treat} = \sum_{i=1}^{k} \sum_{j=1}^{n_i} (\bar{y}_i - \bar{y})^2
\end{equation}

and the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) for one-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1})

\begin{equation} \label{eq:anova1-pss-anova1-rss}
\mathrm{SS}_\mathrm{res} = \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2 \; ,
\end{equation}

we finally have:

\begin{equation} \label{eq:anova1-pss-anova1-pss-qed}
\mathrm{SS}_\mathrm{tot} = \mathrm{SS}_\mathrm{treat} + \mathrm{SS}_\mathrm{res} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Analysis of variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-11-15; URL: \url{https://en.wikipedia.org/wiki/Analysis_of_variance#Partitioning_of_the_sum_of_squares}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{F-test for main effect in one-way ANOVA}]{F-test for main effect in one-way ANOVA} \label{sec:anova1-f}
\setcounter{equation}{0}

\textbf{Theorem:} Assume the one-way analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1}) model

\begin{equation} \label{eq:anova1-f-anova1}
y_{ij} = \mu_i + \varepsilon_{ij}, \; \varepsilon_{ij} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2), \; i = 1, \ldots, k, \; j = 1, \dots, n_i \; .
\end{equation}

Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:anova1-f-anova1-f}
F = \frac{\frac{1}{k-1} \sum_{i=1}^{k} n_i (\bar{y}_i - \bar{y})^2}{\frac{1}{n-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2}
\end{equation}

follows an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f})

\begin{equation} \label{eq:anova1-f-anova1-f-h0}
F \sim \mathrm{F}(k-1, n-k)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})

\begin{equation} \label{eq:anova1-f-anova1-h0}
\begin{split}
H_0: &\; \mu_1 = \ldots = \mu_k \\
H_1: &\; \mu_i \neq \mu_j \quad \text{for at least one} \quad i,j \in \left\lbrace 1, \ldots, k \right\rbrace, \; i \neq j \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} Denote sample sizes as

\begin{equation} \label{eq:anova1-f-samp-size}
\begin{split}
n_i &- \text{number of samples in category} \; i \\
n &= \sum_{i=1}^{k} n_ij
\end{split}
\end{equation}

and denote sample means as

\begin{equation} \label{eq:anova1-f-mean-samp}
\begin{split}
\bar{y}_i &= \frac{1}{n_i} \sum_{j=1}^{n_i} y_{ij} \\
\bar{y} &= \frac{1}{n} \sum_{i=1}^{k} \sum_{j=1}^{n_i} y_{ij} \; .
\end{split}
\end{equation}

Let $\mu$ be the common mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) according to $H_0$ given by \eqref{eq:anova1-f-anova1-h0}, i.e. $\mu_1 = \ldots = \mu_k = \mu$. Under this null hypothesis, we have:

\begin{equation} \label{eq:anova1-f-yij-h0}
y_{ij} \sim \mathcal{N}(\mu, \sigma^2) \quad \text{for all} \quad i = 1, \ldots, k, \; j = 1, \ldots, n_i \; .
\end{equation}

Thus, the random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $U_{ij} = (y_{ij} - \mu)/\sigma$ follows a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-snorm})

\begin{equation} \label{eq:anova1-f-Uij-h0}
U_{ij} = \frac{y_{ij} - \mu}{\sigma} \sim \mathcal{N}(0, 1) \; .
\end{equation}

Now consider the following sum:

\begin{equation} \label{eq:anova1-f-sum-Uij-s1}
\begin{split}
\sum_{i=1}^{k} \sum_{j=1}^{n_i} U_{ij}^2 &= \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left( \frac{y_{ij} - \mu}{\sigma} \right)^2 \\
&= \frac{1}{\sigma^2} \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left( (y_{ij} - \bar{y}_i) + (\bar{y}_i - \bar{y}) + (\bar{y} - \mu) \right)^2 \\
&= \frac{1}{\sigma^2} \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left[ (y_{ij} - \bar{y}_i)^2 + (\bar{y}_i - \bar{y})^2 + (\bar{y} - \mu)^2 + 2 (y_{ij} - \bar{y}_i) (\bar{y}_i - \bar{y}) + 2 (y_{ij} - \bar{y}_i) (\bar{y} - \mu) + 2 (\bar{y}_i - \bar{y}) (\bar{y} - \mu) \right] \; .
\end{split}
\end{equation}

Because the following sum over $j$ is zero for all $i$

\begin{equation} \label{eq:anova1-f-sum-yij}
\begin{split}
\sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i) &= \sum_{j=1}^{n_i} y_{ij} - n_i \bar{y}_i \\
&= \sum_{j=1}^{n_i} y_{ij}  - n_i \cdot \frac{1}{n_i} \sum_{j=1}^{n_i} y_{ij} \\
&= 0, \; i = 1, \ldots, k
\end{split}
\end{equation}

and the following sum over $i$ and $j$ is also zero

\begin{equation} \label{eq:anova1-f-sum-yib}
\begin{split}
\sum_{i=1}^{k} \sum_{j=1}^{n_i} (\bar{y}_i - \bar{y}) &= \sum_{i=1}^{k} n_i (\bar{y}_i - \bar{y}) \\
&= \sum_{i=1}^{k} n_i \bar{y}_i - \bar{y} \sum_{i=1}^{k} n_i \\
&= \sum_{i=1}^{k} n_i \cdot \frac{1}{n_i} \sum_{j=1}^{n_i} y_{ij} - n \cdot \frac{1}{n} \sum_{i=1}^{k} \sum_{j=1}^{n_i} y_{ij} \\
&= 0 \; ,
\end{split}
\end{equation}

non-square products in \eqref{eq:anova1-f-sum-Uij-s1} disappear and the sum reduces to

\begin{equation} \label{eq:anova1-f-sum-Uij-s2}
\begin{split}
\sum_{i=1}^{k} \sum_{j=1}^{n_i} U_{ij}^2 &= \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left[ \left( \frac{y_{ij} - \bar{y}_i}{\sigma} \right)^2 + \left( \frac{\bar{y}_i - \bar{y}}{\sigma} \right)^2 + \left( \frac{\bar{y} - \mu}{\sigma} \right)^2 \right] \\
&= \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left( \frac{y_{ij} - \bar{y}_i}{\sigma} \right)^2 + \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left( \frac{\bar{y}_i - \bar{y}}{\sigma} \right)^2 + \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left( \frac{\bar{y} - \mu}{\sigma} \right)^2 \; .
\end{split}
\end{equation}

Cochran's theorem states that, if a sum of squared standard normal ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) can be written as a sum of squared forms

\begin{equation} \label{eq:anova1-f-cochran-p1}
\begin{split}
\sum_{i=1}^{n} U_i^2 = \sum_{j=1}^{m} Q_j \quad &\text{where} \quad Q_j = U^\mathrm{T} B^{(j)} U \\
&\text{with} \quad \sum_{j=1}^{m} B^{(j)} = I_n \\
&\text{and} \quad r_j = \mathrm{rank}(B^{(j)}) \; ,
\end{split}
\end{equation}

then the terms $Q_j$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) and each term $Q_j$ follows a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) with $r_j$ degrees of freedom:

\begin{equation} \label{eq:anova1-f-cochran-p2}
Q_j \sim \chi^2(r_j), \; j = 1, \ldots, m \; .
\end{equation}

Let $U$ be the $n \times 1$ column vector of all observations

\begin{equation} \label{eq:anova1-f-U}
U = \left[ \begin{matrix} u_1 \\ \vdots \\ u_k \end{matrix} \right]
\end{equation}

where the group-wise $n_i \times 1$ column vectors are

\begin{equation} \label{yi}
u_1 = \left[ \begin{matrix} (y_{1,1}-\mu)/\sigma \\ \vdots \\ (y_{1,n_1}-\mu)/\sigma \end{matrix} \right], \quad \ldots, \quad u_k = \left[ \begin{matrix} (y_{k,1}-\mu)/\sigma \\ \vdots \\ (y_{k,n_k}-\mu)/\sigma \end{matrix} \right] \; .
\end{equation}

Then, we observe that the sum in \eqref{eq:anova1-f-sum-Uij-s2} can be represented in the form of \eqref{eq:anova1-f-cochran-p1} using the matrices

\begin{equation} \label{eq:anova1-f-B}
\begin{split}
B^{(1)} &= I_n - \mathrm{diag}\left( \frac{1}{n_1} J_{n_1}, \; \ldots, \; \frac{1}{n_k} J_{n_k} \right) \\
B^{(2)} &= \mathrm{diag}\left( \frac{1}{n_1} J_{n_1}, \; \ldots, \; \frac{1}{n_k} J_{n_k} \right) - \frac{1}{n} J_n \\
B^{(3)} &= \frac{1}{n} J_n
\end{split}
\end{equation}

where $J_n$ is an $n \times n$ matrix of ones and $\mathrm{diag}\left( A_1, \ldots, A_n \right)$ denotes a block-diagonal matrix composed of $A_1, \ldots, A_n$. We observe that those matrices satisfy

\begin{equation} \label{eq:anova1-f-U-Q-B}
\sum_{i=1}^{k} \sum_{j=1}^{n_i} U_{ij}^2 = Q_1 + Q_2 + Q_3 = U^\mathrm{T} B^{(1)} U + U^\mathrm{T} B^{(2)} U + U^\mathrm{T} B^{(3)} U
\end{equation}

as well as

\begin{equation} \label{eq:anova1-f-B-In}
B^{(1)} + B^{(2)} + B^{(3)} = I_n
\end{equation}

and their ranks are:

\begin{equation} \label{eq:anova1-f-B-rk}
\begin{split}
\mathrm{rank}\left( B^{(1)} \right) &= n-k \\
\mathrm{rank}\left( B^{(2)} \right) &= k-1 \\
\mathrm{rank}\left( B^{(3)} \right) &= 1 \; .
\end{split}
\end{equation}

Let's write down the explained sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess}) and the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) for one-way analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1}) as

\begin{equation} \label{eq:anova1-f-ess-rss}
\begin{split}
\mathrm{ESS} &= \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left( \bar{y}_i - \bar{y} \right)^2 \\
\mathrm{RSS} &= \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left( y_{ij} - \bar{y}_i \right)^2 \; .
\end{split}
\end{equation}

Then, using \eqref{eq:anova1-f-sum-Uij-s2}, \eqref{eq:anova1-f-cochran-p1}, \eqref{eq:anova1-f-cochran-p2}, \eqref{eq:anova1-f-B} and \eqref{eq:anova1-f-B-rk}, we find that

\begin{equation} \label{eq:anova1-f-ess-rss-dist}
\begin{split}
\frac{\mathrm{ESS}}{\sigma^2} = \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left( \frac{\bar{y}_i - \bar{y}}{\sigma} \right)^2 &= Q_2 = U^\mathrm{T} B^{(2)} U \sim \chi^2(k-1) \\
\frac{\mathrm{RSS}}{\sigma^2} = \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left( \frac{y_{ij} - \bar{y}_i}{\sigma} \right)^2 &= Q_1 = U^\mathrm{T} B^{(1)} U \sim \chi^2(n-k) \; .
\end{split}
\end{equation}

Because $\mathrm{ESS}/\sigma^2$ and $\mathrm{RSS}/\sigma^2$ are also independent by \eqref{eq:anova1-f-cochran-p2}, the F-statistic from \eqref{eq:anova1-f-anova1-f} is equal to the ratio of two independent chi-squared distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) divided by their degrees of freedom

\begin{equation} \label{eq:anova1-f-anova1-f-ess-tss}
\begin{split}
F &= \frac{(\mathrm{ESS}/\sigma^2)/(k-1)}{(\mathrm{RSS}/\sigma^2)/(n-k)} \\
&= \frac{\mathrm{ESS}/(k-1)}{\mathrm{RSS}/(n-k)} \\
&= \frac{\frac{1}{k-1} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (\bar{y}_i - \bar{y})^2}{\frac{1}{n-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2} \\
&= \frac{\frac{1}{k-1} \sum_{i=1}^{k} n_i (\bar{y}_i - \bar{y})^2}{\frac{1}{n-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2}
\end{split}
\end{equation}

which, by definition of the F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}), is distributed as

\begin{equation} \label{eq:anova1-f-anova1-f-qed}
F \sim \mathrm{F}(k-1, n-k)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) for the main effect.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Denziloe (2018): "Derive the distribution of the ANOVA F-statistic under the alternative hypothesis"; in: \textit{StackExchange CrossValidated}, retrieved on 2022-11-06; URL: \url{https://stats.stackexchange.com/questions/355594/derive-the-distribution-of-the-anova-f-statistic-under-the-alternative-hypothesi}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{F-statistic in terms of OLS estimates}]{F-statistic in terms of OLS estimates} \label{sec:anova1-fols}
\setcounter{equation}{0}

\textbf{Theorem:} Given the one-way analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1}) assumption

\begin{equation} \label{eq:anova1-fols-anova1}
y_{ij} = \mu_i + \varepsilon_{ij}, \; \varepsilon_{ij} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2),
\end{equation}

1) the F-statistic for the main effect ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1-f}) can be expressed in terms of ordinary least squares parameter estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1-ols}) as

\begin{equation} \label{eq:anova1-fols-anova1-fols-v1}
F = \frac{\frac{1}{k-1} \sum_{i=1}^{k} n_i (\hat{\mu}_i - \bar{y})^2}{\frac{1}{n-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \hat{\mu}_i)^2}
\end{equation}

2) or, when using the reparametrized version of one-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1-repara}), the F-statistic can be expressed as

\begin{equation} \label{eq:anova1-fols-anova1-fols-v2}
F = \frac{\frac{1}{k-1} \sum_{i=1}^{k} n_i \hat{\delta}_i^2}{\frac{1}{n-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \hat{\mu} - \hat{\delta}_i)^2} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The F-statistic for the main effect in one-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1-f}) is given in terms of the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) as

\begin{equation} \label{eq:anova1-fols-anova1-f}
F = \frac{\frac{1}{k-1} \sum_{i=1}^{k} n_i (\bar{y}_i - \bar{y})^2}{\frac{1}{n-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2}
\end{equation}

where $\bar{y}\_i$ is the average of all values $y\_{ij}$ from category $i$ and $\bar{y}$ is the grand mean of all values $y\_{ij}$ from all categories $i = 1, \ldots, k$.

1) The ordinary least squares estimates for one-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1-ols}) are

\begin{equation} \label{eq:anova1-fols-anova1-ols}
\hat{\mu}_i = \bar{y}_i \; ,
\end{equation}

such that

\begin{equation} \label{eq:anova1-fols-anova1-fols-v1-qed}
F = \frac{\frac{1}{k-1} \sum_{i=1}^{k} n_i (\hat{\mu}_i - \bar{y})^2}{\frac{1}{n-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \hat{\mu}_i)^2} \; .
\end{equation}

2) The OLS estimates for reparametrized one-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1-repara}) are

\begin{equation} \label{eq:anova1-fols-anova1-repara-ols}
\begin{split}
\hat{\mu} &= \bar{y} \\
\hat{\delta}_i &= \bar{y}_i - \bar{y} \; ,
\end{split}
\end{equation}

such that

\begin{equation} \label{eq:anova1-fols-anova1-fols-v2-qed}
F = \frac{\frac{1}{k-1} \sum_{i=1}^{k} n_i \hat{\delta}_i^2}{\frac{1}{n-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \hat{\mu} - \hat{\delta}_i)^2} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Reparametrization of one-way ANOVA}]{Reparametrization of one-way ANOVA} \label{sec:anova1-repara}
\setcounter{equation}{0}

\textbf{Theorem:} The one-way analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1}) model

\begin{equation} \label{eq:anova1-repara-anova1}
y_{ij} = \mu_i + \varepsilon_{ij}, \; \varepsilon_{ij} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)
\end{equation}

can be rewritten using paraneters $\mu$ and $\delta_i$ instead of $\mu_i$

\begin{equation} \label{eq:anova1-repara-anova1-repara}
y_{ij} = \mu + \delta_i + \varepsilon_{ij}, \; \varepsilon_{ij} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)
\end{equation}

with the constraint

\begin{equation} \label{eq:anova1-repara-anova1-constr}
\sum_{i=1}^{k} \frac{n_i}{n} \delta_i = 0 \; ,
\end{equation}

in which case

1) the model parameters are related to each other as

\begin{equation} \label{eq:anova1-repara-anova1-repara-c1}
\delta_i = \mu_i - \mu, \; i = 1, \ldots, k \; ;
\end{equation}

2) the ordinary least squares estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1-ols}) are given by

\begin{equation} \label{eq:anova1-repara-anova1-repara-c2}
\hat{\delta}_i = \bar{y}_i - \bar{y} = \frac{1}{n_i} \sum_{j=1}^{n_i} y_{ij} - \frac{1}{n} \sum_{i=1}^{k} \sum_{j=1}^{n_i} y_{ij} \; ;
\end{equation}

3) the following sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1-pss}) is chi-square distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2})

\begin{equation} \label{eq:anova1-repara-anova1-repara-c3}
\frac{1}{\sigma^2} \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left( \hat{\delta}_i - \delta_i \right)^2 \sim \chi^2(k-1) \; ;
\end{equation}

4) and the following test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat}) is F-distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f})

\begin{equation} \label{eq:anova1-repara-anova1-repara-c4}
F = \frac{\frac{1}{k-1} \sum_{i=1}^{k} n_i \hat{\delta}_i^2}{\frac{1}{n-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2} \sim \mathrm{F}(k-1, n-k)
\end{equation}

under the null hypothesis for the main effect ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1-f})

\begin{equation} \label{eq:anova1-repara-anova1-repara-c4-h0}
H_0: \; \delta_1 = \ldots = \delta_k = 0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:}

1) Equating \eqref{eq:anova1-repara-anova1} with \eqref{eq:anova1-repara-anova1-repara}, we get:

\begin{equation} \label{eq:anova1-repara-anova1-repara-c1-qed}
\begin{split}
y_{ij} = \mu + \delta_i + \varepsilon_{ij} &= \mu_i + \varepsilon_{ij} = y_{ij} \\
\mu + \delta_i &= \mu_i \\
\delta_i &= \mu_i - \mu \; .
\end{split}
\end{equation}

2) Equation \eqref{eq:anova1-repara-anova1-repara} is a special case of the two-way analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) with (i) just one factor $A$ and (ii) no interaction term. Thus, OLS estimates are identical to that of two-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-ols}), i.e. given by

\begin{equation} \label{eq:anova1-repara-anova1-repara-c2-qed}
\begin{split}
\hat{\mu} &= \bar{y}_{\bullet \bullet} = \frac{1}{n} \sum_{i=1}^{k} \sum_{j=1}^{n_i} y_{ij} \\
\hat{\delta}_i &= \bar{y}_{i \bullet} - \bar{y}_{\bullet \bullet} = \frac{1}{n_i} \sum_{j=1}^{n_i} y_{ij} - \frac{1}{n} \sum_{i=1}^{k} \sum_{j=1}^{n_i} y_{ij} \; .
\end{split}
\end{equation}

3) Let $U_{ij} = (y_{ij} - \mu - \delta_i)/\sigma$, such that ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-snorm}) $U_{ij} \sim \mathcal{N}(0, 1)$ and consider the sum of all squared random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $U_{ij}$:

\begin{equation} \label{eq:anova1-repara-anova1-repara-c3-s1}
\begin{split}
\sum_{i=1}^{k} \sum_{j=1}^{n_i} U_{ij}^2 &= \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left( \frac{y_{ij} - \mu - \delta_i}{\sigma} \right)^2 \\
&= \frac{1}{\sigma^2} \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left[ (y_{ij} - \bar{y}_i) + ([\bar{y}_i - \bar{y}] - \delta_i) + (\bar{y} - \mu) \right]^2 \; .
\end{split}
\end{equation}

This square of sums, using a number of intermediate steps, can be developed ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1-f}) into a sum of squares:

\begin{equation} \label{eq:anova1-repara-anova1-repara-c3-s2}
\begin{split}
\sum_{i=1}^{k} \sum_{j=1}^{n_i} U_{ij}^2 &= \frac{1}{\sigma^2} \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left[ (y_{ij} - \bar{y}_i)^2 + ([\bar{y}_i - \bar{y}] - \delta_i)^2 + (\bar{y} - \mu)^2 \right] \\
&= \frac{1}{\sigma^2} \left[ \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2 + \sum_{i=1}^{k} \sum_{j=1}^{n_i} ([\bar{y}_i - \bar{y}] - \delta_i)^2 + \sum_{i=1}^{k} \sum_{j=1}^{n_i} (\bar{y} - \mu)^2 \right] \; .
\end{split}
\end{equation}

To this sum, Cochran's theorem for one-way analysis of variance can be applied ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova1-f}), yielding the distributions:

\begin{equation} \label{eq:anova1-repara-anova1-repara-c3-qed}
\begin{split}
\frac{1}{\sigma^2} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2 &\sim \chi^2(n-k) \\
\frac{1}{\sigma^2} \sum_{i=1}^{k} \sum_{j=1}^{n_i} ([\bar{y}_i - \bar{y}] - \delta_i)^2 \overset{\eqref{eq:anova1-repara-anova1-repara-c2-qed}}{=} \frac{1}{\sigma^2} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (\hat{\delta}_i - \delta_i)^2 &\sim \chi^2(k-1) \; .
\end{split}
\end{equation}

4) The ratio of two chi-square distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}), divided by their degrees of freedom, is defined to be F-distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}), so that

\begin{equation} \label{eq:anova1-repara-anova1-repara-c4-s1}
\begin{split}
F &= \frac{\left( \frac{1}{\sigma^2} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (\hat{\delta}_i - \delta_i)^2 \right)/(k-1)}{\left( \frac{1}{\sigma^2} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2 \right)/(n-k)} \\
&= \frac{\frac{1}{k-1} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (\hat{\delta}_i - \delta_i)^2}{\frac{1}{n-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2} \\
&= \frac{\frac{1}{k-1} \sum_{i=1}^{k} n_i (\hat{\delta}_i - \delta_i)^2}{\frac{1}{n-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2} \\
&\overset{\eqref{eq:anova1-repara-anova1-repara-c4-h0}}{=} \frac{\frac{1}{k-1} \sum_{i=1}^{k} n_i \hat{\delta}_i^2}{\frac{1}{n-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2}
\end{split}
\end{equation}

follows the F-distribution

\begin{equation} \label{eq:anova1-repara-anova1-repara-c4-qed}
F \sim \mathrm{F}(k-1, n-k)
\end{equation}

under the null hypothesis.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Analysis of variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-11-15; URL: \url{https://en.wikipedia.org/wiki/Analysis_of_variance#For_a_single_factor}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Two-way ANOVA}]{Two-way ANOVA} \label{sec:anova2}
\setcounter{equation}{0}

\textbf{Definition:} Let there be two factors $A$ and $B$ with levels $i = 1, \ldots, a$ and $j = 1, \ldots, b$ that are used to group measurements $y_{ijk} \in \mathbb{R}$ from distinct objects $k = 1, \ldots, n_{ij}$ into $a \cdot b$ categories $(i,j) \in \left\lbrace 1, \ldots, a \right\rbrace \times \left\lbrace 1, \ldots, b \right\rbrace$.

Then, in two-way analysis of variance (ANOVA), these measurements are assumed to come from normal distributions ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm})

\begin{equation} \label{eq:anova2-anova2-p1}
y_{ijk} \sim \mathcal{N}(\mu_{ij}, \sigma^2) \quad \text{for all} \quad i = 1, \ldots, a, \quad j = 1, \ldots, b, \quad \text{and} \quad k = 1, \dots, n_{ij}
\end{equation}

with

\begin{equation} \label{eq:anova2-anova2-p2}
\mu_{ij} = \mu + \alpha_i + \beta_j + \gamma_{ij}
\end{equation}

where

\begin{itemize}

\item $\mu$ is called the "grand mean";

\item $\alpha_i$ is the additive "main effect" of the $i$-th level of factor $A$;

\item $\beta_j$ is the additive "main effect" of the $j$-th level of factor $B$;

\item $\gamma_{ij}$ is the non-additive "interaction effect" of category $(i,j)$;

\item $\mu_{ij}$ is the expected value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) in category $(i,j)$; and

\item $\sigma^2$ is common variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) across all categories.

\end{itemize}

Alternatively, the model may be written as

\begin{equation} \label{eq:anova2-anova2-alt}
\begin{split}
y_{ijk} &= \mu + \alpha_i + \beta_j + \gamma_{ij} + \varepsilon_{ijk} \\
\varepsilon_{ijk} &\sim \mathcal{N}(0, \sigma^2)
\end{split}
\end{equation}

where $\varepsilon_{ijk}$ is the error term ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) corresponding to observation $k$ belonging to the $i$-th level of $A$ and the $j$-th level of $B$.

As the two-way ANOVA model is underdetermined, the parameters of the model are additionally subject to the constraints

\begin{equation} \label{eq:anova2-anova2-cons}
\begin{split}
\sum_{i=1}^{a} w_{ij} \alpha_i &= 0 \quad \text{for all} \quad j = 1, \ldots, b \\
\sum_{j=1}^{b} w_{ij} \beta_j &= 0 \quad \text{for all} \quad i = 1, \ldots, a \\
\sum_{i=1}^{a} w_{ij} \gamma_{ij} &= 0 \quad \text{for all} \quad j = 1, \ldots, b \\
\sum_{j=1}^{b} w_{ij} \gamma_{ij} &= 0 \quad \text{for all} \quad i = 1, \ldots, a
\end{split}
\end{equation}

where the weights are $w_{ij} = n_{ij}/n$ and the total sample size is $n = \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij}$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Bortz, JÃ¼rgen (1977): "Zwei- und mehrfaktorielle Varianzanalyse"; in: \textit{Lehrbuch der Statistik. FÃ¼r Sozialwissenschaftler}, ch. 12.2, pp. 538ff.; URL: \url{https://books.google.de/books?id=lNCyBgAAQBAJ}.
\item ttd (2021): "Proof on SSAB/s2~chi2(I-1)(J-1) under the null hypothesis HAB: dij=0 for i=1,...,I and j=1,...,J"; in: \textit{StackExchange CrossValidated}, retrieved on 2022-11-06; URL: \url{https://stats.stackexchange.com/questions/545807/proof-on-ss-ab-sigma2-sim-chi2-i-1j-1-under-the-null-hypothesis}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Interaction sum of squares}]{Interaction sum of squares} \label{sec:iass}
\setcounter{equation}{0}

\textbf{Definition:} Let there be an analysis of variance (ANOVA) model with two ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) or more factors influencing the measured data $y$ (here, using the standard formulation ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-pss}) of two-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2})):

\begin{equation} \label{eq:iass-anova}
y_{ijk} = \mu + \alpha_i + \beta_j + \gamma_{ij} + \varepsilon_{ijk}, \; \varepsilon_{ijk} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; .
\end{equation}

Then, the interaction sum of squares is defined as the explained sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess}) (ESS) for each interaction, i.e. as the sum of squared deviations of the average for each cell from the average across all observations, controlling for the treatment sums of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:trss}) of the corresponding factors:

\begin{equation} \label{eq:iass-iass}
\begin{split}
\mathrm{SS}_\mathrm{A \times B} &= \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} ([\bar{y}_{i j \bullet} - \bar{y}_{\bullet \bullet \bullet}] - [\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}] - [\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}])^2 \\
&= \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet})^2 \; .
\end{split}
\end{equation}

Here, $\bar{y}\_{i j \bullet}$ is the mean for the $(i,j)$-th cell (out of $a \times b$ cells), computed from $n\_{ij}$ values $y\_{ijk}$, $\bar{y}\_{i \bullet \bullet}$ and $\bar{y}\_{\bullet j \bullet}$ are the level means for the two factors and and $\bar{y}\_{\bullet \bullet \bullet}$ is the mean across all values $y\_{ijk}$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Nandy, Siddhartha (2018): "Two-Way Analysis of Variance"; in: \textit{Stat 512: Applied Regression Analysis}, Purdue University, Summer 2018, Ch. 19; URL: \url{https://www.stat.purdue.edu/~snandy/stat512/topic7.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Ordinary least squares for two-way ANOVA}]{Ordinary least squares for two-way ANOVA} \label{sec:anova2-ols}
\setcounter{equation}{0}

\textbf{Theorem:} Given the two-way analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) assumption

\begin{equation} \label{eq:anova2-ols-anova2}
\begin{split}
y_{ijk} &= \mu + \alpha_i + \beta_j + \gamma_{ij} + \varepsilon_{ijk} \\
\varepsilon_{ijk} &\overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2), \; i = 1, \ldots, a, \; j = 1, \ldots, b, \; k = 1, \dots, n_{ij} \; ,
\end{split}
\end{equation}

the parameters minimizing the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) and satisfying the constraints for the model parameters ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) are given by

\begin{equation} \label{eq:anova2-ols-anova2-ols}
\begin{split}
\hat{\mu} &= \bar{y}_{\bullet \bullet \bullet} \\
\hat{\alpha}_i &= \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet} \\
\hat{\beta}_j &= \bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet} \\
\hat{\gamma}_{ij} &= \bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}
\end{split}
\end{equation}

where $\bar{y} _{\bullet \bullet \bullet}$, $\bar{y} _{i \bullet \bullet}$, $\bar{y} _{\bullet j \bullet}$ and $\bar{y} _{i j \bullet}$ are the following sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}):

\begin{equation} \label{eq:anova2-ols-mean-samp}
\begin{split}
\bar{y}_{\bullet \bullet \bullet} &= \frac{1}{n} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} \\
\bar{y}_{i \bullet \bullet} &= \frac{1}{n_{i \bullet}} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} \\
\bar{y}_{\bullet j \bullet} &= \frac{1}{n_{\bullet j}} \sum_{i=1}^{a} \sum_{k=1}^{n_{ij}} y_{ijk} \\
\bar{y}_{i j \bullet} &= \frac{1}{n_{ij}} \sum_{k=1}^{n_{ij}} y_{ijk}
\end{split}
\end{equation}

with the sample size numbers

\begin{equation} \label{eq:anova2-ols-samp-size}
\begin{split}
n_{ij} &- \text{number of samples in category} \; (i,j) \\
n_{i \bullet} &= \sum_{j=1}^{b} n_{ij} \\
n_{\bullet j} &= \sum_{i=1}^{a} n_{ij} \\
n &= \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} In two-way ANOVA, model parameters are subject to the constraints ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2})

\begin{equation} \label{eq:anova2-ols-anova2-cons}
\begin{split}
\sum_{i=1}^{a} w_{ij} \alpha_i &= 0 \quad \text{for all} \quad j = 1, \ldots, b \\
\sum_{j=1}^{b} w_{ij} \beta_j &= 0 \quad \text{for all} \quad i = 1, \ldots, a \\
\sum_{i=1}^{a} w_{ij} \gamma_{ij} &= 0 \quad \text{for all} \quad j = 1, \ldots, b \\
\sum_{j=1}^{b} w_{ij} \gamma_{ij} &= 0 \quad \text{for all} \quad i = 1, \ldots, a
\end{split}
\end{equation}

where $w_{ij} = n_{ij}/n$. The residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) for this model is

\begin{equation} \label{eq:anova2-ols-rss}
\mathrm{RSS}(\mu,\alpha,\beta,\gamma) = \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} \varepsilon_{ijk}^2 = \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ij} - \mu - \alpha_i - \beta_j - \gamma_{ij})^2
\end{equation}

and the derivatives of $\mathrm{RSS}$ with respect to $\mu$, $\alpha$, $\beta$ and $\gamma$ are

\begin{equation} \label{eq:anova2-ols-rss-der-mu}
\begin{split}
\frac{\mathrm{d}\mathrm{RSS}}{\mathrm{d}\mu} &= \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} \frac{\mathrm{d}}{\mathrm{d}\mu} (y_{ijk} - \mu - \alpha_i - \beta_j - \gamma_{ij})^2 \\
&= \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} -2 (y_{ijk} - \mu - \alpha_i - \beta_j - \gamma_{ij}) \\
&= \sum_{i=1}^{a} \sum_{j=1}^{b} \left( 2 n_{ij} \mu + 2 n_{ij} (\alpha_i + \beta_j + \gamma_{ij}) - 2 \sum_{k=1}^{n_{ij}} y_{ijk} \right) \\
&= 2 n \mu + 2 \left( \sum_{i=1}^{a} n_{i \bullet} \alpha_i + \sum_{j=1}^{b} n_{\bullet j} \beta_j + \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} \gamma_{ij} \right) - 2 \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk}
\end{split}
\end{equation}

\begin{equation} \label{eq:anova2-ols-rss-der-alpha}
\begin{split}
\frac{\mathrm{d}\mathrm{RSS}}{\mathrm{d}\alpha_i} &= \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} \frac{\mathrm{d}}{\mathrm{d}\alpha_i} (y_{ijk} - \mu - \alpha_i - \beta_j - \gamma_{ij})^2 \\
&= \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} -2 (y_{ijk} - \mu - \alpha_i - \beta_j - \gamma_{ij}) \\
&= 2 n_{i \bullet} \mu + 2 n_{i \bullet} \alpha_i + 2 \left( \sum_{j=1}^{b} n_{ij} \beta_j + \sum_{j=1}^{b} n_{ij} \gamma_{ij} \right) - 2 \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk}
\end{split}
\end{equation}

\begin{equation} \label{eq:anova2-ols-rss-der-beta}
\begin{split}
\frac{\mathrm{d}\mathrm{RSS}}{\mathrm{d}\beta_j} &= \sum_{i=1}^{a} \sum_{k=1}^{n_{ij}} \frac{\mathrm{d}}{\mathrm{d}\beta_j} (y_{ijk} - \mu - \alpha_i - \beta_j - \gamma_{ij})^2 \\
&= \sum_{i=1}^{a} \sum_{k=1}^{n_{ij}} -2 (y_{ijk} - \mu - \alpha_i - \beta_j - \gamma_{ij}) \\
&= 2 n_{\bullet j} \mu + 2 n_{\bullet j} \beta_j + 2 \left( \sum_{i=1}^{a} n_{ij} \alpha_i + \sum_{i=1}^{a} n_{ij} \gamma_{ij} \right) - 2 \sum_{i=1}^{a} \sum_{k=1}^{n_{ij}} y_{ijk}
\end{split}
\end{equation}

\begin{equation} \label{eq:anova2-ols-rss-der-gamma}
\begin{split}
\frac{\mathrm{d}\mathrm{RSS}}{\mathrm{d}\gamma_{ij}} &= \sum_{k=1}^{n_{ij}} \frac{\mathrm{d}}{\mathrm{d}\gamma_{ij}} (y_{ijk} - \mu - \alpha_i - \beta_j - \gamma_{ij})^2 \\
&= \sum_{k=1}^{n_{ij}} -2 (y_{ijk} - \mu - \alpha_i - \beta_j - \gamma_{ij}) \\
&= 2 n_{ij} (\mu + \alpha_i + \beta_j + \gamma_{ij}) - 2 \sum_{k=1}^{n_{ij}} y_{ijk} \; .
\end{split}
\end{equation}

Setting these derivatives to zero, we obtain the estimates of $\mu_i$, $\alpha_i$, $\beta_j$ and $\gamma_{ij}$:

\begin{equation} \label{eq:anova2-ols-rss-der-mu-zero}
\begin{split}
0 &= 2 n \hat{\mu} + 2 \left( \sum_{i=1}^{a} n_{i \bullet} \alpha_i + \sum_{j=1}^{b} n_{\bullet j} \beta_j + \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} \gamma_{ij} \right) - 2 \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} \\
\hat{\mu} &= \frac{1}{n} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} -  \sum_{i=1}^{a} \frac{n_{i \bullet}}{n} \alpha_i - \sum_{j=1}^{b} \frac{n_{\bullet j}}{n} \beta_j - \sum_{i=1}^{a} \sum_{j=1}^{b} \frac{n_{ij}}{n} \gamma_{ij} \\
&\overset{\eqref{eq:anova2-ols-samp-size}}{=} \frac{1}{n} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} - \sum_{j=1}^{b} \sum_{i=1}^{a} \frac{n_{ij}}{n} \alpha_i - \sum_{i=1}^{a} \sum_{j=1}^{b} \frac{n_{ij}}{n} \beta_j - \sum_{i=1}^{a} \sum_{j=1}^{b} \frac{n_{ij}}{n} \gamma_{ij} \\
&\overset{\eqref{eq:anova2-ols-anova2-cons}}{=} \frac{1}{n} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} \\
&\overset{\eqref{eq:anova2-ols-mean-samp}}{=} \bar{y}_{\bullet \bullet \bullet}
\end{split}
\end{equation}

\begin{equation} \label{eq:anova2-ols-rss-der-alpha-zero}
\begin{split}
0 &= 2 n_{i \bullet} \hat{\mu} + 2 n_{i \bullet} \hat{\alpha}_i + 2 \left( \sum_{j=1}^{b} n_{ij} \beta_j + \sum_{j=1}^{b} n_{ij} \gamma_{ij} \right) - 2 \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} \\
\hat{\alpha}_i &= \frac{1}{n_{i \bullet}} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} - \hat{\mu} - \sum_{j=1}^{b} \frac{n_{ij}}{n_{i \bullet}} \beta_j - \sum_{j=1}^{b} \frac{n_{ij}}{n_{i \bullet}} \gamma_{ij} \\
&= \frac{1}{n_{i \bullet}} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} - \hat{\mu} - \frac{n}{n_{i \bullet}} \sum_{j=1}^{b} \frac{n_{ij}}{n} \beta_j - \frac{n}{n_{i \bullet}} \sum_{j=1}^{b} \frac{n_{ij}}{n} \gamma_{ij} \\
&\overset{\eqref{eq:anova2-ols-anova2-cons}}{=} \frac{1}{n_{i \bullet}} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} - \frac{1}{n} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} \\
&\overset{\eqref{eq:anova2-ols-mean-samp}}{=} \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}
\end{split}
\end{equation}

\begin{equation} \label{eq:anova2-ols-rss-der-beta-zero}
\begin{split}
0 &= 2 n_{\bullet j} \hat{\mu} + 2 n_{\bullet j} \hat{\beta}_j + 2 \left( \sum_{i=1}^{a} n_{ij} \alpha_i + \sum_{i=1}^{a} n_{ij} \gamma_{ij} \right) - 2 \sum_{i=1}^{a} \sum_{k=1}^{n_{ij}} y_{ijk} \\
\hat{\beta}_j &= \frac{1}{n_{\bullet j}} \sum_{i=1}^{a} \sum_{k=1}^{n_{ij}} y_{ijk} - \hat{\mu} - \sum_{i=1}^{a} \frac{n_{ij}}{n_{\bullet j}} \alpha_i - \sum_{i=1}^{a} \frac{n_{ij}}{n_{\bullet j}} \gamma_{ij} \\
&= \frac{1}{n_{\bullet j}} \sum_{i=1}^{a} \sum_{k=1}^{n_{ij}} y_{ijk} - \hat{\mu} - \frac{n}{n_{\bullet j}} \sum_{i=1}^{a} \frac{n_{ij}}{n} \alpha_i - \frac{n}{n_{\bullet j}} \sum_{i=1}^{a} \frac{n_{ij}}{n} \gamma_{ij} \\
&\overset{\eqref{eq:anova2-ols-anova2-cons}}{=} \frac{1}{n_{\bullet j}} \sum_{i=1}^{a} \sum_{k=1}^{n_{ij}} y_{ijk} - \frac{1}{n} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} \\
&\overset{\eqref{eq:anova2-ols-mean-samp}}{=} \bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}
\end{split}
\end{equation}

\begin{equation} \label{eq:anova2-ols-rss-der-gamma-zero}
\begin{split}
0 &= 2 n_{ij} (\hat{\mu} + \hat{\alpha}_i + \hat{\beta}_j + \hat{\gamma_{ij}}) - 2 \sum_{k=1}^{n_{ij}} y_{ijk} \\
\hat{\gamma_{ij}} &= \frac{1}{n_{ij}} \sum_{k=1}^{n_{ij}} y_{ijk} - \hat{\alpha}_i - \hat{\beta}_j - \hat{\mu} \\
&= \frac{1}{n_{ij}} \sum_{k=1}^{n_{ij}} y_{ijk} - \frac{1}{n_{i \bullet}} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} - \frac{1}{n_{\bullet j}} \sum_{i=1}^{a} \sum_{k=1}^{n_{ij}} y_{ijk} + \frac{1}{n} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} \\
&\overset{\eqref{eq:anova2-ols-mean-samp}}{=} \bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Olbricht, Gayla R. (2011): "Two-Way ANOVA: Interaction"; in: \textit{Stat 512: Applied Regression Analysis}, Purdue University, Spring 2011, Lect. 27; URL: \url{https://www.stat.purdue.edu/~ghobbs/STAT_512/Lecture_Notes/ANOVA/Topic_27.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Sums of squares in two-way ANOVA}]{Sums of squares in two-way ANOVA} \label{sec:anova2-pss}
\setcounter{equation}{0}

\textbf{Theorem:} Given two-way analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}),

\begin{equation} \label{eq:anova2-pss-anova2}
y_{ijk} = \mu + \alpha_i + \beta_j + \gamma_{ij} + \varepsilon_{ijk}, \; \varepsilon_{ijk} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)
\end{equation}

sums of squares can be partitioned as follows

\begin{equation} \label{eq:anova2-pss-anova2-pss}
\mathrm{SS}_\mathrm{tot} = \mathrm{SS}_{A} + \mathrm{SS}_{B} + \mathrm{SS}_{A \times B} + \mathrm{SS}_\mathrm{res}
\end{equation}

where $\mathrm{SS}\_\mathrm{tot}$ is the total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}), $\mathrm{SS}\_{A}$, $\mathrm{SS}\_{B}$ and $\mathrm{SS}\_{A \times B}$ are treatment ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:trss}) and interaction sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:iass}) (summing into the explained sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess})) and $\mathrm{SS}\_\mathrm{res}$ is the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}).


\vspace{1em}
\textbf{Proof:} The total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}) for two-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) is given by

\begin{equation} \label{eq:anova2-pss-anova2-tss}
\mathrm{SS}_\mathrm{tot} = \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{\bullet \bullet \bullet})^2
\end{equation}

where $\bar{y}\_{\bullet \bullet \bullet}$ is the mean across all values $y\_{ijk}$. This can be rewritten as

\begin{equation} \label{eq:anova2-pss-anova2-pss-s1}
\begin{split}
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{\bullet \bullet \bullet})^2 = \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} & \left[ (y_{ijk} - \bar{y}_{i j \bullet}) + (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}) + (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}) + \right. \\
\\ & \left. (\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}) \right]^2 \\
\end{split}
\end{equation}

It can be shown ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-cochran}) that the following sums are all zero:

\begin{equation} \label{eq:anova2-pss-anova2-pss-s2}
\begin{split}
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet}) &= 0 \\
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}) &= 0 \\
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}) &= 0 \\
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}) &= 0 \; .
\end{split}
\end{equation}

This means that the sum in \eqref{eq:anova2-pss-anova2-pss-s1} reduces to

\begin{equation} \label{eq:anova2-pss-anova2-pss-s3}
\begin{split}
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{\bullet \bullet \bullet})^2 = \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} & \left[ (y_{ijk} - \bar{y}_{i j \bullet})^2 + (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet})^2 + (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet})^2 + \right. \\
\\ & \left. (\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet})^2 \right] \\
= \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} & (y_{ijk} - \bar{y}_{i j \bullet})^2 + \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet})^2 + \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet})^2 + \\
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} & (\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet})^2 \; .
\end{split}
\end{equation}

With the treatment sums of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:trss})

\begin{equation} \label{eq:anova2-pss-anova2-trss}
\begin{split}
\mathrm{SS}_A &= \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet})^2 \\
\mathrm{SS}_B &= \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet})^2 \; ,
\end{split}
\end{equation}

the interaction sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:iass})

\begin{equation} \label{eq:anova2-pss-anova2-iass}
\mathrm{SS}_\mathrm{A \times B} = \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet})^2
\end{equation}

and the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) for two-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2})

\begin{equation} \label{eq:anova2-pss-anova2-rss}
\mathrm{SS}_\mathrm{res} = \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2 \; ,
\end{equation}

we finally have:

\begin{equation} \label{eq:anova2-pss-anova2-pss-qed}
\mathrm{SS}_\mathrm{tot} = \mathrm{SS}_{A} + \mathrm{SS}_{B} + \mathrm{SS}_{A \times B} + \mathrm{SS}_\mathrm{res} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Nandy, Siddhartha (2018): "Two-Way Analysis of Variance"; in: \textit{Stat 512: Applied Regression Analysis}, Purdue University, Summer 2018, Ch. 19; URL: \url{https://www.stat.purdue.edu/~snandy/stat512/topic7.pdf}.
\item Wikipedia (2022): "Analysis of variance"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-11-15; URL: \url{https://en.wikipedia.org/wiki/Analysis_of_variance#Partitioning_of_the_sum_of_squares}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Cochran's theorem for two-way ANOVA}]{Cochran's theorem for two-way ANOVA} \label{sec:anova2-cochran}
\setcounter{equation}{0}

\textbf{Theorem:} Assume the two-way analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) model

\begin{equation} \label{eq:anova2-cochran-anova2}
\begin{split}
y_{ijk} &= \mu + \alpha_i + \beta_j + \gamma_{ij} + \varepsilon_{ijk} \\
\varepsilon_{ijk} &\overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2), \; i = 1, \ldots, a, \; j = 1, \ldots, b, \; k = 1, \dots, n_{ij}
\end{split}
\end{equation}

under the well-known constraints for the model parameters ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2})

\begin{equation} \label{eq:anova2-cochran-anova2-constr}
\begin{split}
\sum_{i=1}^{a} \frac{n_{ij}}{n} \alpha_i &= 0 \quad \text{for all} \quad j = 1, \ldots, b \\
\sum_{j=1}^{b} \frac{n_{ij}}{n} \beta_j &= 0 \quad \text{for all} \quad i = 1, \ldots, a \\
\sum_{i=1}^{a} \frac{n_{ij}}{n} \gamma_{ij} &= 0 \quad \text{for all} \quad j = 1, \ldots, b \\
\sum_{j=1}^{b} \frac{n_{ij}}{n} \gamma_{ij} &= 0 \quad \text{for all} \quad i = 1, \ldots, a \; .
\end{split}
\end{equation}

Then, the following sums of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-pss}) are chi-square distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2})

\begin{equation} \label{eq:anova2-cochran-anova2-cochran}
\begin{split}
\frac{1}{\sigma^2} n (\bar{y}_{\bullet \bullet \bullet} - \mu)^2 = \frac{\mathrm{SS}_M}{\sigma^2} &\sim \chi^2(1) \\
\frac{1}{\sigma^2} \sum_{i=1}^{a} n_{i \bullet} ([\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \alpha_i)^2 = \frac{\mathrm{SS}_A}{\sigma^2} &\sim \chi^2(a-1) \\
\frac{1}{\sigma^2} \sum_{j=1}^{b} n_{\bullet j} ([\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \beta_j)^2 = \frac{\mathrm{SS}_B}{\sigma^2} &\sim \chi^2(b-1) \\
\frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} ([\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}] - \gamma_{ij})^2 = \frac{\mathrm{SS}_{A \times B}}{\sigma^2} &\sim \chi^2\left( (a-1)(b-1) \right) \\
\frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2 = \frac{\mathrm{SS}_\mathrm{res}}{\sigma^2} &\sim \chi^2(n-ab) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} Denote sample sizes as

\begin{equation} \label{eq:anova2-cochran-samp-size}
\begin{split}
n_{ij} &- \text{number of samples in category} \; (i,j) \\
n_{i \bullet} &= \sum_{j=1}^{b} n_{ij} \\
n_{\bullet j} &= \sum_{i=1}^{a} n_{ij} \\
n &= \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij}
\end{split}
\end{equation}

and denote sample means as

\begin{equation} \label{eq:anova2-cochran-mean-samp}
\begin{split}
\bar{y}_{\bullet \bullet \bullet} &= \frac{1}{n} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} \\
\bar{y}_{i \bullet \bullet} &= \frac{1}{n_{i \bullet}} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} \\
\bar{y}_{\bullet j \bullet} &= \frac{1}{n_{\bullet j}} \sum_{i=1}^{a} \sum_{k=1}^{n_{ij}} y_{ijk} \\
\bar{y}_{i j \bullet} &= \frac{1}{n_{ij}} \sum_{k=1}^{n_{ij}} y_{ijk} \; .
\end{split}
\end{equation}

According to the model given by \eqref{eq:anova2-cochran-anova2}, the observations are distributed as:

\begin{equation} \label{eq:anova2-cochran-yijk-h0}
y_{ijk} \sim \mathcal{N}(\mu + \alpha_i + \beta_j + \gamma_{ij}, \sigma^2) \quad \text{for all} \quad i, j, k \; .
\end{equation}

Thus, the random variable ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) $U_{ijk} = (y_{ijk} - \mu - \alpha_i - \beta_j - \gamma_{ij})/\sigma$ follows a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-snorm})

\begin{equation} \label{eq:anova2-cochran-Uijk-h0}
U_{ijk} = \frac{y_{ijk} - \mu - \alpha_i - \beta_j - \gamma_{ij}}{\sigma} \sim \mathcal{N}(0, 1) \; .
\end{equation}

Now consider the following sum

\begin{equation} \label{eq:anova2-cochran-sum-Uijk-s1}
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} U_{ijk}^2 = \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} \left( \frac{y_{ijk} - \mu - \alpha_i - \beta_j - \gamma_{ij}}{\sigma} \right)^2 \\
\end{equation}

which can be rewritten as follows:

\begin{equation} \label{eq:anova2-cochran-sum-Uijk-s2}
\begin{split}
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} U_{ijk}^2 = \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} & \left[ (y_{ijk} - \mu - \alpha_i - \beta_j - \gamma_{ij}) - \right. \\
&\left. [\bar{y}_{\bullet \bullet \bullet} + (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}) + (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}) + (\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet})] \right. + \\
&\left. [\bar{y}_{\bullet \bullet \bullet} + (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}) + (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}) + (\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet})] \right]^2 \\
= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} & \left[ (y_{ijk} - [\bar{y}_{\bullet \bullet \bullet} + (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}) + (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}) + (\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet})]) + \right. \\
&\left. (\bar{y}_{\bullet \bullet \bullet} - \mu) + ([\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \alpha_i) + ([\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \beta_j) \right. + \\
&\left. ([\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}] - \gamma_{ij}) \right]^2 \\
= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} & \left[ (y_{ijk} - \bar{y}_{i j \bullet}) + (\bar{y}_{\bullet \bullet \bullet} - \mu) + ([\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \alpha_i) + \right. \\
&\left. ([\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \beta_j) + ([\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}] - \gamma_{ij}) \right]^2 \; .
\end{split}
\end{equation}

Note that the following sums are all zero:

\begin{equation} \label{eq:anova2-cochran-sum-Uijk-s3a}
\begin{split}
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet}) &= \sum_{i=1}^{a} \sum_{j=1}^{b} \left[ \sum_{k=1}^{n_{ij}} y_{ijk} - n_{ij} \cdot \bar{y}_{i j \bullet} \right] \\
&\overset{\eqref{eq:anova2-cochran-mean-samp}}{=} \sum_{i=1}^{a} \sum_{j=1}^{b} \left[ \sum_{k=1}^{n_{ij}} y_{ijk} - n_{ij} \cdot \frac{1}{n_{ij}} \sum_{k=1}^{n_{ij}} y_{ijk} \right] \\
&= \sum_{i=1}^{a} \sum_{j=1}^{b} 0 = 0
\end{split}
\end{equation}

\begin{equation} \label{eq:anova2-cochran-sum-Uijk-s3b}
\begin{split}
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} ([\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \alpha_i) &= \sum_{i=1}^{a} n_{i \bullet} \cdot (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet} - \alpha_i) \\
&= \sum_{i=1}^{a} n_{i \bullet} \cdot \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet} \sum_{i=1}^{a} n_{i \bullet} - \sum_{i=1}^{a} n_{i \bullet} \alpha_i \\
&\overset{\eqref{eq:anova2-cochran-mean-samp}}{=} \sum_{i=1}^{a} n_{i \bullet} \cdot \frac{1}{n_{i \bullet}} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} - n \cdot \frac{1}{n} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} - \sum_{i=1}^{a} n_{i \bullet} \alpha_i \\
&= - \sum_{i=1}^{a} n_{i \bullet} \alpha_i \overset{\eqref{eq:anova2-cochran-samp-size}}{=} - n \sum_{i=1}^{a} \sum_{j=1}^{b} \frac{n_{ij}}{n} \alpha_i \overset{\eqref{eq:anova2-cochran-anova2-constr}}{=} 0
\end{split}
\end{equation}

\begin{equation} \label{eq:anova2-cochran-sum-Uijk-s3c}
\begin{split}
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} ([\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \beta_j) &= \sum_{j=1}^{b} n_{\bullet j} \cdot (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet} - \beta_j) \\
&= \sum_{j=1}^{b} n_{\bullet j} \cdot \bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet} \sum_{j=1}^{b} n_{\bullet j} - \sum_{j=1}^{b} n_{\bullet j} \beta_j \\
&\overset{\eqref{eq:anova2-cochran-mean-samp}}{=} \sum_{j=1}^{b} n_{\bullet j} \cdot \frac{1}{n_{\bullet j}} \sum_{i=1}^{a} \sum_{k=1}^{n_{ij}} y_{ijk} - n \cdot \frac{1}{n} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} - \sum_{j=1}^{b} n_{\bullet j} \beta_j \\
&= - \sum_{j=1}^{b} n_{\bullet j} \beta_j \overset{\eqref{eq:anova2-cochran-samp-size}}{=} - n \sum_{j=1}^{b} \sum_{i=1}^{a} \frac{n_{ij}}{n} \beta_j \overset{\eqref{eq:anova2-cochran-anova2-constr}}{=} 0
\end{split}
\end{equation}

\begin{equation} \label{eq:anova2-cochran-sum-Uijk-s3d}
\begin{split}
&\hphantom{=} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} ([\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}] - \gamma_{ij}) \\
&= \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} \left[ (\bar{y}_{i j \bullet} - \bar{y}_{\bullet \bullet \bullet}) - (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}) - (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}) - \gamma_{ij} \right] \\
&= \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{i j \bullet} - \bar{y}_{\bullet \bullet \bullet} - \gamma_{ij}) - \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}) - \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}) \\
&\overset{\eqref{eq:anova2-cochran-sum-Uijk-s3c}}{=} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{i j \bullet} - \bar{y}_{\bullet \bullet \bullet} - \gamma_{ij}) - \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}) \\
&\overset{\eqref{eq:anova2-cochran-sum-Uijk-s3b}}{=} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{i j \bullet} - \bar{y}_{\bullet \bullet \bullet} - \gamma_{ij}) \\
&= \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} \bar{y}_{i j \bullet} - \bar{y}_{\bullet \bullet \bullet} \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} - \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} \gamma_{ij} \\
&\overset{\eqref{eq:anova2-cochran-mean-samp}}{=} \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} \cdot \frac{1}{n_{ij}} \sum_{k=1}^{n_{ij}} y_{ijk} - n \cdot \frac{1}{n} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} - \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} \gamma_{ij} \\
&= - \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} \gamma_{ij} = - \frac{1}{n} \sum_{i=1}^{a} \sum_{j=1}^{b} \frac{n_{ij}}{n} \gamma_{ij} \overset{\eqref{eq:anova2-cochran-anova2-constr}}{=} 0 \; .
\end{split}
\end{equation}

Note further that $\bar{y}_{\bullet \bullet \bullet}$ and $\mu$ are not dependent on $i$, $j$ and $k$:

\begin{equation} \label{eq:anova2-cochran-yb-mu-const}
\bar{y}_{\bullet \bullet \bullet} = \text{const.} \quad \text{and} \quad \mu = \text{const.}
\end{equation}

Thus, all the non-square products in \eqref{eq:anova2-cochran-sum-Uijk-s2} disappear and the sum reduces to

\begin{equation} \label{eq:anova2-cochran-sum-Uijk-s4}
\begin{split}
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} U_{ijk}^2 = \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} & \left[ (y_{ijk} - \bar{y}_{i j \bullet})^2 + (\bar{y}_{\bullet \bullet \bullet} - \mu)^2 + ([\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \alpha_i)^2 + \right. \\
&\left. ([\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \beta_j)^2 + ([\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}] - \gamma_{ij})^2 \right] \\
= \frac{1}{\sigma^2} \left[ \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} \right. & (y_{ijk} - \bar{y}_{i j \bullet})^2 + \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{\bullet \bullet \bullet} - \mu)^2 + \\
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} &\left. ([\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \alpha_i)^2 + \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} ([\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \beta_j)^2 + \right. \\
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} &\left. ([\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}] - \gamma_{ij})^2 \right] \; .
\end{split}
\end{equation}

Cochran's theorem states that, if a sum of squared standard normal ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) can be written as a sum of squared forms

\begin{equation} \label{eq:anova2-cochran-cochran-p1}
\begin{split}
\sum_{i=1}^{n} U_i^2 = \sum_{j=1}^{m} Q_j \quad &\text{where} \quad Q_j = U^\mathrm{T} B^{(j)} U \\
&\text{with} \quad \sum_{j=1}^{m} B^{(j)} = I_n \\
&\text{and} \quad r_j = \mathrm{rank}(B^{(j)}) \; ,
\end{split}
\end{equation}

then the terms $Q_j$ are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) and each term $Q_j$ follows a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) with $r_j$ degrees of freedom:

\begin{equation} \label{eq:anova2-cochran-cochran-p2}
Q_j \sim \chi^2(r_j), \; j = 1, \ldots, m \; .
\end{equation}

First, we define the $n \times 1$ vector $U$:

\begin{equation} \label{eq:anova2-cochran-U}
U = \left[ \begin{matrix} u_{1 \bullet} \\ \vdots \\ u_{a \bullet} \end{matrix} \right] \quad \text{where} \quad u_{i \bullet} = \left[ \begin{matrix} u_{i1} \\ \vdots \\ u_{ib} \end{matrix} \right] \quad \text{where} \quad u_{ij} = \left[ \begin{matrix} (y_{i,j,1} - \mu - \alpha_i - \beta_j - \gamma_{ij})/\sigma \\ \vdots \\ (y_{i,j,n_{ij}} - \mu - \alpha_i - \beta_j - \gamma_{ij})/\sigma \end{matrix} \right] \; .
\end{equation}

Next, we specify the $n \times n$ matrices $B$

\begin{equation} \label{eq:anova2-cochran-B}
\begin{split}
B^{(1)} &= I_n - \mathrm{diag}\left[ \mathrm{diag}\left( \frac{1}{n_{11}} J_{n_{11}}, \; \ldots, \; \frac{1}{n_{1b}} J_{n_{1b}} \right), \; \ldots, \; \mathrm{diag}\left( \frac{1}{n_{a1}} J_{n_{a1}}, \; \ldots, \; \frac{1}{n_{ab}} J_{n_{ab}} \right) \right] \\
B^{(2)} &= \frac{1}{n} J_n \\
B^{(3)} &= \mathrm{diag}\left( \frac{1}{n_{1 \bullet}} J_{n_{1 \bullet}}, \; \ldots, \; \frac{1}{n_{a \bullet}} J_{n_{a \bullet}} \right) - \frac{1}{n} J_n \\
B^{(4)} &= M_B - \frac{1}{n} J_n \\
B^{(5)} &= \mathrm{diag}\left[ \mathrm{diag}\left( \frac{1}{n_{11}} J_{n_{11}}, \; \ldots, \; \frac{1}{n_{1b}} J_{n_{1b}} \right), \; \ldots, \; \mathrm{diag}\left( \frac{1}{n_{a1}} J_{n_{a1}}, \; \ldots, \; \frac{1}{n_{ab}} J_{n_{ab}} \right) \right] \\
&- \mathrm{diag}\left( \frac{1}{n_{1 \bullet}} J_{n_{1 \bullet}}, \; \ldots, \; \frac{1}{n_{a \bullet}} J_{n_{a \bullet}} \right) - M_B + \frac{1}{n} J_n
\end{split}
\end{equation}

with the factor B matrix $M_B$ given by

\begin{equation} \label{eq:anova2-cochran-MB}
M_B = \left[ \begin{matrix} \mathrm{diag}\left( \frac{1}{n_{\bullet 1}} J_{n_{11},n_{11}}, \; \ldots, \; \frac{1}{n_{\bullet b}} J_{n_{1b},n_{1b}} \right) & \cdots & \mathrm{diag}\left( \frac{1}{n_{\bullet 1}} J_{n_{11},n_{a1}}, \; \ldots, \; \frac{1}{n_{\bullet b}} J_{n_{1b},n_{ab}} \right) \\ \vdots & \ddots & \vdots \\ \mathrm{diag}\left( \frac{1}{n_{\bullet 1}} J_{n_{a1},n_{11}}, \; \ldots, \; \frac{1}{n_{\bullet b}} J_{n_{ab},n_{1b}} \right) & \cdots & \mathrm{diag}\left( \frac{1}{n_{\bullet 1}} J_{n_{a1},n_{a1}}, \; \ldots, \; \frac{1}{n_{\bullet b}} J_{n_{ab},n_{ab}} \right) \end{matrix} \right] \; .
\end{equation}

where $J_n$ is an $n \times n$ matrix of ones, $J_{n,m}$ is an $n \times m$ matrix of ones and $\mathrm{diag}\left( A_1, \ldots, A_n \right)$ denotes a block-diagonal matrix composed of $A_1, \ldots, A_n$. We observe that those matrices satisfy

\begin{equation} \label{eq:anova2-cochran-U-Q-B}
\sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} U_{ijk}^2 = \sum_{l=1}^{5} Q_l = \sum_{l=1}^{5} U^\mathrm{T} B^{(l)} U
\end{equation}

as well as

\begin{equation} \label{eq:anova2-cochran-B-In}
\sum_{l=1}^{5} B^{(l)} = I_n
\end{equation}

and their ranks are

\begin{equation} \label{eq:anova2-cochran-B-rk}
\begin{split}
\mathrm{rank}\left( B^{(1)} \right) &= n - a b \\
\mathrm{rank}\left( B^{(2)} \right) &= 1 \\
\mathrm{rank}\left( B^{(3)} \right) &= a - 1 \\
\mathrm{rank}\left( B^{(4)} \right) &= b - 1 \\
\mathrm{rank}\left( B^{(5)} \right) &= (a-1)(b-1) \; .
\end{split}
\end{equation}

Thus, the conditions for applying Cochran's theorem given by \eqref{eq:anova2-cochran-cochran-p1} are fulfilled and we can use \eqref{eq:anova2-cochran-sum-Uijk-s4}, \eqref{eq:anova2-cochran-cochran-p2}, \eqref{eq:anova2-cochran-B} and \eqref{eq:anova2-cochran-B-rk} to conclude that

\begin{equation} \label{eq:anova2-cochran-anova2-cochran-s1}
\begin{split}
\frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{\bullet \bullet \bullet} - \mu)^2 &= Q_2 = U^\mathrm{T} B^{(2)} U \sim \chi^2(1) \\
\frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} ([\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \alpha_i)^2 &= Q_3 = U^\mathrm{T} B^{(3)} U \sim \chi^2(a-1) \\
\frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} ([\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \beta_j)^2 &= Q_4 = U^\mathrm{T} B^{(4)} U \sim \chi^2(b-1) \\
\frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} ([\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}] - \gamma_{ij})^2 &= Q_5 = U^\mathrm{T} B^{(5)} U \sim \chi^2\left( (a-1)(b-1) \right) \\
\frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2 &= Q_1 = U^\mathrm{T} B^{(1)} U \sim \chi^2(n-ab) \; .
\end{split}
\end{equation}

Finally, we identify the terms $Q$ with sums of squares in two-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-pss}) and simplify them to reach the expressions given by \eqref{eq:anova2-cochran-anova2-cochran}:

\begin{equation} \label{eq:anova2-cochran-anova2-cochran-s2}
\begin{split}
\frac{\mathrm{SS}_M}{\sigma^2} &= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{\bullet \bullet \bullet} - \mu)^2 = \frac{1}{\sigma^2} n (\bar{y}_{\bullet \bullet \bullet} - \mu)^2 \\
\frac{\mathrm{SS}_A}{\sigma^2} &= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} ([\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \alpha_i)^2 = \frac{1}{\sigma^2} \sum_{i=1}^{a} n_{i \bullet} ([\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \alpha_i)^2 \\
\frac{\mathrm{SS}_B}{\sigma^2} &= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} ([\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \beta_j)^2 = \frac{1}{\sigma^2} \sum_{j=1}^{b} n_{\bullet j} ([\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \beta_j)^2 \\
\frac{\mathrm{SS}_{A \times B}}{\sigma^2} &= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} ([\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}] - \gamma_{ij})^2 = \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} ([\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}] - \gamma_{ij})^2 \\
\frac{\mathrm{SS}_\mathrm{res}}{\sigma^2} &= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2 = \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Nandy, Siddhartha (2018): "Two-Way Analysis of Variance"; in: \textit{Stat 512: Applied Regression Analysis}, Purdue University, Summer 2018, Ch. 19; URL: \url{https://www.stat.purdue.edu/~snandy/stat512/topic7.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{F-test for main effect in two-way ANOVA}]{F-test for main effect in two-way ANOVA} \label{sec:anova2-fme}
\setcounter{equation}{0}

\textbf{Theorem:} Assume the two-way analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) model

\begin{equation} \label{eq:anova2-fme-anova2}
\begin{split}
y_{ijk} &= \mu + \alpha_i + \beta_j + \gamma_{ij} + \varepsilon_{ijk} \\
\varepsilon_{ijk} &\overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2), \; i = 1, \ldots, a, \; j = 1, \ldots, b, \; k = 1, \dots, n_{ij} \; .
\end{split}
\end{equation}

Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:anova2-fme-anova2-fme-A}
F_A = \frac{\frac{1}{a-1} \sum_{i=1}^{a} n_{i \bullet} (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet})^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2}
\end{equation}

follows an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f})

\begin{equation} \label{eq:anova2-fme-anova2-fme-h0-A}
F_A \sim \mathrm{F}(a-1, n-ab)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) for the main effect ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) of factor A

\begin{equation} \label{eq:anova2-fme-anova2-h0-A}
\begin{split}
H_0: &\; \alpha_1 = \ldots = \alpha_a = 0 \\
H_1: &\; \alpha_i \neq 0 \quad \text{for at least one} \quad i \in \left\lbrace 1, \ldots, a \right\rbrace
\end{split}
\end{equation}

and the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:anova2-fme-anova2-fme-B}
F_B = \frac{\frac{1}{b-1} \sum_{j=1}^{b} n_{\bullet j} (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet})^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2}
\end{equation}

follows an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f})

\begin{equation} \label{eq:anova2-fme-anova2-fme-h0-B}
F_B \sim \mathrm{F}(b-1, n-ab)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) for the main effect ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) of factor B

\begin{equation} \label{eq:anova2-fme-anova2-h0-B}
\begin{split}
H_0: &\; \beta_1 = \ldots = \beta_b = 0 \\
H_1: &\; \beta_j \neq 0 \quad \text{for at least one} \quad j \in \left\lbrace 1, \ldots, b \right\rbrace \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} Applying Cochran's theorem for two-analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-cochran}), we find that the following squared sums

\begin{equation} \label{eq:anova2-fme-anova2-ss-dist}
\begin{split}
\frac{\mathrm{SS}_A}{\sigma^2} &= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} ([\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \alpha_i)^2 = \frac{1}{\sigma^2} \sum_{i=1}^{a} n_{i \bullet} ([\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \alpha_i)^2 \\
\frac{\mathrm{SS}_B}{\sigma^2} &= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} ([\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \beta_j)^2 = \frac{1}{\sigma^2} \sum_{j=1}^{b} n_{\bullet j} ([\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \beta_j)^2 \\
\frac{\mathrm{SS}_\mathrm{res}}{\sigma^2} &= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2 = \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2
\end{split}
\end{equation}

are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) and chi-squared distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}):

\begin{equation} \label{eq:anova2-fme-anova2-cochran-s1}
\begin{split}
\frac{\mathrm{SS}_A}{\sigma^2} &\sim \chi^2(a-1) \\
\frac{\mathrm{SS}_B}{\sigma^2} &\sim \chi^2(b-1) \\
\frac{\mathrm{SS}_\mathrm{res}}{\sigma^2} &\sim \chi^2(n-ab) \; .
\end{split}
\end{equation}

1) Thus, the F-statistic from \eqref{eq:anova2-fme-anova2-fme-A} is equal to the ratio of two independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) chi-squared distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) divided by their degrees of freedom

\begin{equation} \label{eq:anova2-fme-anova2-fme-A-ss}
\begin{split}
F_A &= \frac{(\mathrm{SS}_A/\sigma^2)/(a-1)}{(\mathrm{SS}_\mathrm{res}/\sigma^2)/(n-ab)} \\
&= \frac{\mathrm{SS}_A/(a-1)}{\mathrm{SS}_\mathrm{res}/(n-ab)} \\
&\overset{\eqref{eq:anova2-fme-anova2-ss-dist}}{=} \frac{\frac{1}{a-1} \sum_{i=1}^{a} n_{i \bullet} ([\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \alpha_i)^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2} \\
&\overset{\eqref{eq:anova2-fme-anova2-h0-A}}{=} \frac{\frac{1}{a-1} \sum_{i=1}^{a} n_{i \bullet} (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet})^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2}
\end{split}
\end{equation}

which, by definition of the F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}), is distributed as

\begin{equation} \label{eq:anova2-fme-anova2-fme-A-qed}
F_A \sim \mathrm{F}(a-1, n-ab)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) for main effect of $A$.

2) Similarly, the F-statistic from \eqref{eq:anova2-fme-anova2-fme-B} is equal to the ratio of two independent chi-squared distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) divided by their degrees of freedom

\begin{equation} \label{eq:anova2-fme-anova2-fme-B-ss}
\begin{split}
F_B &= \frac{(\mathrm{SS}_B/\sigma^2)/(b-1)}{(\mathrm{SS}_\mathrm{res}/\sigma^2)/(n-ab)} \\
&= \frac{\mathrm{SS}_B/(b-1)}{\mathrm{SS}_\mathrm{res}/(n-ab)} \\
&\overset{\eqref{eq:anova2-fme-anova2-ss-dist}}{=} \frac{\frac{1}{b-1} \sum_{j=1}^{b} n_{\bullet j} ([\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}] - \beta_j)^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2} \\
&\overset{\eqref{eq:anova2-fme-anova2-h0-B}}{=} \frac{\frac{1}{b-1} \sum_{j=1}^{b} n_{\bullet j} (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet})^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2}
\end{split}
\end{equation}

which, by definition of the F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}), is distributed as

\begin{equation} \label{eq:anova2-fme-anova2-fme-B-qed}
F_B \sim \mathrm{F}(b-1, n-ab)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) for main effect of $B$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item ttd (2021): "Proof on SSAB/s2~chi2(I-1)(J-1) under the null hypothesis HAB: dij=0 for i=1,...,I and j=1,...,J"; in: \textit{StackExchange CrossValidated}, retrieved on 2022-11-10; URL: \url{https://stats.stackexchange.com/questions/545807/proof-on-ss-ab-sigma2-sim-chi2-i-1j-1-under-the-null-hypothesis}.
\item JohnK (2014): "In a two-way ANOVA, how can the F-statistic for one factor have a central distribution if the null is false for the other factor?"; in: \textit{StackExchange CrossValidated}, retrieved on 2022-11-10; URL: \url{https://stats.stackexchange.com/questions/124166/in-a-two-way-anova-how-can-the-f-statistic-for-one-factor-have-a-central-distri}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{F-test for interaction in two-way ANOVA}]{F-test for interaction in two-way ANOVA} \label{sec:anova2-fia}
\setcounter{equation}{0}

\textbf{Theorem:} Assume the two-way analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) model

\begin{equation} \label{eq:anova2-fia-anova2}
\begin{split}
y_{ijk} &= \mu + \alpha_i + \beta_j + \gamma_{ij} + \varepsilon_{ijk} \\
\varepsilon_{ijk} &\overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2), \; i = 1, \ldots, a, \; j = 1, \ldots, b, \; k = 1, \dots, n_{ij} \; .
\end{split}
\end{equation}

Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:anova2-fia-anova2-fia}
F_{A \times B} = \frac{\frac{1}{(a-1)(b-1)} \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} (\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet})^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2}
\end{equation}

follows an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f})

\begin{equation} \label{eq:anova2-fia-anova2-fia-h0}
F_{A \times B} \sim \mathrm{F}\left( (a-1)(b-1), n-ab \right)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) for the interaction effect ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) of factors A and B

\begin{equation} \label{eq:anova2-fia-anova2-h0}
\begin{split}
H_0: &\; \gamma_{11} = \ldots = \gamma_{ab} = 0 \\
H_1: &\; \gamma_{ij} \neq 0 \quad \text{for at least one} \quad (i,j) \in \left\lbrace 1, \ldots, a \right\rbrace \times \left\lbrace 1, \ldots, b \right\rbrace \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} Applying Cochran's theorem for two-analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-cochran}), we find that the following squared sums

\begin{equation} \label{eq:anova2-fia-anova2-ss-dist}
\begin{split}
\frac{\mathrm{SS}_{A \times B}}{\sigma^2} &= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} ([\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}] - \gamma_{ij})^2 \\
&= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} ([\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}] - \gamma_{ij})^2 \\
\frac{\mathrm{SS}_\mathrm{res}}{\sigma^2} &= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2 \\
&= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2
\end{split}
\end{equation}

are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) and chi-squared distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}):

\begin{equation} \label{eq:anova2-fia-anova2-cochran-s1}
\begin{split}
\frac{\mathrm{SS}_{A \times B}}{\sigma^2} &\sim \chi^2\left( (a-1)(b-1) \right) \\
\frac{\mathrm{SS}_\mathrm{res}}{\sigma^2} &\sim \chi^2(n-ab) \; .
\end{split}
\end{equation}

Thus, the F-statistic from \eqref{eq:anova2-fia-anova2-fia} is equal to the ratio of two independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) chi-squared distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) divided by their degrees of freedom

\begin{equation} \label{eq:anova2-fia-anova2-fia-ess-tss}
\begin{split}
F_{A \times B} &= \frac{(\mathrm{SS}_{A \times B}/\sigma^2)/\left( (a-1)(b-1) \right)}{(\mathrm{SS}_\mathrm{res}/\sigma^2)/(n-ab)} \\
&= \frac{\mathrm{SS}_{A \times B}/\left( (a-1)(b-1) \right)}{\mathrm{SS}_\mathrm{res}/(n-ab)} \\
&\overset{\eqref{eq:anova2-fia-anova2-ss-dist}}{=} \frac{\frac{1}{(a-1)(b-1)} \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} ([\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}] - \gamma_{ij})^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2} \\
&\overset{\eqref{eq:anova2-fia-anova2-fia-h0}}{=} \frac{\frac{1}{(a-1)(b-1)} \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} (\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet})^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2}
\end{split}
\end{equation}

which, by definition of the F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}), is distributed as

\begin{equation} \label{eq:anova2-fia-anova2-fia-qed}
F_{A \times B} \sim \mathrm{F}\left( (a-1)(b-1), n-ab \right)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) for an interaction of A and B.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Nandy, Siddhartha (2018): "Two-Way Analysis of Variance"; in: \textit{Stat 512: Applied Regression Analysis}, Purdue University, Summer 2018, Ch. 19; URL: \url{https://www.stat.purdue.edu/~snandy/stat512/topic7.pdf}.
\item ttd (2021): "Proof on SSAB/s2~chi2(I-1)(J-1) under the null hypothesis HAB: dij=0 for i=1,...,I and j=1,...,J"; in: \textit{StackExchange CrossValidated}, retrieved on 2022-11-10; URL: \url{https://stats.stackexchange.com/questions/545807/proof-on-ss-ab-sigma2-sim-chi2-i-1j-1-under-the-null-hypothesis}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{F-test for grand mean in two-way ANOVA}]{F-test for grand mean in two-way ANOVA} \label{sec:anova2-fgm}
\setcounter{equation}{0}

\textbf{Theorem:} Assume the two-way analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) model

\begin{equation} \label{eq:anova2-fgm-anova2}
\begin{split}
y_{ijk} &= \mu + \alpha_i + \beta_j + \gamma_{ij} + \varepsilon_{ijk} \\
\varepsilon_{ijk} &\overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2), \; i = 1, \ldots, a, \; j = 1, \ldots, b, \; k = 1, \dots, n_{ij} \; .
\end{split}
\end{equation}

Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:anova2-fgm-anova2-fgm}
F_M = \frac{n (\bar{y}_{\bullet \bullet \bullet})^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2}
\end{equation}

follows an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f})

\begin{equation} \label{eq:anova2-fgm-anova2-fgm-h0}
F_M \sim \mathrm{F}\left( 1, n-ab \right)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) for the grand mean ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2})

\begin{equation} \label{eq:anova2-fgm-anova2-h0}
\begin{split}
H_0: &\; \mu = 0 \\
H_1: &\; \mu \neq 0 \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} Applying Cochran's theorem for two-analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-cochran}), we find that the following squared sums

\begin{equation} \label{eq:anova2-fgm-anova2-ss-dist}
\begin{split}
\frac{\mathrm{SS}_M}{\sigma^2} &= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (\bar{y}_{\bullet \bullet \bullet} - \mu)^2 = \frac{1}{\sigma^2} n (\bar{y}_{\bullet \bullet \bullet} - \mu)^2 \\
\frac{\mathrm{SS}_\mathrm{res}}{\sigma^2} &= \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2 = \frac{1}{\sigma^2} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2
\end{split}
\end{equation}

are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) and chi-squared distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}):

\begin{equation} \label{eq:anova2-fgm-anova2-cochran-s1}
\begin{split}
\frac{\mathrm{SS}_M}{\sigma^2} &\sim \chi^2(1) \\
\frac{\mathrm{SS}_\mathrm{res}}{\sigma^2} &\sim \chi^2(n-ab) \; .
\end{split}
\end{equation}

Thus, the F-statistic from \eqref{eq:anova2-fgm-anova2-fgm} is equal to the ratio of two independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) chi-squared distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}) divided by their degrees of freedom

\begin{equation} \label{eq:anova2-fgm-anova2-fgm-ess-tss}
\begin{split}
F_M &= \frac{(\mathrm{SS}_M/\sigma^2)/(1)}{(\mathrm{SS}_\mathrm{res}/\sigma^2)/(n-ab)} \\
&= \frac{\mathrm{SS}_M/(1)}{\mathrm{SS}_\mathrm{res}/(n-ab)} \\
&\overset{\eqref{eq:anova2-fgm-anova2-ss-dist}}{=} \frac{n (\bar{y}_{\bullet \bullet \bullet} - \mu)^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2} \\
&\overset{\eqref{eq:anova2-fgm-anova2-h0}}{=} \frac{n (\bar{y}_{\bullet \bullet \bullet})^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2}
\end{split}
\end{equation}

which, by definition of the F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}), is distributed as

\begin{equation} \label{eq:anova2-fgm-anova2-fia-qed}
F_M \sim \mathrm{F}(1, n-ab)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) for the grand mean.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Nandy, Siddhartha (2018): "Two-Way Analysis of Variance"; in: \textit{Stat 512: Applied Regression Analysis}, Purdue University, Summer 2018, Ch. 19; URL: \url{https://www.stat.purdue.edu/~snandy/stat512/topic7.pdf}.
\item Olbricht, Gayla R. (2011): "Two-Way ANOVA: Interaction"; in: \textit{Stat 512: Applied Regression Analysis}, Purdue University, Spring 2011, Lect. 27; URL: \url{https://www.stat.purdue.edu/~ghobbs/STAT_512/Lecture_Notes/ANOVA/Topic_27.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{F-statistics in terms of OLS estimates}]{F-statistics in terms of OLS estimates} \label{sec:anova2-fols}
\setcounter{equation}{0}

\textbf{Theorem:} Given the two-way analysis of variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) assumption

\begin{equation} \label{eq:anova2-fols-anova2}
y_{ijk} = \mu + \alpha_i + \beta_j + \gamma_{ij} + \varepsilon_{ijk}, \; \varepsilon_{ijk} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; ,
\end{equation}

the F-statistics for the grand mean ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-fgm}), the main effects ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-fme}) and the interaction ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-fia}) can be expressed in terms of ordinary least squares parameter estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-ols}) as

\begin{equation} \label{eq:anova2-fols-anova2-fols}
\begin{split}
F_M &= \frac{n \hat{\mu}^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \hat{y}_{ijk})^2} \\
F_A &= \frac{\frac{1}{a-1} \sum_{i=1}^{a} n_{i \bullet} \hat{\alpha}_i^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \hat{y}_{ijk})^2} \\
F_B &= \frac{\frac{1}{b-1} \sum_{j=1}^{b} n_{\bullet j} \hat{\beta}_j^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \hat{y}_{ijk})^2} \\
F_{A \times B} &= \frac{\frac{1}{(a-1)(b-1)} \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} \hat{\gamma}_{ij}^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \hat{y}_{ijk})^2}
\end{split}
\end{equation}

where the predicted values $\hat{y}_{ijk}$ are given by

\begin{equation} \label{eq:anova2-fols-y-est}
\hat{y}_{ijk} = \hat{\mu} + \hat{\alpha}_i + \hat{\beta}_j + \hat{\gamma}_{ij} \; .
\end{equation}


\textbf{Theorem:} The F-statistics for the grand mean ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-fgm}), the main effects ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-fme}) and the interaction ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-fia}) in two-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2}) are calculated as

\begin{equation} \label{eq:anova2-fols-anova2-f}
\begin{split}
F_M &= \frac{n (\bar{y}_{\bullet \bullet \bullet})^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2} \\
F_A &= \frac{\frac{1}{a-1} \sum_{i=1}^{a} n_{i \bullet} (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet})^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2} \\
F_B &= \frac{\frac{1}{b-1} \sum_{j=1}^{b} n_{\bullet j} (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet})^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2} \\
F_{A \times B} &= \frac{\frac{1}{(a-1)(b-1)} \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} (\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet})^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \bar{y}_{i j \bullet})^2}
\end{split}
\end{equation}

and the ordinary least squares estimates for two-way ANOVA ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:anova2-ols}) are

\begin{equation} \label{eq:anova2-fols-anova2-ols}
\begin{split}
\hat{\mu} &= \bar{y}_{\bullet \bullet \bullet} \\
\hat{\alpha}_i &= \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet} \\
\hat{\beta}_j &= \bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet} \\
\hat{\gamma}_{ij} &= \bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}
\end{split}
\end{equation}

where the the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) are given by

\begin{equation} \label{eq:anova2-fols-mean-samp}
\begin{split}
\bar{y}_{\bullet \bullet \bullet} &= \frac{1}{n} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} \\
\bar{y}_{i \bullet \bullet} &= \frac{1}{n_{i \bullet}} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} y_{ijk} \\
\bar{y}_{\bullet j \bullet} &= \frac{1}{n_{\bullet j}} \sum_{i=1}^{a} \sum_{k=1}^{n_{ij}} y_{ijk} \\
\bar{y}_{i j \bullet} &= \frac{1}{n_{ij}} \sum_{k=1}^{n_{ij}} y_{ijk} \; .
\end{split}
\end{equation}

We first note that the predicted values can be evaluated as

\begin{equation} \label{eq:anova2-fols-y-est-qed}
\begin{split}
\hat{y}_{ijk} &= \hat{\mu} + \hat{\alpha}_i + \hat{\beta}_j + \hat{\gamma}_{ij} \\
&= \bar{y}_{\bullet \bullet \bullet} + (\bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet \bullet \bullet}) + (\bar{y}_{\bullet j \bullet} - \bar{y}_{\bullet \bullet \bullet}) + (\bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} + \bar{y}_{\bullet \bullet \bullet}) \\
&= \bar{y}_{i \bullet \bullet} + \bar{y}_{\bullet j \bullet} + \bar{y}_{i j \bullet} - \bar{y}_{i \bullet \bullet} - \bar{y}_{\bullet j \bullet} \\
&= \bar{y}_{i j \bullet} \; .
\end{split}
\end{equation}

Substituting this \eqref{eq:anova2-fols-y-est-qed} and the OLS estimates \eqref{eq:anova2-fols-anova2-ols} into the F-formulas \eqref{eq:anova2-fols-anova2-f}, we obtain:

\begin{equation} \label{eq:anova2-fols-anova2-fols-qed}
\begin{split}
F_M &= \frac{n \hat{\mu}^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \hat{y}_{ijk})^2} \\
F_A &= \frac{\frac{1}{a-1} \sum_{i=1}^{a} n_{i \bullet} \hat{\alpha}_i^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \hat{y}_{ijk})^2} \\
F_B &= \frac{\frac{1}{b-1} \sum_{j=1}^{b} n_{\bullet j} \hat{\beta}_j^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \hat{y}_{ijk})^2} \\
F_{A \times B} &= \frac{\frac{1}{(a-1)(b-1)} \sum_{i=1}^{a} \sum_{j=1}^{b} n_{ij} \hat{\gamma}_{ij}^2}{\frac{1}{n-ab} \sum_{i=1}^{a} \sum_{j=1}^{b} \sum_{k=1}^{n_{ij}} (y_{ijk} - \hat{y}_{ijk})^2} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{Simple linear regression}

\subsubsection[\textit{Definition}]{Definition} \label{sec:slr}
\setcounter{equation}{0}

\textbf{Definition:} Let $y$ and $x$ be two $n \times 1$ vectors.

Then, a statement asserting a linear relationship between $x$ and $y$

\begin{equation} \label{eq:slr-slr-model}
y = \beta_0 + \beta_1 x + \varepsilon \; ,
\end{equation}

together with a statement asserting a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) for $\varepsilon$

\begin{equation} \label{eq:slr-slr-noise}
\varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

is called a univariate simple regression model or simply, "simple linear regression".

\begin{itemize}

\item $y$ is called "dependent variable", "measured data" or "signal";

\item $x$ is called "independent variable", "predictor" or "covariate";

\item $V$ is called "covariance matrix" or "covariance structure";

\item $\beta_1$ is called "slope of the regression line ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:regline})";

\item $\beta_0$ is called "intercept of the regression line ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:regline})";

\item $\varepsilon$ is called "noise", "errors" or "error terms";

\item $\sigma^2$ is called "noise variance" or "error variance";

\item $n$ is the number of observations.

\end{itemize}

When the covariance structure $V$ is equal to the $n \times n$ identity matrix, this is called simple linear regression with independent and identically distributed (i.i.d.) observations:

\begin{equation} \label{eq:slr-mlr-noise-iid}
V = I_n \quad \Rightarrow \quad \varepsilon \sim \mathcal{N}(0, \sigma^2 I_n) \quad \Rightarrow \quad \varepsilon_i \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; .
\end{equation}

In this case, the linear regression model can also be written as

\begin{equation} \label{eq:slr-slr-model-sum}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2) \; .
\end{equation}

Otherwise, it is called simple linear regression with correlated observations.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Simple linear regression"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-27; URL: \url{https://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Special case of multiple linear regression}]{Special case of multiple linear regression} \label{sec:slr-mlr}
\setcounter{equation}{0}

\textbf{Theorem:} Simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) is a special case of multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with design matrix $X$ and regression coefficients $\beta$

\begin{equation} \label{eq:slr-mlr-slr-mlr}
X = \left[ \begin{matrix} 1_n & x \end{matrix} \right] \quad \text{and} \quad \beta = \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right]
\end{equation}

where $1_n$ is an $n \times 1$ vector of ones, $x$ is the $n \times 1$ single predictor variable, $\beta_0$ is the intercept and $\beta_1$ is the slope of the regression line ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:regline}).


\vspace{1em}
\textbf{Proof:} Without loss of generality, consider the simple linear regression case with uncorrelated errors ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}):

\begin{equation} \label{eq:slr-mlr-slr}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n \; .
\end{equation}

In matrix notation and using the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}), this can also be written as

\begin{equation} \label{eq:slr-mlr-slr-mlr-s1}
\begin{split}
y &= \beta_0 1_n + \beta_1 x + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, I_n) \\
y &= \left[ \begin{matrix} 1_n & x \end{matrix} \right] \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right] + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, I_n) \; .
\end{split}
\end{equation}

Comparing with the multiple linear regression equations for uncorrelated errors ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}), we finally note:

\begin{equation} \label{eq:slr-mlr-slr-mlr-s3}
y = X\beta + \varepsilon \quad \text{with} \quad X = \left[ \begin{matrix} 1_n & x \end{matrix} \right] \quad \text{and} \quad \beta = \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right] \; .
\end{equation}

In the case of correlated observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}), the error distribution changes to ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}):

\begin{equation} \label{eq:slr-mlr-mlr-noise}
\varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Ordinary least squares}]{Ordinary least squares} \label{sec:slr-ols}
\setcounter{equation}{0}

\textbf{Theorem:} Given a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) with independent observations

\begin{equation} \label{eq:slr-ols-slr}
y = \beta_0 + \beta_1 x + \varepsilon, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n \; ,
\end{equation}

the parameters minimizing the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) are given by

\begin{equation} \label{eq:slr-ols-slr-ols}
\begin{split}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{s_{xy}}{s_x^2}
\end{split}
\end{equation}

where $\bar{x}$ and $\bar{y}$ are the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}), $s_x^2$ is the sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) of $x$ and $s_{xy}$ is the sample covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-samp}) between $x$ and $y$.


\vspace{1em}
\textbf{Proof:} The residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) is defined as

\begin{equation} \label{eq:slr-ols-rss}
\mathrm{RSS}(\beta_0,\beta_1) = \sum_{i=1}^n \varepsilon_i^2 = \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2 \; .
\end{equation}

The derivatives of $\mathrm{RSS}(\beta_0,\beta_1)$ with respect to $\beta_0$ and $\beta_1$ are

\begin{equation} \label{eq:slr-ols-rss-der}
\begin{split}
\frac{\mathrm{d}\mathrm{RSS}(\beta_0,\beta_1)}{\mathrm{d}\beta_0} &= \sum_{i=1}^n 2 (y_i - \beta_0 - \beta_1 x_i) (-1) \\
&= -2 \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i) \\
\frac{\mathrm{d}\mathrm{RSS}(\beta_0,\beta_1)}{\mathrm{d}\beta_1} &= \sum_{i=1}^n 2 (y_i - \beta_0 - \beta_1 x_i) (-x_i) \\
&= -2 \sum_{i=1}^n (x_i y_i - \beta_0 x_i - \beta_1 x_i^2)
\end{split}
\end{equation}

and setting these derivatives to zero

\begin{equation} \label{eq:slr-ols-rss-der-zero}
\begin{split}
0 &= -2 \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \\
0 &= -2 \sum_{i=1}^n (x_i y_i - \hat{\beta}_0 x_i - \hat{\beta}_1 x_i^2)
\end{split}
\end{equation}

yields the following equations:

\begin{equation} \label{eq:slr-ols-slr-norm-eq}
\begin{split}
\hat{\beta}_1 \sum_{i=1}^n x_i + \hat{\beta}_0 \cdot n &= \sum_{i=1}^n y_i \\
\hat{\beta}_1 \sum_{i=1}^n x_i^2 + \hat{\beta}_0 \sum_{i=1}^n x_i &= \sum_{i=1}^n x_i y_i \; .
\end{split}
\end{equation}

From the first equation, we can derive the estimate for the intercept:

\begin{equation} \label{eq:slr-ols-slr-ols-int}
\begin{split}
\hat{\beta}_0 &= \frac{1}{n} \sum_{i=1}^n y_i - \hat{\beta}_1 \cdot \frac{1}{n} \sum_{i=1}^n x_i \\
&= \bar{y} - \hat{\beta}_1 \bar{x} \; .
\end{split}
\end{equation}

From the second equation, we can derive the estimate for the slope:

\begin{equation} \label{eq:slr-ols-slr-ols-sl}
\begin{split}
\hat{\beta}_1 \sum_{i=1}^n x_i^2 + \hat{\beta}_0 \sum_{i=1}^n x_i &= \sum_{i=1}^n x_i y_i \\
\hat{\beta}_1 \sum_{i=1}^n x_i^2 + \left( \bar{y} - \hat{\beta}_1 \bar{x} \right) \sum_{i=1}^n x_i &\overset{\eqref{eq:slr-ols-slr-ols-int}}{=} \sum_{i=1}^n x_i y_i \\
\hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - \bar{x} \sum_{i=1}^n x_i \right) &= \sum_{i=1}^n x_i y_i - \bar{y} \sum_{i=1}^n x_i \\
\hat{\beta}_1 &= \frac{\sum_{i=1}^n x_i y_i - \bar{y} \sum_{i=1}^n x_i}{\sum_{i=1}^n x_i^2 - \bar{x} \sum_{i=1}^n x_i} \; .
\end{split}
\end{equation}

Note that the numerator can be rewritten as

\begin{equation} \label{eq:slr-ols-slr-ols-sl-num}
\begin{split}
\sum_{i=1}^n x_i y_i - \bar{y} \sum_{i=1}^n x_i &= \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} \\
&= \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} - n \bar{x} \bar{y} + n \bar{x} \bar{y} \\
&= \sum_{i=1}^n x_i y_i - \bar{y} \sum_{i=1}^n x_i - \bar{x} \sum_{i=1}^n y_i + \sum_{i=1}^n \bar{x} \bar{y} \\
&= \sum_{i=1}^n \left( x_i y_i - x_i \bar{y} - \bar{x} y_i + \bar{x} \bar{y} \right) \\
&= \sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})
\end{split}
\end{equation}

and that the denominator can be rewritten as

\begin{equation} \label{eq:slr-ols-slr-ols-sl-den}
\begin{split}
\sum_{i=1}^n x_i^2 - \bar{x} \sum_{i=1}^n x_i &= \sum_{i=1}^n x_i^2 - n \bar{x}^2 \\
&= \sum_{i=1}^n x_i^2 - 2 n \bar{x} \bar{x} + n \bar{x}^2 \\
&= \sum_{i=1}^n x_i^2 - 2 \bar{x} \sum_{i=1}^n x_i - \sum_{i=1}^n \bar{x}^2 \\
&= \sum_{i=1}^n \left( x_i^2 - 2 \bar{x} x_i + \bar{x}^2 \right) \\
&= \sum_{i=1}^n (x_i - \bar{x})^2 \; .
\end{split}
\end{equation}

With \eqref{eq:slr-ols-slr-ols-sl-num} and \eqref{eq:slr-ols-slr-ols-sl-den}, the estimate from \eqref{eq:slr-ols-slr-ols-sl} can be simplified as follows:

\begin{equation} \label{eq:slr-ols-slr-ols-sl-qed}
\begin{split}
\hat{\beta}_1 &= \frac{\sum_{i=1}^n x_i y_i - \bar{y} \sum_{i=1}^n x_i}{\sum_{i=1}^n x_i^2 - \bar{x} \sum_{i=1}^n x_i} \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&= \frac{\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2} \\
&= \frac{s_{xy}}{s_x^2} \; .
\end{split}
\end{equation}

Together, \eqref{eq:slr-ols-slr-ols-int} and \eqref{eq:slr-ols-slr-ols-sl-qed} constitute the ordinary least squares parameter estimates for simple linear regression.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Penny, William (2006): "Linear regression"; in: \textit{Mathematics for Brain Imaging}, ch. 1.2.2, pp. 14-16, eqs. 1.24/1.25; URL: \url{https://ueapsylabs.co.uk/sites/wpenny/mbi/mbi_course.pdf}.
\item Wikipedia (2021): "Proofs involving ordinary least squares"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-27; URL: \url{https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares#Derivation_of_simple_linear_regression_estimators}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Ordinary least squares}]{Ordinary least squares} \label{sec:slr-ols2}
\setcounter{equation}{0}

\textbf{Theorem:} Given a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) with independent observations

\begin{equation} \label{eq:slr-ols2-slr}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n \; ,
\end{equation}

the parameters minimizing the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) are given by

\begin{equation} \label{eq:slr-ols2-slr-ols}
\begin{split}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{s_{xy}}{s_x^2}
\end{split}
\end{equation}

where $\bar{x}$ and $\bar{y}$ are the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}), $s_x^2$ is the sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) of $x$ and $s_{xy}$ is the sample covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-samp}) between $x$ and $y$.


\vspace{1em}
\textbf{Proof:} Simple linear regression is a special case of multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-mlr}) with

\begin{equation} \label{eq:slr-ols2-slr-mlr}
X = \left[ \begin{matrix} 1_n & x \end{matrix} \right] \quad \text{and} \quad \beta = \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right]
\end{equation}

and ordinary least squares estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}) are given by

\begin{equation} \label{eq:slr-ols2-mlr-ols}
\hat{\beta} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \; .
\end{equation}

Writing out equation \eqref{eq:slr-ols2-mlr-ols}, we have

\begin{equation} \label{eq:slr-ols2-slr-ols-b}
\begin{split}
\hat{\beta} &= \left( \left[ \begin{matrix} 1_n^\mathrm{T} \\ x^\mathrm{T} \end{matrix} \right] \left[ \begin{matrix} 1_n & x \end{matrix} \right] \right)^{-1} \left[ \begin{matrix} 1_n^\mathrm{T} \\ x^\mathrm{T} \end{matrix} \right] y \\
&= \left( \left[ \begin{matrix} n & n\bar{x} \\ n\bar{x} & x^\mathrm{T} x \end{matrix} \right] \right)^{-1} \left[ \begin{matrix} n \bar{y} \\ x^\mathrm{T} y \end{matrix} \right] \\
&= \frac{1}{n x^\mathrm{T} x - (n\bar{x})^2} \left[ \begin{matrix} x^\mathrm{T} x & -n\bar{x} \\ -n\bar{x} & n \end{matrix} \right]  \left[ \begin{matrix} n \bar{y} \\ x^\mathrm{T} y \end{matrix} \right] \\
&= \frac{1}{n x^\mathrm{T} x - (n\bar{x})^2} \left[ \begin{matrix} n \bar{y} \, x^\mathrm{T} x - n \bar{x} \, x^\mathrm{T} y \\ n \, x^\mathrm{T} y - (n \bar{x})(n \bar{y}) \end{matrix} \right] \; .
\end{split}
\end{equation}

Thus, the second entry of $\hat{\beta}$ is equal to ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}):

\begin{equation} \label{eq:slr-ols2-slr-ols-b1}
\begin{split}
\hat{\beta}_1 &= \frac{n \, x^\mathrm{T} y - (n \bar{x})(n \bar{y})}{n x^\mathrm{T} x - (n\bar{x})^2} \\
&= \frac{x^\mathrm{T} y - n \bar{x} \bar{y}}{x^\mathrm{T} x - n \bar{x}^2} \\
&= \frac{\sum_{i=1}^n x_i y_i - \sum_{i=1}^n \bar{x} \bar{y}}{\sum_{i=1}^n x_i^2 - \sum_{i=1}^n \bar{x}^2} \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&= \frac{s_{xy}}{s_x^2} \; .
\end{split}
\end{equation}

Moreover, the first entry of $\hat{\beta}$ is equal to:

\begin{equation} \label{eq:slr-ols2-slr-ols-b2}
\begin{split}
\hat{\beta}_0 &= \frac{n \bar{y} \, x^\mathrm{T} x - n \bar{x} \, x^\mathrm{T} y}{n x^\mathrm{T} x - (n\bar{x})^2} \\
&= \frac{\bar{y} \, x^\mathrm{T} x - \bar{x} \, x^\mathrm{T} y}{x^\mathrm{T} x - n \bar{x}^2} \\
&= \frac{\bar{y} \, x^\mathrm{T} x - \bar{x} \, x^\mathrm{T} y + n \bar{x}^2 \bar{y} - n \bar{x}^2 \bar{y}}{x^\mathrm{T} x - n \bar{x}^2} \\
&= \frac{\bar{y} (x^\mathrm{T} x - n \bar{x}^2) - \bar{x} (x^\mathrm{T} y - n \bar{x} \bar{y})}{x^\mathrm{T} x - n \bar{x}^2} \\
&= \frac{\bar{y} (x^\mathrm{T} x - n \bar{x}^2)}{x^\mathrm{T} x - n \bar{x}^2} - \frac{\bar{x} (x^\mathrm{T} y - n \bar{x} \bar{y})}{x^\mathrm{T} x - n \bar{x}^2} \\
&= \bar{y} - \bar{x} \, \frac{\sum_{i=1}^n x_i y_i - \sum_{i=1}^n \bar{x} \bar{y}}{\sum_{i=1}^n x_i^2 - \sum_{i=1}^n \bar{x}^2} \\
&= \bar{y} - \hat{\beta}_1 \bar{x} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Expectation of estimates}]{Expectation of estimates} \label{sec:slr-olsmean}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) with independent observations

\begin{equation} \label{eq:slr-olsmean-slr}
y = \beta_0 + \beta_1 x + \varepsilon, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n
\end{equation}

and consider estimation using ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}). Then, the expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the estimated parameters are

\begin{equation} \label{eq:slr-olsmean-slr-ols-mean}
\begin{split}
\mathrm{E}(\hat{\beta}_0) &= \beta_0 \\
\mathrm{E}(\hat{\beta}_1) &= \beta_1
\end{split}
\end{equation}

which means that the ordinary least squares solution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}) produces unbiased estimators.


\vspace{1em}
\textbf{Proof:} According to the simple linear regression model in \eqref{eq:slr-olsmean-slr}, the expectation of a single data point is

\begin{equation} \label{eq:slr-olsmean-E-yi}
\mathrm{E}(y_i) = \beta_0 + \beta_1 x_i \; .
\end{equation}

The ordinary least squares estimates for simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}) are given by

\begin{equation} \label{eq:slr-olsmean-slr-ols}
\begin{split}
\hat{\beta}_0 &= \frac{1}{n} \sum_{i=1}^n y_i - \hat{\beta}_1 \cdot \frac{1}{n} \sum_{i=1}^n x_i \\
\hat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \; .
\end{split}
\end{equation}

If we define the following quantity

\begin{equation} \label{eq:slr-olsmean-ci}
c_i = \frac{x_i - \bar{x}}{\sum_{i=1}^n (x_i - \bar{x})^2} \; ,
\end{equation}

we note that

\begin{equation} \label{eq:slr-olsmean-sum-ci}
\begin{split}
\sum_{i=1}^n c_i &= \frac{\sum_{i=1}^n (x_i - \bar{x})}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{\sum_{i=1}^n x_i - n \bar{x}}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&= \frac{n \bar{x} - n \bar{x}}{\sum_{i=1}^n (x_i - \bar{x})^2} = 0 \; , \\
\end{split}
\end{equation}

and

\begin{equation} \label{eq:slr-olsmean-sum-ci-xi}
\begin{split}
\sum_{i=1}^n c_i x_i &= \frac{\sum_{i=1}^n (x_i - \bar{x}) x_i}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{\sum_{i=1}^n \left( x_i^2 - \bar{x} x_i \right)}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&= \frac{\sum_{i=1}^n x_i^2 - 2 n \bar{x} \bar{x} + n \bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{\sum_{i=1}^n \left( x_i^2 - 2 \bar{x} x_i + \bar{x}^2 \right)}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{\sum_{i=1}^n (x_i - \bar{x})^2} = 1 \; .
\end{split}
\end{equation}

With \eqref{eq:slr-olsmean-ci}, the estimate for the slope from \eqref{eq:slr-olsmean-slr-ols} becomes

\begin{equation} \label{eq:slr-olsmean-slr-ols-sl}
\begin{split}
\hat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&= \sum_{i=1}^n c_i (y_i - \bar{y}) \\
&= \sum_{i=1}^n c_i y_i - \bar{y} \sum_{i=1}^n c_i
\end{split}
\end{equation}

and with \eqref{eq:slr-olsmean-E-yi}, \eqref{eq:slr-olsmean-sum-ci} and \eqref{eq:slr-olsmean-sum-ci-xi}, its expectation becomes:

\begin{equation} \label{eq:slr-olsmean-E-b1}
\begin{split}
\mathrm{E}(\hat{\beta}_1) &= \mathrm{E}\left( \sum_{i=1}^n c_i y_i - \bar{y} \sum_{i=1}^n c_i \right) \\
&= \sum_{i=1}^n c_i \mathrm{E}(y_i) - \bar{y} \sum_{i=1}^n c_i \\
&= \beta_1 \sum_{i=1}^n c_i x_i + \beta_0 \sum_{i=1}^n c_i - \bar{y} \sum_{i=1}^n c_i \\
&= \beta_1 \; .
\end{split}
\end{equation}

Finally, with \eqref{eq:slr-olsmean-E-yi} and \eqref{eq:slr-olsmean-E-b1}, the expectation of the intercept estimate from \eqref{eq:slr-olsmean-slr-ols} becomes

\begin{equation} \label{eq:slr-olsmean-E-b0}
\begin{split}
\mathrm{E}(\hat{\beta}_0) &= \mathrm{E}\left( \frac{1}{n} \sum_{i=1}^n y_i - \hat{\beta}_1 \cdot \frac{1}{n} \sum_{i=1}^n x_i \right) \\
&= \frac{1}{n} \sum_{i=1}^n \mathrm{E}(y_i) - \mathrm{E}(\hat{\beta}_1) \cdot \bar{x} \\
&= \frac{1}{n} \sum_{i=1}^n (\beta_0 + \beta_1 x_i) - \beta_1 \cdot \bar{x} \\
&= \beta_0 + \beta_1 \bar{x} - \beta_1 \bar{x} \\
&= \beta_0 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Penny, William (2006): "Finding the uncertainty in estimating the slope"; in: \textit{Mathematics for Brain Imaging}, ch. 1.2.4, pp. 18-20, eq. 1.37; URL: \url{https://ueapsylabs.co.uk/sites/wpenny/mbi/mbi_course.pdf}.
\item Wikipedia (2021): "Proofs involving ordinary least squares"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-27; URL: \url{https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares#Unbiasedness_and_variance_of_%7F'%22%60UNIQ--postMath-00000037-QINU%60%22'%7F}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Variance of estimates}]{Variance of estimates} \label{sec:slr-olsvar}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) with independent observations

\begin{equation} \label{eq:slr-olsvar-slr}
y = \beta_0 + \beta_1 x + \varepsilon, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n
\end{equation}

and consider estimation using ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}). Then, the variances ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) of the estimated parameters are

\begin{equation} \label{eq:slr-olsvar-slr-ols-var}
\begin{split}
\mathrm{Var}(\hat{\beta}_0) &= \frac{x^\mathrm{T} x}{n} \cdot \frac{\sigma^2}{(n-1) s_x^2} \\
\mathrm{Var}(\hat{\beta}_1) &= \frac{\sigma^2}{(n-1) s_x^2}
\end{split}
\end{equation}

where $s_x^2$ is the sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) of $x$ and $x^\mathrm{T} x$ is the sum of squared values of the covariate.


\vspace{1em}
\textbf{Proof:} According to the simple linear regression model in \eqref{eq:slr-olsvar-slr}, the variance of a single data point is

\begin{equation} \label{eq:slr-olsvar-Var-yi}
\mathrm{Var}(y_i) = \mathrm{Var}(\varepsilon_i) = \sigma^2 \; .
\end{equation}

The ordinary least squares estimates for simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}) are given by

\begin{equation} \label{eq:slr-olsvar-slr-ols}
\begin{split}
\hat{\beta}_0 &= \frac{1}{n} \sum_{i=1}^n y_i - \hat{\beta}_1 \cdot \frac{1}{n} \sum_{i=1}^n x_i \\
\hat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \; .
\end{split}
\end{equation}

If we define the following quantity

\begin{equation} \label{eq:slr-olsvar-ci}
c_i = \frac{x_i - \bar{x}}{\sum_{i=1}^n (x_i - \bar{x})^2} \; ,
\end{equation}

we note that

\begin{equation} \label{eq:slr-olsvar-sum-ci2}
\begin{split}
\sum_{i=1}^n c_i^2 &= \sum_{i=1}^n \left( \frac{x_i - \bar{x}}{\sum_{i=1}^n (x_i - \bar{x})^2} \right)^2 \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{\left[ \sum_{i=1}^n (x_i - \bar{x})^2 \right]^2} \\
&= \frac{1}{\sum_{i=1}^n (x_i - \bar{x})^2} \; .
\end{split}
\end{equation}

With \eqref{eq:slr-olsvar-ci}, the estimate for the slope from \eqref{eq:slr-olsvar-slr-ols} becomes

\begin{equation} \label{eq:slr-olsvar-slr-ols-sl}
\begin{split}
\hat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&= \sum_{i=1}^n c_i (y_i - \bar{y}) \\
&= \sum_{i=1}^n c_i y_i - \bar{y} \sum_{i=1}^n c_i
\end{split}
\end{equation}

and with \eqref{eq:slr-olsvar-Var-yi} and \eqref{eq:slr-olsvar-sum-ci2} as well as invariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-inv}), scaling ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-scal}) and additivity ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-add}) of the variance, the variance of $\hat{\beta}_1$ is:

\begin{equation} \label{eq:slr-olsvar-Var-b1}
\begin{split}
\mathrm{Var}(\hat{\beta}_1) &= \mathrm{Var}\left( \sum_{i=1}^n c_i y_i - \bar{y} \sum_{i=1}^n c_i \right) \\
&= \mathrm{Var}\left( \sum_{i=1}^n c_i y_i \right) \\
&= \sum_{i=1}^n c_i^2 \mathrm{Var}(y_i) \\
&= \sigma^2 \sum_{i=1}^n c_i^2 \\
&= \sigma^2 \frac{1}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&= \frac{\sigma^2}{(n-1) \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2} \\
&= \frac{\sigma^2}{(n-1) s_x^2} \; .
\end{split}
\end{equation}

Finally, with \eqref{eq:slr-olsvar-Var-yi} and \eqref{eq:slr-olsvar-Var-b1}, the variance of the intercept estimate from \eqref{eq:slr-olsvar-slr-ols} becomes:

\begin{equation} \label{eq:slr-olsvar-Var-b0-s1}
\begin{split}
\mathrm{Var}(\hat{\beta}_0) &= \mathrm{Var}\left( \frac{1}{n} \sum_{i=1}^n y_i - \hat{\beta}_1 \cdot \frac{1}{n} \sum_{i=1}^n x_i \right) \\
&= \mathrm{Var}\left( \frac{1}{n} \sum_{i=1}^n y_i \right) + \mathrm{Var}\left( \hat{\beta}_1 \cdot \bar{x} \right) \\
&= \left( \frac{1}{n} \right)^2 \sum_{i=1}^n \mathrm{Var}(y_i) + \bar{x}^2 \cdot \mathrm{Var}(\hat{\beta}_1) \\
&= \frac{1}{n^2} \sum_{i=1}^n \sigma^2 + \bar{x}^2 \frac{\sigma^2}{(n-1) s_x^2} \\
&= \frac{\sigma^2}{n} + \frac{\sigma^2 \bar{x}^2 }{(n-1) s_x^2} \; .
\end{split}
\end{equation}

Applying the formula for the sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) $s_x^2$, we finally get:

\begin{equation} \label{eq:slr-olsvar-Var-b0-s2}
\begin{split}
\mathrm{Var}(\hat{\beta}_0) &= \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \right) \\
&= \sigma^2 \left( \frac{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2}{\sum_{i=1}^n (x_i - \bar{x})^2} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \right) \\
&= \sigma^2 \left( \frac{\frac{1}{n}\sum_{i=1}^n \left( x_i^2 - 2 \bar{x} x_i + \bar{x}^2 \right) + \bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \right) \\
&= \sigma^2 \left( \frac{\left( \frac{1}{n}\sum_{i=1}^n x_i^2 - 2 \bar{x} \frac{1}{n}\sum_{i=1}^n x_i + \bar{x}^2 \right) + \bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \right) \\
&= \sigma^2 \left( \frac{\frac{1}{n}\sum_{i=1}^n x_i^2 - 2 \bar{x}^2 + 2 \bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \right) \\
&= \sigma^2 \left( \frac{\frac{1}{n}\sum_{i=1}^n x_i^2}{(n-1) \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2} \right) \\
&= \frac{x^\mathrm{T} x}{n} \cdot \frac{\sigma^2}{(n-1) s_x^2} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Penny, William (2006): "Finding the uncertainty in estimating the slope"; in: \textit{Mathematics for Brain Imaging}, ch. 1.2.4, pp. 18-20, eq. 1.37; URL: \url{https://ueapsylabs.co.uk/sites/wpenny/mbi/mbi_course.pdf}.
\item Wikipedia (2021): "Proofs involving ordinary least squares"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-27; URL: \url{https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares#Unbiasedness_and_variance_of_%7F'%22%60UNIQ--postMath-00000037-QINU%60%22'%7F}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Distribution of estimates}]{Distribution of estimates} \label{sec:slr-olsdist}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) with independent observations

\begin{equation} \label{eq:slr-olsdist-slr}
y = \beta_0 + \beta_1 x + \varepsilon, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n
\end{equation}

and consider estimation using ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}). Then, the estimated parameters are normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) as

\begin{equation} \label{eq:slr-olsdist-slr-olsdist}
\left[ \begin{matrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{matrix} \right] \sim \mathcal{N}\left( \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right], \, \frac{\sigma^2}{(n-1) \, s_x^2} \cdot \left[ \begin{matrix} x^\mathrm{T}x/n & -\bar{x} \\ -\bar{x} & 1 \end{matrix} \right] \right)
\end{equation}

where $\bar{x}$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) and $s_x^2$ is the sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) of $x$.


\vspace{1em}
\textbf{Proof:} Simple linear regression is a special case of multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-mlr}) with

\begin{equation} \label{eq:slr-olsdist-slr-mlr}
X = \left[ \begin{matrix} 1_n & x \end{matrix} \right] \quad \text{and} \quad \beta = \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right] \; ,
\end{equation}

such that \eqref{eq:slr-olsdist-slr} can also be written as

\begin{equation} \label{eq:slr-olsdist-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 I_n)
\end{equation}

and ordinary least sqaures estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}) are given by

\begin{equation} \label{eq:slr-olsdist-mlr-ols}
\hat{\beta} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \; .
\end{equation}

From \eqref{eq:slr-olsdist-mlr} and the linear transformation theorem for the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}), it follows that

\begin{equation} \label{eq:slr-olsdist-y-dist}
y \sim \mathcal{N}\left( X\beta, \, \sigma^2 I_n \right) \; .
\end{equation}

From \eqref{eq:slr-olsdist-mlr-ols}, in combination with \eqref{eq:slr-olsdist-y-dist} and the transformation theorem ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}), it follows that

\begin{equation} \label{eq:slr-olsdist-b-est-dist}
\begin{split}
\hat{\beta} &\sim \mathcal{N}\left( (X^\mathrm{T} X)^{-1} X^\mathrm{T} X\beta, \, \sigma^2 (X^\mathrm{T} X)^{-1} X^\mathrm{T} I_n X (X^\mathrm{T} X)^{-1} \right) \\
&\sim \mathcal{N}\left( \beta, \, \sigma^2 (X^\mathrm{T} X)^{-1} \right) \; .
\end{split}
\end{equation}

Applying \eqref{eq:slr-olsdist-slr-mlr}, the covariance matrix ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) can be further developed as follows:

\begin{equation} \label{eq:slr-olsdist-b-est-cov}
\begin{split}
\sigma^2 (X^\mathrm{T} X)^{-1} &= \sigma^2 \left( \left[ \begin{matrix} 1_n^\mathrm{T} \\ x^\mathrm{T} \end{matrix} \right] \left[ \begin{matrix} 1_n & x \end{matrix} \right] \right)^{-1} \\
&= \sigma^2 \left( \left[ \begin{matrix} n & n\bar{x} \\ n\bar{x} & x^\mathrm{T} x \end{matrix} \right] \right)^{-1} \\
&= \frac{\sigma^2}{n x^\mathrm{T} x - (n\bar{x})^2} \left[ \begin{matrix} x^\mathrm{T} x & -n\bar{x} \\ -n\bar{x} & n \end{matrix} \right] \\
&= \frac{\sigma^2}{x^\mathrm{T} x - n\bar{x}^2} \left[ \begin{matrix} x^\mathrm{T} x/n & -\bar{x} \\ -\bar{x} & 1 \end{matrix} \right] \; .
\end{split}
\end{equation}

Note that the denominator in the first factor is equal to

\begin{equation} \label{eq:slr-olsdist-b-est-cov-den}
\begin{split}
x^\mathrm{T} x - n\bar{x}^2 &= x^\mathrm{T} x - 2 n\bar{x}^2 + n\bar{x}^2 \\
&= \sum_{i=1}^{n} x_i^2 - 2 n \bar{x} \frac{1}{n} \sum_{i=1}^{n} x_i + \sum_{i=1}^{n} \bar{x}^2 \\
&= \sum_{i=1}^{n} x_i^2 - 2 \sum_{i=1}^{n} x_i \bar{x} + \sum_{i=1}^{n} \bar{x}^2 \\
&= \sum_{i=1}^{n} \left( x_i^2 - 2 x_i \bar{x} + \bar{x}^2 \right) \\
&= \sum_{i=1}^{n} \left( x_i^2 - \bar{x} \right)^2 \\
&= (n-1) \, s_x^2 \; .
\end{split}
\end{equation}

Thus, combining \eqref{eq:slr-olsdist-b-est-dist}, \eqref{eq:slr-olsdist-b-est-cov} and \eqref{eq:slr-olsdist-b-est-cov-den}, we have

\begin{equation} \label{eq:slr-olsdist-slr-olsdist-qed}
\hat{\beta} \sim \mathcal{N}\left( \beta, \, \frac{\sigma^2}{(n-1) \, s_x^2} \cdot \left[ \begin{matrix} x^\mathrm{T}x/n & -\bar{x} \\ -\bar{x} & 1 \end{matrix} \right] \right)
\end{equation}

which is equivalent to equation \eqref{eq:slr-olsdist-slr-olsdist}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Proofs involving ordinary least squares"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-11-09; URL: \url{https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares#Unbiasedness_and_variance_of_%7F'%22%60UNIQ--postMath-00000037-QINU%60%22'%7F}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Correlation of estimates}]{Correlation of estimates} \label{sec:slr-olscorr}
\setcounter{equation}{0}

\textbf{Theorem:} In simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}), when the independent variable $x$ is mean-centered ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), the ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}) estimates for slope and intercept are uncorrelated ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr}).


\vspace{1em}
\textbf{Proof:} The parameter estimates for simple linear regression are bivariate normally distributed under ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-olsdist}):

\begin{equation} \label{eq:slr-olscorr-slr-olsdist}
\left[ \begin{matrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{matrix} \right] \sim \mathcal{N}\left( \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right], \, \frac{\sigma^2}{(n-1) \, s_x^2} \cdot \left[ \begin{matrix} x^\mathrm{T}x/n & -\bar{x} \\ -\bar{x} & 1 \end{matrix} \right] \right)
\end{equation}

Because the covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) contains the pairwise covariances of the random variables ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvar}), we can deduce that the covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov}) of $\hat{\beta}_0$ and $\hat{\beta}_1$ is:

\begin{equation} \label{eq:slr-olscorr-slr-olscov}
\mathrm{Cov}\left( \hat{\beta}_0, \hat{\beta}_1 \right) = -\frac{\sigma^2 \, \bar{x}}{(n-1) \, s_x^2}
\end{equation}

where $\sigma^2$ is the noise variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}), $s_x^2$ is the sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) of $x$ and $n$ is the number of observations. When $x$ is mean-centered, we have $\bar{x} = 0$, such that:

\begin{equation} \label{eq:slr-olscorr-slr-olscov-meancent}
\mathrm{Cov}\left( \hat{\beta}_0, \hat{\beta}_1 \right) = 0 \; .
\end{equation}

Because correlation is equal to covariance divided by standard deviations ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr}), we can conclude that the correlation of $\hat{\beta}_0$ and $\hat{\beta}_1$ is also zero:

\begin{equation} \label{eq:slr-olscorr-slr-olscorr-qed}
\mathrm{Corr}\left( \hat{\beta}_0, \hat{\beta}_1 \right) = 0 \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Effects of mean-centering}]{Effects of mean-centering} \label{sec:slr-meancent}
\setcounter{equation}{0}

\textbf{Theorem:} In simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}), when the dependent variable $y$ and/or the independent variable $x$ are mean-centered ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}), the ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}) estimate for the intercept changes, but that of the slope does not.

\vspace{1em}
\textbf{Proof:}

1) Under unaltered $y$ and $x$, ordinary least squares estimates for simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}) are

\begin{equation} \label{eq:slr-meancent-slr-ols}
\begin{split}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{s_{xy}}{s_x^2}
\end{split}
\end{equation}

with sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) $\bar{x}$ and $\bar{y}$, sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) $s_x^2$ and sample covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-samp}) $s_{xy}$, such that $\beta_0$ estimates "the mean $y$ at $x = 0$".

\vspace{1em}
2) Let $\tilde{x}$ be the mean-centered covariate vector ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}):

\begin{equation} \label{eq:slr-meancent-slr-meancent-x}
\tilde{x}_i = x_i - \bar{x} \quad \Rightarrow \quad \bar{\tilde{x}} = 0 \; .
\end{equation}

Under this condition, the parameter estimates become

\begin{equation} \label{eq:slr-meancent-slr-ols-meancent-x}
\begin{split}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{\tilde{x}} \\
&= \bar{y} \\
\hat{\beta}_1 &= \frac{\sum_{i=1}^n (\tilde{x}_i - \bar{\tilde{x}}) (y_i - \bar{y})}{\sum_{i=1}^n (\tilde{x}_i - \bar{\tilde{x}})^2} \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{s_{xy}}{s_x^2}
\end{split}
\end{equation}

and we can see that $\hat{\beta}_1(\tilde{x},y) = \hat{\beta}_1(x,y)$, but $\hat{\beta}_0(\tilde{x},y) \neq \hat{\beta}_0(x,y)$, specifically $\beta_0$ now estimates "the mean $y$ at the mean $x$".


\vspace{1em} 
3) Let $\tilde{y}$ be the mean-centered data vector ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}):

\begin{equation} \label{eq:slr-meancent-slr-meancent-y}
\tilde{y}_i = y_i - \bar{y} \quad \Rightarrow \quad \bar{\tilde{y}} = 0 \; .
\end{equation}

Under this condition, the parameter estimates become

\begin{equation} \label{eq:slr-meancent-slr-ols-meancent-y}
\begin{split}
\hat{\beta}_0 &= \bar{\tilde{y}} - \hat{\beta}_1 \bar{x} \\
&= - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i - \bar{x}) (\tilde{y}_i - \bar{\tilde{y}})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{s_{xy}}{s_x^2}
\end{split}
\end{equation}

and we can see that $\hat{\beta}_1(x,\tilde{y}) = \hat{\beta}_1(x,y)$, but $\hat{\beta}_0(x,\tilde{y}) \neq \hat{\beta}_0(x,y)$, specifically $\beta_0$ now estimates "the mean $x$, multiplied with the negative slope".

\vspace{1em} 
4) Finally, consider mean-centering both $x$ and $y$::

\begin{equation} \label{eq:slr-meancent-slr-meancent-xy}
\begin{split}
\tilde{x}_i = x_i - \bar{x} \quad &\Rightarrow \quad \bar{\tilde{x}} = 0 \\
\tilde{y}_i = y_i - \bar{y} \quad &\Rightarrow \quad \bar{\tilde{y}} = 0 \; .
\end{split}
\end{equation}

Under this condition, the parameter estimates become

\begin{equation} \label{eq:slr-meancent-slr-ols-meancent-xy}
\begin{split}
\hat{\beta}_0 &= \bar{\tilde{y}} - \hat{\beta}_1 \bar{\tilde{x}} \\
&= 0 \\
\hat{\beta}_1 &= \frac{\sum_{i=1}^n (\tilde{x}_i - \bar{\tilde{x}}) (\tilde{y}_i - \bar{\tilde{y}})}{\sum_{i=1}^n (\tilde{x}_i - \bar{\tilde{x}})^2} \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{s_{xy}}{s_x^2}
\end{split}
\end{equation}

and we can see that $\hat{\beta}_1(\tilde{x},\tilde{y}) = \hat{\beta}_1(x,y)$, but $\hat{\beta}_0(\tilde{x},\tilde{y}) \neq \hat{\beta}_0(x,y)$, specifically $\beta_0$ is now forced to become zero.

\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textit{Regression line}]{Regression line} \label{sec:regline}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a simple linear regression with independent observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) using dependent variable $y$ and independent variable $x$:

\begin{equation} \label{eq:regline-slr}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2) \; .
\end{equation}

Then, given some parameters $\beta_0, \beta_1 \in \mathbb{R}$, the set

\begin{equation} \label{eq:regline-regline}
L(\beta_0, \beta_1) = \left\lbrace (x,y) \in \mathbb{R}^2 \mid y = \beta_0 + \beta_1 x \right\rbrace
\end{equation}

is called a "regression line" and the set

\begin{equation} \label{eq:regline-regline-ols}
L(\hat{\beta}_0, \hat{\beta}_1)
\end{equation}

is called the "fitted regression line", with estimated regression coefficients $\hat{\beta}_0, \hat{\beta}_1$, e.g. obtained via ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Simple linear regression"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-27; URL: \url{https://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Regression line includes center of mass}]{Regression line includes center of mass} \label{sec:slr-comp}
\setcounter{equation}{0}

\textbf{Theorem:} In simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}), the regression line ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:regline}) estimated using ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}) includes the point $M(\bar{x},\bar{y})$.

\vspace{1em}
\textbf{Proof:} The fitted regression line ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:regline}) is described by the equation

\begin{equation} \label{eq:slr-comp-slr-ols-regline}
y = \hat{\beta}_0 + \hat{\beta}_1 x \quad \text{where} \quad x,y \in \mathbb{R} \; .
\end{equation}

Plugging in the coordinates of $M$ and the ordinary least squares estimate of the intercept ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}), we obtain

\begin{equation} \label{eq:slr-comp-slr-ols}
\begin{split}
\bar{y} &= \hat{\beta}_0 + \hat{\beta}_1 \bar{x} \\
\bar{y} &= \bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 \bar{x} \\
\bar{y} &= \bar{y} \; .
\end{split}
\end{equation}

which is a true statement. Thus, the regression line ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:regline}) goes through the center of mass point $(\bar{x},\bar{y})$, if the model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) includes an intercept term $\beta_0$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Simple linear regression"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-27; URL: \url{https://en.wikipedia.org/wiki/Simple_linear_regression#Numerical_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Projection of data point to regression line}]{Projection of data point to regression line} \label{sec:slr-proj}
\setcounter{equation}{0}

\textbf{Theorem:} Consider simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) and an estimated regression line ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:regline}) specified by

\begin{equation} \label{eq:slr-proj-slr-regline}
y = \hat{\beta}_0 + \hat{\beta}_1 x \quad \text{where} \quad x,y \in \mathbb{R} \; .
\end{equation}

For any given data point $O(x_o \vert y_o)$, the point on the regression line $P(x_p \vert y_p)$ that is closest to this data point is given by:

\begin{equation} \label{eq:slr-proj-slr-proj}
P\left(w \mid \hat{\beta}_0 + \hat{\beta}_1 w\right) \quad \text{with} \quad w = \frac{x_0 + (y_o - \hat{\beta}_0) \hat{\beta}_1}{1 + \hat{\beta}_1^2} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The intersection point of the regression line ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:regline}) with the y-axis is

\begin{equation} \label{eq:slr-proj-S}
S(0 \vert \hat{\beta}_0) \; .
\end{equation}

Let $a$ be a vector describing the direction of the regression line, let $b$ be the vector pointing from $S$ to $O$ and let $p$ be the vector pointing from $S$ to $P$.

Because $\hat{\beta}_1$ is the slope of the regression line, we have

\begin{equation} \label{eq:slr-proj-a}
a = \left( \begin{matrix} 1 \\ \hat{\beta}_1 \end{matrix} \right) \; .
\end{equation}

Moreover, with the points $O$ and $S$, we have

\begin{equation} \label{eq:slr-proj-b}
b = \left( \begin{matrix} x_o \\ y_o \end{matrix} \right) - \left( \begin{matrix} 0 \\ \hat{\beta}_0 \end{matrix} \right) = \left( \begin{matrix} x_o \\ y_o - \hat{\beta}_0 \end{matrix} \right) \; .
\end{equation}

Because $P$ is located on the regression line, $p$ is collinear with $a$ and thus a scalar multiple of this vector:

\begin{equation} \label{eq:slr-proj-p}
p = w \cdot a \; .
\end{equation}

Moreover, as $P$ is the point on the regression line which is closest to $O$, this means that the vector $b-p$ is orthogonal to $a$, such that the inner product of these two vectors is equal to zero:

\begin{equation} \label{eq:slr-proj-a-b-p-orth}
a^\mathrm{T} (b-p) = 0 \; .
\end{equation}

Rearranging this equation gives

\begin{equation} \label{eq:slr-proj-w}
\begin{split}
a^\mathrm{T} (b-p) &= 0 \\
a^\mathrm{T} (b - w \cdot a) &= 0 \\
a^\mathrm{T} b - w \cdot a^\mathrm{T} a &= 0 \\
w \cdot a^\mathrm{T} a &= a^\mathrm{T} b \\
w &= \frac{a^\mathrm{T} b}{a^\mathrm{T} a} \; .
\end{split}
\end{equation}

With \eqref{eq:slr-proj-a} and \eqref{eq:slr-proj-b}, $w$ can be calculated as

\begin{equation} \label{eq:slr-proj-w-qed}
\begin{split}
w &= \frac{a^\mathrm{T} b}{a^\mathrm{T} a} \\
w &= \frac{\left( \begin{matrix} 1 \\ \hat{\beta}_1 \end{matrix} \right)^\mathrm{T} \left( \begin{matrix} x_o \\ y_o - \hat{\beta}_0 \end{matrix} \right)}{\left( \begin{matrix} 1 \\ \hat{\beta}_1 \end{matrix} \right)^\mathrm{T} \left( \begin{matrix} 1 \\ \hat{\beta}_1 \end{matrix} \right)} \\
w &= \frac{x_0 + (y_o - \hat{\beta}_0) \hat{\beta}_1}{1 + \hat{\beta}_1^2}
\end{split}
\end{equation}

Finally, with the point $S$ \eqref{eq:slr-proj-S} and the vector $p$ \eqref{eq:slr-proj-p}, the coordinates of $P$ are obtained as

\begin{equation} \label{eq:slr-proj-P-qed}
\left( \begin{matrix} x_p \\ y_p \end{matrix} \right) = \left( \begin{matrix} 0 \\ \hat{\beta}_0 \end{matrix} \right) + w \cdot \left( \begin{matrix} 1 \\ \hat{\beta}_1 \end{matrix} \right) = \left( \begin{matrix} w \\ \hat{\beta}_0 + \hat{\beta}_1 w \end{matrix} \right) \; .
\end{equation}

Together, \eqref{eq:slr-proj-P-qed} and \eqref{eq:slr-proj-w-qed} constitute the proof of equation \eqref{eq:slr-proj-slr-proj}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Penny, William (2006): "Projections"; in: \textit{Mathematics for Brain Imaging}, ch. 1.4.10, pp. 34-35, eqs. 1.87/1.88; URL: \url{https://ueapsylabs.co.uk/sites/wpenny/mbi/mbi_course.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Sums of squares}]{Sums of squares} \label{sec:slr-sss}
\setcounter{equation}{0}

\textbf{Theorem:} Under ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}) for simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}), total ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}), explained ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess}) and residual ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) sums of squares are given by

\begin{equation} \label{eq:slr-sss-slr-sss}
\begin{split}
\mathrm{TSS} &= (n-1) \, s_y^2 \\
\mathrm{ESS} &= (n-1) \, \frac{s_{xy}^2}{s_x^2} \\
\mathrm{RSS} &= (n-1) \left( s_y^2 - \frac{s_{xy}^2}{s_x^2} \right)
\end{split}
\end{equation}

where $s_x^2$ and $s_y^2$ are the sample variances ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) of $x$ and $y$ and $s_{xy}$ is the sample covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-samp}) between $x$ and $y$.


\vspace{1em}
\textbf{Proof:} The ordinary least squares parameter estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}) are given by

\begin{equation} \label{eq:slr-sss-slr-ols}
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} \quad \text{and} \quad \hat{\beta}_1 = \frac{s_{xy}}{s_x^2} \; .
\end{equation}

\vspace{1em}
1) The total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}) is defined as

\begin{equation} \label{eq:slr-sss-TSS}
\mathrm{TSS} = \sum_{i=1}^{n} (y_i - \bar{y})^2
\end{equation}

which can be reformulated as follows:

\begin{equation} \label{eq:slr-sss-TSS-qed}
\begin{split}
\mathrm{TSS} &= \sum_{i=1}^{n} (y_i - \bar{y})^2 \\
&= (n-1) \frac{1}{n-1} \sum_{i=1}^{n} (y_i - \bar{y})^2 \\
&= (n-1) s_y^2 \; .
\end{split}
\end{equation}

\vspace{1em}
2) The explained sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess}) is defined as

\begin{equation} \label{eq:slr-sss-ESS}
\mathrm{ESS} = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2 \quad \text{where} \quad \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i
\end{equation}

which, with the OLS parameter estimates, becomes:

\begin{equation} \label{eq:slr-sss-ESS-qed}
\begin{split}
\mathrm{ESS} &= \sum_{i=1}^n (\hat{y}_i - \bar{y})^2 \\
&= \sum_{i=1}^n (\hat{\beta}_0 + \hat{\beta}_1 x_i - \bar{y})^2 \\
&\overset{\eqref{eq:slr-sss-slr-ols}}{=} \sum_{i=1}^n (\bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 x_i - \bar{y})^2 \\
&= \sum_{i=1}^n \left( \hat{\beta}_1 (x_i - \bar{x}) \right)^2 \\
&\overset{\eqref{eq:slr-sss-slr-ols}}{=} \sum_{i=1}^n \left( \frac{s_{xy}}{s_x^2} (x_i - \bar{x}) \right)^2 \\
&= \left( \frac{s_{xy}}{s_x^2} \right)^2 \sum_{i=1}^n (x_i - \bar{x})^2 \\
&= \left( \frac{s_{xy}}{s_x^2} \right)^2 (n-1) s_x^2 \\
&= (n-1) \, \frac{s_{xy}^2}{s_x^2} \; .
\end{split}
\end{equation}

\vspace{1em}
3) The residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) is defined as

\begin{equation} \label{eq:slr-sss-RSS}
\mathrm{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 \quad \text{where} \quad \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i
\end{equation}

which, with the OLS parameter estimates, becomes:

\begin{equation} \label{eq:slr-sss-RSS-qed}
\begin{split}
\mathrm{RSS} &= \sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
&= \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \\
&\overset{\eqref{eq:slr-sss-slr-ols}}{=} \sum_{i=1}^n (y_i - \bar{y} + \hat{\beta}_1 \bar{x} - \hat{\beta}_1 x_i)^2 \\
&= \sum_{i=1}^n \left( (y_i - \bar{y}) - \hat{\beta}_1 (x_i - \bar{x}) \right)^2 \\
&= \sum_{i=1}^n \left( (y_i - \bar{y})^2 - 2 \hat{\beta}_1 (x_i - \bar{x}) (y_i - \bar{y}) + \hat{\beta}_1^2 (x_i - \bar{x})^2 \right) \\
&= \sum_{i=1}^n (y_i - \bar{y})^2 - 2 \hat{\beta}_1 \sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y}) + \hat{\beta}_1^2 \sum_{i=1}^n (x_i - \bar{x})^2 \\
&= (n-1) \, s_y^2 - 2 (n-1) \, \hat{\beta}_1 \, s_{xy} + (n-1) \, \hat{\beta}_1^2 \, s_x^2 \\
&\overset{\eqref{eq:slr-sss-slr-ols}}{=} (n-1) \, s_y^2 - 2 (n-1) \left( \frac{s_{xy}}{s_x^2} \right) s_{xy} + (n-1) \left( \frac{s_{xy}}{s_x^2} \right)^2 s_x^2 \\
&= (n-1) \, s_y^2 - (n-1) \, \frac{s_{xy}^2}{s_x^2} \\
&= (n-1) \left( s_y^2 - \frac{s_{xy}^2}{s_x^2} \right) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Partition of sums of squares}]{Partition of sums of squares} \label{sec:slr-pss}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) with independent observations

\begin{equation} \label{eq:slr-pss-slr}
y = \beta_0 + \beta_1 x + \varepsilon, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n
\end{equation}

where $\beta_0$ and $\beta_1$ are intercept and slope parameter ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}), respectively. Then, it holds that

\begin{equation} \label{eq:slr-pss-slr-pss}
\mathrm{TSS} = \mathrm{ESS} + \mathrm{RSS}
\end{equation}

where $\mathrm{TSS}$ is the total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}), $\mathrm{ESS}$ is the explained sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess}) and $\mathrm{RSS}$ is the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}).


\vspace{1em}
\textbf{Proof:} For simple linear regression, total, explained and residual sum squares are given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-sss})

\begin{equation} \label{eq:slr-pss-slr-sss}
\begin{split}
\mathrm{TSS} &= \sum_{i=1}^{n} (y_i - \bar{y})^2 \\
\mathrm{ESS} &= \sum_{i=1}^n (\hat{y}_i - \bar{y})^2 = \sum_{i=1}^n (\hat{\beta}_0 + \hat{\beta}_1 x_i - \bar{y})^2 \\
\mathrm{RSS} &= \sum_{i=1}^n (y_i - \hat{y}_i)^2  = \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2
\end{split}
\end{equation}

where $\hat{\beta}_0$ and $\hat{\beta}_1$ are the estimated regression coefficients obtained via ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols})

\begin{equation} \label{eq:slr-pss-slr-ols}
\begin{split}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{s_{xy}}{s_x^2}
\end{split}
\end{equation}

where $\bar{x}$ and $\bar{y}$ are the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) of $x$ and $y$, $s_{xy}$ is the unbiased sample covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-samp}) of $x$ and $y$ and $s_x^2$ is the unbiased sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-samp}) of $x$:

\begin{equation} \label{eq:slr-pss-cov-var-samp}
\begin{split}
s_{xy} &= \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y}) \\
s_x^2  &= \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \; .
\end{split}
\end{equation}

With that in mind, we start working out the total sum of squares:

\begin{equation} \label{eq:slr-pss-slr-tss}
\begin{split}
\mathrm{TSS}
&= \sum_{i=1}^{n} (y_i - \bar{y})^2 \\
&= \sum_{i=1}^{n} (y_i - \hat{y}_i + \hat{y}_i - \bar{y})^2 \\
&= \sum_{i=1}^{n} \left( (y_i - \hat{y}_i) + (\hat{y}_i - \bar{y}) \right)^2 \\
&= \sum_{i=1}^{n} \left( (y_i - \hat{y}_i)^2 + 2 (y_i - \hat{y}_i) (\hat{y}_i - \bar{y}) + (\hat{y}_i - \bar{y})^2 \right) \\
&= \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \sum_{i=1}^{n} 2 (y_i - \hat{y}_i) (\hat{y}_i - \bar{y}) + \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 \\
&\overset{\eqref{eq:slr-pss-slr-sss}}{=} \mathrm{ESS} + \mathrm{RSS} + \sum_{i=1}^{n} 2 (y_i - \hat{y}_i) (\hat{y}_i - \bar{y}) \; .
\end{split}
\end{equation}

Thus, what remains to be shown is that the following sum is zero:

\begin{equation} \label{eq:slr-pss-tss-sum}
\sum_{i=1}^{n} 2 (y_i - \hat{y}_i) (\hat{y}_i - \bar{y})
\end{equation}

Using the expression $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$ for the fitted signal values ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:regline}), we proceed as follows:

\begin{equation} \label{eq:slr-pss-tss-sum-s1}
\begin{split}
\sum_{i=1}^{n} 2 (y_i - \hat{y}_i) (\hat{y}_i - \bar{y})
&= \sum_{i=1}^{n} 2 (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) (\hat{\beta}_0 + \hat{\beta}_1 x_i - \bar{y}) \\
&\overset{\eqref{eq:slr-pss-slr-ols}}{=} \sum_{i=1}^{n} 2 (y_i - \bar{y} + \hat{\beta}_1 \bar{x} - \hat{\beta}_1 x_i) (\bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 x_i - \bar{y}) \\
&= \sum_{i=1}^{n} 2 \left( (y_i - \bar{y}) - (\hat{\beta}_1 x_i - \hat{\beta}_1 \bar{x}) \right) (\hat{\beta}_1 x_i - \hat{\beta}_1 \bar{x}) \\
&= 2 \sum_{i=1}^{n} \left( (y_i - \bar{y}) - \hat{\beta}_1 (x_i - \bar{x}) \right) \hat{\beta}_1 (x_i - \bar{x}) \\
&= 2 \sum_{i=1}^{n} \left( (y_i - \hat{y}_i) \hat{\beta}_1 (x_i - \bar{x}) - \hat{\beta}_1 (x_i - \bar{x}) \hat{\beta}_1 (x_i - \bar{x}) \right) \\
&= 2 \left[ \hat{\beta}_1 \sum_{i=1}^{n} (y_i - \hat{y}_i) (x_i - \bar{x}) - \hat{\beta}_1^2 \sum_{i=1}^{n} (x_i - \bar{x}) (x_i - \bar{x}) \right] \; .
\end{split}
\end{equation}

Next, we recognize the sample covariance and sample variance terms from \eqref{eq:slr-pss-cov-var-samp}:

\begin{equation} \label{eq:slr-pss-tss-sum-s2}
\begin{split}
\sum_{i=1}^{n} 2 (y_i - \hat{y}_i) (\hat{y}_i - \bar{y})
&= 2 \left[ \hat{\beta}_1 (n-1) s_{xy} - \hat{\beta}_1^2 (n-1) s_x^2 \right] \\
&= 2 (n-1) \left[ \hat{\beta}_1 s_{xy} - \hat{\beta}_1^2 s_x^2 \right] \; .
\end{split}
\end{equation}

Now, we can apply to functional form of the estimate $\hat{\beta}_1$ from \eqref{eq:slr-pss-slr-ols} to get:

\begin{equation} \label{eq:slr-pss-tss-sum-s3}
\begin{split}
\sum_{i=1}^{n} 2 (y_i - \hat{y}_i) (\hat{y}_i - \bar{y})
&= 2 (n-1) \left[ \left( \frac{s_{xy}}{s_x^2} \right) s_{xy} - \left( \frac{s_{xy}}{s_x^2} \right)^2 s_x^2 \right] \\
&= 2 (n-1) \left[ \frac{s_{xy}^2}{s_x^2} - \frac{s_{xy}^2}{s_x^2} \right] \\
&= 2 (n-1) \cdot 0 \\
&= 0 \; .
\end{split}
\end{equation}

Plugging the result from \eqref{eq:slr-pss-tss-sum-s3} into \eqref{eq:slr-pss-slr-tss}, we finally get:

\begin{equation} \label{eq:slr-pss-slr-pss-qed}
\mathrm{TSS} = \mathrm{ESS} + \mathrm{RSS} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Ostwald, Dirk (2023): "Korrelation"; in: \textit{Allgemeines Lineares Modell}, Einheit (2), Folien 19-23; URL: \url{https://www.ipsy.ovgu.de/ipsy_media/Methodenlehre+I/Sommersemester+2023/Allgemeines+Lineares+Modell/2_Korrelation.pdf}.
\item Manabu, Hayashi (2021): "TSS = RSS + ESS | Simple Linear Regression"; in: \textit{YouTube}, retrieved on 2024-07-12; URL: \url{https://www.youtube.com/watch?v=N7pHym1L9b0}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Transformation matrices}]{Transformation matrices} \label{sec:slr-mat}
\setcounter{equation}{0}

\textbf{Theorem:} Under ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}) for simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}), estimation ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:emat}), projection ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:pmat}) and residual-forming ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rfmat}) matrices are given by

\begin{equation} \label{eq:slr-mat-slr-mat}
\begin{split}
E &= \frac{1}{(n-1)\,s_x^2} \left[ \begin{matrix} (x^\mathrm{T} x/n) \, 1_n^\mathrm{T} - \bar{x} \, x^\mathrm{T} \\ - \bar{x} \, 1_n^\mathrm{T} + x^\mathrm{T} \end{matrix} \right] \\
P &= \frac{1}{(n-1)\,s_x^2} \left[ \begin{matrix} (x^\mathrm{T} x/n) - 2 \bar{x} x_1 + x_1^2 & \cdots & (x^\mathrm{T} x/n) - \bar{x} (x_1 + x_n) + x_1 x_n \\ \vdots & \ddots & \vdots \\ (x^\mathrm{T} x/n) - \bar{x} (x_1 + x_n) + x_1 x_n & \cdots & (x^\mathrm{T} x/n) - 2 \bar{x} x_n + x_n^2 \end{matrix} \right] \\
R &= \frac{1}{(n-1)\,s_x^2} \left[ \begin{matrix} (n-1) (x^\mathrm{T} x/n) + \bar{x} (2 x_1 - n\bar{x}) - x_1^2 & \cdots & -(x^\mathrm{T} x/n) + \bar{x} (x_1 + x_n) - x_1 x_n \\ \vdots & \ddots & \vdots \\ -(x^\mathrm{T} x/n) + \bar{x} (x_1 + x_n) - x_1 x_n & \cdots &  (n-1) (x^\mathrm{T} x/n) + \bar{x} (2 x_n - n\bar{x}) - x_n^2 \end{matrix} \right]
\end{split}
\end{equation}

where $1_n$ is an $n \times 1$ vector of ones, $x$ is the $n \times 1$ single predictor variable, $\bar{x}$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) of $x$ and $s_x^2$ is the sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) of $x$.


\vspace{1em}
\textbf{Proof:} Simple linear regression is a special case of multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-mlr}) with

\begin{equation} \label{eq:slr-mat-slr-mlr}
X = \left[ \begin{matrix} 1_n & x \end{matrix} \right] \quad \text{and} \quad \beta = \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right] \; ,
\end{equation}

such that the simple linear regression model can also be written as

\begin{equation} \label{eq:slr-mat-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 I_n) \; .
\end{equation}

Moreover, we note the following equality ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-olsdist}):

\begin{equation} \label{eq:slr-mat-b-est-cov-den}
x^\mathrm{T} x - n\bar{x}^2 = (n-1) \, s_x^2 \; .
\end{equation}

\vspace{1em}
1) The estimation matrix is given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat})

\begin{equation} \label{eq:slr-mat-E}
E = (X^\mathrm{T} X)^{-1} X^\mathrm{T}
\end{equation}

which is a $2 \times n$ matrix and can be reformulated as follows:

\begin{equation} \label{eq:slr-mat-E-qed}
\begin{split}
E &= (X^\mathrm{T} X)^{-1} X^\mathrm{T} \\
&= \left( \left[ \begin{matrix} 1_n^\mathrm{T} \\ x^\mathrm{T} \end{matrix} \right] \left[ \begin{matrix} 1_n & x \end{matrix} \right] \right)^{-1} \left[ \begin{matrix} 1_n^\mathrm{T} \\ x^\mathrm{T} \end{matrix} \right] \\
&= \left( \left[ \begin{matrix} n & n\bar{x} \\ n\bar{x} & x^\mathrm{T} x \end{matrix} \right] \right)^{-1} \left[ \begin{matrix} 1_n^\mathrm{T} \\ x^\mathrm{T} \end{matrix} \right] \\
&= \frac{1}{n x^\mathrm{T} x - (n\bar{x})^2} \left[ \begin{matrix} x^\mathrm{T} x & -n\bar{x} \\ -n\bar{x} & n \end{matrix} \right] \left[ \begin{matrix} 1_n^\mathrm{T} \\ x^\mathrm{T} \end{matrix} \right] \\
&= \frac{1}{x^\mathrm{T} x - n\bar{x}^2} \left[ \begin{matrix} x^\mathrm{T} x/n & -\bar{x} \\ -\bar{x} & 1 \end{matrix} \right] \left[ \begin{matrix} 1_n^\mathrm{T} \\ x^\mathrm{T} \end{matrix} \right] \\
&\overset{\eqref{eq:slr-mat-b-est-cov-den}}{=} \frac{1}{(n-1)\,s_x^2} \left[ \begin{matrix} (x^\mathrm{T} x/n) \, 1_n^\mathrm{T} - \bar{x} \, x^\mathrm{T} \\ - \bar{x} \, 1_n^\mathrm{T} + x^\mathrm{T} \end{matrix} \right] \; .
\end{split}
\end{equation}

\vspace{1em}
2) The projection matrix is given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat})

\begin{equation} \label{eq:slr-mat-P}
P = X (X^\mathrm{T} X)^{-1} X^\mathrm{T} = X \, E
\end{equation}

which is an $n \times n$ matrix and can be reformulated as follows:

\begin{equation} \label{eq:slr-mat-P-qed}
\begin{split}
P &= X \, E = \left[ \begin{matrix} 1_n & x \end{matrix} \right] \left[ \begin{matrix} e_1 \\ e_2 \end{matrix} \right] \\
&= \frac{1}{(n-1)\,s_x^2} \left[ \begin{matrix} 1 & x_1 \\ \vdots & \vdots \\ 1 & x_n \end{matrix} \right] \left[ \begin{matrix} (x^\mathrm{T} x/n) - \bar{x} x_1 & \cdots & (x^\mathrm{T} x/n) - \bar{x} x_n \\ -\bar{x} + x_1 & \cdots & -\bar{x} + x_n \end{matrix} \right] \\
&= \frac{1}{(n-1)\,s_x^2} \left[ \begin{matrix} (x^\mathrm{T} x/n) - 2 \bar{x} x_1 + x_1^2 & \cdots & (x^\mathrm{T} x/n) - \bar{x} (x_1 + x_n) + x_1 x_n \\ \vdots & \ddots & \vdots \\ (x^\mathrm{T} x/n) - \bar{x} (x_1 + x_n) + x_1 x_n & \cdots & (x^\mathrm{T} x/n) - 2 \bar{x} x_n + x_n^2 \end{matrix} \right] \; .
\end{split}
\end{equation}

\vspace{1em}
3) The residual-forming matrix is given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat})

\begin{equation} \label{eq:slr-mat-R}
R = I_n - X (X^\mathrm{T} X)^{-1} X^\mathrm{T} = I_n - P
\end{equation}

which also is an $n \times n$ matrix and can be reformulated as follows:

\begin{equation} \label{eq:slr-mat-R-qed}
\begin{split}
R &= I_n - P = \left[ \begin{matrix} 1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 1 \end{matrix} \right] - \left[ \begin{matrix} p_{11} & \cdots & p_{1n} \\ \vdots & \ddots & \vdots \\ p_{n1} & \cdots & p_{nn} \end{matrix} \right] \\
&\overset{\eqref{eq:slr-mat-b-est-cov-den}}{=} \frac{1}{(n-1)\,s_x^2} \left[ \begin{matrix} x^\mathrm{T} x - n\bar{x}^2 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & x^\mathrm{T} x - n\bar{x}^2 \end{matrix} \right] \\
&- \frac{1}{(n-1)\,s_x^2} \left[ \begin{matrix} (x^\mathrm{T} x/n) - 2 \bar{x} x_1 + x_1^2 & \cdots & (x^\mathrm{T} x/n) - \bar{x} (x_1 + x_n) + x_1 x_n \\ \vdots & \ddots & \vdots \\ (x^\mathrm{T} x/n) - \bar{x} (x_1 + x_n) + x_1 x_n & \cdots & (x^\mathrm{T} x/n) - 2 \bar{x} x_n + x_n^2 \end{matrix} \right] \\
&= \frac{1}{(n-1)\,s_x^2} \left[ \begin{matrix} (n-1) (x^\mathrm{T} x/n) + \bar{x} (2 x_1 - n\bar{x}) - x_1^2 & \cdots & -(x^\mathrm{T} x/n) + \bar{x} (x_1 + x_n) - x_1 x_n \\ \vdots & \ddots & \vdots \\ -(x^\mathrm{T} x/n) + \bar{x} (x_1 + x_n) - x_1 x_n & \cdots &  (n-1) (x^\mathrm{T} x/n) + \bar{x} (2 x_n - n\bar{x}) - x_n^2 \end{matrix} \right] \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Weighted least squares}]{Weighted least squares} \label{sec:slr-wls}
\setcounter{equation}{0}

\textbf{Theorem:} Given a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) with correlated observations

\begin{equation} \label{eq:slr-wls-slr}
y = \beta_0 + \beta_1 x + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; ,
\end{equation}

the parameters minimizing the weighted residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) are given by

\begin{equation} \label{eq:slr-wls-slr-wls}
\begin{split}
\hat{\beta}_0 &= \frac{x^\mathrm{T} V^{-1} x \, 1_n^\mathrm{T} V^{-1} y - 1_n^\mathrm{T} V^{-1} x \, x^\mathrm{T} V^{-1} y}{x^\mathrm{T} V^{-1} x \, 1_n^\mathrm{T} V^{-1} 1_n - 1_n^\mathrm{T} V^{-1} x \, x^\mathrm{T} V^{-1} 1_n} \\
\hat{\beta}_1 &= \frac{1_n^\mathrm{T} V^{-1} 1_n \, x^\mathrm{T} V^{-1} y - x^\mathrm{T} V^{-1} 1_n \, 1_n^\mathrm{T} V^{-1} y}{1_n^\mathrm{T} V^{-1} 1_n \, x^\mathrm{T} V^{-1} x - x^\mathrm{T} V^{-1} 1_n \, 1_n^\mathrm{T} V^{-1} x}
\end{split}
\end{equation}

where $1_n$ is an $n \times 1$ vector of ones.


\vspace{1em}
\textbf{Proof:} Let there be an $n \times n$ square matrix $W$, such that

\begin{equation} \label{eq:slr-wls-W-def}
W V W^\mathrm{T} = I_n \; .
\end{equation}

Since $V$ is a covariance matrix and thus symmetric, $W$ is also symmetric and can be expressed as the matrix square root of the inverse of $V$:

\begin{equation} \label{eq:slr-wls-W-V}
W V W = I_n \quad \Leftrightarrow \quad V = W^{-1} W^{-1} \quad \Leftrightarrow \quad V^{-1} = W W \quad \Leftrightarrow \quad W = V^{-1/2} \; .
\end{equation}

Because $\beta_0$ is a scalar, \eqref{eq:slr-wls-slr} may also be written as

\begin{equation} \label{eq:slr-wls-slr-s1}
y = \beta_0 1_n + \beta_1 x + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; ,
\end{equation}

Left-multiplying \eqref{eq:slr-wls-slr-s1} with $W$, the linear transformation theorem ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}) implies that

\begin{equation} \label{eq:slr-wls-slr-s2}
W y = \beta_0 W 1_n + \beta_1 W x + W \varepsilon, \; W \varepsilon \sim \mathcal{N}(0, \sigma^2 W V W^\mathrm{T}) \; .
\end{equation}

Applying \eqref{eq:slr-wls-W-def}, we see that \eqref{eq:slr-wls-slr-s2} is actually a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent observations

\begin{equation} \label{eq:slr-wls-slr-s3}
\tilde{y} = \left[ \begin{matrix} \tilde{x}_0 & \tilde{x} \end{matrix} \right] \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right] + \tilde{\varepsilon}, \; \tilde{\varepsilon} \sim \mathcal{N}(0, \sigma^2 I_n)
\end{equation}

where $\tilde{y} = Wy$, $\tilde{x}_0 = W 1_n$, $\tilde{x} = W x$ and $\tilde{\varepsilon} = W\varepsilon$, such that we can apply the ordinary least squares solution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}) giving:

\begin{equation} \label{eq:slr-wls-slr-wls-s1}
\begin{split}
\hat{\beta} &= (\tilde{X}^\mathrm{T} \tilde{X})^{-1} \tilde{X}^\mathrm{T} \tilde{y} \\
&= \left( \left[ \begin{matrix} \tilde{x}_0^\mathrm{T} \\ \tilde{x}^\mathrm{T} \end{matrix} \right] \left[ \begin{matrix} \tilde{x}_0 & \tilde{x} \end{matrix} \right] \right)^{-1} \left[ \begin{matrix} \tilde{x}_0^\mathrm{T} \\ \tilde{x}^\mathrm{T} \end{matrix} \right] \tilde{y} \\
&= \left[ \begin{matrix} \tilde{x}_0^\mathrm{T} \tilde{x}_0 & \tilde{x}_0^\mathrm{T} \tilde{x} \\ \tilde{x}^\mathrm{T} \tilde{x}_0 & \tilde{x}^\mathrm{T} \tilde{x} \end{matrix} \right]^{-1} \left[ \begin{matrix} \tilde{x}_0^\mathrm{T} \\ \tilde{x}^\mathrm{T} \end{matrix} \right] \tilde{y} \; .
\end{split}
\end{equation}

Applying the inverse of a $2 \times 2$ matrix, this reformulates to:

\begin{equation} \label{eq:slr-wls-slr-wls-s2}
\begin{split}
\hat{\beta} &= \frac{1}{\tilde{x}_0^\mathrm{T} \tilde{x}_0 \, \tilde{x}^\mathrm{T} \tilde{x} - \tilde{x}_0^\mathrm{T} \tilde{x} \, \tilde{x}^\mathrm{T} \tilde{x}_0} \left[ \begin{matrix} \tilde{x}^\mathrm{T} \tilde{x} & -\tilde{x}_0^\mathrm{T} \tilde{x} \\ -\tilde{x}^\mathrm{T} \tilde{x}_0 & \tilde{x}_0^\mathrm{T} \tilde{x}_0 \end{matrix} \right]^{-1} \left[ \begin{matrix} \tilde{x}_0^\mathrm{T} \\ \tilde{x}^\mathrm{T} \end{matrix} \right] \tilde{y} \\
&= \frac{1}{\tilde{x}_0^\mathrm{T} \tilde{x}_0 \, \tilde{x}^\mathrm{T} \tilde{x} - \tilde{x}_0^\mathrm{T} \tilde{x} \, \tilde{x}^\mathrm{T} \tilde{x}_0} \left[ \begin{matrix} \tilde{x}^\mathrm{T} \tilde{x} \, \tilde{x}_0^\mathrm{T} - \tilde{x}_0^\mathrm{T} \tilde{x} \, \tilde{x}^\mathrm{T} \\ \tilde{x}_0^\mathrm{T} \tilde{x}_0 \, \tilde{x}^\mathrm{T} - \tilde{x}^\mathrm{T} \tilde{x}_0 \, \tilde{x}_0^\mathrm{T} \end{matrix} \right] \tilde{y} \\
&= \frac{1}{\tilde{x}_0^\mathrm{T} \tilde{x}_0 \, \tilde{x}^\mathrm{T} \tilde{x} - \tilde{x}_0^\mathrm{T} \tilde{x} \, \tilde{x}^\mathrm{T} \tilde{x}_0} \left[ \begin{matrix} \tilde{x}^\mathrm{T} \tilde{x} \, \tilde{x}_0^\mathrm{T} \tilde{y} - \tilde{x}_0^\mathrm{T} \tilde{x} \, \tilde{x}^\mathrm{T} \tilde{y} \\ \tilde{x}_0^\mathrm{T} \tilde{x}_0 \, \tilde{x}^\mathrm{T} \tilde{y} - \tilde{x}^\mathrm{T} \tilde{x}_0 \, \tilde{x}_0^\mathrm{T} \tilde{y} \end{matrix} \right] \; .
\end{split}
\end{equation}

Applying $\tilde{x}_0 = W 1_n$, $\tilde{x} = W x$ and $W^\mathrm{T} W = W W = V^{-1}$, we finally have

\begin{equation} \label{eq:slr-wls-slr-wls-s3}
\begin{split}
\hat{\beta} &= \frac{1}{1_n^\mathrm{T} W^\mathrm{T} W 1_n \, x^\mathrm{T} W^\mathrm{T} W x - 1_n^\mathrm{T} W^\mathrm{T} W x \, x^\mathrm{T} W^\mathrm{T} W 1_n} \left[ \begin{matrix} x^\mathrm{T} W^\mathrm{T} W x \, 1_n^\mathrm{T} W^\mathrm{T} W y - 1_n^\mathrm{T} W^\mathrm{T} W x \, x^\mathrm{T} W^\mathrm{T} W y \\ 1_n^\mathrm{T} W^\mathrm{T} W 1_n \, x^\mathrm{T} W^\mathrm{T} W y - x^\mathrm{T} W^\mathrm{T} W 1_n \, 1_n^\mathrm{T} W^\mathrm{T} W y \end{matrix} \right] \\
&= \frac{1}{x^\mathrm{T} V^{-1} x \, 1_n^\mathrm{T} V^{-1} 1_n - 1_n^\mathrm{T} V^{-1} x \, x^\mathrm{T} V^{-1} 1_n} \left[ \begin{matrix} x^\mathrm{T} V^{-1} x \, 1_n^\mathrm{T} V^{-1} y - 1_n^\mathrm{T} V^{-1} x \, x^\mathrm{T} V^{-1} y \\ 1_n^\mathrm{T} V^{-1} 1_n \, x^\mathrm{T} V^{-1} y - x^\mathrm{T} V^{-1} 1_n \, 1_n^\mathrm{T} V^{-1} y \end{matrix} \right] \\
&= \left[ \begin{matrix} \frac{x^\mathrm{T} V^{-1} x \, 1_n^\mathrm{T} V^{-1} y - 1_n^\mathrm{T} V^{-1} x \, x^\mathrm{T} V^{-1} y}{x^\mathrm{T} V^{-1} x \, 1_n^\mathrm{T} V^{-1} 1_n - 1_n^\mathrm{T} V^{-1} x \, x^\mathrm{T} V^{-1} 1_n} \\ 
\frac{1_n^\mathrm{T} V^{-1} 1_n \, x^\mathrm{T} V^{-1} y - x^\mathrm{T} V^{-1} 1_n \, 1_n^\mathrm{T} V^{-1} y}{1_n^\mathrm{T} V^{-1} 1_n \, x^\mathrm{T} V^{-1} x - x^\mathrm{T} V^{-1} 1_n \, 1_n^\mathrm{T} V^{-1} x} \end{matrix} \right]
\end{split}
\end{equation}

which corresponds to the weighted least squares solution \eqref{eq:slr-wls-slr-wls}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Weighted least squares}]{Weighted least squares} \label{sec:slr-wls2}
\setcounter{equation}{0}

\textbf{Theorem:} Given a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) with correlated observations

\begin{equation} \label{eq:slr-wls2-slr}
y = \beta_0 + \beta_1 x + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; ,
\end{equation}

the parameters minimizing the weighted residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) are given by

\begin{equation} \label{eq:slr-wls2-slr-wls}
\begin{split}
\hat{\beta}_0 &= \frac{x^\mathrm{T} V^{-1} x \, 1_n^\mathrm{T} V^{-1} y - 1_n^\mathrm{T} V^{-1} x \, x^\mathrm{T} V^{-1} y}{x^\mathrm{T} V^{-1} x \, 1_n^\mathrm{T} V^{-1} 1_n - 1_n^\mathrm{T} V^{-1} x \, x^\mathrm{T} V^{-1} 1_n} \\
\hat{\beta}_1 &= \frac{1_n^\mathrm{T} V^{-1} 1_n \, x^\mathrm{T} V^{-1} y - x^\mathrm{T} V^{-1} 1_n \, 1_n^\mathrm{T} V^{-1} y}{1_n^\mathrm{T} V^{-1} 1_n \, x^\mathrm{T} V^{-1} x - x^\mathrm{T} V^{-1} 1_n \, 1_n^\mathrm{T} V^{-1} x}
\end{split}
\end{equation}

where $1_n$ is an $n \times 1$ vector of ones.


\vspace{1em}
\textbf{Proof:} Simple linear regression is a special case of multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-mlr}) with

\begin{equation} \label{eq:slr-wls2-slr-mlr}
X = \left[ \begin{matrix} 1_n & x \end{matrix} \right] \quad \text{and} \quad \beta = \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right]
\end{equation}

and weighted least squares estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wls}) are given by

\begin{equation} \label{eq:slr-wls2-mlr-wls}
\hat{\beta} = (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \; .
\end{equation}

Writing out equation \eqref{eq:slr-wls2-mlr-wls}, we have

\begin{equation} \label{eq:slr-wls2-slr-wls-b}
\begin{split}
\hat{\beta} &= \left( \left[ \begin{matrix} 1_n^\mathrm{T} \\ x^\mathrm{T} \end{matrix} \right] V^{-1} \left[ \begin{matrix} 1_n & x \end{matrix} \right] \right)^{-1} \left[ \begin{matrix} 1_n^\mathrm{T} \\ x^\mathrm{T} \end{matrix} \right] V^{-1} y \\
&= \left[ \begin{matrix} 1_n^\mathrm{T} V^{-1} 1_n & 1_n^\mathrm{T} V^{-1} x \\ x^\mathrm{T} V^{-1} 1_n & x^\mathrm{T} V^{-1} x \end{matrix} \right]^{-1} \left[ \begin{matrix} 1_n^\mathrm{T} V^{-1} y \\ x^\mathrm{T} V^{-1} y \end{matrix} \right] \\
&= \frac{1}{x^\mathrm{T} V^{-1} x \, 1_n^\mathrm{T} V^{-1} 1_n - 1_n^\mathrm{T} V^{-1} x \, x^\mathrm{T} V^{-1} 1_n} \left[ \begin{matrix} x^\mathrm{T} V^{-1} x & -1_n^\mathrm{T} V^{-1} x \\ -x^\mathrm{T} V^{-1} 1_n & 1_n^\mathrm{T} V^{-1} 1_n \end{matrix} \right] \left[ \begin{matrix} 1_n^\mathrm{T} V^{-1} y \\ x^\mathrm{T} V^{-1} y \end{matrix} \right] \\
&= \frac{1}{x^\mathrm{T} V^{-1} x \, 1_n^\mathrm{T} V^{-1} 1_n - 1_n^\mathrm{T} V^{-1} x \, x^\mathrm{T} V^{-1} 1_n} \left[ \begin{matrix} x^\mathrm{T} V^{-1} x \, 1_n^\mathrm{T} V^{-1} y - 1_n^\mathrm{T} V^{-1} x \, x^\mathrm{T} V^{-1} y \\ 1_n^\mathrm{T} V^{-1} 1_n \, x^\mathrm{T} V^{-1} y - x^\mathrm{T} V^{-1} 1_n \, 1_n^\mathrm{T} V^{-1} y \end{matrix} \right] \; .
\end{split}
\end{equation}

Thus, the first entry of $\hat{\beta}$ is equal to:

\begin{equation} \label{eq:slr-wls2-slr-wls-b1}
\hat{\beta}_0 = \frac{x^\mathrm{T} V^{-1} x \, 1_n^\mathrm{T} V^{-1} y - 1_n^\mathrm{T} V^{-1} x \, x^\mathrm{T} V^{-1} y}{x^\mathrm{T} V^{-1} x \, 1_n^\mathrm{T} V^{-1} 1_n - 1_n^\mathrm{T} V^{-1} x \, x^\mathrm{T} V^{-1} 1_n} \; .
\end{equation}

Moreover, the second entry of $\hat{\beta}$ is equal to ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-wls}):

\begin{equation} \label{eq:slr-wls2-slr-wls-b2}
\hat{\beta}_1 = \frac{1_n^\mathrm{T} V^{-1} 1_n \, x^\mathrm{T} V^{-1} y - x^\mathrm{T} V^{-1} 1_n \, 1_n^\mathrm{T} V^{-1} y}{1_n^\mathrm{T} V^{-1} 1_n \, x^\mathrm{T} V^{-1} x - x^\mathrm{T} V^{-1} 1_n \, 1_n^\mathrm{T} V^{-1} x} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Maximum likelihood estimation}]{Maximum likelihood estimation} \label{sec:slr-mle}
\setcounter{equation}{0}

\textbf{Theorem:} Given a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent observations

\begin{equation} \label{eq:slr-mle-slr}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n \; ,
\end{equation}

the maximum likelihood estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) of $\beta_0$, $\beta_1$ and $\sigma^2$ are given by

\begin{equation} \label{eq:slr-mle-slr-mle}
\begin{split}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{s_{xy}}{s_x^2} \\
\hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2
\end{split}
\end{equation}

where $\bar{x}$ and $\bar{y}$ are the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}), $s_x^2$ is the sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) of $x$ and $s_{xy}$ is the sample covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-samp}) between $x$ and $y$.


\vspace{1em}
\textbf{Proof:} With the probability density function of the normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm-pdf}) and probability under independence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the linear regression equation \eqref{eq:slr-mle-slr} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:slr-mle-slr-lf}
\begin{split}
p(y|\beta_0,\beta_1,\sigma^2) &= \prod_{i=1}^n p(y_i|\beta_0,\beta_1,\sigma^2) \\
&= \prod_{i=1}^n \mathcal{N}(y_i; \beta_0 + \beta_1 x_i, \sigma^2) \\
&= \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma}} \cdot \exp \left[ -\frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2 \sigma^2} \right] \\
&= \frac{1}{\sqrt{(2 \pi \sigma^2)^n}} \cdot \exp\left[ -\frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2 \right]
\end{split}
\end{equation}

and the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf})

\begin{equation} \label{eq:slr-mle-slr-ll}
\begin{split}
\mathrm{LL}(\beta_0,\beta_1,\sigma^2) &= \log p(y|\beta_0,\beta_1,\sigma^2) \\
&= -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log (\sigma^2) -\frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2 \; .
\end{split}
\end{equation}

\vspace{1em}
The derivative of the log-likelihood function \eqref{eq:slr-mle-slr-ll} with respect to $\beta_0$ is

\begin{equation} \label{eq:slr-mle-dLL-dbeta0}
\frac{\mathrm{d}\mathrm{LL}(\beta_0,\beta_1,\sigma^2)}{\mathrm{d}\beta_0} = \frac{1}{\sigma^2} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)
\end{equation}

and setting this derivative to zero gives the MLE for $\beta_0$:

\begin{equation} \label{eq:slr-mle-beta0-mle}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{\beta}_0,\hat{\beta}_1,\hat{\sigma}^2)}{\mathrm{d}\beta_0} &= 0 \\
0 &= \frac{1}{\hat{\sigma}^2} \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \\
0 &= \sum_{i=1}^n y_i - n \hat{\beta}_0 - \hat{\beta}_1 \sum_{i=1}^n x_i \\
\hat{\beta}_0 &= \frac{1}{n} \sum_{i=1}^n y_i - \hat{\beta}_1 \frac{1}{n} \sum_{i=1}^n x_i \\
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \; .
\end{split}
\end{equation}

\vspace{1em}
The derivative of the log-likelihood function \eqref{eq:slr-mle-slr-ll} at $\hat{\beta}_0$ with respect to $\beta_1$ is

\begin{equation} \label{eq:slr-mle-dLL-dbeta1}
\frac{\mathrm{d}\mathrm{LL}(\hat{\beta}_0,\beta_1,\sigma^2)}{\mathrm{d}\beta_1} = \frac{1}{\sigma^2} \sum_{i=1}^n (x_i y_i - \hat{\beta}_0 x_i - \beta_1 x_i^2) \\
\end{equation}

and setting this derivative to zero gives the MLE for $\beta_1$:

\begin{equation} \label{eq:slr-mle-beta1-mle}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{\beta}_0,\hat{\beta}_1,\hat{\sigma}^2)}{\mathrm{d}\beta_1} &= 0 \\
0 &= \frac{1}{\hat{\sigma}^2} \sum_{i=1}^n (x_i y_i - \hat{\beta}_0 x_i - \hat{\beta}_1 x_i^2) \\
0 &= \sum_{i=1}^n x_i y_i - \hat{\beta}_0 \sum_{i=1}^n x_i - \hat{\beta}_1 \sum_{i=1}^n x_i^2) \\
0 &\overset{\eqref{eq:slr-mle-beta0-mle}}{=} \sum_{i=1}^n x_i y_i - (\bar{y} - \hat{\beta}_1 \bar{x}) \sum_{i=1}^n x_i - \hat{\beta}_1 \sum_{i=1}^n x_i^2 \\
0 &= \sum_{i=1}^n x_i y_i - \bar{y} \sum_{i=1}^n x_i + \hat{\beta}_1 \bar{x} \sum_{i=1}^n x_i - \hat{\beta}_1 \sum_{i=1}^n x_i^2 \\
0 &= \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} + \hat{\beta}_1 n \bar{x}^2 - \hat{\beta}_1 \sum_{i=1}^n x_i^2 \\
\hat{\beta}_1 &= \frac{\sum_{i=1}^n x_i y_i - \sum_{i=1}^n \bar{x} \bar{y}}{\sum_{i=1}^n x_i^2 - \sum_{i=1}^n \bar{x}^2} \\
\hat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
\hat{\beta}_1 &= \frac{s_{xy}}{s_x^2} \; .
\end{split}
\end{equation}

\vspace{1em}
The derivative of the log-likelihood function \eqref{eq:slr-mle-slr-ll} at $(\hat{\beta}_0,\hat{\beta}_1)$ with respect to $\sigma^2$ is

\begin{equation} \label{eq:slr-mle-dLL-ds2}
\frac{\mathrm{d}\mathrm{LL}(\hat{\beta}_0,\hat{\beta}_1,\sigma^2)}{\mathrm{d}\sigma^2} = - \frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2
\end{equation}

and setting this derivative to zero gives the MLE for $\sigma^2$:

\begin{equation} \label{eq:slr-mle-s2-mle}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{\beta}_0,\hat{\beta}_1,\hat{\sigma}^2)}{\mathrm{d}\sigma^2} &= 0 \\
0 &= - \frac{n}{2\hat{\sigma}^2} + \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \\
\frac{n}{2\hat{\sigma}^2} &= \frac{1}{2(\hat{\sigma}^2)^2} \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \\
\hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \; .
\end{split}
\end{equation}

\vspace{1em}
Together, \eqref{eq:slr-mle-beta0-mle}, \eqref{eq:slr-mle-beta1-mle} and \eqref{eq:slr-mle-s2-mle} constitute the MLE for simple linear regression.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Maximum likelihood estimation}]{Maximum likelihood estimation} \label{sec:slr-mle2}
\setcounter{equation}{0}

\textbf{Theorem:} Given a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent observations

\begin{equation} \label{eq:slr-mle2-slr}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n \; ,
\end{equation}

the maximum likelihood estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) of $\beta_0$, $\beta_1$ and $\sigma^2$ are given by

\begin{equation} \label{eq:slr-mle2-slr-mle}
\begin{split}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{s_{xy}}{s_x^2} \\
\hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2
\end{split}
\end{equation}

where $\bar{x}$ and $\bar{y}$ are the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}), $s_x^2$ is the sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) of $x$ and $s_{xy}$ is the sample covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-samp}) between $x$ and $y$.


\vspace{1em}
\textbf{Proof:} Simple linear regression is a special case of multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-mlr}) with

\begin{equation} \label{eq:slr-mle2-slr-mlr}
X = \left[ \begin{matrix} 1_n & x \end{matrix} \right] \quad \text{and} \quad \beta = \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right]
\end{equation}

and weighted least sqaures estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle}) are given by

\begin{equation} \label{eq:slr-mle2-mlr-mle}
\begin{split}
\hat{\beta} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \\
\hat{\sigma}^2 &= \frac{1}{n} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) \; .
\end{split}
\end{equation}

Under independent observations, the covariance matrix is

\begin{equation} \label{eq:slr-mle2-mlr-ind}
V = I_n, \quad \text{such that} \quad V^{-1} = I_n \; .
\end{equation}

Thus, we can write out the estimate of $\beta$

\begin{equation} \label{eq:slr-mle2-slr-mle-b}
\begin{split}
\hat{\beta} &= \left( \left[ \begin{matrix} 1_n^\mathrm{T} \\ x^\mathrm{T} \end{matrix} \right] V^{-1} \left[ \begin{matrix} 1_n & x \end{matrix} \right] \right)^{-1} \left[ \begin{matrix} 1_n^\mathrm{T} \\ x^\mathrm{T} \end{matrix} \right] V^{-1} y \\
&= \left( \left[ \begin{matrix} 1_n^\mathrm{T} \\ x^\mathrm{T} \end{matrix} \right] \left[ \begin{matrix} 1_n & x \end{matrix} \right] \right)^{-1} \left[ \begin{matrix} 1_n^\mathrm{T} \\ x^\mathrm{T} \end{matrix} \right] y
\end{split}
\end{equation}

which is equal to the ordinary least squares solution for simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols2}):

\begin{equation} \label{eq:slr-mle2-slr-mle-b-qed}
\begin{split}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{s_{xy}}{s_x^2} \; .
\end{split}
\end{equation}

Additionally, we can write out the estimate of $\sigma^2$:

\begin{equation} \label{eq:slr-mle2-slr-mle-s2}
\begin{split}
\hat{\sigma}^2 &= \frac{1}{n} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) \\
&= \frac{1}{n} \left( y - \left[ \begin{matrix} 1_n & x \end{matrix} \right] \left[ \begin{matrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{matrix} \right] \right)^\mathrm{T} \left( y - \left[ \begin{matrix} 1_n & x \end{matrix} \right] \left[ \begin{matrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{matrix} \right] \right) \\
&= \frac{1}{n} \left( y - \hat{\beta}_0 - \hat{\beta}_1 x \right)^\mathrm{T} \left( y - \hat{\beta}_0 - \hat{\beta}_1 x \right) \\
&= \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{t-test for intercept parameter}]{t-test for intercept parameter} \label{sec:slr-tint}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) with independent observations

\begin{equation} \label{eq:slr-tint-slr}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n \; ,
\end{equation}

and the parameter estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-mle})

\begin{equation} \label{eq:slr-tint-slr-est}
\begin{split}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{s_{xy}}{s_x^2} \\
\hat{\sigma}^2 &= \frac{1}{n-2} \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \; .
\end{split}
\end{equation}

where $\bar{x}$ and $\bar{y}$ are the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) of the $x_i$ and $y_i$, $s_{xy}$ is the sample covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-samp}) of the $x_i$ and $y_i$ and $s_x^2$ is the sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) of the $x_i$.

Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:slr-tint-slr-t-int}
t_0 = \frac{\bar{y} - \hat{\beta}_1 \bar{x}}{\sqrt{\hat{\sigma}^2 \; \sigma_0}}
\end{equation}

with $\sigma_0$ equal to the first diagonal element of the parameter covariance matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-olsdist})

\begin{equation} \label{eq:slr-tint-slr-t-int-sig}
\sigma_0 = \frac{x^\mathrm{T}x/n}{(n-1) \, s_x^2} \quad \text{where} \quad s_x^2 = \frac{1}{n-1} \sum_{i=1}^{n} \left( x_i - \bar{x} \right)^2
\end{equation}

follows a t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t})

\begin{equation} \label{eq:slr-tint-slr-t-int-dist}
t_0 \sim \mathrm{t}(n-2)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that the intercept parameter ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) is zero:

\begin{equation} \label{eq:slr-tint-slr-t-int-h0}
H_0: \; \beta_0 = 0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} In multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}), the contrast-based t-test ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-t}) is based on the t-statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:slr-tint-mlr-t}
t = \frac{c^\mathrm{T} \hat{\beta}}{\sqrt{\hat{\sigma}^2 c^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} c}}
\end{equation}

which follows a t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that the scalar product of the contrast vector ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tcon}) and the regression coefficients ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) is zero: 

\begin{equation} \label{eq:slr-tint-mlr-t-dist-h0}
t \sim \mathrm{t}(n-p), \quad \text{if} \quad c^\mathrm{T} \beta = 0 \; .
\end{equation}

Since simple linear regression is a special case of multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-mlr}), in the present case we have the following quantities:

\begin{equation} \label{eq:slr-tint-slr-mlr}
\beta = \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right], \;
\hat{\beta} = \left[ \begin{matrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{matrix} \right], \;
c_0 = \left[ \begin{matrix} 1 \\ 0 \end{matrix} \right], \;
X = \left[ \begin{matrix} 1_n & x \end{matrix} \right], \;
V = I_n \; .
\end{equation}

Thus, we have the null hypothesis

\begin{equation} \label{eq:slr-tint-slr-t-int-h0-qed}
H_0: \; c_0^\mathrm{T} \beta = \left[ \begin{matrix} 1 \\ 0 \end{matrix} \right]^\mathrm{T} \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right] = \beta_0 = 0
\end{equation}

and the contrast estimate

\begin{equation} \label{eq:slr-tint-slr-t-int-cTb}
c_0^\mathrm{T} \hat{\beta} = \left[ \begin{matrix} 1 \\ 0 \end{matrix} \right]^\mathrm{T} \left[ \begin{matrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{matrix} \right] = \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} \; .
\end{equation}

Moreover, when deriving the distribution of ordinary least squares parameter estimates for simple linear regression with independent observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-olsdist}), we have identified the parameter covariance matrix as

\begin{equation} \label{eq:slr-tint-slr-XTX-inv}
(X^\mathrm{T} X)^{-1} = \frac{1}{(n-1) \, s_x^2} \cdot \left[ \begin{matrix} x^\mathrm{T}x/n & -\bar{x} \\ -\bar{x} & 1 \end{matrix} \right] \; .
\end{equation}

Plugging \eqref{eq:slr-tint-slr-mlr}, \eqref{eq:slr-tint-slr-t-int-cTb}, \eqref{eq:slr-tint-slr-XTX-inv} and \eqref{eq:slr-tint-slr-est} into \eqref{eq:slr-tint-mlr-t}, the test statistic becomes

\begin{equation} \label{eq:slr-tint-slr-t-int-qed}
\begin{split}
t_0 &= \frac{c_0^\mathrm{T} \hat{\beta}}{\sqrt{\hat{\sigma}^2 \; c_0^\mathrm{T} (X^\mathrm{T} X)^{-1} c_0}} \\
&= \frac{\left[ \begin{matrix} 1 & 0 \end{matrix} \right] \left[ \begin{matrix} \hat{\beta}_0 & \hat{\beta}_1 \end{matrix} \right]^\mathrm{T}}{\sqrt{\hat{\sigma}^2 \left[ \begin{matrix} 1 & 0 \end{matrix} \right] (X^\mathrm{T} X)^{-1} \left[ \begin{matrix} 1 & 0 \end{matrix} \right]^\mathrm{T}}} \\
&= \frac{\left[ \begin{matrix} 1 & 0 \end{matrix} \right] \left[ \begin{matrix} \hat{\beta}_0 & \hat{\beta}_1 \end{matrix} \right]^\mathrm{T}}{\sqrt{\hat{\sigma}^2 \left[ \begin{matrix} 1 & 0 \end{matrix} \right] \left( \frac{1}{(n-1) \, s_x^2} \cdot \left[ \begin{matrix} x^\mathrm{T}x/n & -\bar{x} \\ -\bar{x} & 1 \end{matrix} \right] \right) \left[ \begin{matrix} 1 & 0 \end{matrix} \right]^\mathrm{T}}} \\
&= \frac{\hat{\beta}_0}{\sqrt{\hat{\sigma}^2 \left( \frac{x^\mathrm{T}x/n}{(n-1) \, s_x^2} \right)}} \\
&= \frac{\bar{y} - \hat{\beta}_1 \bar{x}}{\sqrt{\hat{\sigma}^2 \; \sigma_0}} \; .
\end{split}
\end{equation}

Finally, because $X = \left[ \begin{matrix} 1_n & x \end{matrix} \right]$ is an $n \times 2$ matrix, we have $p = 2$, such that from \eqref{eq:slr-tint-mlr-t-dist-h0}, it follows that

\begin{equation} \label{eq:slr-tint-slr-t-int-dist-qed}
\begin{split}
t_0 \sim \mathrm{t}(n-2), \quad \text{if} \quad \beta_0 = 0 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{t-test for slope parameter}]{t-test for slope parameter} \label{sec:slr-tslo}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) with independent observations

\begin{equation} \label{eq:slr-tslo-slr}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n \; ,
\end{equation}

and the parameter estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-mle})

\begin{equation} \label{eq:slr-tslo-slr-est}
\begin{split}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{s_{xy}}{s_x^2} \\
\hat{\sigma}^2 &= \frac{1}{n-2} \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \; .
\end{split}
\end{equation}

where $\bar{x}$ and $\bar{y}$ are the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) of the $x_i$ and $y_i$, $s_{xy}$ is the sample covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-samp}) of the $x_i$ and $y_i$ and $s_x^2$ is the sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) of the $x_i$.

Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:slr-tslo-slr-t-slo}
t_1 = \frac{s_{xy}/s_x^2}{\sqrt{\hat{\sigma}^2 \; \sigma_1}}
\end{equation}

with $\sigma_1$ equal to the first diagonal element of the parameter covariance matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-olsdist})

\begin{equation} \label{eq:slr-tslo-slr-t-slo-sig}
\sigma_1 = \frac{1}{\sum_{i=1}^{n} \left( x_i - \bar{x} \right)^2}
\end{equation}

follows a t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t})

\begin{equation} \label{eq:slr-tslo-slr-t-slo-dist}
t_1 \sim \mathrm{t}(n-2)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that the slope parameter ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) is zero:

\begin{equation} \label{eq:slr-tslo-slr-t-slo-h0}
H_0: \; \beta_1 = 0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} In multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}), the contrast-based t-test ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-t}) is based on the t-statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:slr-tslo-mlr-t}
t = \frac{c^\mathrm{T} \hat{\beta}}{\sqrt{\hat{\sigma}^2 c^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} c}}
\end{equation}

which follows a t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that the scalar product of the contrast vector ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tcon}) and the regression coefficients ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) is zero: 

\begin{equation} \label{eq:slr-tslo-mlr-t-dist-h0}
t \sim \mathrm{t}(n-p), \quad \text{if} \quad c^\mathrm{T} \beta = 0 \; .
\end{equation}

Since simple linear regression is a special case of multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-mlr}), in the present case we have the following quantities:

\begin{equation} \label{eq:slr-tslo-slr-mlr}
\beta = \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right], \;
\hat{\beta} = \left[ \begin{matrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{matrix} \right], \;
c_1 = \left[ \begin{matrix} 0 \\ 1 \end{matrix} \right], \;
X = \left[ \begin{matrix} 1_n & x \end{matrix} \right], \;
V = I_n \; .
\end{equation}

Thus, we have the null hypothesis

\begin{equation} \label{eq:slr-tslo-slr-t-slo-h0-qed}
H_0: \; c_1^\mathrm{T} \beta = \left[ \begin{matrix} 0 \\ 1 \end{matrix} \right]^\mathrm{T} \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right] = \beta_1 = 0
\end{equation}

and the contrast estimate

\begin{equation} \label{eq:slr-tslo-slr-t-slo-cTb}
c_1^\mathrm{T} \hat{\beta} = \left[ \begin{matrix} 0 \\ 1 \end{matrix} \right]^\mathrm{T} \left[ \begin{matrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{matrix} \right] = \hat{\beta}_1 = \frac{s_{xy}}{s_x^2} \; .
\end{equation}

Moreover, when deriving the distribution of ordinary least squares parameter estimates for simple linear regression with independent observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-olsdist}), we have identified the parameter covariance matrix as

\begin{equation} \label{eq:slr-tslo-slr-XTX-inv}
(X^\mathrm{T} X)^{-1} = \frac{1}{(n-1) \, s_x^2} \cdot \left[ \begin{matrix} x^\mathrm{T}x/n & -\bar{x} \\ -\bar{x} & 1 \end{matrix} \right] \; .
\end{equation}

Plugging \eqref{eq:slr-tslo-slr-mlr}, \eqref{eq:slr-tslo-slr-t-slo-cTb}, \eqref{eq:slr-tslo-slr-XTX-inv} and \eqref{eq:slr-tslo-slr-est} into \eqref{eq:slr-tslo-mlr-t}, the test statistic becomes

\begin{equation} \label{eq:slr-tslo-slr-t-slo-qed}
\begin{split}
t_1 &= \frac{c_1^\mathrm{T} \hat{\beta}}{\sqrt{\hat{\sigma}^2 \; c_1^\mathrm{T} (X^\mathrm{T} X)^{-1} c_1}} \\
&= \frac{\left[ \begin{matrix} 0 & 1 \end{matrix} \right] \left[ \begin{matrix} \hat{\beta}_0 & \hat{\beta}_1 \end{matrix} \right]^\mathrm{T}}{\sqrt{\hat{\sigma}^2 \left[ \begin{matrix} 0 & 1 \end{matrix} \right] (X^\mathrm{T} X)^{-1} \left[ \begin{matrix} 0 & 1 \end{matrix} \right]^\mathrm{T}}} \\
&= \frac{\left[ \begin{matrix} 0 & 1 \end{matrix} \right] \left[ \begin{matrix} \hat{\beta}_0 & \hat{\beta}_1 \end{matrix} \right]^\mathrm{T}}{\sqrt{\hat{\sigma}^2 \left[ \begin{matrix} 0 & 1 \end{matrix} \right] \left( \frac{1}{(n-1) \, s_x^2} \cdot \left[ \begin{matrix} x^\mathrm{T}x/n & -\bar{x} \\ -\bar{x} & 1 \end{matrix} \right] \right) \left[ \begin{matrix} 0 & 1 \end{matrix} \right]^\mathrm{T}}} \\
&= \frac{\hat{\beta}_1}{\sqrt{\hat{\sigma}^2 \left( \frac{1}{(n-1) \, s_x^2} \right)}} \\
&= \frac{\hat{\beta}_1}{\sqrt{\hat{\sigma}^2 / \sum_{i=1}^{n} \left( x_i - \bar{x} \right)^2}} \\
&= \frac{s_{xy}/s_x^2}{\sqrt{\hat{\sigma}^2 \; \sigma_1}} \; .
\end{split}
\end{equation}

Finally, because $X = \left[ \begin{matrix} 1_n & x \end{matrix} \right]$ is an $n \times 2$ matrix, we have $p = 2$, such that from \eqref{eq:slr-tslo-mlr-t-dist-h0} it follows that

\begin{equation} \label{eq:slr-tslo-slr-t-slo-dist-qed}
\begin{split}
t_1 \sim \mathrm{t}(n-2), \quad \text{if} \quad \beta_1 = 0 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{F-test for model comparison}]{F-test for model comparison} \label{sec:slr-fcomp}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) with independent observations

\begin{equation} \label{eq:slr-fcomp-slr}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n \; ,
\end{equation}

and the parameter estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-mle})

\begin{equation} \label{eq:slr-fcomp-slr-est}
\begin{split}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{s_{xy}}{s_x^2} \\
\hat{\sigma}^2 &= \frac{1}{n-2} \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \; .
\end{split}
\end{equation}

where $\bar{x}$ and $\bar{y}$ are the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) of the $x_i$ and $y_i$, $s_{xy}$ is the sample covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-samp}) of the $x_i$ and $y_i$ and $s_x^2$ is the sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) of the $x_i$.

Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:slr-fcomp-slr-f-comp}
F = \frac{s_{xy}^2/s_x^2}{\hat{\sigma}^2/(n-1)}
\end{equation}

follows an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f})

\begin{equation} \label{eq:slr-fcomp-slr-f-comp-dist}
F \sim \mathrm{F}(1, n-2)
\end{equation}

under the scenario that the data were generated using a model in which the slope parameter ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) is zero:

\begin{equation} \label{eq:slr-fcomp-slr-f-comp-h0}
H_0: \; \beta_1 = 0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} In multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}), the contrast-based F-test ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-f}) is based on the F-statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:slr-fcomp-mlr-f}
F = \hat{\beta}^\mathrm{T} C \left( \hat{\sigma}^2 C^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C \right)^{-1} C^\mathrm{T} \hat{\beta} / q
\end{equation}

which follows an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}) under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that the product of the contrast matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:fcon}) $C \in \mathbb{R}^{p \times q}$ and the regression coefficients ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) is a zero vector: 

\begin{equation} \label{eq:slr-fcomp-mlr-f-dist-h0}
F \sim \mathrm{F}(q, n-p), \quad \text{if} \quad C^\mathrm{T} \beta = 0_q = \left[ 0, \ldots, 0 \right]^\mathrm{T} \; .
\end{equation}

Since simple linear regression is a special case of multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-mlr}), we have the following quantities, if we want to compare the regression model against a model without the slope parameter:

\begin{equation} \label{eq:slr-fcomp-slr-mlr}
\beta = \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right], \;
\hat{\beta} = \left[ \begin{matrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{matrix} \right], \;
C = \left[ \begin{matrix} 0 \\ 1 \end{matrix} \right], \;
X = \left[ \begin{matrix} 1_n & x \end{matrix} \right], \;
V = I_n \; .
\end{equation}

Thus, we have the null hypothesis

\begin{equation} \label{eq:slr-fcomp-slr-f-comp-h0-qed}
H_0: \; C^\mathrm{T} \beta = \left[ \begin{matrix} 0 \\ 1 \end{matrix} \right]^\mathrm{T} \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right] = \beta_1 = 0
\end{equation}

and the contrast estimate

\begin{equation} \label{eq:slr-fcomp-slr-f-comp-CTb}
C^\mathrm{T} \hat{\beta} = \left[ \begin{matrix} 0 \\ 1 \end{matrix} \right]^\mathrm{T} \left[ \begin{matrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{matrix} \right] = \hat{\beta}_1 = \frac{s_{xy}}{s_x^2} \; .
\end{equation}

Moreover, when deriving the distribution of ordinary least squares parameter estimates for simple linear regression with independent observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-olsdist}), we have identified the parameter covariance matrix as

\begin{equation} \label{eq:slr-fcomp-slr-XTX-inv}
(X^\mathrm{T} X)^{-1} = \frac{1}{(n-1) \, s_x^2} \cdot \left[ \begin{matrix} x^\mathrm{T}x/n & -\bar{x} \\ -\bar{x} & 1 \end{matrix} \right] \; .
\end{equation}

Plugging \eqref{eq:slr-fcomp-slr-mlr}, \eqref{eq:slr-fcomp-slr-f-comp-CTb}, \eqref{eq:slr-fcomp-slr-XTX-inv} and \eqref{eq:slr-fcomp-slr-est} into \eqref{eq:slr-fcomp-mlr-f}, the test statistic becomes

\begin{equation} \label{eq:slr-fcomp-slr-f-comp-qed}
\begin{split}
F &= \hat{\beta}^\mathrm{T} C \left( \hat{\sigma}^2 C^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C \right)^{-1} C^\mathrm{T} \hat{\beta} / q \\
&= \left( \frac{s_{xy}}{s_x^2} \right) \left( \hat{\sigma}^2 \left[ \begin{matrix} 0 & 1 \end{matrix} \right] \left( \frac{1}{(n-1) \, s_x^2} \cdot \left[ \begin{matrix} x^\mathrm{T}x/n & -\bar{x} \\ -\bar{x} & 1 \end{matrix} \right] \right) \left[ \begin{matrix} 0 & 1 \end{matrix} \right]^\mathrm{T} \right)^{-1} \left( \frac{s_{xy}}{s_x^2} \right) / 1 \\
&= \frac{s_{xy}^2/(s_x^2)^2}{\hat{\sigma}^2/((n-1) \, s_x^2)} \\
&= \frac{s_{xy}^2/s_x^2}{\hat{\sigma}^2/(n-1)} \; .
\end{split}
\end{equation}

Finally, because $C = \left[ \begin{matrix} 0 & 1 \end{matrix} \right]^\mathrm{T} \in \mathbb{R}^{2 \times 1}$ and $X = \left[ \begin{matrix} 1_n & x \end{matrix} \right] \in \mathbb{R}^{n \times 2}$, we have $p = 2$ and $q = 1$, such that from \eqref{eq:slr-fcomp-mlr-f-dist-h0} it follows that

\begin{equation} \label{eq:slr-fcomp-slr-f-comp-dist-qed}
\begin{split}
F \sim \mathrm{F}(1, n-2), \quad \text{if} \quad \beta_1 = 0 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Sum of residuals is zero}]{Sum of residuals is zero} \label{sec:slr-ressum}
\setcounter{equation}{0}

\textbf{Theorem:} In simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}), the sum of the residuals ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) is zero when estimated using ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}).

\vspace{1em}
\textbf{Proof:} The residuals are defined as the estimated error terms ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr})

\begin{equation} \label{eq:slr-ressum-slr-res}
\hat{\varepsilon}_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i
\end{equation}

where $\hat{\beta}_0$ and $\hat{\beta}_1$ are parameter estimates obtained using ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}):

\begin{equation} \label{eq:slr-ressum-slr-ols}
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} \quad \text{and} \quad \hat{\beta}_1 = \frac{s_{xy}}{s_x^2} \; .
\end{equation}

With that, we can calculate the sum of the residuals:

\begin{equation} \label{eq:slr-ressum-slr-ressum}
\begin{split}
\sum_{i=1}^n \hat{\varepsilon}_i &= \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \\
&= \sum_{i=1}^n (y_i - \bar{y} + \hat{\beta}_1 \bar{x} - \hat{\beta}_1 x_i) \\
&= \sum_{i=1}^n y_i - n \bar{y} + \hat{\beta}_1 n \bar{x} - \hat{\beta}_1 \sum_{i=1}^n x_i \\
&= n \bar{y} - n \bar{y} + \hat{\beta}_1 n \bar{x} - \hat{\beta}_1 n \bar{x} \\
&= 0 \; .
\end{split}
\end{equation}

Thus, the sum of the residuals ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) is zero under ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}), if the model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) includes an intercept term $\beta_0$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Simple linear regression"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-27; URL: \url{https://en.wikipedia.org/wiki/Simple_linear_regression#Numerical_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Correlation with covariate is zero}]{Correlation with covariate is zero} \label{sec:slr-rescorr}
\setcounter{equation}{0}

\textbf{Theorem:} In simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}), the residuals ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) and the covariate ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) are uncorrelated ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr}) when estimated using ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}).

\vspace{1em}
\textbf{Proof:} The residuals are defined as the estimated error terms ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr})

\begin{equation} \label{eq:slr-rescorr-slr-res}
\hat{\varepsilon}_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i
\end{equation}

where $\hat{\beta}_0$ and $\hat{\beta}_1$ are parameter estimates obtained using ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}):

\begin{equation} \label{eq:slr-rescorr-slr-ols}
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} \quad \text{and} \quad \hat{\beta}_1 = \frac{s_{xy}}{s_x^2} \; .
\end{equation}

With that, we can calculate the inner product of the covariate and the residuals vector:

\begin{equation} \label{eq:slr-rescorr-slr-rescorr}
\begin{split}
\sum_{i=1}^n x_i \hat{\varepsilon}_i &= \sum_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) \\
&= \sum_{i=1}^n \left( x_i y_i - \hat{\beta}_0 x_i - \hat{\beta}_1 x_i^2 \right) \\
&= \sum_{i=1}^n \left( x_i y_i - x_i (\bar{y} - \hat{\beta}_1 \bar{x}) - \hat{\beta}_1 x_i^2 \right) \\
&= \sum_{i=1}^n \left( x_i (y_i - \bar{y}) + \hat{\beta}_1 (\bar{x} x_i - x_i^2 \right) \\
&= \sum_{i=1}^n x_i y_i - \bar{y} \sum_{i=1}^n x_i - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - \bar{x} \sum_{i=1}^n x_i \right) \\
&= \left( \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} - n \bar{x} \bar{y} + n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - 2 n \bar{x} \bar{x} + n \bar{x}^2 \right) \\
&= \left( \sum_{i=1}^n x_i y_i - \bar{y} \sum_{i=1}^n x_i - \bar{x} \sum_{i=1}^n y_i + n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - 2 \bar{x} \sum_{i=1}^n x_i + n \bar{x}^2 \right) \\
&= \sum_{i=1}^n \left( x_i y_i - \bar{y} x_i - \bar{x} y_i + \bar{x} \bar{y} \right) - \hat{\beta}_1 \sum_{i=1}^n \left( x_i^2 - 2 \bar{x} x_i + \bar{x}^2 \right) \\
&= \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) - \hat{\beta}_1 \sum_{i=1}^n (x_i - \bar{x})^2 \\
&= (n-1) s_{xy} - \frac{s_{xy}}{s_x^2} (n-1) s_x^2 \\
&= (n-1) s_{xy} - (n-1) s_{xy} \\
&= 0 \; .
\end{split}
\end{equation}

Because an inner product of zero also implies zero correlation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr}), this demonstrates that residuals ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) and covariate ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) values are uncorrelated under ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}).
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Simple linear regression"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-27; URL: \url{https://en.wikipedia.org/wiki/Simple_linear_regression#Numerical_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Residual variance in terms of sample variance}]{Residual variance in terms of sample variance} \label{sec:slr-resvar}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) with independent observations

\begin{equation} \label{eq:slr-resvar-slr}
y = \beta_0 + \beta_1 x + \varepsilon, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n
\end{equation}

and consider estimation using ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}). Then, residual variance ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:resvar}) and sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) are related to each other via the correlation coefficient ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr}):

\begin{equation} \label{eq:slr-resvar-slr-vars}
\hat{\sigma}^2 = \left( 1 - r_{xy}^2 \right) s_y^2 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The residual variance ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:resvar}) can be expressed in terms of the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}):

\begin{equation} \label{eq:slr-resvar-slr-res}
\hat{\sigma}^2 = \frac{1}{n-1} \, \mathrm{RSS}(\hat{\beta}_0,\hat{\beta}_1)
\end{equation}

and the residual sum of squares for simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-sss}) is

\begin{equation} \label{eq:slr-resvar-slr-rss}
\mathrm{RSS}(\hat{\beta}_0,\hat{\beta}_1) = (n-1) \left( s_y^2 - \frac{s_{xy}^2}{s_x^2} \right) \; .
\end{equation}

Combining \eqref{eq:slr-resvar-slr-res} and \eqref{eq:slr-resvar-slr-rss}, we obtain:

\begin{equation} \label{eq:slr-resvar-slr-vars-s1}
\begin{split}
\hat{\sigma}^2 &= \left( s_y^2 - \frac{s_{xy}^2}{s_x^2} \right) \\
&= \left( 1 - \frac{s_{xy}^2}{s_x^2 s_y^2} \right) s_y^2 \\
&= \left( 1 - \left( \frac{s_{xy}}{s_x \, s_y} \right)^2 \right) s_y^2 \; .
\end{split}
\end{equation}

Using the relationship between correlation, covariance and standard deviation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr})

\begin{equation} \label{eq:slr-resvar-corr-cov-std}
\mathrm{Corr}(X,Y) = \frac{\mathrm{Cov}(X,Y)}{\sqrt{\mathrm{Var}(X)} \sqrt{\mathrm{Var}(Y)}}
\end{equation}

which also holds for sample correlation, sample covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-samp}) and sample standard deviation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:std})

\begin{equation} \label{eq:slr-resvar-corr-cov-std-samp}
r_{xy} = \frac{s_{xy}}{s_x \, s_y} \; ,
\end{equation}

we get the final result:

\begin{equation} \label{eq:slr-resvar-slr-vars-s2}
\hat{\sigma}^2 = \left( 1 - r_{xy}^2 \right) s_y^2 \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Penny, William (2006): "Relation to correlation"; in: \textit{Mathematics for Brain Imaging}, ch. 1.2.3, p. 18, eq. 1.28; URL: \url{https://ueapsylabs.co.uk/sites/wpenny/mbi/mbi_course.pdf}.
\item Wikipedia (2021): "Simple linear regression"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-27; URL: \url{https://en.wikipedia.org/wiki/Simple_linear_regression#Numerical_properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Correlation coefficient in terms of slope estimate}]{Correlation coefficient in terms of slope estimate} \label{sec:slr-corr}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) with independent observations

\begin{equation} \label{eq:slr-corr-slr}
y = \beta_0 + \beta_1 x + \varepsilon, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n
\end{equation}

and consider estimation using ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}). Then, correlation coefficient ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr-samp}) and the estimated value of the slope parameter ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) are related to each other via the sample ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}) standard deviations ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:std}):

\begin{equation} \label{eq:slr-corr-slr-corr}
r_{xy} = \frac{s_x}{s_y} \, \hat{\beta}_1 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The ordinary least squares estimate of the slope ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}) is given by

\begin{equation} \label{eq:slr-corr-slr-ols-sl}
\hat{\beta}_1 = \frac{s_{xy}}{s_x^2} \; .
\end{equation}

Using the relationship between covariance and correlation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-corr})

\begin{equation} \label{eq:slr-corr-cov-corr}
\mathrm{Cov}(X,Y) = \sigma_X \, \mathrm{Corr}(X,Y) \, \sigma_Y
\end{equation}

which also holds for sample correlation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr-samp}) and sample covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-samp})

\begin{equation} \label{eq:slr-corr-cov-corr-samp}
s_{xy} = s_x \, r_{xy} \, s_y \; ,
\end{equation}

we get the final result:

\begin{equation} \label{eq:slr-corr-slr-corr-qed}
\begin{split}
\hat{\beta}_1 &= \frac{s_{xy}}{s_x^2} \\
\hat{\beta}_1 &= \frac{s_x \, r_{xy} \, s_y}{s_x^2} \\
\hat{\beta}_1 &= \frac{s_y}{s_x} \, r_{xy} \\
\Leftrightarrow \quad r_{xy} &= \frac{s_x}{s_y} \, \hat{\beta}_1 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Penny, William (2006): "Relation to correlation"; in: \textit{Mathematics for Brain Imaging}, ch. 1.2.3, p. 18, eq. 1.27; URL: \url{https://ueapsylabs.co.uk/sites/wpenny/mbi/mbi_course.pdf}.
\item Wikipedia (2021): "Simple linear regression"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-27; URL: \url{https://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Coefficient of determination in terms of correlation coefficient}]{Coefficient of determination in terms of correlation coefficient} \label{sec:slr-rsq}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a simple linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr}) with independent observations

\begin{equation} \label{eq:slr-rsq-slr}
y = \beta_0 + \beta_1 x + \varepsilon, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n
\end{equation}

and consider estimation using ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}). Then, the coefficient of determination ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq}) is equal to the squared correlation coefficient ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corr-samp}) between $x$ and $y$:

\begin{equation} \label{eq:slr-rsq-slr-R2}
R^2 = r_{xy}^2 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The ordinary least squares estimates for simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-ols}) are

\begin{equation} \label{eq:slr-rsq-slr-ols}
\begin{split}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &= \frac{s_{xy}}{s_x^2} \; .
\end{split}
\end{equation}

The coefficient of determination ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq}) $R^2$ is defined as the proportion of the variance explained by the independent variables, relative to the total variance in the data. This can be quantified as the ratio of explained sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess}) to total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}):

\begin{equation} \label{eq:slr-rsq-slr-R2-s1}
R^2 = \frac{\mathrm{ESS}}{\mathrm{TSS}} \; .
\end{equation}

Using the explained and total sum of squares for simple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-sss}), we have:

\begin{equation} \label{eq:slr-rsq-slr-R2-s2}
\begin{split}
R^2 &= \frac{\sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} \\
&= \frac{\sum_{i=1}^{n} (\hat{\beta}_0 + \hat{\beta}_1 x_i - \bar{y})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} \; .
\end{split}
\end{equation}

By applying \eqref{eq:slr-rsq-slr-ols}, we can further develop the coefficient of determination:

\begin{equation} \label{eq:slr-rsq-slr-R2-s3}
\begin{split}
R^2 &= \frac{\sum_{i=1}^{n} (\bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 x_i - \bar{y})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} \\
&= \frac{\sum_{i=1}^{n} \left( \hat{\beta}_1 (x_i - \bar{x}) \right)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} \\
&= \hat{\beta}_1^2 \, \frac{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2}{\frac{1}{n-1} \sum_{i=1}^{n} (y_i - \bar{y})^2} \\
&= \hat{\beta}_1^2 \, \frac{s_x^2}{s_y^2} \\ 
&= \left( \frac{s_x}{s_y} \, \hat{\beta}_1 \right)^2 \; .
\end{split}
\end{equation}

Using the relationship between correlation coefficient and slope estimate ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:slr-corr}), we conclude:

\begin{equation} \label{eq:slr-rsq-slr-R2-qed}
R^2 = \left( \frac{s_x}{s_y} \, \hat{\beta}_1 \right)^2 = r_{xy}^2 \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2021): "Simple linear regression"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-27; URL: \url{https://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line}.
\item Wikipedia (2021): "Coefficient of determination"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-27; URL: \url{https://en.wikipedia.org/wiki/Coefficient_of_determination#As_squared_correlation_coefficient}.
\item Wikipedia (2021): "Correlation"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-10-27; URL: \url{https://en.wikipedia.org/wiki/Correlation#Sample_correlation_coefficient}.
\end{itemize}
\vspace{1em}



\subsection{Multiple linear regression}

\subsubsection[\textit{Definition}]{Definition} \label{sec:mlr}
\setcounter{equation}{0}

\textbf{Definition:} Let $y$ be an $n \times 1$ vector and let $X$ be an $n \times p$ matrix.

Then, a statement asserting a linear combination of $X$ into $y$

\begin{equation} \label{eq:mlr-mlr-model}
y = X\beta + \varepsilon \; ,
\end{equation}

together with a statement asserting a normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) for $\varepsilon$

\begin{equation} \label{eq:mlr-mlr-noise}
\varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

is called a univariate linear regression model or simply, "multiple linear regression".

\begin{itemize}

\item $y$ is called "measured data", "dependent variable" or "measurements";

\item $X$ is called "design matrix", "set of independent variables" or "predictors";

\item $V$ is called "covariance matrix" or "covariance structure";

\item $\beta$ are called "regression coefficients" or "weights";

\item $\varepsilon$ is called "noise", "errors" or "error terms";

\item $\sigma^2$ is called "noise variance" or "error variance";

\item $n$ is the number of observations;

\item $p$ is the number of predictors.

\end{itemize}

Alternatively, the linear combination may also be written as

\begin{equation} \label{eq:mlr-mlr-model-sum}
y = \sum_{i=1}^{p} \beta_i x_i + \varepsilon
\end{equation}

or, when the model includes an intercept term, as

\begin{equation} \label{eq:mlr-mlr-model-sum-base}
y = \beta_0 + \sum_{i=1}^{p} \beta_i x_i + \varepsilon
\end{equation}

which is equivalent to adding a constant regressor $x_0 = 1_n$ to the design matrix $X$.

When the covariance structure $V$ is equal to the $n \times n$ identity matrix, this is called multiple linear regression with independent and identically distributed (i.i.d.) observations:

\begin{equation} \label{eq:mlr-mlr-noise-iid}
V = I_n \quad \Rightarrow \quad \varepsilon \sim \mathcal{N}(0, \sigma^2 I_n) \quad \Rightarrow \quad \varepsilon_i \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; .
\end{equation}

Otherwise, it is called multiple linear regression with correlated observations.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Linear regression"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-21; URL: \url{https://en.wikipedia.org/wiki/Linear_regression#Simple_and_multiple_linear_regression}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Special case of general linear model}]{Special case of general linear model} \label{sec:mlr-glm}
\setcounter{equation}{0}

\textbf{Theorem:} Multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) is a special case of the general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with number of measurements $v = 1$, such that data matrix $Y$, regression coefficients $B$, noise matrix $E$ and noise covariance $\Sigma$ equate as

\begin{equation} \label{eq:mlr-glm-mlr-glm}
Y = y, \quad B = \beta, \quad E = \varepsilon \quad \text{and} \quad \Sigma = \sigma^2
\end{equation}

where $y$, $\beta$, $\varepsilon$ and $\sigma^2$ are the data vector, regression coefficients, noise vector and noise variance from multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}).


\vspace{1em}
\textbf{Proof:} The linear regression model with correlated errors ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) is given by:

\begin{equation} \label{eq:mlr-glm-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; .
\end{equation}

Because $\varepsilon$ is an $n \times 1$ vector and $\sigma^2$ is scalar, we have the following identities:

\begin{equation}
\begin{split}
\mathrm{vec}(\varepsilon) &= \varepsilon \\
\sigma^2 \otimes V &= \sigma^2 V \; .
\end{split}
\end{equation}

Thus, using the relationship between multivariate normal and matrix normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-mvn}), equation \eqref{eq:mlr-glm-mlr} can also be written as

\begin{equation} \label{eq:mlr-glm-mlr-dev}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{MN}(0, V, \sigma^2) \; .
\end{equation}

Comparing with the general linear model with correlated observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm})

\begin{equation} \label{eq:mlr-glm-glm}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma) \; ,
\end{equation}

we finally note the equivalences given in equation \eqref{eq:mlr-glm-mlr-glm}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "General linear model"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-07-21; URL: \url{https://en.wikipedia.org/wiki/General_linear_model#Comparison_to_multiple_linear_regression}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Ordinary least squares}]{Ordinary least squares} \label{sec:mlr-ols}
\setcounter{equation}{0}

\textbf{Theorem:} Given a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent observations

\begin{equation} \label{eq:mlr-ols-MLR}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; ,
\end{equation}

the parameters minimizing the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) are given by

\begin{equation} \label{eq:mlr-ols-OLS}
\hat{\beta} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Let $\hat{\beta}$ be the ordinary least squares (OLS) solution and let $\hat{\varepsilon} = y - X\hat{\beta}$ be the resulting vector of residuals. Then, this vector must be orthogonal to the design matrix,

\begin{equation} \label{eq:mlr-ols-X-e-orth}
X^\mathrm{T} \hat{\varepsilon} = 0 \; ,
\end{equation}

because if it wasn't, there would be another solution $\tilde{\beta}$ giving another vector $\tilde{\varepsilon}$ with a smaller residual sum of squares. From \eqref{eq:mlr-ols-X-e-orth}, the OLS formula can be directly derived:

\begin{equation} \label{eq:mlr-ols-OLS-qed}
\begin{split}
X^\mathrm{T} \hat{\varepsilon} &= 0 \\
X^\mathrm{T} \left( y - X\hat{\beta} \right) &= 0 \\
X^\mathrm{T} y - X^\mathrm{T} X\hat{\beta} &= 0 \\
X^\mathrm{T} X\hat{\beta} &= X^\mathrm{T} y \\
\hat{\beta} &= (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Stephan, Klaas Enno (2010): "The General Linear Model (GLM)"; in: \textit{Methods and models for fMRI data analysis in neuroeconomics}, Lecture 3, Slides 10/11; URL: \url{http://www.socialbehavior.uzh.ch/teaching/methodsspring10.html}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Ordinary least squares}]{Ordinary least squares} \label{sec:mlr-ols2}
\setcounter{equation}{0}

\textbf{Theorem:} Given a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent observations

\begin{equation} \label{eq:mlr-ols2-MLR}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; ,
\end{equation}

the parameters minimizing the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) are given by

\begin{equation} \label{eq:mlr-ols2-OLS}
\hat{\beta} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) is defined as

\begin{equation} \label{eq:mlr-ols2-RSS}
\mathrm{RSS}(\beta) = \sum_{i=1}^n \varepsilon_i^2 = \varepsilon^\mathrm{T} \varepsilon = (y-X\beta)^\mathrm{T} (y-X\beta)
\end{equation}

which can be developed into

\begin{equation} \label{eq:mlr-ols2-RSS-dev}
\begin{split}
\mathrm{RSS}(\beta) &= y^\mathrm{T} y - y^\mathrm{T} X \beta - \beta^\mathrm{T} X^\mathrm{T} y + \beta^\mathrm{T} X^\mathrm{T} X \beta \\
&= y^\mathrm{T} y - 2 \beta^\mathrm{T} X^\mathrm{T} y + \beta^\mathrm{T} X^\mathrm{T} X \beta \; .
\end{split}
\end{equation}

The derivative of $\mathrm{RSS}(\beta)$ with respect to $\beta$ is

\begin{equation} \label{eq:mlr-ols2-RSS-der}
\frac{\mathrm{d}\mathrm{RSS}(\beta)}{\mathrm{d}\beta} = - 2 X^\mathrm{T} y + 2 X^\mathrm{T} X \beta
\end{equation}

and setting this deriative to zero, we obtain:

\begin{equation} \label{eq:mlr-ols2-OLS-qed}
\begin{split}
\frac{\mathrm{d}\mathrm{RSS}(\hat{\beta})}{\mathrm{d}\beta} &= 0 \\
0 &= - 2 X^\mathrm{T} y + 2 X^\mathrm{T} X \hat{\beta} \\
X^\mathrm{T} X \hat{\beta} &= X^\mathrm{T} y \\
\hat{\beta} &= (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \; .
\end{split}
\end{equation}

Since the quadratic form $y^\mathrm{T} y$ in \eqref{eq:mlr-ols2-RSS-dev} is positive, $\hat{\beta}$ minimizes $\mathrm{RSS}(\beta)$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Proofs involving ordinary least squares"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-03; URL: \url{https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares#Least_squares_estimator_for_%CE%B2}.
\item ad (2015): "Derivation of the Least Squares Estimator for Beta in Matrix Notation"; in: \textit{Economic Theory Blog}, retrieved on 2021-05-27; URL: \url{https://economictheoryblog.com/2015/02/19/ols_estimator/}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Ordinary least squares}]{Ordinary least squares} \label{sec:mlr-ols3}
\setcounter{equation}{0}

\textbf{Theorem:} Given a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent observations

\begin{equation} \label{eq:mlr-ols3-MLR}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; ,
\end{equation}

the parameters minimizing the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) are given by

\begin{equation} \label{eq:mlr-ols3-OLS}
\hat{\beta} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} We consider the sum of squared differences between $y$ and $X\beta$:

\begin{equation} \label{eq:mlr-ols3-mlr-rss}
\sum_{i=1}^n \varepsilon_i^2
= \varepsilon^\mathrm{T} \varepsilon
= (y - X\beta)^\mathrm{T} (y - X\beta) \; .
\end{equation}

First, we note that the residual vector $\hat{\varepsilon}$ implied by the ordinary least squares solution $\hat{\beta}$ is orthogonal to the columns of the design matrix, such that the result of their multiplication is the $p$-dimensional zero vector (where $X \in \mathbb{R}^{n \times p}$):

\begin{equation} \label{eq:mlr-ols3-XTe}
\begin{split}
X^\mathrm{T} (y - X \hat{\beta})
&= X^\mathrm{T} y - X \hat{\beta} \\
&= X^\mathrm{T} y - X^\mathrm{T} X (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \\
&= X^\mathrm{T} y - X^\mathrm{T} y \\
&= 0_p \; .
\end{split}
\end{equation}

Second, since $X^\mathrm{T} X$ is a positive semi-definite matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat-psd}), the following product is non-negative for each $p$-dimensional real vector $z$:

\begin{equation} \label{eq:mlr-ols3-XTX-psd}
z^\mathrm{T} X^\mathrm{T} X z \geq 0 \quad \text{for each} \quad z \in \mathbb{R}^p \; .
\end{equation}

We continue developping the sum of squared differences from \eqref{eq:mlr-ols3-mlr-rss}:

\begin{equation} \label{eq:mlr-ols3-rss-s1}
\begin{split}
(y - X\beta)^\mathrm{T} (y - X\beta)
&= (y - X\hat{\beta} + X\hat{\beta} - X\beta)^\mathrm{T} (y - X\hat{\beta} + X\hat{\beta} - X\beta) \\
&= \left( (y - X\hat{\beta}) + X(\hat{\beta} - \beta) \right)^\mathrm{T} \left( (y - X\hat{\beta}) + X(\hat{\beta} - \beta) \right) \\
&= (y - X\hat{\beta})^\mathrm{T} (y - X\hat{\beta}) + (y - X\hat{\beta})^\mathrm{T} X (\hat{\beta} - \beta) + (\hat{\beta} - \beta)^\mathrm{T} X^\mathrm{T} (y - X\hat{\beta}) + (\hat{\beta} - \beta)^\mathrm{T} X^\mathrm{T} X (\hat{\beta} - \beta) \\
&\overset{\eqref{eq:mlr-ols3-XTe}}{=} (y - X\hat{\beta})^\mathrm{T} (y - X\hat{\beta}) + 0_p^\mathrm{T} (\hat{\beta} - \beta) + (\hat{\beta} - \beta)^\mathrm{T} 0_p + (\hat{\beta} - \beta)^\mathrm{T} X^\mathrm{T} X (\hat{\beta} - \beta) \\
&= (y - X\hat{\beta})^\mathrm{T} (y - X\hat{\beta}) + (\hat{\beta} - \beta)^\mathrm{T} X^\mathrm{T} X (\hat{\beta} - \beta) \; .
\end{split}
\end{equation}

By virtue of \eqref{eq:mlr-ols3-XTX-psd}, the second term on the right-hand side must be non-zero:

\begin{equation} \label{eq:mlr-ols3-bbTXTXbb}
(\hat{\beta} - \beta)^\mathrm{T} X^\mathrm{T} X (\hat{\beta} - \beta) \geq 0 \; .
\end{equation}

Thus, the residual sum of squares must be greater than or equal to the first term

\begin{equation} \label{eq:mlr-ols3-rss-s2}
(y - X\beta)^\mathrm{T} (y - X\beta) \geq (y - X\hat{\beta})^\mathrm{T} (y - X\hat{\beta})
\end{equation}

and its minimum value is reached when the the second term is zero:

\begin{equation} \label{eq:mlr-ols3-rss-s3}
\begin{split}
(\hat{\beta} - \beta)^\mathrm{T} X^\mathrm{T} X (\hat{\beta} - \beta) &= 0 \\
\Leftrightarrow \quad
(\hat{\beta} - \beta) &= 0 \\
\Leftrightarrow \quad
\beta &= \hat{\beta} \; .
\end{split}
\end{equation}

Thus, the residual sum of squares is minimized when $\beta = \hat{\beta}$:

\begin{equation} \label{eq:mlr-ols3-mlr-ols-qed}
\hat{\beta} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y = \operatorname*{arg\,min\,}_\beta (y - X\beta)^\mathrm{T} (y - X\beta) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Ostwald, Dirk (2023): "ParameterschÃ¤tzung"; in: \textit{Allgemeines Lineares Modell}, Einheit (6), Folien 10-12; URL: \url{https://www.ipsy.ovgu.de/ipsy_media/Methodenlehre+I/Sommersemester+2023/Allgemeines+Lineares+Modell/6_Parametersch%C3%A4tzung.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Ordinary least squares for two regressors}]{Ordinary least squares for two regressors} \label{sec:mlr-olstr}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) in which the design matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) has two columns:

\begin{equation}
 \label{eq:mlr-olstr-mlr-tr}
y = X\beta + \varepsilon \quad \text{where} \quad y \in \mathbb{R}^{n \times 1} \quad \text{and} \quad X = \left[ \begin{matrix} x_1 & x_2 \end{matrix} \right] \in \mathbb{R}^{n \times 2} \; .
\end{equation}

Then,

1) the ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}) estimates for $\beta_1$ and $\beta_2$ are given by

\begin{equation} \label{eq:mlr-olstr-mlr-ols-tr}
\hat{\beta}_1 = \frac{x_2^\mathrm{T} x_2 x_1^\mathrm{T} y - x_1^\mathrm{T} x_2 x_2^\mathrm{T} y}{x_1^\mathrm{T} x_1 x_2^\mathrm{T} x_2 - x_1^\mathrm{T} x_2 x_2^\mathrm{T} x_1} \quad \text{and} \quad \hat{\beta}_2 = \frac{x_1^\mathrm{T} x_1 x_2^\mathrm{T} y - x_2^\mathrm{T} x_1 x_1^\mathrm{T} y}{x_1^\mathrm{T} x_1 x_2^\mathrm{T} x_2 - x_1^\mathrm{T} x_2 x_2^\mathrm{T} x_1}
\end{equation}

2) and, if the two regressors are orthogonal to each other, they simplify to

\begin{equation} \label{eq:mlr-olstr-mlr-ols-tr-orth}
\hat{\beta}_1 = \frac{x_1^\mathrm{T} y}{x_1^\mathrm{T} x_1} \quad \text{and} \quad \hat{\beta}_2 = \frac{x_2^\mathrm{T} y}{x_2^\mathrm{T} x_2}, \quad \text{if} \quad x_1 \perp x_2 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The model in \eqref{eq:mlr-olstr-mlr-tr} is a special case of multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) and the ordinary least squares solution for multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}) is:

\begin{equation} \label{eq:mlr-olstr-mlr-ols}
\hat{\beta} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \; .
\end{equation}

1) Plugging $X = \left[ \begin{matrix} x_1 & x_2 \end{matrix} \right]$ into this equation, we obtain:

\begin{equation} \label{eq:mlr-olstr-mlr-ols-tr-s1}
\begin{split}
\hat{\beta} &= \left( \left[ \begin{matrix} x_1^\mathrm{T} \\ x_2^\mathrm{T} \end{matrix} \right] \left[ \begin{matrix} x_1 & x_2 \end{matrix} \right] \right)^{-1} \left[ \begin{matrix} x_1^\mathrm{T} \\ x_2^\mathrm{T} \end{matrix} \right] y \\
& = \left( \begin{matrix} x_1^\mathrm{T} x_1 & x_1^\mathrm{T} x_2 \\ x_2^\mathrm{T} x_1 & x_2^\mathrm{T} x_2 \end{matrix} \right)^{-1} \left( \begin{matrix} x_1^\mathrm{T} y \\ x_2^\mathrm{T} y \end{matrix} \right) \; .
\end{split}
\end{equation}

Using the inverse of of a $2 \times 2$ matrix

\begin{equation} \label{eq:mlr-olstr-inv-2x2}
\left[ \begin{matrix} a & b \\ c & d \end{matrix} \right]^{-1} = \frac{1}{a d - b c} \left[ \begin{matrix} d & -b \\ -c & a \end{matrix} \right] \; ,
\end{equation}

this can be further developped into

\begin{equation} \label{eq:mlr-olstr-mlr-ols-tr-s2}
\begin{split}
\hat{\beta} &= \frac{1}{x_1^\mathrm{T} x_1 x_2^\mathrm{T} x_2 - x_1^\mathrm{T} x_2 x_2^\mathrm{T} x_1} \left( \begin{matrix} x_2^\mathrm{T} x_2 & -x_1^\mathrm{T} x_2 \\ -x_2^\mathrm{T} x_1 & x_1^\mathrm{T} x_1 \end{matrix} \right) \left( \begin{matrix} x_1^\mathrm{T} y \\ x_2^\mathrm{T} y \end{matrix} \right) \\
&= \frac{1}{x_1^\mathrm{T} x_1 x_2^\mathrm{T} x_2 - x_1^\mathrm{T} x_2 x_2^\mathrm{T} x_1} \left( \begin{matrix} x_2^\mathrm{T} x_2 x_1^\mathrm{T} y - x_1^\mathrm{T} x_2 x_2^\mathrm{T} y \\ x_1^\mathrm{T} x_1 x_2^\mathrm{T} y - x_2^\mathrm{T} x_1 x_1^\mathrm{T} y \end{matrix} \right)
\end{split}
\end{equation}

which can also be written as

\begin{equation} \label{eq:mlr-olstr-mlr-ols-tr-qed}
\begin{split}
\hat{\beta}_1 &= \frac{x_2^\mathrm{T} x_2 x_1^\mathrm{T} y - x_1^\mathrm{T} x_2 x_2^\mathrm{T} y}{x_1^\mathrm{T} x_1 x_2^\mathrm{T} x_2 - x_1^\mathrm{T} x_2 x_2^\mathrm{T} x_1} \\
\hat{\beta}_2 &= \frac{x_1^\mathrm{T} x_1 x_2^\mathrm{T} y - x_2^\mathrm{T} x_1 x_1^\mathrm{T} y}{x_1^\mathrm{T} x_1 x_2^\mathrm{T} x_2 - x_1^\mathrm{T} x_2 x_2^\mathrm{T} x_1} \; .
\end{split}
\end{equation}

2) If two regressors are orthogonal to each other, this means that the inner product of the corresponding vectors is zero:

\begin{equation} \label{eq:mlr-olstr-reg-orth}
x_1 \perp x_2 \quad \Leftrightarrow \quad x_1^\mathrm{T} x_2 = x_2^\mathrm{T} x_1 = 0 \; .
\end{equation}

Applying this to equation \eqref{eq:mlr-olstr-mlr-ols-tr-qed}, we obtain:

\begin{equation} \label{eq:mlr-olstr-mlr-ols-tr-orth-qed}
\begin{split}
\hat{\beta}_1 &= \frac{x_2^\mathrm{T} x_2 x_1^\mathrm{T} y}{x_1^\mathrm{T} x_1 x_2^\mathrm{T} x_2} = \frac{x_1^\mathrm{T} y}{x_1^\mathrm{T} x_1} \\
\hat{\beta}_2 &= \frac{x_1^\mathrm{T} x_1 x_2^\mathrm{T} y}{x_1^\mathrm{T} x_1 x_2^\mathrm{T} x_2} = \frac{x_2^\mathrm{T} y}{x_2^\mathrm{T} x_2} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textit{Total sum of squares}]{Total sum of squares} \label{sec:tss}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a multiple linear regression with independent observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) using measured data $y$ and design matrix $X$:

\begin{equation} \label{eq:tss-mlr}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; .
\end{equation}

Then, the total sum of squares (TSS) is defined as the sum of squared deviations of the measured signal from the average signal:

\begin{equation} \label{eq:tss-tss}
\mathrm{TSS} = \sum_{i=1}^n (y_i - \bar{y})^2 \quad \text{where} \quad \bar{y} = \frac{1}{n} \sum_{i=1}^n y_i \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Total sum of squares"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-21; URL: \url{https://en.wikipedia.org/wiki/Total_sum_of_squares}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Explained sum of squares}]{Explained sum of squares} \label{sec:ess}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a multiple linear regression with independent observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) using measured data $y$ and design matrix $X$:

\begin{equation} \label{eq:ess-mlr}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; .
\end{equation}

Then, the explained sum of squares (ESS) is defined as the sum of squared deviations of the fitted signal from the average signal:

\begin{equation} \label{eq:ess-ess}
\mathrm{ESS} = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2 \quad \text{where} \quad \hat{y} = X \hat{\beta} \quad \text{and} \quad \bar{y} = \frac{1}{n} \sum_{i=1}^n y_i
\end{equation}

with estimated regression coefficients $\hat{\beta}$, e.g. obtained via ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Explained sum of squares"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-21; URL: \url{https://en.wikipedia.org/wiki/Explained_sum_of_squares}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Residual sum of squares}]{Residual sum of squares} \label{sec:rss}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a multiple linear regression with independent observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) using measured data $y$ and design matrix $X$:

\begin{equation} \label{eq:rss-mlr}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; .
\end{equation}

Then, the residual sum of squares (RSS) is defined as the sum of squared deviations of the measured signal from the fitted signal:

\begin{equation} \label{eq:rss-rss}
\mathrm{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 \quad \text{where} \quad \hat{y} = X \hat{\beta}
\end{equation}

with estimated regression coefficients $\hat{\beta}$, e.g. obtained via ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}).

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Residual sum of squares"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-21; URL: \url{https://en.wikipedia.org/wiki/Residual_sum_of_squares}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Total, explained and residual sum of squares}]{Total, explained and residual sum of squares} \label{sec:mlr-pss}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent observations

\begin{equation} \label{eq:mlr-pss-mlr}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)
\end{equation}

and let $X$ contain a constant regressor $1_n$ modelling the intercept term. Then, it holds that

\begin{equation} \label{eq:mlr-pss-mlr-pss}
\mathrm{TSS} = \mathrm{ESS} + \mathrm{RSS}
\end{equation}

where $\mathrm{TSS}$ is the total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}), $\mathrm{ESS}$ is the explained sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess}) and $\mathrm{RSS}$ is the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}).


\vspace{1em}
\textbf{Proof:} The total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}) is given by

\begin{equation} \label{eq:mlr-pss-TSS}
\mathrm{TSS} = \sum_{i=1}^{n} (y_i - \bar{y})^2
\end{equation}

where $\bar{y}$ is the mean across all $y_i$. The $\mathrm{TSS}$ can be rewritten as

\begin{equation} \label{eq:mlr-pss-TSS-s1}
\begin{split}
\mathrm{TSS} &= \sum_{i=1}^{n} (y_i - \bar{y} + \hat{y}_i - \hat{y}_i)^2 \\
&= \sum_{i=1}^{n} \left( (\hat{y}_i - \bar{y}) + (y_i - \hat{y}_i) \right)^2 \\
&= \sum_{i=1}^{n} \left( (\hat{y}_i - \bar{y}) + \hat{\varepsilon}_i \right)^2 \\
&= \sum_{i=1}^{n} \left( (\hat{y}_i - \bar{y})^2 + 2 \, \hat{\varepsilon}_i (\hat{y}_i - \bar{y}) + \hat{\varepsilon}_i^2 \right) \\
&= \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^{n} \hat{\varepsilon}_i^2 + 2 \sum_{i=1}^{n} \hat{\varepsilon}_i (\hat{y}_i - \bar{y}) \\
&= \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^{n} \hat{\varepsilon}_i^2 + 2 \sum_{i=1}^{n} \hat{\varepsilon}_i (x_i \hat{\beta} - \bar{y}) \\
&= \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^{n} \hat{\varepsilon}_i^2 + 2 \sum_{i=1}^{n} \hat{\varepsilon}_i \left( \sum_{j=1}^{p} x_{ij} \hat{\beta}_j \right) - 2 \sum_{i=1}^{n} \hat{\varepsilon}_i \, \bar{y} \\
&= \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^{n} \hat{\varepsilon}_i^2 + 2 \sum_{j=1}^{p} \hat{\beta}_j \sum_{i=1}^{n} \hat{\varepsilon}_i x_{ij} - 2 \bar{y} \sum_{i=1}^{n} \hat{\varepsilon}_i \\
\end{split}
\end{equation}

The fact that the design matrix includes a constant regressor ensures that

\begin{equation} \label{eq:mlr-pss-e-est-sum}
\sum_{i=1}^{n} \hat{\varepsilon}_i = \hat{\varepsilon}^\mathrm{T} 1_n = 0
\end{equation}

and because the residuals are orthogonal to the design matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}), we have

\begin{equation} \label{eq:mlr-pss-X-e-orth}
\sum_{i=1}^{n} \hat{\varepsilon}_i x_{ij} = \hat{\varepsilon}^\mathrm{T} x_j = 0 \; .
\end{equation}

Applying \eqref{eq:mlr-pss-e-est-sum} and \eqref{eq:mlr-pss-X-e-orth} to \eqref{eq:mlr-pss-TSS-s1}, this becomes

\begin{equation} \label{eq:mlr-pss-TSS-s2}
\mathrm{TSS} = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^{n} \hat{\varepsilon}_i^2
\end{equation}

and, with the definitions of explained ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess}) and residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}), it is

\begin{equation} \label{eq:mlr-pss-TSS-s3}
\mathrm{TSS} = \mathrm{ESS} + \mathrm{RSS} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Partition of sums of squares"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-09; URL: \url{https://en.wikipedia.org/wiki/Partition_of_sums_of_squares#Partitioning_the_sum_of_squares_in_linear_regression}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Estimation matrix}]{Estimation matrix} \label{sec:emat}
\setcounter{equation}{0}

\textbf{Definition:} In multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}), the estimation matrix is the matrix $E$ that results in ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}) or weighted least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wls}) parameter estimates when right-multiplied with the measured data:

\begin{equation} \label{eq:emat-em}
Ey = \hat{\beta} \; .
\end{equation}


\subsubsection[\textit{Projection matrix}]{Projection matrix} \label{sec:pmat}
\setcounter{equation}{0}

\textbf{Definition:} In multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}), the projection matrix is the matrix $P$ that results in the fitted signal explained by estimated parameters ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:emat}) when right-multiplied with the measured data:

\begin{equation} \label{eq:pmat-pm}
Py = \hat{y} = X \hat{\beta} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Projection matrix"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-07-22; URL: \url{https://en.wikipedia.org/wiki/Projection_matrix#Overview}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Residual-forming matrix}]{Residual-forming matrix} \label{sec:rfmat}
\setcounter{equation}{0}

\textbf{Definition:} In multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}), the residual-forming matrix is the matrix $R$ that results in the vector of residuals left over by estimated parameters ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:emat}) when right-multiplied with the measured data:

\begin{equation} \label{eq:rfmat-pm}
Ry = \hat{\varepsilon} = y - \hat{y} = y - X \hat{\beta} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Projection matrix"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-07-22; URL: \url{https://en.wikipedia.org/wiki/Projection_matrix#Properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Estimation, projection and residual-forming matrix}]{Estimation, projection and residual-forming matrix} \label{sec:mlr-mat}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent observations

\begin{equation} \label{eq:mlr-mat-mlr}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)
\end{equation}

and consider estimation using ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}). Then, the estimated parameters, fitted signal and residuals are given by

\begin{equation} \label{eq:mlr-mat-mlr-est}
\begin{split}
\hat{\beta} &= E y \\
\hat{y} &= P y \\
\hat{\varepsilon} &= R y
\end{split}
\end{equation}

where 

\begin{equation} \label{eq:mlr-mat-mlr-mat}
\begin{split}
E &= (X^\mathrm{T} X)^{-1} X^\mathrm{T} \\
P &= X (X^\mathrm{T} X)^{-1} X^\mathrm{T} \\
R &= I_n - X (X^\mathrm{T} X)^{-1} X^\mathrm{T}
\end{split}
\end{equation}

are the estimation matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:emat}), projection matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:pmat}) and residual-forming matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rfmat}) and $n$ is the number of observations.


\vspace{1em}
\textbf{Proof:}

1) Ordinary least squares parameter estimates of $\beta$ are defined as minimizing the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss})

\begin{equation} \label{eq:mlr-mat-ols}
\hat{\beta} = \operatorname*{arg\,min}_{\beta} \left[ (y-X\beta)^\mathrm{T} (y-X\beta) \right]
\end{equation}

and the solution to this ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}) is given by

\begin{equation} \label{eq:mlr-mat-b-est-qed}
\begin{split}
\hat{\beta} &= (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \\
&\overset{\eqref{eq:mlr-mat-mlr-mat}}{=} E y \; .
\end{split}
\end{equation}

\vspace{1em}
2) The fitted signal is given by multiplying the design matrix with the estimated regression coefficients

\begin{equation} \label{eq:mlr-mat-y-est}
\hat{y} = X\hat{\beta}
\end{equation}

and using \eqref{eq:mlr-mat-b-est-qed}, this becomes

\begin{equation} \label{eq:mlr-mat-y-est-qed}
\begin{split}
\hat{y} &= X (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \\
&\overset{\eqref{eq:mlr-mat-mlr-mat}}{=} P y \; .
\end{split}
\end{equation}

\vspace{1em}
3) The residuals of the model are calculated by subtracting the fitted signal from the measured signal

\begin{equation} \label{eq:mlr-mat-e-est}
\hat{\varepsilon} = y - \hat{y}
\end{equation}

and using \eqref{eq:mlr-mat-y-est-qed}, this becomes

\begin{equation} \label{eq:mlr-mat-e-est-qed}
\begin{split}
\hat{\varepsilon} &= y - X (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \\
&= (I_n - X (X^\mathrm{T} X)^{-1} X^\mathrm{T}) y \\
&\overset{\eqref{eq:mlr-mat-mlr-mat}}{=} R y \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Stephan, Klaas Enno (2010): "The General Linear Model (GLM)"; in: \textit{Methods and models for fMRI data analysis in neuroeconomics}, Lecture 3, Slide 10; URL: \url{http://www.socialbehavior.uzh.ch/teaching/methodsspring10.html}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Symmetry of projection and residual-forming matrix}]{Symmetry of projection and residual-forming matrix} \label{sec:mlr-symm}
\setcounter{equation}{0}

\textbf{Theorem:} The projection matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:pmat}) and the residual-forming matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rfmat}) are symmetric:

\begin{equation} \label{eq:mlr-symm-P-R-symm}
\begin{split}
P^\mathrm{T} &= P \\
R^\mathrm{T} &= R \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} Let $X$ be the design matrix from the linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}). Then, the matrix $X^\mathrm{T} X$ is symmetric, because 

\begin{equation} \label{eq:mlr-symm-XTX-symm}
(X^\mathrm{T} X)^\mathrm{T} = X^\mathrm{T} {X^\mathrm{T}}^\mathrm{T} = X^\mathrm{T} X \; .
\end{equation}

Thus, the inverse of $X^\mathrm{T} X$ is also symmetric, i.e.

\begin{equation} \label{eq:mlr-symm-XTX-inv-symm}
\left((X^\mathrm{T} X)^{-1}\right)^\mathrm{T} = (X^\mathrm{T} X)^{-1} \; .
\end{equation}

1) The projection matrix for ordinary least squares is given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat})

\begin{equation} \label{eq:mlr-symm-P}
P = X (X^\mathrm{T} X)^{-1} X^\mathrm{T} \; ,
\end{equation}

such that

\begin{equation} \label{eq:mlr-symm-P-symm}
\begin{split}
P^\mathrm{T} &= \left( X (X^\mathrm{T} X)^{-1} X^\mathrm{T} \right)^\mathrm{T} \\
&= {X^\mathrm{T}}^\mathrm{T} \left((X^\mathrm{T} X)^{-1}\right)^\mathrm{T} X^\mathrm{T} \\
&= X (X^\mathrm{T} X)^{-1} X^\mathrm{T} \\
&\overset{\eqref{eq:mlr-symm-P}}{=} P \; .
\end{split}
\end{equation}

2) The residual-forming matrix for ordinary least squares is given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat})

\begin{equation} \label{eq:mlr-symm-R}
R = I_n - X (X^\mathrm{T} X)^{-1} X^\mathrm{T} = I_n - P \; ,
\end{equation}

such that

\begin{equation} \label{eq:mlr-symm-R-symm}
\begin{split}
R^\mathrm{T} &= (I_n - P)^\mathrm{T} \\
&= I_n^\mathrm{T} - P^\mathrm{T} \\
&\overset{\eqref{eq:mlr-symm-P-symm}}{=} I_n - P \\
&\overset{\eqref{eq:mlr-symm-R}}{=} R \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Projection matrix"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-12-22; URL: \url{https://en.wikipedia.org/wiki/Projection_matrix#Properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Idempotence of projection and residual-forming matrix}]{Idempotence of projection and residual-forming matrix} \label{sec:mlr-idem}
\setcounter{equation}{0}

\textbf{Theorem:} The projection matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:pmat}) and the residual-forming matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rfmat}) are idempotent:

\begin{equation} \label{eq:mlr-idem-P^2-R^2}
\begin{split}
P^2 &= P \\
R^2 &= R \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:}

1) The projection matrix for ordinary least squares is given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat})

\begin{equation} \label{eq:mlr-idem-P}
P = X (X^\mathrm{T} X)^{-1} X^\mathrm{T} \; ,
\end{equation}

such that

\begin{equation} \label{eq:mlr-idem-P^2}
\begin{split}
P^2 &= X (X^\mathrm{T} X)^{-1} X^\mathrm{T} X (X^\mathrm{T} X)^{-1} X^\mathrm{T} \\
&= X (X^\mathrm{T} X)^{-1} X^\mathrm{T} \\
&\overset{\eqref{eq:mlr-idem-P}}{=} P \; .
\end{split}
\end{equation}

\vspace{1em}
2) The residual-forming matrix for ordinary least squares is given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat})

\begin{equation} \label{eq:mlr-idem-R}
R = I_n - X (X^\mathrm{T} X)^{-1} X^\mathrm{T} = I_n - P \; ,
\end{equation}

such that

\begin{equation} \label{eq:mlr-idem-R^2}
\begin{split}
R^2 &= (I_n - P) (I_n - P) \\
&= I_n - P - P + P^2 \\
&\overset{\eqref{eq:mlr-idem-P^2}}{=} I_n - 2 P + P \\
&= I_n - P \\
&\overset{\eqref{eq:mlr-idem-R}}{=} R \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Projection matrix"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-07-22; URL: \url{https://en.wikipedia.org/wiki/Projection_matrix#Properties}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Independence of estimated parameters and residuals}]{Independence of estimated parameters and residuals} \label{sec:mlr-ind}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with correlated observations

\begin{equation} \label{eq:mlr-ind-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

and consider estimation using weighted least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wls}). Then, the estimated parameters and the vector of residuals ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wlsdist}) are independent from each other:

\begin{equation} \label{eq:mlr-ind-mlr-ind}
\begin{split}
\hat{\beta} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \quad \text{and} \\ \hat{\varepsilon} &= y - X \hat{\beta} \quad \text{ind.}
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} Equation \eqref{eq:mlr-ind-mlr} implies the following distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wlsdist}) for the random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $y$:

\begin{equation} \label{eq:mlr-ind-y-dist}
\begin{split}
y &\sim \mathcal{N}\left( X \beta, \sigma^2 V \right) \\
&\sim \mathcal{N}\left( X \beta, \Sigma \right) \\
\text{with} \quad \Sigma &= \sigma^2 V \; .
\end{split}
\end{equation}

Note that the estimated parameters and residuals can be written as projections from the same random vector ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat}) $y$:

\begin{equation} \label{eq:mlr-ind-b-proj}
\begin{split}
\hat{\beta} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \\
&= A y \\
\text{with} \quad A &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1}
\end{split}
\end{equation}

\begin{equation} \label{eq:mlr-ind-e-proj}
\begin{split}
\hat{\varepsilon} &= y - X \hat{\beta} \\
&= (I_n - X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1}) y \\
&= B y \\
\text{with} \quad B &= (I_n - X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1}) \; .
\end{split}
\end{equation}

Two projections $AZ$ and $BZ$ from the same multivariate normal ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) random vector ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:rvec}) $Z \sim \mathcal{N}(\mu, \Sigma)$ are independent, if and only if the following condition holds ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ind}):

\begin{equation} \label{eq:mlr-ind-mvn-ind}
A \Sigma B^\mathrm{T} = 0 \; .
\end{equation}

Combining \eqref{eq:mlr-ind-y-dist}, \eqref{eq:mlr-ind-b-proj} and \eqref{eq:mlr-ind-e-proj}, we check whether this is fulfilled in the present case:

\begin{equation} \label{eq:mlr-ind-mlr-ind-qed}
\begin{split}
A \Sigma B^\mathrm{T} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} (\sigma^2 V) (I_n - X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1})^\mathrm{T} \\
&= \sigma^2 \left[ (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} V - (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} V V^{-1} X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} \right] \\
&= \sigma^2 \left[ (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} - (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} \right] \\
&= \sigma^2 \cdot 0_{pn} \\
&= 0 \; .
\end{split}
\end{equation}

This demonstrates that $\hat{\beta}$ and $\hat{\varepsilon}$ -- and likewise, all pairs of terms separately derived ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-t}) from $\hat{\beta}$ and $\hat{\varepsilon}$ -- are statistically independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}).
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item jld (2018): "Understanding t-test for linear regression"; in: \textit{StackExchange CrossValidated}, retrieved on 2022-12-13; URL: \url{https://stats.stackexchange.com/a/344008}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Distribution of OLS estimates, signal and residuals}]{Distribution of OLS estimates, signal and residuals} \label{sec:mlr-olsdist}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent observations

\begin{equation} \label{eq:mlr-olsdist-mlr}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)
\end{equation}

and consider estimation using ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}). Then, the estimated parameters, fitted signal and residuals are distributed as

\begin{equation} \label{eq:mlr-olsdist-mlr-dist}
\begin{split}
\hat{\beta} &\sim \mathcal{N}\left( \beta, \sigma^2 (X^\mathrm{T} X)^{-1} \right) \\
\hat{y} &\sim \mathcal{N}\left( X \beta, \sigma^2 P \right) \\
\hat{\varepsilon} &\sim \mathcal{N}\left( 0, \sigma^2 (I_n - P) \right)
\end{split}
\end{equation}

where $P$ is the projection matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:pmat}) for ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols})

\begin{equation} \label{eq:mlr-olsdist-mlr-pmat}
P = X (X^\mathrm{T} X)^{-1} X^\mathrm{T} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} We will use the linear transformation theorem for the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}):

\begin{equation} \label{eq:mlr-olsdist-mvn-ltt}
x \sim \mathcal{N}(\mu, \Sigma) \quad \Rightarrow \quad y = Ax + b \sim \mathcal{N}(A\mu + b, A \Sigma A^\mathrm{T}) \; .
\end{equation}

The distributional assumption in \eqref{eq:mlr-olsdist-mlr} is equivalent to ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ind}):

\begin{equation} \label{eq:mlr-olsdist-mlr-vect}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 I_n) \; .
\end{equation}

Applying \eqref{eq:mlr-olsdist-mvn-ltt} to \eqref{eq:mlr-olsdist-mlr-vect}, the measured data are distributed as

\begin{equation} \label{eq:mlr-olsdist-y-dist}
y \sim \mathcal{N}\left( X \beta, \sigma^2 I_n \right) \; .
\end{equation}

1) The parameter estimates from ordinary least sqaures ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}) are given by

\begin{equation} \label{eq:mlr-olsdist-b-est}
\hat{\beta} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y
\end{equation}

and thus, by applying \eqref{eq:mlr-olsdist-mvn-ltt} to \eqref{eq:mlr-olsdist-b-est}, they are distributed as

\begin{equation} \label{eq:mlr-olsdist-b-est-dist}
\begin{split}
\hat{\beta} &\sim \mathcal{N}\left( \left[ (X^\mathrm{T} X)^{-1} X^\mathrm{T} \right] X \beta, \, \sigma^2 \left[ (X^\mathrm{T} X)^{-1} X^\mathrm{T} \right] I_n \left[ X (X^\mathrm{T} X)^{-1} \right] \right) \\
&\sim \mathcal{N}\left( \beta, \, \sigma^2 (X^\mathrm{T} X)^{-1} \right) \; .
\end{split}
\end{equation}

2) The fitted signal in multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat}) is given by

\begin{equation} \label{eq:mlr-olsdist-y-est}
\hat{y} = X \hat{\beta} = X (X^\mathrm{T} X)^{-1} X^\mathrm{T} y = P y
\end{equation}

and thus, by applying \eqref{eq:mlr-olsdist-mvn-ltt} to \eqref{eq:mlr-olsdist-y-est}, they are distributed as

\begin{equation} \label{eq:mlr-olsdist-y-est-dist}
\begin{split}
\hat{y} &\sim \mathcal{N}\left( X \beta, \, \sigma^2 X (X^\mathrm{T} X)^{-1} X^\mathrm{T} \right) \\
&\sim \mathcal{N}\left( X \beta, \, \sigma^2 P \right) \; .
\end{split}
\end{equation}

3) The residuals of the linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat}) are given by

\begin{equation} \label{eq:mlr-olsdist-e-est}
\hat{\varepsilon} = y - X \hat{\beta} = \left( I_n - X (X^\mathrm{T} X)^{-1} X^\mathrm{T} \right) y = \left( I_n - P \right) y
\end{equation}

and thus, by applying \eqref{eq:mlr-olsdist-mvn-ltt} to \eqref{eq:mlr-olsdist-e-est}, they are distributed as

\begin{equation} \label{eq:mlr-olsdist-e-est-dist-s1}
\begin{split}
\hat{\varepsilon} &\sim \mathcal{N}\left( \left[ I_n - X (X^\mathrm{T} X)^{-1} X^\mathrm{T} \right] X \beta, \, \sigma^2 \left[ I_n - P \right] I_n \left[ I_n - P \right]^\mathrm{T} \right) \\
&\sim \mathcal{N}\left( X \beta - X \beta, \, \sigma^2 \left[ I_n - P \right] \left[ I_n - P \right]^\mathrm{T} \right) \; .
\end{split}
\end{equation}

Because the residual-forming matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rfmat}) is symmetric ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-symm}) and idempotent ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-idem}), this becomes:

\begin{equation} \label{eq:mlr-olsdist-e-est-dist-s2}
\hat{\varepsilon} \sim \mathcal{N}\left( 0, \sigma^2 (I_n - P) \right) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Koch, Karl-Rudolf (2007): "Linear Model"; in: \textit{Introduction to Bayesian Statistics}, Springer, Berlin/Heidelberg, 2007, ch. 4, eqs. 4.2, 4.30; URL: \url{https://www.springer.com/de/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\item Penny, William (2006): "Multiple Regression"; in: \textit{Mathematics for Brain Imaging}, ch. 1.5, pp. 39-41, eqs. 1.106-1.110; URL: \url{https://ueapsylabs.co.uk/sites/wpenny/mbi/mbi_course.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Distribution of WLS estimates, signal and residuals}]{Distribution of WLS estimates, signal and residuals} \label{sec:mlr-wlsdist}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with correlated observations

\begin{equation} \label{eq:mlr-wlsdist-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

and consider estimation using weighted least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wls}). Then, the estimated parameters, fitted signal and residuals are distributed as

\begin{equation} \label{eq:mlr-wlsdist-mlr-dist}
\begin{split}
\hat{\beta} &\sim \mathcal{N}\left( \beta, \sigma^2 (X^\mathrm{T} V^{-1} X)^{-1} \right) \\
\hat{y} &\sim \mathcal{N}\left( X \beta, \sigma^2 (PV) \right) \\
\hat{\varepsilon} &\sim \mathcal{N}\left( 0, \sigma^2 (I_n - P) V \right)
\end{split}
\end{equation}

where $P$ is the projection matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:pmat}) for weighted least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wls})

\begin{equation} \label{eq:mlr-wlsdist-mlr-pmat}
P = X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} We will use the linear transformation theorem for the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}):

\begin{equation} \label{eq:mlr-wlsdist-mvn-ltt}
x \sim \mathcal{N}(\mu, \Sigma) \quad \Rightarrow \quad y = Ax + b \sim \mathcal{N}(A\mu + b, A \Sigma A^\mathrm{T}) \; .
\end{equation}

Applying \eqref{eq:mlr-wlsdist-mvn-ltt} to \eqref{eq:mlr-wlsdist-mlr}, the measured data are distributed as

\begin{equation} \label{eq:mlr-wlsdist-y-dist}
y \sim \mathcal{N}\left( X \beta, \sigma^2 V \right) \; .
\end{equation}

1) The parameter estimates from weighted least sqaures ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wls}) are given by

\begin{equation} \label{eq:mlr-wlsdist-b-est}
\hat{\beta} = (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y
\end{equation}

and thus, by applying \eqref{eq:mlr-wlsdist-mvn-ltt} to \eqref{eq:mlr-wlsdist-b-est}, they are distributed as

\begin{equation} \label{eq:mlr-wlsdist-b-est-dist}
\begin{split}
\hat{\beta} &\sim \mathcal{N}\left( \left[ (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} \right] X \beta, \, \sigma^2 \left[ (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} \right] V \left[ V^{-1} X (X^\mathrm{T} V^{-1} X)^{-1} \right] \right) \\
&\sim \mathcal{N}\left( \beta, \, \sigma^2 (X^\mathrm{T} V^{-1} X)^{-1} \right) \; .
\end{split}
\end{equation}

2) The fitted signal in multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat}) is given by

\begin{equation} \label{eq:mlr-wlsdist-y-est}
\hat{y} = X \hat{\beta} = X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y = P y
\end{equation}

and thus, by applying \eqref{eq:mlr-wlsdist-mvn-ltt} to \eqref{eq:mlr-wlsdist-y-est}, they are distributed as

\begin{equation} \label{eq:mlr-wlsdist-y-est-dist}
\begin{split}
\hat{y} &\sim \mathcal{N}\left( X \beta, \, \sigma^2 X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} \right) \\
&\sim \mathcal{N}\left( X \beta, \, \sigma^2 (PV) \right) \; .
\end{split}
\end{equation}

3) The residuals of the linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat}) are given by

\begin{equation} \label{eq:mlr-wlsdist-e-est}
\hat{\varepsilon} = y - X \hat{\beta} = \left( I_n - X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} \right) y = \left( I_n - P \right) y
\end{equation}

and thus, by applying \eqref{eq:mlr-wlsdist-mvn-ltt} to \eqref{eq:mlr-wlsdist-e-est}, they are distributed as

\begin{equation} \label{eq:mlr-wlsdist-e-est-dist}
\begin{split}
\hat{\varepsilon} &\sim \mathcal{N}\left( \left[ I_n - X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} \right] X \beta, \, \sigma^2 \left[ I_n - P \right] V \left[ I_n - P \right]^\mathrm{T} \right) \\
&\sim \mathcal{N}\left( X \beta - X \beta, \, \sigma^2 \left[ V - V P^\mathrm{T} - P V + P V P^\mathrm{T} \right] \right) \\
&\sim \mathcal{N}\left( 0, \, \sigma^2 \left[ V - V V^{-1} X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} - X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} V + P V P^\mathrm{T} \right] \right) \\
&\sim \mathcal{N}\left( 0, \, \sigma^2 \left[ V - 2 P V + X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} V V^{-1} X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} \right] \right) \\
&\sim \mathcal{N}\left( 0, \, \sigma^2 \left[ V - 2 P V + P V \right] \right) \\
&\sim \mathcal{N}\left( 0, \, \sigma^2 \left[ V - P V \right] \right) \\
&\sim \mathcal{N}\left( 0, \, \sigma^2 \left[ I_n - P \right] V \right) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Koch, Karl-Rudolf (2007): "Linear Model"; in: \textit{Introduction to Bayesian Statistics}, Springer, Berlin/Heidelberg, 2007, ch. 4, eqs. 4.2, 4.30; URL: \url{https://www.springer.com/de/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\item Penny, William (2006): "Multiple Regression"; in: \textit{Mathematics for Brain Imaging}, ch. 1.5, pp. 39-41, eqs. 1.106-1.110; URL: \url{https://ueapsylabs.co.uk/sites/wpenny/mbi/mbi_course.pdf}.
\item Soch J, Allefeld C, Haynes JD (2020): "Inverse transformed encoding models â€“ a solution to the problem of correlated trial-by-trial parameter estimates in fMRI decoding"; in: \textit{NeuroImage}, vol. 209, art. 116449, eq. A.10; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811919310407}; DOI: 10.1016/j.neuroimage.2019.116449.
\item Soch J, Meyer AP, Allefeld C, Haynes JD (2017): "How to improve parameter estimates in GLM-based fMRI data analysis: cross-validated Bayesian model averaging"; in: \textit{NeuroImage}, vol. 158, pp. 186-195, eq. A.2; URL: \url{https://www.sciencedirect.com/science/article/pii/S105381191730527X}; DOI: 10.1016/j.neuroimage.2017.06.056.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Distribution of residual sum of squares}]{Distribution of residual sum of squares} \label{sec:mlr-rssdist}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with correlated observations

\begin{equation} \label{eq:mlr-rssdist-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

and consider estimation using weighted least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wls}). Then, the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) $\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}$, divided by the true error variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $\sigma^2$, follows a chi-squared distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2}) with $n-p$ degrees of freedom

\begin{equation} \label{eq:mlr-rssdist-mlr-rss-dist}
\frac{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}}{\sigma^2} \sim \chi^2(n-p)
\end{equation}

where $n$ and $p$ are the dimensions of the $n \times p$ design matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $X$.


\vspace{1em}
\textbf{Proof:} Consider an $n \times n$ square matrix $W$, such that

\begin{equation} \label{eq:mlr-rssdist-W-def}
W V W^\mathrm{T} = I_n \; .
\end{equation}

Then, left-multiplying the regression model in \eqref{eq:mlr-rssdist-mlr} with $W$ gives

\begin{equation} \label{eq:mlr-rssdist-mlr-W-s1}
Wy = WX\beta + W\varepsilon, \; W\varepsilon \sim \mathcal{N}(0, \sigma^2 W V W^\mathrm{T})
\end{equation}

which can be rewritten as

\begin{equation} \label{eq:mlr-rssdist-mlr-W-s2}
\tilde{y} = \tilde{X}\beta + \tilde{\varepsilon}, \; \tilde{\varepsilon} \sim \mathcal{N}(0, \sigma^2 I_n)
\end{equation}

where $\tilde{y} = Wy$, $\tilde{X} = WX$ and $\tilde{\varepsilon} = W\varepsilon$. This implies the distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt})

\begin{equation} \label{eq:mlr-rssdist-y-tilde-dist}
\tilde{y} \sim \mathcal{N}(\tilde{X} \beta, \sigma^2 I_n) \; .
\end{equation}

With that, we have obtained a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent observations. Cochran's theorem for multivariate normal variables states that, for an $n \times 1$ normal random vector ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) whose covariance matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) is a scalar multiple of the identity matrix, a specific squared form will follow a non-central chi-squared distribution where the degrees of freedom and the non-centrality paramter depend on the matrix in the quadratic form:

\begin{equation} \label{eq:mlr-rssdist-mvn-cochran}
x \sim \mathcal{N}(\mu, \sigma^2 I_n) \quad \Rightarrow \quad y = x^\mathrm{T} A x /\sigma^2 \sim \chi^2\left( \mathrm{tr}(A), \mu^\mathrm{T} A \mu \right) \; .
\end{equation}

First, we formulate the residuals ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat}) in terms of transformed measurements $\tilde{y}$:

\begin{equation} \label{eq:mlr-rssdist-rss-y-s1}
\begin{array}{rlcl}
\hat{\varepsilon} & = \tilde{y} - \tilde{X} \hat{\beta} & \quad \text{where} \quad & \hat{\beta} = (\tilde{X}^\mathrm{T} \tilde{X})^{-1} \tilde{X}^\mathrm{T} \tilde{y} \\
& = (I_n - \tilde{P}) \tilde{y} & \quad \text{where} \quad & \tilde{P} = \tilde{X} (\tilde{X}^\mathrm{T} \tilde{X})^{-1} \tilde{X}^\mathrm{T} \\
& = \tilde{R} \tilde{y} & \quad \text{where} \quad & \tilde{R} = I_n - \tilde{P} \; .
\end{array}
\end{equation}

Next, we observe that the residual sum of squares can be represented as a quadratic form:

\begin{equation} \label{eq:mlr-rssdist-rss-y-s2}
\frac{1}{\sigma^2} \sum_{i=1}^{n} \hat{\varepsilon}_i^2 = \frac{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}}{\sigma^2} = \tilde{y}^\mathrm{T} \tilde{R}^\mathrm{T} \tilde{R} \tilde{y} / \sigma^2
\end{equation}

Because the residual-forming matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rfmat}) $\tilde{R}$ is symmetric ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-symm}) and idempotent ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-idem}), we have $\tilde{R}^\mathrm{T} = \tilde{R}$ and $\tilde{R}^2 = \tilde{R}$, such that:

\begin{equation} \label{eq:mlr-rssdist-rss-y-s3}
\frac{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}}{\sigma^2} = \tilde{y}^\mathrm{T} \tilde{R} \tilde{y} / \sigma^2 \; .
\end{equation}

With that, we can apply Cochran's theorem given by \eqref{eq:mlr-rssdist-mvn-cochran} which yields

\begin{equation} \label{eq:mlr-rssdist-rss-dist}
\begin{split}
\frac{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}}{\sigma^2} &\sim \chi^2\left( \mathrm{tr}(I_n - \tilde{P}), \, \beta^\mathrm{T} \tilde{X}^\mathrm{T} \tilde{R} \tilde{X} \beta \right) \\
&\sim \chi^2\left( \mathrm{tr}(I_n) - \mathrm{tr}( \tilde{P} ), \, \beta^\mathrm{T} \tilde{X}^\mathrm{T} (I_n - \tilde{P}) \tilde{X} \beta \right) \\
&\sim \chi^2\left( \mathrm{tr}(I_n) - \mathrm{tr}( \tilde{X} (\tilde{X}^\mathrm{T} \tilde{X})^{-1} \tilde{X}^\mathrm{T} ), \, \beta^\mathrm{T} (\tilde{X}^\mathrm{T} \tilde{X} - \tilde{X}^\mathrm{T} \tilde{X} (\tilde{X}^\mathrm{T} \tilde{X})^{-1} \tilde{X}^\mathrm{T} \tilde{X}) \beta \right) \\
&\sim \chi^2\left( \mathrm{tr}(I_n) - \mathrm{tr}( \tilde{X}^\mathrm{T} \tilde{X} (\tilde{X}^\mathrm{T} \tilde{X})^{-1} ), \, \beta^\mathrm{T} (\tilde{X}^\mathrm{T} \tilde{X} - \tilde{X}^\mathrm{T} \tilde{X}) \beta \right) \\
&\sim \chi^2\left( \mathrm{tr}(I_n) - \mathrm{tr}(I_p), \, \beta^\mathrm{T} 0_{pp} \beta \right) \\
&\sim \chi^2\left( n - p, \, 0 \right) \; .
\end{split}
\end{equation}

Because a non-central chi-squared distribution with non-centrality parameter of zero reduces to the central chi-squared distribution, we obtain our final result:

\begin{equation} \label{eq:mlr-rssdist-rss-dist-qed}
\frac{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}}{\sigma^2} \sim \chi^2(n-p) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Koch, Karl-Rudolf (2007): "Estimation of the Variance Factor in Traditional Statistics"; in: \textit{Introduction to Bayesian Statistics}, Springer, Berlin/Heidelberg, 2007, ch. 4.2.3, eq. 4.37; URL: \url{https://www.springer.com/de/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\item Penny, William (2006): "Estimating error variance"; in: \textit{Mathematics for Brain Imaging}, ch. 2.2, pp. 49-51, eqs. 2.4-2.8; URL: \url{https://ueapsylabs.co.uk/sites/wpenny/mbi/mbi_course.pdf}.
\item Wikipedia (2022): "Ordinary least squares"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-12-13; URL: \url{https://en.wikipedia.org/wiki/Ordinary_least_squares#Estimation}.
\item ocram (2022): "Why is RSS distributed chi square times n-p?"; in: \textit{StackExchange CrossValidated}, retrieved on 2022-12-21; URL: \url{https://stats.stackexchange.com/a/20230}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Weighted least squares}]{Weighted least squares} \label{sec:mlr-wls}
\setcounter{equation}{0}

\textbf{Theorem:} Given a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with correlated observations

\begin{equation} \label{eq:mlr-wls-MLR}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; ,
\end{equation}

the parameters minimizing the weighted residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) are given by

\begin{equation} \label{eq:mlr-wls-WLS}
\hat{\beta} = (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Let there be an $n \times n$ square matrix $W$, such that

\begin{equation} \label{eq:mlr-wls-W-def}
W V W^\mathrm{T} = I_n \; .
\end{equation}

Since $V$ is a covariance matrix and thus symmetric, $W$ is also symmetric and can be expressed as the matrix square root of the inverse of $V$:

\begin{equation} \label{eq:mlr-wls-W-V}
W V W = I_n \quad \Leftrightarrow \quad V = W^{-1} W^{-1} \quad \Leftrightarrow \quad V^{-1} = W W \quad \Leftrightarrow \quad W = V^{-1/2} \; .
\end{equation}

Left-multiplying the linear regression equation \eqref{eq:mlr-wls-MLR} with $W$, the linear transformation theorem ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}) implies that

\begin{equation} \label{eq:mlr-wls-MLR-W}
Wy = WX\beta + W\varepsilon, \; W\varepsilon \sim \mathcal{N}(0, \sigma^2 W V W^T) \; .
\end{equation}

Applying \eqref{eq:mlr-wls-W-def}, we see that \eqref{eq:mlr-wls-MLR-W} is actually a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent observations

\begin{equation} \label{eq:mlr-wls-MLR-W-dev}
\tilde{y} = \tilde{X}\beta + \tilde{\varepsilon}, \; \tilde{\varepsilon} \sim \mathcal{N}(0, \sigma^2 I_n)
\end{equation}

where $\tilde{y} = Wy$, $\tilde{X} = WX$ and $\tilde{\varepsilon} = W\varepsilon$, such that we can apply the ordinary least squares solution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}) giving

\begin{equation} \label{eq:mlr-wls-WLS-qed}
\begin{split}
\hat{\beta} &= (\tilde{X}^\mathrm{T} \tilde{X})^{-1} \tilde{X}^\mathrm{T} \tilde{y} \\
&= \left( (WX)^\mathrm{T} WX \right)^{-1} (WX)^\mathrm{T} Wy \\
&= \left( X^\mathrm{T} W^\mathrm{T} W X \right)^{-1} X^\mathrm{T} W^\mathrm{T} W y \\
&= \left( X^\mathrm{T} W W X \right)^{-1} X^\mathrm{T} W W y \\
&\overset{\eqref{eq:mlr-wls-W-V}}{=} \left( X^\mathrm{T} V^{-1} X \right)^{-1} X^\mathrm{T} V^{-1} y
\end{split}
\end{equation}

which corresponds to the weighted least squares solution \eqref{eq:mlr-wls-WLS}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Stephan, Klaas Enno (2010): "The General Linear Model (GLM)"; in: \textit{Methods and models for fMRI data analysis in neuroeconomics}, Lecture 3, Slides 20/23; URL: \url{http://www.socialbehavior.uzh.ch/teaching/methodsspring10.html}.
\item Wikipedia (2021): "Weighted least squares"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2021-11-17; URL: \url{https://en.wikipedia.org/wiki/Weighted_least_squares#Motivation}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Weighted least squares}]{Weighted least squares} \label{sec:mlr-wls2}
\setcounter{equation}{0}

\textbf{Theorem:} Given a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with correlated observations

\begin{equation} \label{eq:mlr-wls2-MLR}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; ,
\end{equation}

the parameters minimizing the weighted residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) are given by

\begin{equation} \label{eq:mlr-wls2-WLS}
\hat{\beta} = (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Let there be an $n \times n$ square matrix $W$, such that

\begin{equation} \label{eq:mlr-wls2-W-def}
W V W^\mathrm{T} = I_n \; .
\end{equation}

Since $V$ is a covariance matrix and thus symmetric, $W$ is also symmetric and can be expressed the matrix square root of the inverse of $V$:

\begin{equation} \label{eq:mlr-wls2-W-V}
W V W = I_n \quad \Leftrightarrow \quad V = W^{-1} W^{-1} \quad \Leftrightarrow \quad V^{-1} = W W \quad \Leftrightarrow \quad W = V^{-1/2} \; .
\end{equation}

Left-multiplying the linear regression equation \eqref{eq:mlr-wls2-MLR} with $W$, the linear transformation theorem ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}) implies that

\begin{equation} \label{eq:mlr-wls2-MLR-W}
Wy = WX\beta + W\varepsilon, \; W\varepsilon \sim \mathcal{N}(0, \sigma^2 W V W^T) \; .
\end{equation}

Applying \eqref{eq:mlr-wls2-W-def}, we see that \eqref{eq:mlr-wls2-MLR-W} is actually a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent observations

\begin{equation} \label{eq:mlr-wls2-MLR-W-dev}
Wy = WX\beta + W\varepsilon, \; W\varepsilon \sim \mathcal{N}(0, \sigma^2 I_n) \; .
\end{equation}

With this, we can express the weighted residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) as

\begin{equation} \label{eq:mlr-wls2-wRSS}
\mathrm{wRSS}(\beta) = \sum_{i=1}^n (W \varepsilon)_i^2 = (W \varepsilon)^\mathrm{T} (W \varepsilon) = (Wy-WX\beta)^\mathrm{T} (Wy-WX\beta)
\end{equation}

which can be developed into

\begin{equation} \label{eq:mlr-wls2-wRSS-dev}
\begin{split}
\mathrm{wRSS}(\beta) &= y^\mathrm{T} W^\mathrm{T} W y - y^\mathrm{T} W^\mathrm{T} W X \beta - \beta^\mathrm{T} X^\mathrm{T} W^\mathrm{T} W y + \beta^\mathrm{T} X^\mathrm{T} W^\mathrm{T} W X \beta \\
&= y^\mathrm{T} W W y - 2 \beta^\mathrm{T} X^\mathrm{T} W W y + \beta^\mathrm{T} X^\mathrm{T} W W X \beta \\
&\overset{\eqref{eq:mlr-wls2-W-V}}{=} y^\mathrm{T} V^{-1} y - 2 \beta^\mathrm{T} X^\mathrm{T} V^{-1} y + \beta^\mathrm{T} X^\mathrm{T} V^{-1} X \beta \; .
\end{split}
\end{equation}

The derivative of $\mathrm{wRSS}(\beta)$ with respect to $\beta$ is

\begin{equation} \label{eq:mlr-wls2-wRSS-der}
\frac{\mathrm{d}\mathrm{wRSS}(\beta)}{\mathrm{d}\beta} = - 2 X^\mathrm{T} V^{-1} y + 2 X^\mathrm{T} V^{-1} X \beta
\end{equation}

and setting this derivative to zero, we obtain:

\begin{equation} \label{eq:mlr-wls2-WLS-qed}
\begin{split}
\frac{\mathrm{d}\mathrm{wRSS}(\hat{\beta})}{\mathrm{d}\beta} &= 0 \\
0 &= - 2 X^\mathrm{T} V^{-1} y + 2 X^\mathrm{T} V^{-1} X \hat{\beta} \\
X^\mathrm{T} V^{-1} X \hat{\beta} &= X^\mathrm{T} V^{-1} y \\
\hat{\beta} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \; .
\end{split}
\end{equation}

Since the quadratic form $y^\mathrm{T} V^{-1} y$ in \eqref{eq:mlr-wls2-wRSS-dev} is positive, $\hat{\beta}$ minimizes $\mathrm{wRSS}(\beta)$.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Maximum likelihood estimation}]{Maximum likelihood estimation} \label{sec:mlr-mle}
\setcounter{equation}{0}

\textbf{Theorem:} Given a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with correlated observations

\begin{equation} \label{eq:mlr-mle-MLR}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; ,
\end{equation}

the maximum likelihood estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) of $\beta$ and $\sigma^2$ are given  by

\begin{equation} \label{eq:mlr-mle-MLE-MLE}
\begin{split}
\hat{\beta} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \\
\hat{\sigma}^2 &= \frac{1}{n} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}), the linear regression equation \eqref{eq:mlr-mle-MLR} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:mlr-mle-MLR-LF}
\begin{split}
p(y|\beta,\sigma^2) &= \mathcal{N}(y; X\beta, \sigma^2 V) \\
&= \sqrt{\frac{1}{(2\pi)^n |\sigma^2 V|}} \cdot \exp\left[ -\frac{1}{2} (y - X\beta)^\mathrm{T} (\sigma^2 V)^{-1} (y - X\beta) \right]
\end{split}
\end{equation}

and, using $\lvert \sigma^2 V \rvert = (\sigma^2)^n \lvert V \rvert$, the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf})

\begin{equation} \label{eq:mlr-mle-MLR-LL1}
\begin{split}
\mathrm{LL}(\beta,\sigma^2) = &\log p(y|\beta,\sigma^2) \\
= &- \frac{n}{2} \log(2\pi) - \frac{n}{2} \log (\sigma^2) - \frac{1}{2} \log |V| \\
&- \frac{1}{2 \sigma^2} (y - X\beta)^\mathrm{T} V^{-1} (y - X\beta) \; .
\end{split}
\end{equation}

Substituting the precision matrix $P = V^{-1}$ into \eqref{eq:mlr-mle-MLR-LL1} to ease notation, we have:

\begin{equation} \label{eq:mlr-mle-MLR-LL2}
\begin{split}
\mathrm{LL}(\beta,\sigma^2) = &- \frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2} \log(|V|) \\
&- \frac{1}{2 \sigma^2} \left( y^\mathrm{T} P y - 2 \beta^\mathrm{T} X^\mathrm{T} P y + \beta^\mathrm{T} X^\mathrm{T} P X \beta \right) \; .
\end{split}
\end{equation}

\vspace{1em}
The derivative of the log-likelihood function \eqref{eq:mlr-mle-MLR-LL2} with respect to $\beta$ is

\begin{equation} \label{eq:mlr-mle-dLL-dbeta}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\beta,\sigma^2)}{\mathrm{d}\beta} &= \frac{\mathrm{d}}{\mathrm{d}\beta} \left( - \frac{1}{2 \sigma^2} \left( y^\mathrm{T} P y - 2 \beta^\mathrm{T} X^\mathrm{T} P y + \beta^\mathrm{T} X^\mathrm{T} P X \beta \right) \right) \\
&= \frac{1}{2 \sigma^2} \, \frac{\mathrm{d}}{\mathrm{d}\beta} \left( 2 \beta^\mathrm{T} X^\mathrm{T} P y - \beta^\mathrm{T} X^\mathrm{T} P X \beta \right) \\
&= \frac{1}{2 \sigma^2} \left( 2 X^\mathrm{T} P y - 2 X^\mathrm{T} P X \beta \right) \\
&= \frac{1}{\sigma^2} \left( X^\mathrm{T} P y - X^\mathrm{T} P X \beta \right)
\end{split}
\end{equation}

and setting this derivative to zero gives the MLE for $\beta$:

\begin{equation} \label{eq:mlr-mle-beta-MLE}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{\beta},\sigma^2)}{\mathrm{d}\beta} &= 0 \\
0 &= \frac{1}{\sigma^2} \left( X^\mathrm{T} P y - X^\mathrm{T} P X \hat{\beta} \right) \\
0 &= X^\mathrm{T} P y - X^\mathrm{T} P X \hat{\beta} \\
X^\mathrm{T} P X \hat{\beta} &= X^\mathrm{T} P y \\
\hat{\beta} &= \left( X^\mathrm{T} P X \right)^{-1} X^\mathrm{T} P y
\end{split}
\end{equation}

\vspace{1em}
The derivative of the log-likelihood function \eqref{eq:mlr-mle-MLR-LL1} at $\hat{\beta}$ with respect to $\sigma^2$ is

\begin{equation} \label{eq:mlr-mle-dLL-ds2}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{\beta},\sigma^2)}{\mathrm{d}\sigma^2} &= \frac{\mathrm{d}}{\mathrm{d}\sigma^2} \left( - \frac{n}{2} \log (\sigma^2) - \frac{1}{2 \sigma^2} (y - X\hat{\beta})^\mathrm{T} V^{-1} (y - X\hat{\beta}) \right) \\
&= - \frac{n}{2} \, \frac{1}{\sigma^2} + \frac{1}{2 (\sigma^2)^2} (y - X\hat{\beta})^\mathrm{T} V^{-1} (y - X\hat{\beta}) \\
&= - \frac{n}{2 \sigma^2} + \frac{1}{2 (\sigma^2)^2} (y - X\hat{\beta})^\mathrm{T} V^{-1} (y - X\hat{\beta})
\end{split}
\end{equation}

and setting this derivative to zero gives the MLE for $\sigma^2$:

\begin{equation} \label{eq:mlr-mle-s2-MLE}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{\beta},\hat{\sigma}^2)}{\mathrm{d}\sigma^2} &= 0 \\
0 &= - \frac{n}{2 \hat{\sigma}^2} + \frac{1}{2 (\hat{\sigma}^2)^2} (y - X\hat{\beta})^\mathrm{T} V^{-1} (y - X\hat{\beta}) \\
\frac{n}{2 \hat{\sigma}^2} &= \frac{1}{2 (\hat{\sigma}^2)^2} (y - X\hat{\beta})^\mathrm{T} V^{-1} (y - X\hat{\beta}) \\
\frac{2 (\hat{\sigma}^2)^2}{n} \cdot \frac{n}{2 \hat{\sigma}^2} &= \frac{2 (\hat{\sigma}^2)^2}{n} \cdot \frac{1}{2 (\hat{\sigma}^2)^2} (y - X\hat{\beta})^\mathrm{T} V^{-1} (y - X\hat{\beta}) \\
\hat{\sigma}^2 &= \frac{1}{n} (y - X\hat{\beta})^\mathrm{T} V^{-1} (y - X\hat{\beta})
\end{split}
\end{equation}

\vspace{1em}
Together, \eqref{eq:mlr-mle-beta-MLE} and \eqref{eq:mlr-mle-s2-MLE} constitute the MLE for multiple linear regression.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Maximum log-likelihood}]{Maximum log-likelihood} \label{sec:mlr-mll}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $m$ with correlation structure ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corrmat}) $V$

\begin{equation} \label{eq:mlr-mll-mlr}
m: \; y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; .
\end{equation}

Then, the maximum log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}) for this model is

\begin{equation} \label{eq:mlr-mll-mlr-mll-v1}
\mathrm{MLL}(m) = - \frac{n}{2} \log\left( \frac{\mathrm{RSS}}{n} \right) - \frac{n}{2} \left[ 1 + \log(2\pi) \right]
\end{equation}

under uncorrelated observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}), i.e. if $V = I_n$, and

\begin{equation} \label{eq:mlr-mll-mlr-mll-v2}
\mathrm{MLL}(m) = - \frac{n}{2} \log\left( \frac{\mathrm{wRSS}}{n} \right) - \frac{n}{2} \left[ 1 + \log(2\pi) \right] - \frac{1}{2} \log|V| \; ,
\end{equation}

in the general case, i.e. if $V \neq I_n$, where $\mathrm{RSS}$ is the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) and $\mathrm{wRSS}$ is the weighted residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wls2}).


\vspace{1em}
\textbf{Proof:} The likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) for multiple linear regression is given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle})

\begin{equation} \label{eq:mlr-mll-mlr-lf}
\begin{split}
p(y|\beta,\sigma^2) &= \mathcal{N}(y; X\beta, \sigma^2 V) \\
&= \sqrt{\frac{1}{(2\pi)^n |\sigma^2 V|}} \cdot \exp\left[ -\frac{1}{2} (y - X\beta)^\mathrm{T} (\sigma^2 V)^{-1} (y - X\beta) \right] \; ,
\end{split}
\end{equation}

such that, with $\lvert \sigma^2 V \rvert = (\sigma^2)^n \lvert V \rvert$, the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) for this model becomes ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle})

\begin{equation} \label{eq:mlr-mll-mlr-llf}
\begin{split}
\mathrm{LL}(\beta,\sigma^2) = &\log p(y|\beta,\sigma^2) \\
= &- \frac{n}{2} \log(2\pi) - \frac{n}{2} \log (\sigma^2) - \frac{1}{2} \log |V| - \frac{1}{2 \sigma^2} (y - X\beta)^\mathrm{T} V^{-1} (y - X\beta) \; .
\end{split}
\end{equation}

The maximum likelihood estimate for the noise variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle}) is

\begin{equation} \label{eq:mlr-mll-mlr-mle-s2}
\hat{\sigma}^2 = \frac{1}{n} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta})
\end{equation}

which can also be expressed in terms of the (weighted) residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) as

\begin{equation} \label{eq:mlr-mll-s2-rss}
\frac{1}{n} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) = \frac{1}{n} (Wy-WX\hat{\beta})^\mathrm{T} (Wy-WX\hat{\beta}) = \frac{1}{n} \sum_{i=1}^{n} (W\hat{\varepsilon})_i^2 = \frac{\mathrm{wRSS}}{n}
\end{equation}

where $W = V^{-1/2}$. Plugging \eqref{eq:mlr-mll-mlr-mle-s2} into \eqref{eq:mlr-mll-mlr-llf}, we obtain the maximum log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}) as

\begin{equation} \label{eq:mlr-mll-mlr-mll-v2-qed}
\begin{split}
\mathrm{MLL}(m) &= \mathrm{LL}(\hat{\beta},\hat{\sigma}^2) \\
&= - \frac{n}{2} \log(2\pi) - \frac{n}{2} \log (\hat{\sigma}^2) - \frac{1}{2} \log |V| - \frac{1}{2 \hat{\sigma}^2} (y - X\hat{\beta})^\mathrm{T} V^{-1} (y - X\hat{\beta}) \\
&= - \frac{n}{2} \log(2\pi) - \frac{n}{2} \log\left( \frac{\mathrm{wRSS}}{n} \right) - \frac{1}{2} \log |V| - \frac{1}{2} \cdot \frac{n}{\mathrm{wRSS}} \cdot \mathrm{wRSS} \\
&= - \frac{n}{2} \log\left( \frac{\mathrm{wRSS}}{n} \right) - \frac{n}{2} \left[ 1 + \log(2\pi) \right] - \frac{1}{2} \log|V|
\end{split}
\end{equation}

which proves the result in \eqref{eq:mlr-mll-mlr-mll-v2}. Assuming $V = I_n$, we have

\begin{equation} \label{eq:mlr-mll-mlr-mle-s2-iid}
\hat{\sigma}^2 = \frac{1}{n} (y-X\hat{\beta})^\mathrm{T} (y-X\hat{\beta}) = \frac{1}{n} \sum_{i=1}^{n} \hat{\varepsilon}_i^2 = \frac{\mathrm{RSS}}{n}
\end{equation}

and

\begin{equation} \label{eq:mlr-mll-mlr-logdet-V-iid}
\frac{1}{2} \log|V| = \frac{1}{2} \log|I_n| = \frac{1}{2} \log 1 = 0 \; ,
\end{equation}

such that

\begin{equation} \label{eq:mlr-mll-mlr-mll-v1-qed}
\mathrm{MLL}(m) = - \frac{n}{2} \log\left( \frac{\mathrm{RSS}}{n} \right) - \frac{n}{2} \left[ 1 + \log(2\pi) \right]
\end{equation}

which proves the result in \eqref{eq:mlr-mll-mlr-mll-v1}. This completes the proof.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Claeskens G, Hjort NL (2008): "Akaike's information criterion"; in: \textit{Model Selection and Model Averaging}, ex. 2.2, p. 66; URL: \url{https://www.cambridge.org/core/books/model-selection-and-model-averaging/E6F1EC77279D1223423BB64FC3A12C37}; DOI: 10.1017/CBO9780511790485.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{t-contrast}]{t-contrast} \label{sec:tcon}
\setcounter{equation}{0}

\textbf{Definition:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with $n \times p$ design matrix $X$ and $p \times 1$ regression coefficients $\beta$:

\begin{equation} \label{eq:tcon-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; .
\end{equation}

Then, a t-contrast is specified by a $p \times 1$ vector $c$ and it entails the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that the product of this vector and the regression coefficients is zero:

\begin{equation} \label{eq:tcon-mlr-t-h0}
H_0: \; c^\mathrm{T} \beta = 0 \; .
\end{equation}

Consequently, the alternative hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}) of a two-tailed t-test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp-tail}) is

\begin{equation} \label{eq:tcon-mlr-t-h1}
H_1: \; c^\mathrm{T} \beta \neq 0
\end{equation}

and the alternative hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}) of a one-sided t-test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:hyp-tail}) would be

\begin{equation} \label{eq:tcon-mlr-t-h1lr}
H_1: \; c^\mathrm{T} \beta < 0 \quad \text{or} \quad H_1: \; c^\mathrm{T} \beta > 0 \; .
\end{equation}

Here, $c$ is called the "contrast vector" and $c^\mathrm{T} \beta$ is called the "contrast value". With estimated regression coefficients, $c^\mathrm{T} \hat{\beta}$ is called the "estimated contrast value".

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Stephan, Klaas Enno (2010): "Classical (frequentist) inference"; in: \textit{Methods and models for fMRI data analysis in neuroeconomics}, Lecture 4, Slides 7/9; URL: \url{http://www.socialbehavior.uzh.ch/teaching/methodsspring10.html}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{F-contrast}]{F-contrast} \label{sec:fcon}
\setcounter{equation}{0}

\textbf{Definition:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with $n \times p$ design matrix $X$ and $p \times 1$ regression coefficients $\beta$:

\begin{equation} \label{eq:fcon-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; .
\end{equation}

Then, an F-contrast is specified by a $p \times q$ matrix $C$, yielding a $q \times 1$ vector $\gamma = C^\mathrm{T} \beta$, and it entails the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that each value in this vector is zero:

\begin{equation} \label{eq:fcon-mlr-f-h0}
H_0: \; \gamma_1 = 0 \wedge \ldots \wedge \gamma_q = 0 \; .
\end{equation}

Consequently, the alternative hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}) of the statistical test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}) would be that at least one entry of this vector is non-zero:

\begin{equation} \label{eq:fcon-mlr-f-h1}
H_1: \; \gamma_1 \neq 0 \vee \ldots \vee \gamma_q \neq 0 \; .
\end{equation}

Here, $C$ is called the "contrast matrix" and $C^\mathrm{T} \beta$ are called the "contrast values". With estimated regression coefficients, $C^\mathrm{T} \hat{\beta}$ are called the "estimated contrast values".

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Stephan, Klaas Enno (2010): "Classical (frequentist) inference"; in: \textit{Methods and models for fMRI data analysis in neuroeconomics}, Lecture 4, Slides 23/25; URL: \url{http://www.socialbehavior.uzh.ch/teaching/methodsspring10.html}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Contrast-based t-test}]{Contrast-based t-test} \label{sec:mlr-t}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr})

\begin{equation} \label{eq:mlr-t-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

and a t-contrast ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tcon}) on the model parameters

\begin{equation} \label{eq:mlr-t-tcon}
\gamma = c^\mathrm{T} \beta \quad \text{where} \quad c \in \mathbb{R}^{p \times 1} \; .
\end{equation}

Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:mlr-t-mlr-t}
t = \frac{c^\mathrm{T} \hat{\beta}}{\sqrt{\hat{\sigma}^2 c^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} c}}
\end{equation}

with the parameter estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle})

\begin{equation} \label{eq:mlr-t-mlr-est}
\begin{split}
\hat{\beta} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \\
\hat{\sigma}^2 &= \frac{1}{n-p} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta})
\end{split}
\end{equation}

follows a t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t})

\begin{equation} \label{eq:mlr-t-mlr-t-dist}
t \sim \mathrm{t}(n-p)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})

\begin{equation} \label{eq:mlr-t-mlr-t-h0}
\begin{split}
H_0: &\; c^\mathrm{T} \beta = 0 \\
H_1: &\; c^\mathrm{T} \beta > 0 \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:}

1) We know that the estimated regression coefficients in linear regression follow a multivariate normal distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wlsdist}):

\begin{equation} \label{eq:mlr-t-b-est-dist}
\hat{\beta} \sim \mathcal{N}\left( \beta, \, \sigma^2 (X^\mathrm{T} V^{-1} X)^{-1} \right) \; .
\end{equation}

Thus, the estimated contrast value ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tcon}) $\hat{\gamma} = c^\mathrm{T} \hat{\beta}$ is distributed according to a univariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}):

\begin{equation} \label{eq:mlr-t-g-est-dist}
\hat{\gamma} \sim \mathcal{N}\left( c^\mathrm{T} \beta, \, \sigma^2 c^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} c \right) \; .
\end{equation}

Now, define the random variable $z$ by dividing $\hat{\gamma}$ by its standard deviation:

\begin{equation} \label{eq:mlr-t-z}
z = \frac{c^\mathrm{T} \hat{\beta}}{\sqrt{\sigma^2 c^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} c}} \; .
\end{equation}

Again applying the linear transformation theorem ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}), this is distributed as

\begin{equation} \label{eq:mlr-t-z-dist}
z \sim \mathcal{N}\left( \frac{c^\mathrm{T} \beta}{\sqrt{\sigma^2 c^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} c}}, \, 1 \right)
\end{equation}

and thus follows a standard normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:snorm}) under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}):

\begin{equation} \label{eq:mlr-t-z-dist-h0}
z \sim \mathcal{N}(0, 1), \quad \text{if} \; H_0 \; .
\end{equation}

2) We also know that the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}), divided the true error variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr})

\begin{equation} \label{eq:mlr-t-mlr-rss}
v = \frac{1}{\sigma^2} \sum_{i=1}^{n} \hat{\varepsilon}_i^2 = \frac{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}}{\sigma^2} = \frac{1}{\sigma^2} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta})
\end{equation}

is following a chi-squared distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-rssdist}):

\begin{equation} \label{eq:mlr-t-mlr-rss-dist}
v \sim \chi^2(n-p) \; .
\end{equation}

3) Because the estimated regression coefficients and the vector of residuals are independent from each other ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ind})

\begin{equation} \label{eq:mlr-t-mlr-ind-v1}
\hat{\beta} \quad \text{and} \quad \hat{\varepsilon} \quad \text{ind.}
\end{equation}

and thus, the estimated contrast values are also independent from the function of the residual sum of squares

\begin{equation} \label{eq:mlr-t-mlr-ind-v2}
z = \frac{c^\mathrm{T} \hat{\beta}}{\sqrt{\sigma^2 c^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} c}} \quad \text{and} \quad v = \frac{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}}{\sigma^2} \quad \text{ind.} \; ,
\end{equation}

the following quantity is, by definition, t-distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t})

\begin{equation} \label{eq:mlr-t-mlr-t-s1}
t = \frac{z}{\sqrt{v/(n-p)}} \sim \mathrm{t}(n-p), \quad \text{if} \; H_0
\end{equation}

and the quantity can be evaluated as:

\begin{equation} \label{eq:mlr-t-mlr-t-s2}
\begin{split}
t &\overset{\eqref{eq:mlr-t-mlr-t-s1}}{=} \frac{z}{\sqrt{v/(n-p)}} \\
&\overset{\eqref{eq:mlr-t-mlr-ind-v2}}{=} \frac{c^\mathrm{T} \hat{\beta}}{\sqrt{\sigma^2 c^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} c}} \cdot \sqrt{\frac{n-p}{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon} / \sigma^2}} \\
&= \frac{c^\mathrm{T} \hat{\beta}}{\sqrt{\frac{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}}{n-p} \cdot c^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} c}} \\
&\overset{\eqref{eq:mlr-t-mlr-rss}}{=} \frac{c^\mathrm{T} \hat{\beta}}{\sqrt{\frac{(y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta})}{n-p} \cdot c^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} c}} \\
&\overset{\eqref{eq:mlr-t-mlr-est}}{=} \frac{c^\mathrm{T} \hat{\beta}}{\sqrt{\hat{\sigma}^2 c^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} c}} \; .
\end{split}
\end{equation}

This means that the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) in \eqref{eq:mlr-t-mlr-t-h0} can be rejected when $t$ from \eqref{eq:mlr-t-mlr-t-s2} is as extreme or more extreme than the critical value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cval}) obtained from Student's t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) with $n-p$ degrees of freedom using a significance level ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:alpha}) $\alpha$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Stephan, Klaas Enno (2010): "Classical (frequentist) inference"; in: \textit{Methods and models for fMRI data analysis in neuroeconomics}, Lecture 4, Slides 7/9; URL: \url{http://www.socialbehavior.uzh.ch/teaching/methodsspring10.html}.
\item Walter, Henrik (ed.) (2005): "Datenanalyse fÃ¼r funktionell bildgebende Verfahren"; in: \textit{Funktionelle Bildgebung in Psychiatrie und Psychotherapie}, Schattauer, Stuttgart/New York, 2005, p. 40; URL: \url{https://books.google.de/books?id=edWzKAHi7jQC&source=gbs_navlinks_s}.
\item jld (2018): "Understanding t-test for linear regression"; in: \textit{StackExchange CrossValidated}, retrieved on 2022-12-13; URL: \url{https://stats.stackexchange.com/a/344008}.
\item Soch, Joram (2020): "Distributional Transformation Improves Decoding Accuracy When Predicting Chronological Age From Structural MRI"; in: \textit{Frontiers in Psychiatry}, vol. 11, art. 604268, eqs. 8/9; URL: \url{https://www.frontiersin.org/articles/10.3389/fpsyt.2020.604268/full}; DOI: 10.3389/fpsyt.2020.604268.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Contrast-based F-test}]{Contrast-based F-test} \label{sec:mlr-f}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr})

\begin{equation} \label{eq:mlr-f-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

and an F-contrast ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:fcon}) on the model parameters

\begin{equation} \label{eq:mlr-f-fcon}
\gamma = C^\mathrm{T} \beta \quad \text{where} \quad C \in \mathbb{R}^{p \times q} \; .
\end{equation}

Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:mlr-f-mlr-f}
F = \hat{\beta}^\mathrm{T} C \left( \hat{\sigma}^2 C^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C \right)^{-1} C^\mathrm{T} \hat{\beta} / q
\end{equation}

with the parameter estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle})

\begin{equation} \label{eq:mlr-f-mlr-est}
\begin{split}
\hat{\beta} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \\
\hat{\sigma}^2 &= \frac{1}{n-p} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta})
\end{split}
\end{equation}

follows an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f})

\begin{equation} \label{eq:mlr-f-mlr-f-dist}
F \sim \mathrm{F}(q, n-p)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})

\begin{equation} \label{eq:mlr-f-mlr-f-h0}
\begin{split}
H_0: &\; \gamma_1 = 0 \wedge \ldots \wedge \gamma_q = 0 \\
H_1: &\; \gamma_1 \neq 0 \vee \ldots \vee \gamma_q \neq 0 \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:}

1) We know that the estimated regression coefficients in linear regression follow a multivariate normal distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wlsdist}):

\begin{equation} \label{eq:mlr-f-b-est-dist}
\hat{\beta} \sim \mathcal{N}\left( \beta, \, \sigma^2 (X^\mathrm{T} V^{-1} X)^{-1} \right) \; .
\end{equation}

Thus, the estimated contrast vector ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:fcon}) $\hat{\gamma} = C^\mathrm{T} \hat{\beta}$ is also distributed according to a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-ltt}):

\begin{equation} \label{eq:mlr-f-g-est-dist-cond}
\hat{\gamma} \sim \mathcal{N}\left( C^\mathrm{T} \beta, \, \sigma^2 C^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C \right) \; .
\end{equation}

Substituting the noise variance $\sigma^2$ with the noise precision $\tau = 1/\sigma^2$, we can also write this down as a conditional distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-cond}):

\begin{equation} \label{eq:mlr-f-g-est-tau-dist-cond}
\hat{\gamma} \vert \tau \sim \mathcal{N}\left( C^\mathrm{T} \beta, (\tau Q)^{-1} \right) \quad \text{with} \quad Q = \left( C^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C \right)^{-1} \; .
\end{equation}

2) We also know that the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}), divided the true error variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr})

\begin{equation} \label{eq:mlr-f-mlr-rss}
\frac{1}{\sigma^2} \sum_{i=1}^{n} \hat{\varepsilon}_i^2 = \frac{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}}{\sigma^2} = \frac{1}{\sigma^2} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta})
\end{equation}

is following a chi-squared distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-rssdist}):

\begin{equation} \label{eq:mlr-f-mlr-rss-dist}
\frac{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}}{\sigma^2} = \tau \, \hat{\varepsilon}^\mathrm{T} \hat{\varepsilon} \sim \chi^2(n-p) \; .
\end{equation}

The chi-squared distribution is a special case of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2-gam})

\begin{equation} \label{eq:mlr-f-chi2-gam}
X \sim \chi^2(k) \quad \Rightarrow \quad X \sim \mathrm{Gam}\left( \frac{k}{2}, \frac{1}{2} \right)
\end{equation}

and the gamma distribution changes under multiplication ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-scal}) in the following way:

\begin{equation} \label{eq:mlr-f-gam-scal}
X \sim \mathrm{Gam}\left( a, b \right) \quad \Rightarrow \quad cX \sim \mathrm{Gam}\left( a, \frac{b}{c} \right) \; .
\end{equation}

Thus, combining \eqref{eq:mlr-f-chi2-gam} and \eqref{eq:mlr-f-gam-scal} with \eqref{eq:mlr-f-mlr-rss-dist}, we obtain the marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of $\tau$ as:

\begin{equation} \label{eq:mlr-f-tau-dist}
\frac{1}{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}} \left( \tau \, \hat{\varepsilon}^\mathrm{T} \hat{\varepsilon} \right) = \tau \sim \mathrm{Gam}\left( \frac{n-p}{2}, \frac{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}}{2} \right) \; .
\end{equation}

3) Note that the joint distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-joint}) of $\hat{\gamma}$ and $\tau$ is, following from \eqref{eq:mlr-f-g-est-tau-dist-cond} and \eqref{eq:mlr-f-tau-dist} and by definition, a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}):

\begin{equation} \label{eq:mlr-f-g-est-tau-dist-joint}
\hat{\gamma}, \tau \sim \mathrm{NG}\left( C^\mathrm{T} \beta, Q, \frac{n-p}{2}, \frac{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}}{2} \right) \; .
\end{equation}

The marginal distribution of a normal-gamma distribution with respect to the normal random variable, is a multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-marg}):

\begin{equation} \label{eq:mlr-f-ng-mvt}
X, Y \sim \mathrm{NG}(\mu, \Lambda, a, b) \quad \Rightarrow \quad X \sim \mathrm{t}\left( \mu, \left( \frac{a}{b} \Lambda\right)^{-1}, 2a \right) \; .
\end{equation}

Thus, the marginal distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) of $\hat{\gamma}$ is:

\begin{equation} \label{eq:mlr-f-g-est-dist-marg}
\hat{\gamma} \sim \mathrm{t}\left( C^\mathrm{T} \beta, \left( \frac{n-p}{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}} Q \right)^{-1}, n-p \right) \; .
\end{equation}

4) Because of the following relationship between the multivariate t-distribution and the F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt-f})

\begin{equation} \label{eq:mlr-f-mvt-f}
X \sim t(\mu, \Sigma, \nu) \quad \Rightarrow \quad (X-\mu)^\mathrm{T} \, \Sigma^{-1} (X-\mu)/n \sim F(n, \nu) \; ,
\end{equation}

the following quantity is, by definition, F-distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f})

\begin{equation} \label{eq:mlr-f-mlr-f-s1}
F = \left( \hat{\gamma} -  C^\mathrm{T} \beta \right)^\mathrm{T} \left( \frac{n-p}{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}} Q \right) \left( \hat{\gamma} -  C^\mathrm{T} \beta \right) / q  \sim \mathrm{F}(q, n-p)
\end{equation}

and under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) \eqref{eq:mlr-f-mlr-f-h0}, it can be evaluated as:

\begin{equation} \label{eq:mlr-f-mlr-f-s2}
\begin{split}
F &\overset{\eqref{eq:mlr-f-mlr-f-s1}}{=} \left( \hat{\gamma} -  C^\mathrm{T} \beta \right)^\mathrm{T} \left( \frac{n-p}{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}} Q \right) \left( \hat{\gamma} -  C^\mathrm{T} \beta \right) / q \\
&\overset{\eqref{eq:mlr-f-mlr-f-h0}}{=} \hat{\gamma}^\mathrm{T} \left( \frac{n-p}{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}} Q \right) \hat{\gamma} / q \\
&\overset{\eqref{eq:mlr-f-fcon}}{=} \hat{\beta}^\mathrm{T} C \left( \frac{n-p}{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}} Q \right) C^\mathrm{T} \hat{\beta} / q \\
&\overset{\eqref{eq:mlr-f-g-est-tau-dist-cond}}{=} \hat{\beta}^\mathrm{T} C \left( \frac{n-p}{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}} \left( C^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C \right)^{-1} \right) C^\mathrm{T} \hat{\beta} / q \\
&\overset{\eqref{eq:mlr-f-mlr-rss}}{=} \hat{\beta}^\mathrm{T} C \left( \frac{n-p}{(y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta})} \left( C^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C \right)^{-1} \right) C^\mathrm{T} \hat{\beta} / q \\
&\overset{\eqref{eq:mlr-f-mlr-est}}{=} \hat{\beta}^\mathrm{T} C \left( \frac{1}{\hat{\sigma}^2} \left( C^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C \right)^{-1} \right) C^\mathrm{T} \hat{\beta} / q \\
&= \hat{\beta}^\mathrm{T} C \left( \hat{\sigma}^2 C^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C \right)^{-1} C^\mathrm{T} \hat{\beta} / q \; .
\end{split}
\end{equation}

This means that the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) in \eqref{eq:mlr-f-mlr-f-h0} can be rejected when $F$ from \eqref{eq:mlr-f-mlr-f-s2} is as extreme or more extreme than the critical value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cval}) obtained from Fisher's F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}) with $q$ numerator and $n-p$ denominator degrees of freedom using a significance level ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:alpha}) $\alpha$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Stephan, Klaas Enno (2010): "Classical (frequentist) inference"; in: \textit{Methods and models for fMRI data analysis in neuroeconomics}, Lecture 4, Slides 23/25; URL: \url{http://www.socialbehavior.uzh.ch/teaching/methodsspring10.html}.
\item Koch, Karl-Rudolf (2007): "Multivariate Distributions"; in: \textit{Introduction to Bayesian Statistics}, Springer, Berlin/Heidelberg, 2007, ch. 2.5, eqs. 2.202, 2.213, 2.211; URL: \url{https://www.springer.com/de/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\item jld (2018): "Understanding t-test for linear regression"; in: \textit{StackExchange CrossValidated}, retrieved on 2022-12-13; URL: \url{https://stats.stackexchange.com/a/344008}.
\item Penny, William (2006): "Comparing nested GLMs"; in: \textit{Mathematics for Brain Imaging}, ch. 2.3, pp. 51-52, eq. 2.9; URL: \url{https://ueapsylabs.co.uk/sites/wpenny/mbi/mbi_course.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{t-test for single regressor}]{t-test for single regressor} \label{sec:mlr-tsingle}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr})

\begin{equation} \label{eq:mlr-tsingle-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

using the $n \times p$ design matrix $X$ and the parameter estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle})

\begin{equation} \label{eq:mlr-tsingle-mlr-est}
\begin{split}
\hat{\beta} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \\
\hat{\sigma}^2 &= \frac{1}{n-p} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) \; .
\end{split}
\end{equation}

Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:mlr-tsingle-mlr-t-single}
t_j = \frac{\hat{\beta}_j}{\sqrt{\left( \hat{\varepsilon}^\mathrm{T} V^{-1} \hat{\varepsilon} \right)/(n-p) \; \sigma_{jj}}}
\end{equation}

with the $n \times 1$ vector of residuals ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat})

\begin{equation} \label{eq:mlr-tsingle-mlr-eps-est}
\hat{\varepsilon} = y - X\hat{\beta}
\end{equation}

and $\sigma_{jj}$ equal to the $j$-th diagonal element of the parameter covariance matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wlsdist})

\begin{equation} \label{eq:mlr-tsingle-mlr-t-single-sig}
\sigma_{jj} = \left[ \left( X^\mathrm{T} V^{-1} X \right)^{-1} \right]_{jj}
\end{equation}

follows a t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t})

\begin{equation} \label{eq:mlr-tsingle-mlr-t-single-dist}
t_j \sim \mathrm{t}(n-p)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that the $j$-th regression coefficient ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) is zero:

\begin{equation} \label{eq:mlr-tsingle-mlr-t-single-h0}
H_0: \; \beta_j = 0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} This is a special case of the contrast-based t-test for multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-t}) based on the following t-statistic ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}):

\begin{equation} \label{eq:mlr-tsingle-mlr-t}
t = \frac{c^\mathrm{T} \hat{\beta}}{\sqrt{\hat{\sigma}^2 c^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} c}} \sim \mathrm{t}(n-p) \; .
\end{equation}

In this special case, the contrast vector ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tcon}) is equal to the $j$-th elementary vector $e_j$ (a $p \times 1$ vector of zeros, with a single $1$ in the $j$-th entry)

\begin{equation} \label{eq:mlr-tsingle-mlr-t-single-con}
c = e_j = \left[ 0, \ldots, 0, 1, 0, \ldots, 0 \right]^\mathrm{T} \; ,
\end{equation}

such that the null hypothesis is given by

\begin{equation} \label{eq:mlr-tsingle-mlr-t-single-h0-qed}
H_0: \; c^\mathrm{T} \beta = e_j^\mathrm{T} \beta = \beta_j = 0
\end{equation}

and the test statistic becomes

\begin{equation} \label{eq:mlr-tsingle-mlr-t-single-qed}
\begin{split}
t_j &= \frac{e_j^\mathrm{T} \hat{\beta}}{\sqrt{\hat{\sigma}^2 e_j^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} e_j}} \\
&= \frac{\left[ 0, \ldots, 0, 1, 0, \ldots, 0 \right] \left[ \hat{\beta}_1, \ldots, \beta_{j-1}, \beta_j, \beta_{j+1}, \ldots, \hat{\beta}_p \right]^\mathrm{T}}{\sqrt{\frac{1}{n-p} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) \left[ 0, \ldots, 1, \ldots, 0 \right] (X^\mathrm{T} V^{-1} X)^{-1} \left[ 0, \ldots, 1, \ldots, 0 \right]^\mathrm{T}}} \\
&= \frac{\hat{\beta}_j}{\sqrt{\frac{1}{n-p} \left( \hat{\varepsilon}^\mathrm{T} V^{-1} \hat{\varepsilon} \right) \left[ \left( X^\mathrm{T} V^{-1} X \right)^{-1} \right]_{jj}}} \\
&= \frac{\hat{\beta}_j}{\sqrt{\left( \hat{\varepsilon}^\mathrm{T} V^{-1} \hat{\varepsilon} \right)/(n-p) \; \sigma_{jj}}} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Ostwald, Dirk (2023): "T-Statistiken"; in: \textit{Allgemeines Lineares Modell}, Einheit (7), Folien 20, 27; URL: \url{https://www.ipsy.ovgu.de/ipsy_media/Methodenlehre+I/Sommersemester+2023/Allgemeines+Lineares+Modell/7_T_Statistiken-p-9968.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{F-test for multiple regressors}]{F-test for multiple regressors} \label{sec:mlr-fomnibus}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr})

\begin{equation} \label{eq:mlr-fomnibus-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

the design matrix and regression coefficients of which are partitioned as

\begin{equation} \label{eq:mlr-fomnibus-mlr-X-b}
\begin{split}
X = \left[ \begin{matrix} X_0 & X_1 \end{matrix} \right] \in \mathbb{R}^{n \times p}
\quad &\text{where} \quad
X_0 \in \mathbb{R}^{n \times p_0} \quad \text{and} \quad X_1 \in \mathbb{R}^{n \times p_1} \\
\beta = \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right] \in \mathbb{R}^{p \times 1}
\quad &\text{where} \quad
\beta_0 \in \mathbb{R}^{p_0 \times 1} \quad \text{and} \quad \beta_1 \in \mathbb{R}^{p_1 \times 1}
\end{split}
\end{equation}

with $p = p_0 + p_1$. Then, the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-omnibus}
F = \frac{(\hat{\varepsilon}_0^\mathrm{T} V^{-1} \hat{\varepsilon}_0 - \hat{\varepsilon}^\mathrm{T} V^{-1} \hat{\varepsilon})/p_1}{\hat{\varepsilon}^\mathrm{T} V^{-1} \hat{\varepsilon}/(n-p)}
\end{equation}

follows an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f})

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-omnibus-dist}
F \sim \mathrm{F}(p_1, n-p)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that all regression coefficients ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $\beta_1$ are zero:

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-omnibus-h0}
H_0: \; \beta_1 = 0_{p_1} \quad \Leftrightarrow \quad
\beta_j = 0 \quad \text{for all} \quad j = (p_0+1),\ldots,p \; .
\end{equation}

In \eqref{eq:mlr-fomnibus-mlr-f-omnibus}, $\hat{\varepsilon}$ and $\hat{\varepsilon}_0$ are the residual vectors ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat}) when using either the full design matrix $X$ or the reduced design matrix $X_0$:

\begin{equation} \label{eq:mlr-fomnibus-mlr-e-e0}
\begin{split}
\hat{\varepsilon} = y - X \hat{\beta} \quad &\text{with} \quad
\hat{\beta} = (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \\
\hat{\varepsilon}_0 = y - X_0 \hat{\beta}_0 \quad &\text{with} \quad
\hat{\beta}_0 = (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} y \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} This is a special case of the contrast-based F-test for multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-f}) based on the F-statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:mlr-fomnibus-mlr-f}
F = \hat{\beta}^\mathrm{T} C \left( \hat{\sigma}^2 C^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C \right)^{-1} C^\mathrm{T} \hat{\beta} / q
\end{equation}

which follows an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}) under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that the product of the contrast matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:fcon}) $C \in \mathbb{R}^{p \times q}$ and the regression coefficients ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) equals zero: 

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-dist-h0}
F \sim \mathrm{F}(q, n-p), \quad \text{if} \quad C^\mathrm{T} \beta = 0_q = \left[ \begin{matrix} 0 \\ \vdots \\ 0 \end{matrix} \right] \; .
\end{equation}

In \eqref{eq:mlr-fomnibus-mlr-f}, $\hat{\sigma}^2$ is an estimate of the noise variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) calculated as the weighted ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wls}) residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}), divided by $n-p$:

\begin{equation} \label{eq:mlr-fomnibus-mlr-sig2-est}
\hat{\sigma}^2 = \frac{1}{n-p} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) \; .
\end{equation}

In the present case, in order to compare the full model specified by $X$ against the reduced model specified by $X_0$, we have to define the contrast matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:fcon}) as a vertical concatenation of a zero matrix on the first $p_0$ components and an identity matrix on the last $p_1$ components of $\beta$,

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-omnibus-C}
\begin{split}
C_1 = \left[ \begin{matrix} 0_{p_0,p_1} \\ I_{p_1} \end{matrix} \right] \in \mathbb{R}^{p \times p_1} \; ,
\end{split}
\end{equation}

i.e. specify an omnibus F-contrast that tests the alternative hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}) that any of the coefficients $\beta_1$ associated with the regressors $X_1$ is different from zero against the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that all those coefficients are zero:

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-omnibus-h0-C}
\begin{split}
&H_0: \; C_1^\mathrm{T} \beta
= \left[ \begin{matrix} 0_{p_0,p_1} \\ I_{p_1} \end{matrix} \right]^\mathrm{T} \left[ \begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right]
= \beta_1 = 0_{p_1} \quad \Leftrightarrow \quad \beta_{1j} = 0 \quad \text{for all} \quad j=1,\ldots,p_1 \\
\Rightarrow \; &H_1 \Leftrightarrow \neg H_0: \;  C_1^\mathrm{T} \beta
= \beta_1 \neq 0_{p_1} \quad \Leftrightarrow \quad  \beta_{1j} \neq 0 \quad \text{for at least one} \quad j=1,\ldots,p_1 \; .
\end{split}
\end{equation}

Thus, plugging $C = C_1$ and $q = p_1$ into \eqref{eq:mlr-fomnibus-mlr-f} and noting that $\hat{\sigma}^2$ from \eqref{eq:mlr-fomnibus-mlr-sig2-est} is a scalar, we obtain:

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-s1}
\begin{split}
F
&= \hat{\beta}^\mathrm{T} C_1 \left( \hat{\sigma}^2 C_1^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C_1 \right)^{-1} C_1^\mathrm{T} \hat{\beta} / p_1 \\
&\overset{\eqref{eq:mlr-fomnibus-mlr-sig2-est}}{=} \frac{\hat{\beta}^\mathrm{T} C_1 \left( C_1^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C_1 \right)^{-1} C_1^\mathrm{T} \hat{\beta} / p_1}{(y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) / (n-p)} \; .
\end{split}
\end{equation}

Here, we take note of the fact that the denominator in \eqref{eq:mlr-fomnibus-mlr-f-s1} is already equal to the denominator in \eqref{eq:mlr-fomnibus-mlr-f-omnibus}:

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-den}
(y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) / (n-p) \overset{\eqref{eq:mlr-fomnibus-mlr-e-e0}}{=} \hat{\varepsilon}^\mathrm{T} V^{-1} \hat{\varepsilon}/(n-p) \; .
\end{equation}

Therefore, what remains to be shown is that the numerator in \eqref{eq:mlr-fomnibus-mlr-f-s1} is equal to the numerator in \eqref{eq:mlr-fomnibus-mlr-f-omnibus}:

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-num}
\hat{\beta}^\mathrm{T} C_1 \left( C_1^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C_1 \right)^{-1} C_1^\mathrm{T} \hat{\beta} / p_1 = (\hat{\varepsilon}_0^\mathrm{T} V^{-1} \hat{\varepsilon}_0 - \hat{\varepsilon}^\mathrm{T} V^{-1} \hat{\varepsilon})/p_1 \; .
\end{equation}

To do this, we start with the inner-most matrix:

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-s2}
\begin{split}
X^\mathrm{T} V^{-1} X
&\overset{\eqref{eq:mlr-fomnibus-mlr-X-b}}{=} \left[ \begin{matrix} X_0 & X_1 \end{matrix} \right]^\mathrm{T} V^{-1} \left[ \begin{matrix} X_0 & X_1 \end{matrix} \right] \\
&= \left[ \begin{matrix} X_0^\mathrm{T} \\ X_1^\mathrm{T} \end{matrix} \right] V^{-1} \left[ \begin{matrix} X_0 & X_1 \end{matrix} \right] \\
&= \left[ \begin{matrix} X_0^\mathrm{T} V^{-1} X_0 & X_0^\mathrm{T} V^{-1} X_1 \\ X_1^\mathrm{T} V^{-1} X_0 & X_1^\mathrm{T} V^{-1} X_1 \end{matrix} \right] \; .
\end{split}
\end{equation}

The inverse of a block matrix is:

\begin{equation} \label{eq:mlr-fomnibus-block-inv}
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}^{-1} =
\begin{bmatrix}
A^{-1} + A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} & -A^{-1}B(D-CA^{-1}B)^{-1} \\
-(D-CA^{-1}B)^{-1}CA^{-1}                & (D-CA^{-1}B)^{-1}
\end{bmatrix} \; .
\end{equation}

Note that, with the contrast matrix $C_1$, we only extract the lower-right part of the inverse block matrix, so that we have:

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-s3}
\begin{split}
\left( C_1^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C_1 \right)^{-1}
&\overset{\eqref{eq:mlr-fomnibus-mlr-f-omnibus-C}}{=} \left( \left[ \begin{matrix} 0_{p_1,p_0} & I_{p_1} \end{matrix} \right] (X^\mathrm{T} V^{-1} X)^{-1} \left[ \begin{matrix} 0_{p_0,p_1} \\ I_{p_1} \end{matrix} \right] \right)^{-1} \\
&\overset{\eqref{eq:mlr-fomnibus-block-inv}}{=} \left( \left( X_1^\mathrm{T} V^{-1} X_1 - X_1^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} X_1 \right)^{-1} \right)^{-1} \\
&= X_1^\mathrm{T} V^{-1} X_1 - X_1^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} X_1 \; .
\end{split}
\end{equation}

We call this $p_1 \times p_1$ matrix $E$ and note that it can be written as

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-E}
\begin{split}
E
&\overset{\eqref{eq:mlr-fomnibus-mlr-f-s3}}{=} X_1^\mathrm{T} \left( V^{-1} - V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} \right) X_1 \\
&= X_1^\mathrm{T} \left( V^{-1} - F \right) X_1
\end{split}
\end{equation}

where the $n \times n$ matrix $F$ is given as follows:

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-F}
F
= V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} \; .
\end{equation}

Let $\hat{\beta}_{0(X)}$ denote the first $p_0$ entries of $\hat{\beta}$, i.e. estimates of the coefficients belonging to $X_0$, but estimated with $X$ (as opposed to $\hat{\beta}_0$ estimated with $X_0$ given by \eqref{eq:mlr-fomnibus-mlr-e-e0}):

\begin{equation} \label{eq:mlr-fomnibus-hat-b-b0X-b1}
\hat{\beta} = \left[ \begin{matrix} \hat{\beta}_{0(X)} \\ \hat{\beta}_1 \end{matrix} \right] \; .
\end{equation}

Then, it obviously holds that

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-s4}
\begin{split}
X \hat{\beta}
&\overset{\eqref{eq:mlr-fomnibus-hat-b-b0X-b1}}{=} \left[ \begin{matrix} X_0 & X_1 \end{matrix} \right] \left[ \begin{matrix} \hat{\beta}_{0(X)} \\ \hat{\beta}_1 \end{matrix} \right] \\
&= X_0 \hat{\beta}_{0(X)} + X_1 \hat{\beta}_1 \\
&\Leftrightarrow \\
X_1 \hat{\beta}_1 &= X \hat{\beta} - X_0 \hat{\beta}_{0(X)} \; .
\end{split}
\end{equation}

Next, we focus on $C_1^\mathrm{T} \hat{\beta}$ which simply extracts $\hat{\beta}_1$:

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-s5}
\begin{split}
C_1^\mathrm{T} \hat{\beta}
&\overset{\eqref{eq:mlr-fomnibus-hat-b-b0X-b1}}{=} \left[ \begin{matrix} 0_{p_1,p_0} & I_{p_1} \end{matrix} \right] \left[ \begin{matrix} \hat{\beta}_{0(X)} \\ \hat{\beta}_1 \end{matrix} \right] \\
&= \hat{\beta}_1 \; .
\end{split}
\end{equation}

With these identities in mind, we can get back to our main quantity of interest from \eqref{eq:mlr-fomnibus-mlr-f-num}:

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-s6}
\begin{split}
&\quad\quad \hat{\beta}^\mathrm{T} C_1 \left( C_1^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C_1 \right)^{-1} C_1^\mathrm{T} \hat{\beta} \\
&\overset{\eqref{eq:mlr-fomnibus-mlr-f-s5}}{=} \hat{\beta}_1^\mathrm{T} \; E \; \hat{\beta}_1 \\
&\overset{\eqref{eq:mlr-fomnibus-mlr-f-E}}{=} \hat{\beta}_1^\mathrm{T} X_1^\mathrm{T} \left( V^{-1} - V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} \right) X_1 \hat{\beta}_1 \\
&\overset{\eqref{eq:mlr-fomnibus-mlr-f-s4}}{=} \left( \hat{\beta}^\mathrm{T} X^\mathrm{T} - \hat{\beta}_{0(X)}^\mathrm{T} X_0^\mathrm{T} \right) \left( V^{-1} - V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} \right) \left( X \hat{\beta} - X_0 \hat{\beta}_{0(X)} \right) \\
&\overset{\eqref{eq:mlr-fomnibus-mlr-f-F}}{=} \left( \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} - \hat{\beta}^\mathrm{T} X^\mathrm{T} F - \hat{\beta}_{0(X)}^\mathrm{T} X_0^\mathrm{T} V^{-1} + \hat{\beta}_{0(X)}^\mathrm{T} X_0^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} \right) \left( X \hat{\beta} - X_0 \hat{\beta}_{0(X)} \right) \\
&= \left( \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} - \hat{\beta}^\mathrm{T} X^\mathrm{T} F - \hat{\beta}_{0(X)}^\mathrm{T} X_0^\mathrm{T} V^{-1} + \hat{\beta}_{0(X)}^\mathrm{T} X_0^\mathrm{T} V^{-1} \right) \left( X \hat{\beta} - X_0 \hat{\beta}_{0(X)} \right) \\
&= \left( \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} - \hat{\beta}^\mathrm{T} X^\mathrm{T} F \right) \left( X \hat{\beta} - X_0 \hat{\beta}_{0(X)} \right) \\
&\overset{\eqref{eq:mlr-fomnibus-mlr-f-F}}{=} \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X \hat{\beta} - \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X_0 \hat{\beta}_{0(X)} - \hat{\beta}^\mathrm{T} X^\mathrm{T} F X \hat{\beta} + \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} X_0 \hat{\beta}_{0(X)} \\
&= \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X \hat{\beta} - \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X_0 \hat{\beta}_{0(X)} - \hat{\beta}^\mathrm{T} X^\mathrm{T} F X \hat{\beta} + \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X_0 \hat{\beta}_{0(X)} \\
&= \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X \hat{\beta} - \hat{\beta}^\mathrm{T} X^\mathrm{T} F X \hat{\beta} \\
&\overset{\eqref{eq:mlr-fomnibus-mlr-f-F}}{=} \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X \hat{\beta} - \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} X \hat{\beta} \; .
\end{split}
\end{equation}

Let the residual vector of the full model be defined as given by \eqref{eq:mlr-fomnibus-mlr-e-e0}

\begin{equation} \label{eq:mlr-fomnibus-hat-e}
\hat{\varepsilon} = y - X \hat{\beta} \quad \Leftrightarrow \quad y = X \hat{\beta} + \hat{\varepsilon}
\end{equation}

and consider the term $X^\mathrm{T} V^{-1} \hat{\varepsilon}$. Using the residual-forming matrix expression of the residual vector ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mat}), we can show that this matrix product is zero:

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-s7a}
\begin{split}
X^\mathrm{T} V^{-1} \hat{\varepsilon}
&= X^\mathrm{T} V^{-1} \left( I_n - X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} \right) y \\
&= X^\mathrm{T} V^{-1} y - X^\mathrm{T} V^{-1} X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \\
&= X^\mathrm{T} V^{-1} y - X^\mathrm{T} V^{-1} y \\
&= 0_p \; .
\end{split}
\end{equation}

From this, it follows that the product $X_0^\mathrm{T} V^{-1} \hat{\varepsilon}$ is also zero:

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-s7b}
\begin{split}
X^\mathrm{T} V^{-1} \hat{\varepsilon} &= 0_p \\
\left[ \begin{matrix} X_0^\mathrm{T} \\ X_1^\mathrm{T} \end{matrix} \right] V^{-1} \hat{\varepsilon} &= 0_p \\
\left[ \begin{matrix} X_0^\mathrm{T} V^{-1} \hat{\varepsilon} \\ X_1^\mathrm{T} V^{-1} \hat{\varepsilon} \end{matrix} \right] &= \left[ \begin{matrix} 0_{p_0} \\ 0_{p_1} \end{matrix} \right] \\
&\Leftrightarrow \\
X_0^\mathrm{T} V^{-1} \hat{\varepsilon} &= 0_{p_0} \; .
\end{split}
\end{equation}

Thus, any term containing $X_0^\mathrm{T} V^{-1} \hat{\varepsilon} = 0_{p_0}$ can be added to a sum without changing the value of this sum. Continuing from above, we therefore write:

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-s8}
\begin{split}
&\quad\quad \hat{\beta}^\mathrm{T} C_1 \left( C_1^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C_1 \right)^{-1} C_1^\mathrm{T} \hat{\beta} \\
&\overset{\eqref{eq:mlr-fomnibus-mlr-f-s6}}{=} \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X \hat{\beta} - \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} X \hat{\beta} \\
&\overset{\eqref{eq:mlr-fomnibus-mlr-f-s7b}}{=} \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X \hat{\beta} - \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} X \hat{\beta} \\
&+ 2 \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} \hat{\varepsilon} + \hat{\varepsilon}^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} \hat{\varepsilon} \\
&= \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X \hat{\beta} - \left( X \hat{\beta} + \hat{\varepsilon} \right)^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} \left( X \hat{\beta} + \hat{\varepsilon} \right) \\
&\overset{\eqref{eq:mlr-fomnibus-hat-e}}{=} \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X \hat{\beta} - y^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} y \; .
\end{split}
\end{equation}

In the next transformations, we will make use of the weighted least squares parameter estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wls})

\begin{equation} \label{eq:mlr-fomnibus-mlr-b-b0}
\begin{split}
\hat{\beta} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \\
\hat{\beta}_0 &= (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} y
\end{split}
\end{equation}

and the fact that matrices and their inverses cancel out:

\begin{equation} \label{eq:mlr-fomnibus-mlr-inv}
\begin{split}
X^\mathrm{T} V^{-1} X (X^\mathrm{T} V^{-1} X)^{-1} = (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} X &= I_p \\
X_0^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} = (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} X_0 &= I_{p_0} \; .
\end{split}
\end{equation}

Continuing from above, we have:

\begin{equation} \label{eq:mlr-fomnibus-mlr-f-s9}
\begin{split}
&\quad\quad \hat{\beta}^\mathrm{T} C_1 \left( C_1^\mathrm{T} (X^\mathrm{T} V^{-1} X)^{-1} C_1 \right)^{-1} C_1^\mathrm{T} \hat{\beta} \\
&\overset{\eqref{eq:mlr-fomnibus-mlr-f-s8}}{=} \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X \hat{\beta} - y^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} y \\
&\overset{\eqref{eq:mlr-fomnibus-mlr-b-b0}}{=} y^\mathrm{T} V^{-1} X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y - y^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} y \\
&= y^\mathrm{T} V^{-1} X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y - y^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} y \\
&= y^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} y - 2 y^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} y \\
&- y^\mathrm{T} V^{-1} X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y + 2 y^\mathrm{T} V^{-1} X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \\
&\overset{\eqref{eq:mlr-fomnibus-mlr-inv}}{=} y^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} y - 2 y^\mathrm{T} V^{-1} X_0 (X_0^\mathrm{T} V^{-1} X_0)^{-1} X_0^\mathrm{T} V^{-1} y \\
&- y^\mathrm{T} V^{-1} X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y + 2 y^\mathrm{T} V^{-1} X (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \\
&\overset{\eqref{eq:mlr-fomnibus-mlr-b-b0}}{=} \hat{\beta}_0^\mathrm{T} X_0^\mathrm{T} V^{-1} X_0 \hat{\beta}_0 - 2 y^\mathrm{T} V^{-1} X_0 \hat{\beta}_0 - \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X \hat{\beta} + 2 y^\mathrm{T} V^{-1} X \hat{\beta} \\
&= y^\mathrm{T} V^{-1} y - 2 y^\mathrm{T} V^{-1} X_0 \hat{\beta}_0 + \hat{\beta}_0^\mathrm{T} X_0^\mathrm{T} V^{-1} X_0 \hat{\beta}_0 - y^\mathrm{T} V^{-1} y + 2 y^\mathrm{T} V^{-1} X \hat{\beta} - \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X \hat{\beta} \\
&= \left( y^\mathrm{T} V^{-1} y - 2 y^\mathrm{T} V^{-1} X_0 \hat{\beta}_0 + \hat{\beta}_0^\mathrm{T} X_0^\mathrm{T} V^{-1} X_0 \hat{\beta}_0 \right) - \left( y^\mathrm{T} V^{-1} y - 2 y^\mathrm{T} V^{-1} X \hat{\beta} + \hat{\beta}^\mathrm{T} X^\mathrm{T} V^{-1} X \hat{\beta} \right) \\
&= (y - X_0 \hat{\beta}_0)^\mathrm{T} V^{-1} (y - X_0 \hat{\beta}_0) - (y - X \hat{\beta})^\mathrm{T} V^{-1} (y - X \hat{\beta}) \\
&\overset{\eqref{eq:mlr-fomnibus-mlr-e-e0}}{=} \hat{\varepsilon}_0^\mathrm{T} V^{-1} \hat{\varepsilon}_0 - \hat{\varepsilon}^\mathrm{T} V^{-1} \hat{\varepsilon} \; .
\end{split}
\end{equation}

With that, it is shown that \eqref{eq:mlr-fomnibus-mlr-f-num} is true which, together with \eqref{eq:mlr-fomnibus-mlr-f-den}, finally demonstrates that the F-value in \eqref{eq:mlr-fomnibus-mlr-f-s1} is equal to the test statistic given by \eqref{eq:mlr-fomnibus-mlr-f-omnibus}. This completes the proof.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Ostwald, Dirk (2023): "F-Statistiken"; in: \textit{Allgemeines Lineares Modell}, Einheit (8), Folien 20, 24; URL: \url{https://www.ipsy.ovgu.de/ipsy_media/Methodenlehre+I/Sommersemester+2023/Allgemeines+Lineares+Modell/8_F_Statistiken-p-9972.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Deviance function}]{Deviance function} \label{sec:mlr-dev}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $m$ with correlation structure ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:corrmat}) $V$

\begin{equation} \label{eq:mlr-dev-mlr}
m: \; y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; .
\end{equation}

Then, the deviance ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:dev}) for this model is

\begin{equation} \label{eq:mlr-dev-mlr-dev-v1}
D(\beta,\sigma^2) = \mathrm{RSS}/\sigma^2 + n \cdot \left[ \log(\sigma^2) + \log(2\pi) \right]
\end{equation}

under uncorrelated observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}), i.e. if $V = I_n$, and

\begin{equation} \label{eq:mlr-dev-mlr-dev-v2}
D(\beta,\sigma^2) = \mathrm{wRSS}/\sigma^2 + n \cdot \left[ \log(\sigma^2) + \log(2\pi) \right] + \log|V| \; ,
\end{equation}

in the general case, i.e. if $V \neq I_n$, where $\mathrm{RSS}$ is the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) and $\mathrm{wRSS}$ is the weighted residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wls2}).


\vspace{1em}
\textbf{Proof:} The likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) for multiple linear regression is given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle})

\begin{equation} \label{eq:mlr-dev-mlr-lf}
\begin{split}
p(y|\beta,\sigma^2) &= \mathcal{N}(y; X\beta, \sigma^2 V) \\
&= \sqrt{\frac{1}{(2\pi)^n |\sigma^2 V|}} \cdot \exp\left[ -\frac{1}{2} (y - X\beta)^\mathrm{T} (\sigma^2 V)^{-1} (y - X\beta) \right] \; ,
\end{split}
\end{equation}

such that, with $\lvert \sigma^2 V \rvert = (\sigma^2)^n \lvert V \rvert$, the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) for this model becomes ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle})

\begin{equation} \label{eq:mlr-dev-mlr-llf}
\begin{split}
\mathrm{LL}(\beta,\sigma^2) &= \log p(y|\beta,\sigma^2) \\
&= - \frac{n}{2} \log(2\pi) - \frac{n}{2} \log (\sigma^2) - \frac{1}{2} \log |V| - \frac{1}{2 \sigma^2} (y - X\beta)^\mathrm{T} V^{-1} (y - X\beta) \; .
\end{split}
\end{equation}


The last term can be expressed in terms of the (weighted) residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) as

\begin{equation} \label{eq:mlr-dev-mll-rss}
\begin{split}
- \frac{1}{2 \sigma^2} (y - X\beta)^\mathrm{T} V^{-1} (y - X\beta) &= - \frac{1}{2 \sigma^2} (Wy-WX\beta)^\mathrm{T} (Wy-WX\beta) \\
&= - \frac{1}{2 \sigma^2} \left( \frac{1}{n} \sum_{i=1}^{n} (W\varepsilon)_i^2 \right) = - \frac{\mathrm{wRSS}}{2 \sigma^2}
\end{split}
\end{equation}

where $W = V^{-1/2}$. Plugging \eqref{eq:mlr-dev-mll-rss} into \eqref{eq:mlr-dev-mlr-llf} and multiplying with $-2$, we obtain the deviance ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:dev}) as

\begin{equation} \label{eq:mlr-dev-mlr-dev-v2-qed}
\begin{split}
D(\beta,\sigma^2) &= -2 \, \mathrm{LL}(\beta,\sigma^2) \\
&= -2 \left( - \frac{\mathrm{wRSS}}{2 \sigma^2} - \frac{n}{2} \log (\sigma^2) - \frac{n}{2} \log(2\pi) - \frac{1}{2} \log |V| \right) \\
&= \mathrm{wRSS}/\sigma^2 + n \cdot \left[ \log(\sigma^2) + \log(2\pi) \right] + \log|V|
\end{split}
\end{equation}

which proves the result in \eqref{eq:mlr-dev-mlr-dev-v2}. Assuming $V = I_n$, we have

\begin{equation} \label{eq:mlr-dev-mll-rss-iid}
\begin{split}
- \frac{1}{2 \sigma^2} (y - X\beta)^\mathrm{T} V^{-1} (y - X\beta) &= - \frac{1}{2 \sigma^2} (y - X\beta)^\mathrm{T} (y - X\beta) \\
&= - \frac{1}{2 \sigma^2} \left( \frac{1}{n} \sum_{i=1}^{n} \varepsilon_i^2 \right) = - \frac{\mathrm{RSS}}{2 \sigma^2}
\end{split}
\end{equation}

and

\begin{equation} \label{eq:mlr-dev-mlr-logdet-V-iid}
\frac{1}{2} \log|V| = \frac{1}{2} \log|I_n| = \frac{1}{2} \log 1 = 0 \; ,
\end{equation}

such that

\begin{equation} \label{eq:mlr-dev-mlr-mll-v1-qed}
D(\beta,\sigma^2) = \mathrm{RSS}/\sigma^2 + n \cdot \left[ \log(\sigma^2) + \log(2\pi) \right]
\end{equation}

which proves the result in \eqref{eq:mlr-dev-mlr-dev-v1}. This completes the proof.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Akaike information criterion}]{Akaike information criterion} \label{sec:mlr-aic}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $m$

\begin{equation} \label{eq:mlr-aic-mlr}
m: \; y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; .
\end{equation}

Then, the Akaike information criterion ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:aic}) for this model is

\begin{equation} \label{eq:mlr-aic-mlr-aic}
\mathrm{AIC}(m) = n \log\left( \frac{\mathrm{wRSS}}{n} \right) + n \left[ 1 + \log(2\pi) \right] + \log|V| + 2 (p + 1)
\end{equation}

where $\mathrm{wRSS}$ is the weighted residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}), $p$ is the number of regressors ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) in the design matrix $X$ and $n$ is the number of observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) in the data vector $y$.


\vspace{1em}
\textbf{Proof:} The Akaike information criterion ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:aic}) is defined as

\begin{equation} \label{eq:mlr-aic-aic}
\mathrm{AIC}(m) = -2 \, \mathrm{MLL}(m) + 2 \, k
\end{equation}

where $\mathrm{MLL}(m)$ is the maximum log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}) is $k$ is the number of free parameters in $m$.

The maximum log-likelihood for multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mll}) is given by

\begin{equation} \label{eq:mlr-aic-mlr-mll}
\mathrm{MLL}(m) = - \frac{n}{2} \log\left( \frac{\mathrm{wRSS}}{n} \right) - \frac{n}{2} \left[ 1 + \log(2\pi) \right] - \frac{1}{2} \log|V|
\end{equation}

and the number of free paramters in multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) is $k = p + 1$, i.e. one for each regressor in the design matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $X$, plus one for the noise variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $\sigma^2$.

Thus, the AIC of $m$ follows from \eqref{eq:mlr-aic-aic} and \eqref{eq:mlr-aic-mlr-mll} as

\begin{equation} \label{eq:mlr-aic-mlr-aic-qed}
\mathrm{AIC}(m) = n \log\left( \frac{\mathrm{wRSS}}{n} \right) + n \left[ 1 + \log(2\pi) \right] + \log|V| + 2 (p + 1) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Claeskens G, Hjort NL (2008): "Akaike's information criterion"; in: \textit{Model Selection and Model Averaging}, ex. 2.2, p. 66; URL: \url{https://www.cambridge.org/core/books/model-selection-and-model-averaging/E6F1EC77279D1223423BB64FC3A12C37}; DOI: 10.1017/CBO9780511790485.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Bayesian information criterion}]{Bayesian information criterion} \label{sec:mlr-bic}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $m$

\begin{equation} \label{eq:mlr-bic-mlr}
m: \; y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; .
\end{equation}

Then, the Bayesian information criterion ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:bic}) for this model is

\begin{equation} \label{eq:mlr-bic-mlr-bic}
\mathrm{BIC}(m) = n \log\left( \frac{\mathrm{wRSS}}{n} \right) + n \left[ 1 + \log(2\pi) \right] + \log|V| + \log(n) \, (p + 1)
\end{equation}

where $\mathrm{wRSS}$ is the weighted residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}), $p$ is the number of regressors ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) in the design matrix $X$ and $n$ is the number of observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) in the data vector $y$.


\vspace{1em}
\textbf{Proof:} The Bayesian information criterion ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:bic}) is defined as

\begin{equation} \label{eq:mlr-bic-bic}
\mathrm{BIC}(m) = -2 \, \mathrm{MLL}(m) + k \log(n)
\end{equation}

where $\mathrm{MLL}(m)$ is the maximum log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}), $k$ is the number of free parameters in $m$ and $n$ is the number of observations.

The maximum log-likelihood for multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mll}) is given by

\begin{equation} \label{eq:mlr-bic-mlr-mll}
\mathrm{MLL}(m) = - \frac{n}{2} \log\left( \frac{\mathrm{wRSS}}{n} \right) - \frac{n}{2} \left[ 1 + \log(2\pi) \right] - \frac{1}{2} \log|V|
\end{equation}

and the number of free paramters in multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) is $k = p + 1$, i.e. one for each regressor in the design matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $X$, plus one for the noise variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $\sigma^2$.

Thus, the BIC of $m$ follows from \eqref{eq:mlr-bic-bic} and \eqref{eq:mlr-bic-mlr-mll} as

\begin{equation} \label{eq:mlr-bic-mlr-bic-qed}
\mathrm{BIC}(m) = n \log\left( \frac{\mathrm{wRSS}}{n} \right) + n \left[ 1 + \log(2\pi) \right] + \log|V| + \log(n) \, (p + 1) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Corrected Akaike information criterion}]{Corrected Akaike information criterion} \label{sec:mlr-aicc}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $m$

\begin{equation} \label{eq:mlr-aicc-mlr}
m: \; y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; .
\end{equation}

Then, the corrected Akaike information criterion ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:aicc}) for this model is

\begin{equation} \label{eq:mlr-aicc-mlr-aicc}
\mathrm{AIC}_\mathrm{c}(m) = n \log\left( \frac{\mathrm{wRSS}}{n} \right) + n \left[ 1 + \log(2\pi) \right] + \log|V| + \frac{2 \, n \, (p+1)}{n-p-2}
\end{equation}

where $\mathrm{wRSS}$ is the weighted residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}), $p$ is the number of regressors ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) in the design matrix $X$ and $n$ is the number of observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) in the data vector $y$.


\vspace{1em}
\textbf{Proof:} The corrected Akaike information criterion ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:aicc}) is defined as

\begin{equation} \label{eq:mlr-aicc-aicc}
\mathrm{AIC}_\mathrm{c}(m) = \mathrm{AIC}(m) + \frac{2k^2 + 2k}{n-k-1}
\end{equation}

where $\mathrm{AIC}(m)$ is the Akaike information criterion ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:aic}), $k$ is the number of free parameters in $m$ and $n$ is the number of observations.

The Akaike information criterion for multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mll}) is given by

\begin{equation} \label{eq:mlr-aicc-mlr-aic}
\mathrm{AIC}(m) = n \log\left( \frac{\mathrm{wRSS}}{n} \right) + n \left[ 1 + \log(2\pi) \right] + \log|V| + 2 (p + 1)
\end{equation}

and the number of free paramters in multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) is $k = p + 1$, i.e. one for each regressor in the design matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $X$, plus one for the noise variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $\sigma^2$.

Thus, the corrected AIC of $m$ follows from \eqref{eq:mlr-aicc-aicc} and \eqref{eq:mlr-aicc-mlr-aic} as

\begin{equation} \label{eq:mlr-aicc-mlr-aicc-qed}
\begin{split}
\mathrm{AIC}_\mathrm{c}(m) &= n \log\left( \frac{\mathrm{wRSS}}{n} \right) + n \left[ 1 + \log(2\pi) \right] + \log|V| + 2 \, k + \frac{2k^2 + 2k}{n-k-1} \\
&= n \log\left( \frac{\mathrm{wRSS}}{n} \right) + n \left[ 1 + \log(2\pi) \right] + \log|V| + \frac{2nk - 2k^2 - 2k}{n-k-1} + \frac{2k^2 + 2k}{n-k-1} \\ 
&= n \log\left( \frac{\mathrm{wRSS}}{n} \right) + n \left[ 1 + \log(2\pi) \right] + \log|V| + \frac{2nk}{n-k-1} \\ 
&= n \log\left( \frac{\mathrm{wRSS}}{n} \right) + n \left[ 1 + \log(2\pi) \right] + \log|V| + \frac{2 \, n \, (p+1)}{n-p-2} \\ \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Claeskens G, Hjort NL (2008): "Akaike's information criterion"; in: \textit{Model Selection and Model Averaging}, ex. 2.5, p. 67; URL: \url{https://www.cambridge.org/core/books/model-selection-and-model-averaging/E6F1EC77279D1223423BB64FC3A12C37}; DOI: 10.1017/CBO9780511790485.
\end{itemize}
\vspace{1em}



\subsection{Bayesian linear regression}

\subsubsection[\textbf{Conjugate prior distribution}]{Conjugate prior distribution} \label{sec:blr-prior}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:blr-prior-GLM}
y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with measured $n \times 1$ data vector $y$, known $n \times p$ design matrix $X$, known $n \times n$ covariance structure $V$ as well as unknown $p \times 1$ regression coefficients $\beta$ and unknown noise variance $\sigma^2$.

Then, the conjugate prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-conj}) for this model is a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng})

\begin{equation} \label{eq:blr-prior-GLM-NG-prior}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0)
\end{equation}

where $\tau = 1/\sigma^2$ is the inverse noise variance or noise precision.


\vspace{1em}
\textbf{Proof:} By definition, a conjugate prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-conj}) is a prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) that, when combined with the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}), leads to a posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) that belongs to the same family of probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}). This is fulfilled when the prior density and the likelihood function are proportional to the model parameters in the same way, i.e. the model parameters appear in the same functional form in both.

Equation \eqref{eq:blr-prior-GLM} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:blr-prior-GLM-LF-class}
p(y|\beta,\sigma^2) = \mathcal{N}(y; X \beta, \sigma^2 V) = \sqrt{\frac{1}{(2 \pi)^n |\sigma^2 V|}} \, \exp\left[ -\frac{1}{2 \sigma^2} (y-X\beta)^\mathrm{T} V^{-1} (y-X\beta) \right]
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:blr-prior-GLM-LF-Bayes}
p(y|\beta,\tau) = \mathcal{N}(y; X \beta, (\tau P)^{-1}) = \sqrt{\frac{|\tau P|}{(2 \pi)^n}} \, \exp\left[ -\frac{\tau}{2} (y-X\beta)^\mathrm{T} P (y-X\beta) \right]
\end{equation}

using the noise precision $\tau = 1/\sigma^2$ and the $n \times n$ precision matrix $P = V^{-1}$.

\vspace{1em}
Seperating constant and variable terms, we have:

\begin{equation} \label{eq:blr-prior-GLM-LF-s1}
p(y|\beta,\tau) = \sqrt{\frac{|P|}{(2 \pi)^n}} \cdot \tau^{n/2} \cdot \exp\left[ -\frac{\tau}{2} (y-X\beta)^\mathrm{T} P (y-X\beta) \right] \; .
\end{equation}

Expanding the product in the exponent, we have:

\begin{equation} \label{eq:blr-prior-GLM-LF-s2}
p(y|\beta,\tau) = \sqrt{\frac{|P|}{(2 \pi)^n}} \cdot \tau^{n/2} \cdot \exp\left[ -\frac{\tau}{2} \left( y^\mathrm{T} P y - y^\mathrm{T} P X \beta - \beta^\mathrm{T} X^\mathrm{T} P y + \beta^\mathrm{T} X^\mathrm{T} P X \beta \right) \right] \; .
\end{equation}

Completing the square over $\beta$, finally gives

\begin{equation} \label{eq:blr-prior-GLM-LF-s3}
p(y|\beta,\tau) = \sqrt{\frac{|P|}{(2 \pi)^n}} \cdot \tau^{n/2} \cdot \exp\left[ -\frac{\tau}{2} \left( (\beta - \tilde{X}y)^\mathrm{T} X^\mathrm{T} P X (\beta - \tilde{X}y) - y^\mathrm{T} Q y + y^\mathrm{T} P y \right) \right]
\end{equation}

where $\tilde{X} = \left( X^\mathrm{T} P X \right)^{-1} X^\mathrm{T} P$ and $Q = \tilde{X}^\mathrm{T} \left( X^\mathrm{T} P X \right) \tilde{X}$.

\vspace{1em}
In other words, the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) is proportional to a power of $\tau$, times an exponential of $\tau$ and an exponential of a squared form of $\beta$, weighted by $\tau$:

\begin{equation} \label{eq:blr-prior-GLM-LF-s4}
p(y|\beta,\tau) \propto \tau^{n/2} \cdot \exp\left[ -\frac{\tau}{2} \left( y^\mathrm{T} P y - y^\mathrm{T} Q y \right) \right] \cdot \exp\left[ -\frac{\tau}{2} (\beta - \tilde{X}y)^\mathrm{T} X^\mathrm{T} P X (\beta - \tilde{X}y) \right] \; .
\end{equation}

The same is true for a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}) over $\beta$ and $\tau$

\begin{equation} \label{eq:blr-prior-BLR-prior-s1}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0)
\end{equation}

the probability density function of which ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-pdf})

\begin{equation} \label{eq:blr-prior-BLR-prior-s2}
p(\beta,\tau) = \sqrt{\frac{|\tau \Lambda_0|}{(2 \pi)^p}} \exp\left[ -\frac{\tau}{2} (\beta-\mu_0)^\mathrm{T} \Lambda_0 (\beta-\mu_0) \right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau]
\end{equation}

exhibits the same proportionality

\begin{equation} \label{eq:blr-prior-BLR-prior-s3}
p(\beta,\tau) \propto \tau^{a_0+p/2-1} \cdot \exp[-\tau b_0] \cdot \exp\left[ -\frac{\tau}{2} (\beta-\mu_0)^\mathrm{T} \Lambda_0 (\beta-\mu_0) \right]
\end{equation}

and is therefore conjugate relative to the likelihood.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop CM (2006): "Bayesian linear regression"; in: \textit{Pattern Recognition for Machine Learning}, pp. 152-161, ex. 3.12, eq. 3.112; URL: \url{https://www.springer.com/gp/book/9780387310732}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Posterior distribution}]{Posterior distribution} \label{sec:blr-post}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:blr-post-GLM}
y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with measured $n \times 1$ data vector $y$, known $n \times p$ design matrix $X$, known $n \times n$ covariance structure $V$ as well as unknown $p \times 1$ regression coefficients $\beta$ and unknown noise variance $\sigma^2$. Moreover, assume a normal-gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-prior}) over the model parameters $\beta$ and $\tau = 1/\sigma^2$:

\begin{equation} \label{eq:blr-post-GLM-NG-prior}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0) \; .
\end{equation}

Then, the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) is also a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng})

\begin{equation} \label{eq:blr-post-GLM-NG-post}
p(\beta,\tau|y) = \mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1}) \cdot \mathrm{Gam}(\tau; a_n, b_n)
\end{equation}

and the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:blr-post-GLM-NG-post-par}
\begin{split}
\mu_n &= \Lambda_n^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \\
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} According to Bayes' theorem ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:bayes-th}), the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) is given by

\begin{equation} \label{eq:blr-post-GLM-NG-BT}
p(\beta,\tau|y) = \frac{p(y|\beta,\tau) \, p(\beta,\tau)}{p(y)} \; .
\end{equation}

Since $p(y)$ is just a normalization factor, the posterior is proportional ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl}) to the numerator:

\begin{equation} \label{eq:blr-post-GLM-NG-post-JL}
p(\beta,\tau|y) \propto p(y|\beta,\tau) \, p(\beta,\tau) = p(y,\beta,\tau) \; .
\end{equation}

Equation \eqref{eq:blr-post-GLM} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:blr-post-GLM-LF-class}
p(y|\beta,\sigma^2) = \mathcal{N}(y; X \beta, \sigma^2 V) = \sqrt{\frac{1}{(2 \pi)^n |\sigma^2 V|}} \, \exp\left[ -\frac{1}{2 \sigma^2} (y-X\beta)^\mathrm{T} V^{-1} (y-X\beta) \right]
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:blr-post-GLM-LF-Bayes}
p(y|\beta,\tau) = \mathcal{N}(y; X \beta, (\tau P)^{-1}) = \sqrt{\frac{|\tau P|}{(2 \pi)^n}} \, \exp\left[ -\frac{\tau}{2} (y-X\beta)^\mathrm{T} P (y-X\beta) \right]
\end{equation}

using the noise precision $\tau = 1/\sigma^2$ and the $n \times n$ precision matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:precmat}) $P = V^{-1}$.

\vspace{1em}
Combining the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) \eqref{eq:blr-post-GLM-LF-Bayes} with the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) \eqref{eq:blr-post-GLM-NG-prior}, the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) of the model is given by

\begin{equation} \label{eq:blr-post-GLM-NG-JL-s1}
\begin{split}
p(y,\beta,\tau) = \; & p(y|\beta,\tau) \, p(\beta,\tau) \\
= \; & \sqrt{\frac{|\tau P|}{(2 \pi)^n}} \, \exp\left[ -\frac{\tau}{2} (y-X\beta)^\mathrm{T} P (y-X\beta) \right] \cdot \\
& \sqrt{\frac{|\tau \Lambda_0|}{(2 \pi)^p}} \, \exp\left[ -\frac{\tau}{2} (\beta-\mu_0)^\mathrm{T} \Lambda_0 (\beta-\mu_0) \right] \cdot \\
& \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \; .
\end{split}
\end{equation}

Collecting identical variables gives:

\begin{equation} \label{eq:blr-post-GLM-NG-JL-s2}
\begin{split}
p(y,\beta,\tau) = \; & \sqrt{\frac{\tau^{n+p}}{(2 \pi)^{n+p}} |P| |\Lambda_0|} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau}{2} \left( (y-X\beta)^\mathrm{T} P (y-X\beta) + (\beta-\mu_0)^\mathrm{T} \Lambda_0 (\beta-\mu_0) \right) \right] \; .
\end{split}
\end{equation}

Expanding the products in the exponent gives:

\begin{equation} \label{eq:blr-post-GLM-NG-JL-s3}
\begin{split}
p(y,\beta,\tau) = \; & \sqrt{\frac{\tau^{n+p}}{(2 \pi)^{n+p}} |P| |\Lambda_0|} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau}{2} \left( y^\mathrm{T} P y - y^\mathrm{T} P X \beta - \beta^\mathrm{T} X^\mathrm{T} P y + \beta^\mathrm{T} X^\mathrm{T} P X \beta + \right. \right. \\
& \hphantom{\exp \left[ -\frac{\tau}{2} \right.} \; \left. \left. \beta^\mathrm{T} \Lambda_0 \beta - \beta^\mathrm{T} \Lambda_0 \mu_0 - \mu_0^\mathrm{T} \Lambda_0 \beta + \mu_0^\mathrm{T} \Lambda_0 \mu_0 \right) \right] \; .
\end{split}
\end{equation}

Completing the square over $\beta$, we finally have

\begin{equation} \label{eq:blr-post-GLM-NG-JL-s4}
\begin{split}
p(y,\beta,\tau) = \; & \sqrt{\frac{\tau^{n+p}}{(2 \pi)^{n+p}} |P| |\Lambda_0|} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau}{2} \left( (\beta-\mu_n)^\mathrm{T} \Lambda_n (\beta-\mu_n) + (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \right) \right]
\end{split}
\end{equation}

with the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post})

\begin{equation} \label{eq:blr-post-GLM-NG-post-beta-par}
\begin{split}
\mu_n &= \Lambda_n^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \; .
\end{split}
\end{equation}

Ergo, the joint likelihood is proportional to

\begin{equation} \label{eq:blr-post-GLM-NG-JL-s5}
p(y,\beta,\tau) \propto \tau^{p/2} \cdot \exp\left[ -\frac{\tau}{2} (\beta-\mu_n)^\mathrm{T} \Lambda_n (\beta-\mu_n) \right] \cdot \tau^{a_n-1} \cdot \exp\left[ -b_n \tau \right]
\end{equation}

with the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post})

\begin{equation} \label{eq:blr-post-GLM-NG-post-tau-par}
\begin{split}
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \; .
\end{split}
\end{equation}

From the term in \eqref{eq:blr-post-GLM-NG-JL-s5}, we can isolate the posterior distribution over $\beta$ given $\tau$:

\begin{equation} \label{eq:blr-post-GLM-NG-post-beta}
p(\beta|\tau,y) = \mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1}) \; .
\end{equation}

From the remaining term, we can isolate the posterior distribution over $\tau$:

\begin{equation} \label{eq:blr-post-GLM-NG-post-tau}
p(\tau|y) = \mathrm{Gam}(\tau; a_n, b_n) \; .
\end{equation}

Together, \eqref{eq:blr-post-GLM-NG-post-beta} and \eqref{eq:blr-post-GLM-NG-post-tau} constitute the joint ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) of $\beta$ and $\tau$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop CM (2006): "Bayesian linear regression"; in: \textit{Pattern Recognition for Machine Learning}, pp. 152-161, ex. 3.12, eq. 3.113; URL: \url{https://www.springer.com/gp/book/9780387310732}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Log model evidence}]{Log model evidence} \label{sec:blr-lme}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:blr-lme-GLM}
m: \; y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with measured $n \times 1$ data vector $y$, known $n \times p$ design matrix $X$, known $n \times n$ covariance structure $V$ as well as unknown $p \times 1$ regression coefficients $\beta$ and unknown noise variance $\sigma^2$. Moreover, assume a normal-gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-prior}) over the model parameters $\beta$ and $\tau = 1/\sigma^2$:

\begin{equation} \label{eq:blr-lme-GLM-NG-prior}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0) \; .
\end{equation}

Then, the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) for this model is

\begin{equation} \label{eq:blr-lme-GLM-NG-LME}
\begin{split}
\log p(y|m) = \frac{1}{2} & \log |P| - \frac{n}{2} \log (2 \pi)  + \frac{1}{2} \log |\Lambda_0| - \frac{1}{2} \log |\Lambda_n| + \\
& \log \Gamma(a_n) - \log \Gamma(a_0) + a_0 \log b_0 - a_n \log b_n
\end{split}
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:blr-lme-GLM-NG-post-par}
\begin{split}
\mu_n &= \Lambda_n^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \\
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} According to the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), the model evidence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml}) for this model is:

\begin{equation} \label{eq:blr-lme-GLM-NG-ME-s1}
p(y|m) = \iint p(y|\beta,\tau) \, p(\beta,\tau) \, \mathrm{d}\beta \, \mathrm{d}\tau \; .
\end{equation}

According to the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), the integrand is equivalent to the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}):

\begin{equation} \label{eq:blr-lme-GLM-NG-ME-s2}
p(y|m) = \iint p(y,\beta,\tau) \, \mathrm{d}\beta \, \mathrm{d}\tau \; .
\end{equation}

Equation \eqref{eq:blr-lme-GLM} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:blr-lme-GLM-LF-class}
p(y|\beta,\sigma^2) = \mathcal{N}(y; X \beta, \sigma^2 V) = \sqrt{\frac{1}{(2 \pi)^n |\sigma^2 V|}} \, \exp\left[ -\frac{1}{2 \sigma^2} (y-X\beta)^\mathrm{T} V^{-1} (y-X\beta) \right]
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:blr-lme-GLM-LF-Bayes}
p(y|\beta,\tau) = \mathcal{N}(y; X \beta, (\tau P)^{-1}) = \sqrt{\frac{|\tau P|}{(2 \pi)^n}} \, \exp\left[ -\frac{\tau}{2} (y-X\beta)^\mathrm{T} P (y-X\beta) \right]
\end{equation}

using the noise precision $\tau = 1/\sigma^2$ and the $n \times n$ precision matrix $P = V^{-1}$.

\vspace{1em}
When deriving the posterior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post}) $p(\beta,\tau|y)$, the joint likelihood $p(y,\beta,\tau)$ is obtained as

\begin{equation} \label{eq:blr-lme-GLM-NG-LME-s1}
\begin{split}
p(y,\beta,\tau) = \; & \sqrt{\frac{\tau^n |P|}{(2 \pi)^n}} \, \sqrt{\frac{\tau^p |\Lambda_0|}{(2 \pi)^p}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau}{2} \left( (\beta-\mu_n)^T \Lambda_n (\beta-\mu_n) + (y^T P y + \mu_0^T \Lambda_0 \mu_0 - \mu_n^T \Lambda_n \mu_n) \right) \right] \; .
\end{split}
\end{equation}

Using the probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}), we can rewrite this as

\begin{equation} \label{eq:blr-lme-GLM-NG-LME-s2}
\begin{split}
p(y,\beta,\tau) = \; & \sqrt{\frac{\tau^n |P|}{(2 \pi)^n}} \, \sqrt{\frac{\tau^p |\Lambda_0|}{(2 \pi)^p}} \, \sqrt{\frac{(2 \pi)^p}{\tau^p |\Lambda_n|}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1}) \, \exp\left[ -\frac{\tau}{2} (y^T P y + \mu_0^T \Lambda_0 \mu_0 - \mu_n^T \Lambda_n \mu_n) \right] \; .
\end{split}
\end{equation}

Now, $\beta$ can be integrated out easily:

\begin{equation} \label{eq:blr-lme-GLM-NG-LME-s3}
\begin{split}
\int p(y,\beta,\tau) \, \mathrm{d}\beta = \; & \sqrt{\frac{\tau^n |P|}{(2 \pi)^n}} \, \sqrt{\frac{|\Lambda_0|}{|\Lambda_n|}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \tau^{a_0-1} \exp[-b_0 \tau] \cdot \\
& \exp\left[ -\frac{\tau}{2} (y^T P y + \mu_0^T \Lambda_0 \mu_0 - \mu_n^T \Lambda_n \mu_n) \right] \; .
\end{split}
\end{equation}

Using the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), we can rewrite this as

\begin{equation} \label{eq:blr-lme-GLM-NG-LME-s4}
\int p(y,\beta,\tau) \, \mathrm{d}\beta = \sqrt{\frac{|P|}{(2 \pi)^n}} \, \sqrt{\frac{|\Lambda_0|}{|\Lambda_n|}} \, \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \, \frac{\Gamma(a_n)}{ {b_n}^{a_n}} \, \mathrm{Gam}(\tau; a_n, b_n) \; .
\end{equation}

Finally, $\tau$ can also be integrated out:

\begin{equation} \label{eq:blr-lme-GLM-NG-LME-s5}
\iint p(y,\beta,\tau) \, \mathrm{d}\beta \, \mathrm{d}\tau = \sqrt{\frac{|P|}{(2 \pi)^n}} \, \sqrt{\frac{|\Lambda_0|}{|\Lambda_n|}} \, \frac{\Gamma(a_n)}{\Gamma(a_0)} \, \frac{ {b_0}^{a_0}}{ {b_n}^{a_n}} = p(y|m) \; .
\end{equation}

Thus, the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) of this model is given by

\begin{equation} \label{eq:blr-lme-GLM-NG-LME-s6}
\begin{split}
\log p(y|m) = \frac{1}{2} & \log |P| - \frac{n}{2} \log (2 \pi)  + \frac{1}{2} \log |\Lambda_0| - \frac{1}{2} \log |\Lambda_n| + \\
& \log \Gamma(a_n) - \log \Gamma(a_0) + a_0 \log b_0 - a_n \log b_n \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop CM (2006): "Bayesian linear regression"; in: \textit{Pattern Recognition for Machine Learning}, pp. 152-161, ex. 3.23, eq. 3.118; URL: \url{https://www.springer.com/gp/book/9780387310732}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Accuracy and complexity}]{Accuracy and complexity} \label{sec:blr-anc}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:blr-anc-GLM}
m: \; y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with measured $n \times 1$ data vector $y$, known $n \times p$ design matrix $X$, known $n \times n$ covariance structure $V$ as well as unknown $p \times 1$ regression coefficients $\beta$ and unknown noise variance $\sigma^2$. Moreover, assume a normal-gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-prior}) over the model parameters $\beta$ and $\tau = 1/\sigma^2$:

\begin{equation} \label{eq:blr-anc-GLM-NG-prior}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0) \; .
\end{equation}

Then, accuracy and complexity ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme-anc}) of this model are

\begin{equation} \label{eq:blr-anc-GLM-NG-AnC}
\begin{split}
\mathrm{Acc}(m) = - &\frac{1}{2} \frac{a_n}{b_n} (y-X\mu_n)^\mathrm{T} P (y-X\mu_n) - \frac{1}{2} \mathrm{tr}(X^\mathrm{T} P X \Lambda_n^{-1}) \\
+ &\frac{1}{2} \log |P| - \frac{n}{2} \log (2 \pi) + \frac{n}{2} (\psi(a_n) - \log(b_n)) \\
\mathrm{Com}(m) = \hphantom{+} &\frac{1}{2} \frac{a_n}{b_n} \left[(\mu_0-\mu_n)^\mathrm{T} \Lambda_0 (\mu_0-\mu_n) - 2(b_n-b_0)\right] + \frac{1}{2} \mathrm{tr}(\Lambda_0 \Lambda_n^{-1}) - \frac{1}{2} \log \frac{|\Lambda_0|}{|\Lambda_n|} - \frac{p}{2} \\
+ & \, a_0 \log \frac{b_n}{b_0} - \log \frac{\Gamma(a_n)}{\Gamma(a_0)} + (a_n - a_0) \psi(a_n) \; .
\end{split}
\end{equation}

where $\mu_n$, $\Lambda_n$, $a_n$ and $b_n$ are the posterior hyperparameters for Bayesian linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post}) and $P$ is the data precision matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:precmat}): $P = V^{-1}$.


\vspace{1em}
\textbf{Proof:} Model accuracy and complexity are defined as ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme-anc})

\begin{equation} \label{eq:blr-anc-lme-anc}
\begin{split}
\mathrm{LME}(m) &= \mathrm{Acc}(m) - \mathrm{Com}(m) \\
\mathrm{Acc}(m) &= \left\langle \log p(y|\beta,\tau,m) \right\rangle_{p(\beta,\tau|y,m)} \\
\mathrm{Com}(m) &= \mathrm{KL} \left[ p(\beta,\tau|y,m) \, || \, p(\beta,\tau|m) \right] \; .
\end{split}
\end{equation}

\vspace{1em}
1) The accuracy term is the expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) $\log p(y|\beta,\tau)$ with respect to the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) $p(\beta,\tau|y)$. This expectation can be rewritten as:

\begin{equation} \label{eq:blr-anc-GLM-NG-Acc-s1}
\begin{split}
\mathrm{Acc}(m) &= \iint p(\beta,\tau|y) \, \log p(y|\beta,\tau) \, \mathrm{d}\beta \, \mathrm{d}\tau \\
&= \int p(\tau|y) \int p(\beta|\tau,y) \, \log p(y|\beta,\tau) \, \mathrm{d}\beta \, \mathrm{d}\tau \\
&= \left\langle \left\langle \log p(y|\beta,\tau) \right\rangle_{p(\beta|\tau,y)} \right\rangle_{p(\tau|y)} \; .
\end{split}
\end{equation}

With the log-likelihood function for multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle}), we have:

\begin{equation} \label{eq:blr-anc-GLM-NG-Acc-s2}
\begin{split}
\mathrm{Acc}(m) &= \left\langle \left\langle \log \left( \sqrt{\frac{1}{(2\pi)^n |\sigma^2 V|}} \cdot \exp\left[ -\frac{1}{2} (y - X\beta)^\mathrm{T} (\sigma^2 V)^{-1} (y - X\beta) \right] \right) \right\rangle_{p(\beta|\tau,y)} \right\rangle_{p(\tau|y)} \\
&= \left\langle \left\langle \log \left( \sqrt{\frac{\tau^n |P|}{(2\pi)^n}} \cdot \exp\left[ -\frac{1}{2} (y - X\beta)^\mathrm{T} (\tau P) (y - X\beta) \right] \right) \right\rangle_{p(\beta|\tau,y)} \right\rangle_{p(\tau|y)} \\
&= \left\langle \left\langle \frac{1}{2} \log |P| + \frac{n}{2} \log \tau - \frac{n}{2} \log (2 \pi) - \frac{1}{2} (y-X\beta)^\mathrm{T} (\tau P) (y-X\beta) \right\rangle_{p(\beta|\tau,y)} \right\rangle_{p(\tau|y)} \\
&= \left\langle \left\langle \frac{1}{2} \log |P| + \frac{n}{2} \log \tau - \frac{n}{2} \log (2 \pi) - \frac{\tau}{2} \left[ y^\mathrm{T} P y - 2 y^\mathrm{T} P X \beta + \beta^\mathrm{T} X^\mathrm{T} P X \beta \right] \right\rangle_{p(\beta|\tau,y)} \right\rangle_{p(\tau|y)} \; .
\end{split}
\end{equation}

With the posterior distribution for Bayesian linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post}), this becomes:

\begin{equation} \label{eq:blr-anc-GLM-NG-Acc-s3}
\mathrm{Acc}(m) = \left\langle \left\langle \frac{1}{2} \log |P| + \frac{n}{2} \log \tau - \frac{n}{2} \log (2 \pi) - \frac{\tau}{2} \left[ y^\mathrm{T} P y - 2 y^\mathrm{T} P X \beta + \beta^\mathrm{T} X^\mathrm{T} P X \beta \right] \right\rangle_{\mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1})} \right\rangle_{\mathrm{Gam}(\tau; a_n, b_n)} \; .
\end{equation}

If $x \sim \mathrm{N}(\mu, \Sigma)$, then its expected value is ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-mean})

\begin{equation} \label{eq:blr-anc-mvn-mean}
\left\langle x \right\rangle = \mu
\end{equation}

and the expectation of a quadratic form is given by ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-qf})

\begin{equation} \label{eq:blr-anc-mvn-meansqr}
\left\langle x^\mathrm{T} A x \right\rangle = \mu^\mathrm{T} A \mu + \mathrm{tr}(A \Sigma) \; .
\end{equation}

Thus, the model accuracy of $m$ evaluates to:

\begin{equation} \label{eq:blr-anc-GLM-NG-Acc-s4}
\begin{split}
\mathrm{Acc}(m) &= \left\langle \frac{1}{2} \log |P| + \frac{n}{2} \log \tau - \frac{n}{2} \log (2 \pi) - \right. \\
&\hphantom{= -} \left. \frac{\tau}{2} \left[ y^\mathrm{T} P y - 2 y^\mathrm{T} P X \mu_n + \mu_n^\mathrm{T} X^\mathrm{T} P X \mu_n + \frac{1}{\tau} \mathrm{tr}(X^\mathrm{T} P X \Lambda_n^{-1}) \right] \right\rangle_{\mathrm{Gam}(\tau; a_n, b_n)} \\
&= \left\langle \frac{1}{2} \log |P| + \frac{n}{2} \log \tau - \frac{n}{2} \log (2 \pi) - \frac{\tau}{2} (y-X\mu_n)^\mathrm{T} P (y-X\mu_n) - \frac{1}{2} \mathrm{tr}(X^\mathrm{T} P X \Lambda_n^{-1}) \right\rangle_{\mathrm{Gam}(\tau; a_n, b_n)} \; .
\end{split}
\end{equation}

If $x \sim \mathrm{Gam}(a, b)$, then its expected value is ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-mean})

\begin{equation} \label{eq:blr-anc-gam-mean}
\left\langle x \right\rangle = \frac{a}{b}
\end{equation}

and its logarithmic expectation is given by ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-logmean})

\begin{equation} \label{eq:blr-anc-gan-logmean}
\left\langle \log x \right\rangle = \psi(a) - \log(b) \; .
\end{equation}

Thus, the model accuracy of $m$ evaluates to

\begin{equation} \label{eq:blr-anc-GLM-NG-Acc-s5}
\begin{split}
\mathrm{Acc}(m) = & - \frac{1}{2} \frac{a_n}{b_n} (y-X\mu_n)^\mathrm{T} P (y-X\mu_n) - \frac{1}{2} \mathrm{tr}(X^\mathrm{T} P X \Lambda_n^{-1}) \\
& + \frac{1}{2} \log |P| - \frac{n}{2} \log (2 \pi) + \frac{n}{2} (\psi(a_n) - \log(b_n))
\end{split}
\end{equation}

which proofs the first part of \eqref{eq:blr-anc-GLM-NG-AnC}.

\vspace{1em}
2) The complexity penalty is the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) $p(\beta,\tau|y)$ from the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) $p(\beta,\tau)$. This can be rewritten as follows:

\begin{equation} \label{eq:blr-anc-GLM-NG-Com-s1}
\begin{split}
\mathrm{Com}(m) &= \iint p(\beta,\tau|y) \, \log \frac{p(\beta,\tau|y)}{p(\beta,\tau)} \, \mathrm{d}\beta \, \mathrm{d}\tau \\
&= \iint p(\beta|\tau,y) \, p(\tau|y) \, \log \left[ \frac{p(\beta|\tau,y)}{p(\beta|\tau)} \, \frac{p(\tau|y)}{p(\tau)} \right] \, \mathrm{d}\beta \, \mathrm{d}\tau \\
&= \int p(\tau|y) \int p(\beta|\tau,y) \, \log \frac{p(\beta|\tau,y)}{p(\beta|\tau)} \, \mathrm{d}\beta \, \mathrm{d}\tau + \int p(\tau|y) \, \log \frac{p(\tau|y)}{p(\tau)} \int p(\beta|\tau,y) \, \mathrm{d}\beta \, \mathrm{d}\tau \\
&= \left\langle \mathrm{KL} \left[ p(\beta|\tau,y)\,||\,p(\beta|\tau) \right] \right\rangle_{p(\tau|y)} + \mathrm{KL} \left[ p(\tau|y)\,||\,p(\tau) \right] \; .
\end{split}
\end{equation}

With the prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-prior}) given by \eqref{eq:blr-anc-GLM-NG-prior} and the posterior distribution for Bayesian linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post}), this becomes:

\begin{equation} \label{eq:blr-anc-GLM-NG-Com-s2}
\begin{split}
\mathrm{Com}(m) &= \left\langle \mathrm{KL} \left[ \mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1})\,||\,\mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \right] \right\rangle_{\mathrm{Gam}(\tau; a_n, b_n)} \\
&+ \mathrm{KL} \left[ \mathrm{Gam}(\tau; a_n, b_n)\,||\,\mathrm{Gam}(\tau; a_0, b_0) \right] \; .
\end{split}
\end{equation}

With the Kullback-Leibler divergence for the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-kl})

\begin{equation} \label{eq:blr-anc-mvn-kl}
\mathrm{KL}[\mathcal{N}(\mu_1, \Sigma_1)\,||\,\mathcal{N}(\mu_2, \Sigma_2)] = \frac{1}{2} \left[ (\mu_2 - \mu_1)^\mathrm{T} \Sigma_2^{-1} (\mu_2 - \mu_1) + \mathrm{tr}(\Sigma_2^{-1} \Sigma_1) - \ln \frac{|\Sigma_1|}{|\Sigma_2|} - n \right]
\end{equation}

and the Kullback-Leibler divergence for the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-kl})

\begin{equation} \label{eq:blr-anc-gam-kl}
\mathrm{KL}[\mathrm{Gam}(a_1, b_1)\,||\,\mathrm{Gam}(a_2, b_2)] = a_2 \, \ln \frac{b_1}{b_2} - \ln \frac{\Gamma(a_1)}{\Gamma(a_2)} + (a_1 - a_2) \, \psi(a_1) - (b_1 - b_2) \, \frac{a_1}{b_1} \; ,
\end{equation}

the model complexity of $m$ evaluates to:

\begin{equation} \label{eq:blr-anc-GLM-NG-Com-s3}
\begin{split}
\mathrm{Com}(m) &= \left\langle \frac{1}{2} \left[ (\mu_0 - \mu_n)^\mathrm{T} (\tau \Lambda_0) (\mu_0 - \mu_n) + \mathrm{tr}((\tau \Lambda_0) (\tau \Lambda_n)^{-1}) - \log \frac{|(\tau \Lambda_n)^{-1}|}{|(\tau \Lambda_0)^{-1}|} - p \right] \right\rangle_{p(\tau|y)} \\
&+ a_0 \, \log \frac{b_n}{b_0} - \log \frac{\Gamma(a_n)}{\Gamma(a_0)} + (a_n - a_0) \, \psi(a_n) - (b_n - b_0) \, \frac{a_n}{b_n} \; .
\end{split}
\end{equation}

Using $x \sim \mathrm{Gam}(a, b) \Rightarrow \left\langle x \right\rangle = a/b$ from \eqref{eq:blr-anc-gam-mean} again, it follows that

\begin{equation} \label{eq:blr-anc-GLM-NG-Com-s4}
\begin{split}
\mathrm{Com}(m) &= \frac{1}{2} \frac{a_n}{b_n} \left[ (\mu_0 - \mu_n)^\mathrm{T} \Lambda_0 (\mu_0 - \mu_n) \right] + \frac{1}{2} \mathrm{tr}(\Lambda_0 \Lambda_n^{-1}) - \frac{1}{2} \log \frac{|\Lambda_0|}{|\Lambda_n|} - \frac{p}{2} \\
&+ a_0 \, \log \frac{b_n}{b_0} - \log \frac{\Gamma(a_n)}{\Gamma(a_0)} + (a_n - a_0) \, \psi(a_n) - (b_n - b_0) \, \frac{a_n}{b_n} \; .
\end{split}
\end{equation}

Thus, the model complexity of $m$ evaluates to

\begin{equation} \label{eq:blr-anc-GLM-NG-Com-s5}
\begin{split}
\mathrm{Com}(m) &= \frac{1}{2} \frac{a_n}{b_n} \left[(\mu_0-\mu_n)^\mathrm{T} \Lambda_0 (\mu_0-\mu_n) - 2(b_n-b_0)\right] + \frac{1}{2} \mathrm{tr}(\Lambda_0 \Lambda_n^{-1}) - \frac{1}{2} \log \frac{|\Lambda_0|}{|\Lambda_n|} - \frac{p}{2} \\
&+ a_0 \log \frac{b_n}{b_0} - \log \frac{\Gamma(a_n)}{\Gamma(a_0)} + (a_n - a_0) \psi(a_n)
\end{split}
\end{equation}

which proofs the second part of \eqref{eq:blr-anc-GLM-NG-AnC}.

\vspace{1em}
3) A control calculation confirms that

\begin{equation} \label{eq:blr-anc-GLM-NG-AnC-LME}
\mathrm{Acc}(m) - \mathrm{Com}(m) = \mathrm{LME}(m)
\end{equation}

where $\mathrm{LME}(m)$ is the log model evidence for Bayesian linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-lme}):

\begin{equation} \label{eq:blr-anc-GLM-NG-LME}
\begin{split}
\log p(y|m) = \frac{1}{2} & \log |P| - \frac{n}{2} \log (2 \pi)  + \frac{1}{2} \log |\Lambda_0| - \frac{1}{2} \log |\Lambda_n| + \\
& \log \Gamma(a_n) - \log \Gamma(a_0) + a_0 \log b_0 - a_n \log b_n
\end{split}
\end{equation}

This requires to recognize that

\begin{equation} \label{eq:blr-anc-GLM-NG-AnC-LME-a1}
-\frac{1}{2} \mathrm{tr}(X^\mathrm{T} P X \Lambda_n^{-1}) - \frac{1}{2} \mathrm{tr}(\Lambda_0 \Lambda_n^{-1}) + \frac{p}{2} = 0
\end{equation}

and 

\begin{equation} \label{eq:blr-anc-GLM-NG-AnC-LME-a2}
\frac{n}{2} (\psi(a_n) - \log(b_n)) - a_0 \log \frac{b_n}{b_0} - (a_n - a_0) \psi(a_n) = a_0 \log b_0 - a_n \log b_n
\end{equation}

thanks to the nature of the posterior hyperparameters for Bayesian linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post}).
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Soch J, Allefeld A (2016): "Kullback-Leibler Divergence for the Normal-Gamma Distribution"; in: \textit{arXiv math.ST}, 1611.01437, eqs. 23/30; URL: \url{https://arxiv.org/abs/1611.01437}.
\item Soch J, Allefeld C, Haynes JD (2016): "How to avoid mismodelling in GLM-based fMRI data analysis: cross-validated Bayesian model selection"; in: \textit{NeuroImage}, vol. 141, pp. 469-489, Appendix C; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811916303615}; DOI: 10.1016/j.neuroimage.2016.07.047.
\item Soch J (2018): "cvBMS and cvBMA: filling in the gaps"; in: \textit{arXiv stat.ME}, sect. 2.2, eqs. 8-24; URL: \url{https://arxiv.org/abs/1807.01585}; DOI: 10.48550/arXiv.1807.01585.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Deviance information criterion}]{Deviance information criterion} \label{sec:blr-dic}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $m$

\begin{equation} \label{eq:blr-dic-mlr}
m: \; y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V), \; \sigma^2 V = (\tau P)^{-1}
\end{equation}

with a normal-gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-prior})

\begin{equation} \label{eq:blr-dic-blr-prior}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0) \; .
\end{equation}

Then, the deviance information criterion ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:dic}) for this model is

\begin{equation} \label{eq:blr-dic-mlr-dic}
\begin{split}
\mathrm{DIC}(m) &= n \cdot \log(2\pi) - n \left[ 2 \psi(a_n) - \log(a_n) - \log(b_n) \right] - \log|P| \\
&+ \frac{a_n}{b_n} (y - X\mu_n)^\mathrm{T} P (y - X\mu_n) + \mathrm{tr}\left( X^\mathrm{T} P X \Lambda_n^{-1} \right)
\end{split}
\end{equation}

where $\mu_n$ and $\Lambda_n$ as well as $a_n$ and $b_n$ are posterior parameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) describing the posterior distribution in Bayesian linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post}).


\vspace{1em}
\textbf{Proof:} The deviance for multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-dev}) is

\begin{equation} \label{eq:blr-dic-mlr-dev-s1}
D(\beta,\sigma^2) = n \cdot \log(2\pi) + n \cdot \log(\sigma^2) + \log|V| + \frac{1}{\sigma^2} (y - X\beta)^\mathrm{T} V^{-1} (y - X\beta)
\end{equation}

which, applying the equalities $\tau = 1/\sigma^2$ and $P = V^{-1}$, becomes

\begin{equation} \label{eq:blr-dic-mlr-dev-s2}
D(\beta,\tau) = n \cdot \log(2\pi) - n \cdot \log(\tau) - \log|P| + \tau \cdot (y - X\beta)^\mathrm{T} P (y - X\beta) \; .
\end{equation}

The deviance information criterion ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:dic}) (DIC) is defined as

\begin{equation} \label{eq:blr-dic-dic}
\mathrm{DIC}(m) = -2 \log p(y|\left\langle \beta \right\rangle, \left\langle \tau \right\rangle, m) + 2 \, p_D
\end{equation}

where $\log p(y \vert \left\langle \beta \right\rangle, \left\langle \tau \right\rangle, m)$ is the log-likelihood function ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mll}) at the posterior expectations ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) and the "effective number of parameters" $p_D$ is the difference between the expectation of the deviance and the deviance at the expectation ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:dic}):

\begin{equation} \label{eq:blr-dic-dic-pD}
p_D = \left\langle D(\beta,\tau) \right\rangle - D(\left\langle \beta \right\rangle, \left\langle \tau \right\rangle) \; .
\end{equation}

With that, the DIC for multiple linear regression becomes:

\begin{equation} \label{eq:blr-dic-mlr-dic-s1}
\begin{split}
\mathrm{DIC}(m) &= -2 \log p(y|\left\langle \beta \right\rangle, \left\langle \tau \right\rangle, m) + 2 \, p_D \\
&= D(\left\langle \beta \right\rangle, \left\langle \tau \right\rangle) + 2 \left[ \left\langle D(\beta,\tau) \right\rangle - D(\left\langle \beta \right\rangle, \left\langle \tau \right\rangle) \right] \\
&= 2 \left\langle D(\beta,\tau) \right\rangle - D(\left\langle \beta \right\rangle, \left\langle \tau \right\rangle) \; .
\end{split}
\end{equation}

The posterior distribution for multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post}) is

\begin{equation} \label{eq:blr-dic-blr-post}
p(\beta,\tau|y) = \mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1}) \cdot \mathrm{Gam}(\tau; a_n, b_n)
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:blr-dic-blr-post-par}
\begin{split}
\mu_n &= \Lambda_n^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \\
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \; .
\end{split}
\end{equation}

Thus, we have the following posterior expectations:

\begin{equation} \label{eq:blr-dic-blr-post-beta}
\left\langle \beta \right\rangle_{\beta,\tau|y} = \mu_n
\end{equation}

\begin{equation} \label{eq:blr-dic-blr-post-tau}
\left\langle \tau \right\rangle_{\beta,\tau|y} = \frac{a_n}{b_n}
\end{equation}

\begin{equation} \label{eq:blr-dic-blr-post-log-tau}
\left\langle \log \tau \right\rangle_{\beta,\tau|y} = \psi(a_n) - \log(b_n)
\end{equation}

\begin{equation} \label{eq:blr-dic-blr-post-beta-qf}
\begin{split}
\left\langle \beta^\mathrm{T} A \beta \right\rangle_{\beta|\tau,y} &= \mu_n^\mathrm{T} A \mu_n + \mathrm{tr}\left( A (\tau \Lambda_n)^{-1} \right) \\
&= \mu_n^\mathrm{T} A \mu_n + \frac{1}{\tau} \mathrm{tr}\left( A \Lambda_n^{-1} \right) \; .
\end{split}
\end{equation}

In these identities, we have used the mean of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-mean}), the mean of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-mean}), the logarithmic expectation of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-logmean}), the expectation of a quadratic form ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-qf}) and the covariance of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-cov}).

With that, the deviance at the expectation is:

\begin{equation} \label{eq:blr-dic-mlr-dev-exp}
\begin{split}
D(\left\langle \beta \right\rangle, \left\langle \tau \right\rangle) &\overset{\eqref{eq:blr-dic-mlr-dev-s2}}{=} n \cdot \log(2\pi) - n \cdot \log(\left\langle \tau \right\rangle) - \log|P| + \tau \cdot (y - X\left\langle \beta \right\rangle)^\mathrm{T} P (y - X\left\langle \beta \right\rangle) \\
&\overset{\eqref{eq:blr-dic-blr-post-beta}}{=} n \cdot \log(2\pi) - n \cdot \log(\left\langle \tau \right\rangle) - \log|P| + \tau \cdot (y - X\mu_n)^\mathrm{T} P (y - X\mu_n) \\
&\overset{\eqref{eq:blr-dic-blr-post-tau}}{=} n \cdot \log(2\pi) - n \cdot \log\left(\frac{a_n}{b_n}\right) - \log|P| + \frac{a_n}{b_n} \cdot (y - X\mu_n)^\mathrm{T} P (y - X\mu_n) \; .
\end{split}
\end{equation}

Moreover, the expectation of the deviance is:

\begin{equation} \label{eq:blr-dic-mlr-exp-dev}
\begin{split}
\left\langle D(\beta,\tau) \right\rangle &\overset{\eqref{eq:blr-dic-mlr-dev-s2}}{=} \left\langle n \cdot \log(2\pi) - n \cdot \log(\tau) - \log|P| + \tau \cdot (y - X\beta)^\mathrm{T} P (y - X\beta) \right\rangle \\
&= n \cdot \log(2\pi) - n \cdot \left\langle \log(\tau) \right\rangle - \log|P| + \left\langle \tau \cdot (y - X\beta)^\mathrm{T} P (y - X\beta) \right\rangle \\
&\overset{\eqref{eq:blr-dic-blr-post-log-tau}}{=} n \cdot \log(2\pi) - n \cdot \left[ \psi(a_n) - \log(b_n) \right] - \log|P| \\
&+ \left\langle \tau \cdot \left\langle (y - X\beta)^\mathrm{T} P (y - X\beta) \right\rangle_{\beta|\tau,y} \right\rangle_{\tau|y} \\
&= n \cdot \log(2\pi) - n \cdot \left[ \psi(a_n) - \log(b_n) \right] - \log|P| \\
&+ \left\langle \tau \cdot \left\langle y^\mathrm{T} P y - y^\mathrm{T} P X\beta - \beta^\mathrm{T} X^\mathrm{T} P y + \beta^\mathrm{T} X^\mathrm{T} P X \beta \right\rangle_{\beta|\tau,y} \right\rangle_{\tau|y} \\
&\overset{\eqref{eq:blr-dic-blr-post-beta-qf}}{=} n \cdot \log(2\pi) - n \cdot \left[ \psi(a_n) - \log(b_n) \right] - \log|P| \\
&+ \left\langle \tau \cdot \left[ y^\mathrm{T} P y - y^\mathrm{T} P X\mu_n - \mu_n^\mathrm{T} X^\mathrm{T} P y + \mu_n^\mathrm{T} X^\mathrm{T} P X \mu_n  + \frac{1}{\tau} \mathrm{tr}\left( X^\mathrm{T} P X \Lambda_n^{-1} \right) \right] \right\rangle_{\tau|y} \\
&= n \cdot \log(2\pi) - n \cdot \left[ \psi(a_n) - \log(b_n) \right] - \log|P| \\
&+ \left\langle \tau \cdot (y - X\mu_n)^\mathrm{T} P (y - X\mu_n) \right\rangle_{\tau|y} + \mathrm{tr}\left( X^\mathrm{T} P X \Lambda_n^{-1} \right) \\
&\overset{\eqref{eq:blr-dic-blr-post-tau}}{=} n \cdot \log(2\pi) - n \cdot \left[ \psi(a_n) - \log(b_n) \right] - \log|P| \\
&+ \frac{a_n}{b_n} \cdot (y - X\mu_n)^\mathrm{T} P (y - X\mu_n) + \mathrm{tr}\left( X^\mathrm{T} P X \Lambda_n^{-1} \right) \; .
\end{split}
\end{equation}

Finally, combining the two terms, we have:

\begin{equation} \label{eq:blr-dic-mlr-dic-s2}
\begin{split}
\mathrm{DIC}(m) &\overset{\eqref{eq:blr-dic-mlr-dic-s1}}{=} 2 \left\langle D(\beta,\tau) \right\rangle - D(\left\langle \beta \right\rangle, \left\langle \tau \right\rangle) \\
&\overset{\eqref{eq:blr-dic-mlr-exp-dev}}{=} 2 \left[ n \cdot \log(2\pi) - n \cdot \left[ \psi(a_n) - \log(b_n) \right] - \log|P| \right. \\
&+ \left. \frac{a_n}{b_n} \cdot (y - X\mu_n)^\mathrm{T} P (y - X\mu_n) + \mathrm{tr}\left( X^\mathrm{T} P X \Lambda_n^{-1} \right) \right] \\
&\overset{\eqref{eq:blr-dic-mlr-dev-exp}}{-} \left[ n \cdot \log(2\pi) - n \cdot \log\left(\frac{a_n}{b_n}\right) - \log|P| + \frac{a_n}{b_n} \cdot (y - X\mu_n)^\mathrm{T} P (y - X\mu_n) \right] \\
&= n \cdot \log(2\pi) - 2 n \psi(a_n) + 2 n \log(b_n) + n \log(a_n) - \log(b_n) - \log|P| \\
&+ \frac{a_n}{b_n} (y - X\mu_n)^\mathrm{T} P (y - X\mu_n) + \mathrm{tr}\left( X^\mathrm{T} P X \Lambda_n^{-1} \right) \\
&= n \cdot \log(2\pi) - n \left[ 2 \psi(a_n) - \log(a_n) - \log(b_n) \right] - \log|P| \\
&+ \frac{a_n}{b_n} (y - X\mu_n)^\mathrm{T} P (y - X\mu_n) + \mathrm{tr}\left( X^\mathrm{T} P X \Lambda_n^{-1} \right) \; .
\end{split}
\end{equation}

This conforms to equation \eqref{eq:blr-dic-mlr-dic}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Maximum-a-posteriori estimation}]{Maximum-a-posteriori estimation} \label{sec:blr-map}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr})

\begin{equation} \label{eq:blr-map-GLM}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V), \; \sigma^2 V = (\tau P)^{-1}
\end{equation}

and assume a normal-gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-prior}) over the model parameters $\beta$ and $\tau = 1/\sigma^2$

\begin{equation} \label{eq:blr-map-GLM-NG-prior}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0) \; .
\end{equation}

Then, the maximum-a-posteriori estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:map}) of $\beta$ and $\tau$ are

\begin{equation} \label{eq:blr-map-GLN-NG-MAP}
\begin{split}
\hat{\beta}_\mathrm{MAP} &= (X^\mathrm{T} P X + \Lambda_0)^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
\hat{\tau}_\mathrm{MAP} &= \left( 2 a_0 + n - 2 \right) \left( 2 b_0 + (y - X \hat{\beta}_\mathrm{MAP})^\mathrm{T} P (y - X \hat{\beta}_\mathrm{MAP}) + (\hat{\beta}_\mathrm{MAP} - \mu_0)^\mathrm{T} \Lambda_0 (\hat{\beta}_\mathrm{MAP} - \mu_0) \right)^{-1}
\end{split}
\end{equation}

where $n$ is the number of data points ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}).


\vspace{1em}
\textbf{Proof:} Given the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) in \eqref{eq:blr-map-GLM-NG-prior}, the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) for multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) is also a normal-gamma distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post})

\begin{equation} \label{eq:blr-map-GLM-NG-post}
p(\beta,\tau|y) = \mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1}) \cdot \mathrm{Gam}(\tau; a_n, b_n)
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are equal to

\begin{equation} \label{eq:blr-map-GLM-NG-post-par}
\begin{split}
\mu_n &= \Lambda_n^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \\
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \; .
\end{split}
\end{equation}

From this, the conditional posterior distribution over $\beta$ follows as ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng})

\begin{equation} \label{eq:blr-map-GLM-NG-post-beta}
p(\beta|\tau,y) = \mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1})
\end{equation}

and the marginal posterior distribution over $\tau$ follows as ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng})

\begin{equation} \label{eq:blr-map-GLM-NG-post-tau}
p(\tau|y) = \mathrm{Gam}(\tau; a_n, b_n) \; .
\end{equation}

The mode of the multivariate normal distribution is given by

\begin{equation} \label{eq:blr-map-mvn-mode}
X \sim \mathcal{N}(\mu, \Sigma) \quad \Rightarrow \quad \mathrm{mode}(X) = \mu
\end{equation}

and the mode of the gamma distribution is given by

\begin{equation} \label{eq:blr-map-gam-mode}
X \sim \mathrm{Gam}(a, b) \quad \Rightarrow \quad \mathrm{mode}(X) = \frac{a-1}{b} \; .
\end{equation}

Applying \eqref{eq:blr-map-mvn-mode} to \eqref{eq:blr-map-GLM-NG-post-beta}, the maximum-a-posteriori estimate ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:map}) of $\beta$ follows as

\begin{equation} \label{eq:blr-map-GLN-NG-MAP-beta}
\begin{split}
\hat{\beta}_\mathrm{MAP} &= \mu_n \\
&= \Lambda_n^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
&= (X^\mathrm{T} P X + \Lambda_0)^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0)
\end{split}
\end{equation}

and applying \eqref{eq:blr-map-gam-mode} to \eqref{eq:blr-map-GLM-NG-post-tau}, the maximum-a-posteriori estimate ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:map}) of $\tau$ follows as

\begin{equation} \label{eq:blr-map-GLN-NG-MAP-tau}
\begin{split}
\hat{\tau}_\mathrm{MAP} &= \frac{a_n-1}{b_n} \\
&= \left( a_0 + \frac{n}{2} - 1 \right) \left( b_0 + \frac{1}{2} (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \right)^{-1} \\
&= \left( 2 a_0 + n - 2 \right) \left( 2 b_0 + y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n \right)^{-1} \\
&= \left( 2 a_0 + n - 2 \right) \left( 2 b_0 + y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \hat{\beta}_\mathrm{MAP}^\mathrm{T} \left( X^\mathrm{T} P X + \Lambda_0 \right) \hat{\beta}_\mathrm{MAP} \right)^{-1} \\
&= \left( 2 a_0 + n - 2 \right) \left( 2 b_0 + y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \hat{\beta}_\mathrm{MAP}^\mathrm{T} X^\mathrm{T} P X \hat{\beta}_\mathrm{MAP} - \hat{\beta}_\mathrm{MAP}^\mathrm{T} \Lambda_0 \hat{\beta}_\mathrm{MAP} \right)^{-1} \\
&= \left( 2 a_0 + n - 2 \right) \left( 2 b_0 + (y - X \hat{\beta}_\mathrm{MAP})^\mathrm{T} P (y - X \hat{\beta}_\mathrm{MAP}) + (\hat{\beta}_\mathrm{MAP} - \mu_0)^\mathrm{T} \Lambda_0 (\hat{\beta}_\mathrm{MAP} - \mu_0) \right)^{-1} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Expression of posterior parameters using error terms}]{Expression of posterior parameters using error terms} \label{sec:blr-posterr}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr})

\begin{equation} \label{eq:blr-posterr-GLM}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V), \; \sigma^2 V = (\tau P)^{-1} \; ,
\end{equation}

assume a normal-gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-prior}) over the model parameters $\beta$ and $\tau = 1/\sigma^2$

\begin{equation} \label{eq:blr-posterr-GLM-NG-prior}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0)
\end{equation}

and consider the Bayesian posterior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post}) over these model parameters:

\begin{equation} \label{eq:blr-posterr-GLM-NG-post}
p(\beta,\tau|y) = \mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1}) \cdot \mathrm{Gam}(\tau; a_n, b_n) \; .
\end{equation}

Then, the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) for the noise precision ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-prior}) $\tau$ can be expressed as

\begin{equation} \label{eq:blr-posterr-GLM-NG-post-tau}
\begin{split}
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} \left( \varepsilon_y^\mathrm{T} P \varepsilon_y +  \varepsilon_\beta^\mathrm{T} \Lambda_0 \varepsilon_\beta \right)
\end{split}
\end{equation}

where $\varepsilon_y$ and $\varepsilon_\beta$ are the "prediction errors" and "parameter errors"

\begin{equation} \label{eq:blr-posterr-GLM-NG-post-tau-err}
\begin{split}
\varepsilon_y &= y - \hat{y} \\
\varepsilon_\beta &= \mu_n - \mu_0
\end{split}
\end{equation}

where $\hat{y}$ is the predicted signal ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:pmat}) at the posterior mean ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post}) regression coefficients ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $\mu_n$:

\begin{equation} \label{eq:blr-posterr-GLM-NG-post-y-hat}
\hat{y} = X \mu_n \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The posterior hyperparameter for Bayesian linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post}) are:

\begin{equation} \label{eq:blr-posterr-GLM-NG-post-par}
\begin{split}
\mu_n &= \Lambda_n^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \\
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \; .
\end{split}
\end{equation}

The shape parameter ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) $a_n$ is given by this equation. The rate parameter ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}) $b_n$ of the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) can be developped as follows:

\begin{equation} \label{eq:blr-posterr-GLM-NG-post-tau-qed}
\begin{split}
b_n &\overset{\eqref{eq:blr-posterr-GLM-NG-post-par}}{=} b_0 + \frac{1}{2} \left( y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n \right) \\
&\overset{\eqref{eq:blr-posterr-GLM-NG-post-par}}{=} b_0 + \frac{1}{2} \left( y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} (X^\mathrm{T} P X + \Lambda_0) \mu_n \right) \\
&= b_0 + \frac{1}{2} \left( y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} X^\mathrm{T} P X \mu_n - \mu_n^\mathrm{T} \Lambda_0 \mu_n \right) \\
&= b_0 + \frac{1}{2} \left( (y^\mathrm{T} P y - \mu_n^\mathrm{T} X^\mathrm{T} P X \mu_n) + (\mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_0 \mu_n) \right) \\
&= b_0 + \frac{1}{2} \left( (y - X \mu_n)^\mathrm{T} P (y - X \mu_n) + (\mu_0 - \mu_n)^\mathrm{T} \Lambda_0 (\mu_0 - \mu_n) \right) \\
&\overset{\eqref{eq:blr-posterr-GLM-NG-post-y-hat}}{=} b_0 + \frac{1}{2} \left( (y - \hat{y})^\mathrm{T} P (y - \hat{y}) + (\mu_n - \mu_0)^\mathrm{T} \Lambda_0 (\mu_n - \mu_0) \right) \\
&\overset{\eqref{eq:blr-posterr-GLM-NG-post-tau-err}}{=} b_0 + \frac{1}{2} \left( \varepsilon_y^\mathrm{T} P \varepsilon_y +  \varepsilon_\beta^\mathrm{T} \Lambda_0 \varepsilon_\beta \right) \; .
\end{split}
\end{equation}

Together with equation (\ref{eq:GLM-NG-post-par}c), this completes the proof.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Posterior probability of alternative hypothesis}]{Posterior probability of alternative hypothesis} \label{sec:blr-pp}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) errors:

\begin{equation} \label{eq:blr-pp-GLM}
y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

and assume a normal-gamma ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}) prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) over the model parameters $\beta$ and $\tau = 1/\sigma^2$:

\begin{equation} \label{eq:blr-pp-GLM-NG-prior}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0) \; .
\end{equation}

Then, the posterior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob}) of the alternative hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1})

\begin{equation} \label{eq:blr-pp-GLM-H1}
\mathrm{H}_1: \, c^\mathrm{T} \beta > 0
\end{equation}

is given by

\begin{equation} \label{eq:blr-pp-GLM-NG-PP}
\mathrm{Pr}\left( \mathrm{H}_1 | y \right) = 1 - \mathrm{T}\left( -\frac{c^\mathrm{T} \mu}{\sqrt{c^\mathrm{T} \Sigma c}}; \nu \right)
\end{equation}

where $c$ is a $p \times 1$ contrast vector ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tcon}), $\mathrm{T}(x; \nu)$ is the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of the t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:t}) with $\nu$ degrees of freedom and $\mu$, $\Sigma$ and $\nu$ can be obtained from the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) of Bayesian linear regression.


\vspace{1em}
\textbf{Proof:} The posterior distribution for Bayesian linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post}) is given by a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}) over $\beta$ and $\tau = 1/\sigma^2$

\begin{equation} \label{eq:blr-pp-GLM-NG-post}
p(\beta,\tau|y) = \mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1}) \cdot \mathrm{Gam}(\tau; a_n, b_n)
\end{equation}

with the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post})

\begin{equation} \label{eq:blr-pp-GLM-NG-post-par}
\begin{split}
\mu_n &= \Lambda_n^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \\
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \; .
\end{split}
\end{equation}

The marginal distribution of a normal-gamma distribution is a multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-marg}), such that the marginal ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) posterior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) distribution of $\beta$ is

\begin{equation} \label{eq:blr-pp-GLM-NG-post-beta}
p(\beta|y) = t(\beta; \mu, \Sigma, \nu)
\end{equation}

with the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post})

\begin{equation} \label{eq:blr-pp-GLM-NG-post-par-beta}
\begin{split}
\mu &= \mu_n \\
\Sigma &= \left( \frac{a_n}{b_n} \Lambda_n \right)^{-1} \\
\nu &= 2 \, a_n \; .
\end{split}
\end{equation}

Define the quantity $\gamma = c^\mathrm{T} \beta$. According to the linear transformation theorem for the multivariate t-distribution, $\gamma$ also follows a multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt}):

\begin{equation} \label{eq:blr-pp-GLM-NG-post-gamma}
p(\gamma|y) = t(\gamma; c^\mathrm{T} \mu, c^\mathrm{T} \Sigma \, c, \nu) \; .
\end{equation}

Because $c^\mathrm{T}$ is a $1 \times p$ vector, $\gamma$ is a scalar and actually has a non-standardized t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:nst}). Therefore, the posterior probability of $H_1$ can be calculated using a one-dimensional integral:

\begin{equation} \label{eq:blr-pp-GLM-NG-post-prob-H0-s1}
\begin{split}
\mathrm{Pr}\left( \mathrm{H}_1 | y \right) &= p(\gamma > 0|y) \\
&= \int_{0}^{+\infty} p(\gamma|y) \, \mathrm{d}\gamma \\
&= 1 - \int_{-\infty}^{0} p(\gamma|y) \, \mathrm{d}\gamma \\
&= 1 - \mathrm{T}_\mathrm{nst}(0; c^\mathrm{T} \mu, c^\mathrm{T} \Sigma \, c, \nu) \; .
\end{split}
\end{equation}

Using the relation between non-standardized t-distribution and standard t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:nst-t}), we can finally write:

\begin{equation} \label{eq:blr-pp-GLM-NG-post-prob-H0-s2}
\begin{split}
\mathrm{Pr}\left( \mathrm{H}_1 | y \right) &= 1 - \mathrm{T}\left( \frac{(0 - c^\mathrm{T} \mu)}{\sqrt{c^\mathrm{T} \Sigma c}}; \nu \right) \\
&= 1 - \mathrm{T}\left( -\frac{c^\mathrm{T} \mu}{\sqrt{c^\mathrm{T} \Sigma c}}; \nu \right) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Koch, Karl-Rudolf (2007): "Multivariate t-distribution"; in: \textit{Introduction to Bayesian Statistics}, Springer, Berlin/Heidelberg, 2007, eqs. 2.235, 2.236, 2.213, 2.210, 2.188; URL: \url{https://www.springer.com/de/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Posterior credibility region excluding null hypothesis}]{Posterior credibility region excluding null hypothesis} \label{sec:blr-pcr}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) errors:

\begin{equation} \label{eq:blr-pcr-GLM}
y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

and assume a normal-gamma ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}) prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) over the model parameters $\beta$ and $\tau = 1/\sigma^2$:

\begin{equation} \label{eq:blr-pcr-GLM-NG-prior}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0) \; .
\end{equation}

Then, the largest posterior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) credibility region that does not contain the omnibus null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})

\begin{equation} \label{eq:blr-pcr-GLM-H0}
\mathrm{H}_0: \, C^\mathrm{T} \beta = 0
\end{equation}

is given by the credibility level

\begin{equation} \label{eq:blr-pcr-GLM-NG-PCR}
(1-\alpha) = \mathrm{F}\left( \left[ \mu^\mathrm{T} C (C^\mathrm{T} \Sigma \, C)^{-1} C^\mathrm{T} \mu \right]/q; q, \nu \right)
\end{equation}

where $C$ is a $p \times q$ contrast matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:fcon}), $\mathrm{F}(x; v, w)$ is the cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) of the F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}) with $v$ numerator degrees of freedom, $w$ denominator degrees of freedom and $\mu$, $\Sigma$ and $\nu$ can be obtained from the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) of Bayesian linear regression.


\vspace{1em}
\textbf{Proof:} The posterior distribution for Bayesian linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post}) is given by a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng}) over $\beta$ and $\tau = 1/\sigma^2$

\begin{equation} \label{eq:blr-pcr-GLM-NG-post}
p(\beta,\tau|y) = \mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1}) \cdot \mathrm{Gam}(\tau; a_n, b_n)
\end{equation}

with the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post})

\begin{equation} \label{eq:blr-pcr-GLM-NG-post-par}
\begin{split}
\mu_n &= \Lambda_n^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \\
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \; .
\end{split}
\end{equation}

The marginal distribution of a normal-gamma distribution is a multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng-marg}), such that the marginal ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-marg}) posterior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) distribution of $\beta$ is

\begin{equation} \label{eq:blr-pcr-GLM-NG-post-beta}
p(\beta|y) = t(\beta; \mu, \Sigma, \nu)
\end{equation}

with the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post})

\begin{equation} \label{eq:blr-pcr-GLM-NG-post-par-beta}
\begin{split}
\mu &= \mu_n \\
\Sigma &= \left( \frac{a_n}{b_n} \Lambda_n \right)^{-1} \\
\nu &= 2 \, a_n \; .
\end{split}
\end{equation}

Define the quantity $\gamma = C^\mathrm{T} \beta$. According to the linear transformation theorem for the multivariate t-distribution, $\gamma$ also follows a multivariate t-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt}):

\begin{equation} \label{eq:blr-pcr-GLM-NG-post-gamma}
p(\gamma|y) = t(\gamma; C^\mathrm{T} \mu, C^\mathrm{T} \Sigma \, C, \nu) \; .
\end{equation}

Because $C^\mathrm{T}$ is a $q \times p$ matrix, $\gamma$ is a $q \times 1$ vector. The quadratic form of a multivariate t-distributed random variable has an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvt-f}), such that we can write:

\begin{equation} \label{eq:blr-pcr-GLM-NG-post-qf}
\mathrm{QF}(\gamma) = (\gamma - C^\mathrm{T} \mu)^\mathrm{T} (C^\mathrm{T} \Sigma \, C)^{-1} (\gamma - C^\mathrm{T} \mu) /q \, \sim \mathrm{F}(q,\nu) \; .
\end{equation}

Therefore, the largest posterior credibility region for $\gamma$ which does not contain $\gamma = 0_q$ (i.e. only touches this origin point) can be obtained by plugging $\mathrm{QF}(0)$ into the cumulative distribution function of the F-distribution:

\begin{equation} \label{eq:blr-pcr-GLM-NG-post-cred-reg-not-H0}
\begin{split}
(1-\alpha) &= \mathrm{F}\left( \mathrm{QF}(0); q, \nu \right) \\
&= \mathrm{F}\left( \left[ \mu^\mathrm{T} C (C^\mathrm{T} \Sigma \, C)^{-1} C^\mathrm{T} \mu \right]/q; q, \nu \right) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Koch, Karl-Rudolf (2007): "Multivariate t-distribution"; in: \textit{Introduction to Bayesian Statistics}, Springer, Berlin/Heidelberg, 2007, eqs. 2.235, 2.236, 2.213, 2.210, 2.211, 2.183; URL: \url{https://www.springer.com/de/book/9783540727231}; DOI: 10.1007/978-3-540-72726-2.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Combined posterior distribution from independent data sets}]{Combined posterior distribution from independent data sets} \label{sec:blr-postind}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = \left\lbrace y_1, \ldots, y_S \right\rbrace$ be a set of $S$ conditionally independent data sets ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind-cond}) assumed to follow linear regression models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with design matrices ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $X_1, \ldots, X_S$, number of data points ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $n_1, \ldots, n_S$ and precision matrices ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-prior}) $P_1, \ldots, P_n$, governed by identical regression coefficients ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $\beta$ and identical noise precision $\tau$:

\begin{equation} \label{eq:blr-postind-GLM-NG-S}
\begin{split}
y_1 &= X_1 \beta + \varepsilon_1, \; \varepsilon_1 \sim \mathcal{N}(0, \sigma^2 V_1), \; \sigma^2 V_1 = (\tau P_1)^{-1} \\
&\;\;\vdots \\
y_S &= X_S \beta + \varepsilon_S, \; \varepsilon_S \sim \mathcal{N}(0, \sigma^2 V_S), \; \sigma^2 V_S = (\tau P_S)^{-1} \; .
\end{split}
\end{equation}

Moreover, assume a normal-gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-prior}) over the model parameters $\beta$ and $\tau = 1/\sigma^2$:

\begin{equation} \label{eq:blr-postind-GLM-NG-prior}
p(\beta,\tau) = \mathcal{N}(\beta; \mu_0, (\tau \Lambda_0)^{-1}) \cdot \mathrm{Gam}(\tau; a_0, b_0) \; .
\end{equation}

Then, the combined posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-ind}) from observing these conditionally independent data sets ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind-cond}) is also given by a normal-gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:ng})

\begin{equation} \label{eq:blr-postind-GLM-NG-post}
p(\beta,\tau|y) = \mathcal{N}(\beta; \mu_n, (\tau \Lambda_n)^{-1}) \cdot \mathrm{Gam}(\tau; a_n, b_n)
\end{equation}

with the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post})

\begin{equation} \label{eq:blr-postind-GLM-NG-S-post-par}
\begin{split}
\mu_n &= \Lambda_n^{-1} \left( \sum_{i=1}^S X_i^\mathrm{T} P_i y_i + \Lambda_0 \mu_0 \right) \\
\Lambda_n &= \sum_{i=1}^S X_i^\mathrm{T} P_i X_i + \Lambda_0 \\
a_n &= a_0 + \frac{1}{2} \sum_{i=1}^S n_i \\
b_n &= b_0 + \frac{1}{2} \left( \sum_{i=1}^S y_i^\mathrm{T} P_i y_i + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n \right) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} This can be seen by sequentially applying Bayes' theorem ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:bayes-th}) for calculating the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl}), while using the posterior after one iteration as the prior for the next iteration.

Let $\mu_0^{(i)}, \Lambda_0^{(i)}, a_0^{(i)}, b_0^{(i)}$ denote the prior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) before analyzing the $i$-th data set, such that e.g. $\mu_0^{(1)}$ is identical to $\mu_0$ in \eqref{eq:blr-postind-GLM-NG-prior}:

\begin{equation} \label{eq:blr-postind-GLM-NG-S-prior-y1}
\begin{split}
\mu_0^{(1)} &= \mu_0 \\
\Lambda_0^{(1)} &= \Lambda_0 \\
a_0^{(1)} &= a_0 \\
b_0^{(1)} &= b_0 \; .
\end{split}
\end{equation}

Moreover, let $\mu_n^{(i)}, \Lambda_n^{(i)}, a_n^{(i)}, b_n^{(i)}$ denote the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) after analyzing the $i$-th data set, such that e.g. $\mu_n^{(S)}$ is identical to $\mu_n$ in \eqref{eq:blr-postind-GLM-NG-post}:

\begin{equation} \label{eq:blr-postind-GLM-NG-S-post-yS}
\begin{split}
\mu_n^{(S)} &= \mu_n \\
\Lambda_n^{(S)} &= \Lambda_n \\
a_n^{(S)} &= a_n \\
b_n^{(S)} &= b_n \; .
\end{split}
\end{equation}

The posterior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) after seeing the $i$-th data set is equal to the prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) before seeing the $(i+1)$-th data set, so we have the relation:

\begin{equation} \label{eq:blr-postind-GLM-NG-S-prior-post}
\begin{split}
\mu_0^{(i+1)} &= \mu_n^{(i)} \\
\Lambda_0^{(i+1)} &= \Lambda_n^{(i)} \\
a_0^{(i+1)} &= a_n^{(i)} \\
b_0^{(i+1)} &= b_n^{(i)} \; .
\end{split}
\end{equation}

The posterior distribution for Bayesian linear regression when observing a single data set is given by the following hyperparameter equations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post}):

\begin{equation} \label{eq:blr-postind-GLM-NG-post-par}
\begin{split}
\mu_n &= \Lambda_n^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \\
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \; .
\end{split}
\end{equation}

We can apply \eqref{eq:blr-postind-GLM-NG-post-par} to calculate the posterior hyperparameters after seeing the first data set:

\begin{equation} \label{eq:blr-postind-GLM-NG-S-post-y1}
\begin{split}
\mu_n^{(1)} &= {\Lambda_n^{(1)}}^{-1} \left( X_1^\mathrm{T} P_1 y_1 + \Lambda_0^{(1)} \mu_0^{(1)} \right) \\
&= {\Lambda_n^{(1)}}^{-1} \left( X_1^\mathrm{T} P_1 y_1 + \Lambda_0 \mu_0 \right) \\
\Lambda_n^{(1)} &= X_1^\mathrm{T} P_1 X_1 + \Lambda_0^{(1)} \\
&= X_1^\mathrm{T} P_1 X_1 + \Lambda_0 \\
a_n^{(1)} &= a_0^{(1)} + \frac{1}{2} n_1 \\
&= a_0 + \frac{1}{2} n_1 \\
b_n^{(1)} &= b_0^{(1)} + \frac{1}{2} \left( y_1^\mathrm{T} P_1 y_1 + {\mu_0^{(1)}}^\mathrm{T} \Lambda_0^{(1)} \mu_0^{(1)} - {\mu_n^{(1)}}^\mathrm{T} \Lambda_n^{(1)} \mu_n^{(1)} \right) \\
&= b_0 + \frac{1}{2} \left( y_1^\mathrm{T} P_1 y_1 + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - {\mu_n^{(1)}}^\mathrm{T} \Lambda_n^{(1)} \mu_n^{(1)} \right) \; .
\end{split}
\end{equation}

These are the prior hyperparameters before seeing the second data set:

\begin{equation} \label{eq:blr-postind-GLM-NG-S-prior-y2}
\begin{split}
\mu_0^{(2)} &= \mu_n^{(1)} \\
\Lambda_0^{(2)} &= \Lambda_n^{(1)} \\
a_0^{(2)} &= a_n^{(1)} \\
b_0^{(2)} &= b_n^{(1)} \; .
\end{split}
\end{equation}

Thus, we can again use \eqref{eq:blr-postind-GLM-NG-post-par} to calculate the posterior hyperparameters after seeing the second data set:

\begin{equation} \label{eq:blr-postind-GLM-NG-S-post-y2}
\begin{split}
\mu_n^{(2)} &= {\Lambda_n^{(2)}}^{-1} \left( X_2^\mathrm{T} P_2 y_2 + \Lambda_0^{(2)} \mu_0^{(2)} \right) \\
&= {\Lambda_n^{(2)}}^{-1} \left( X_2^\mathrm{T} P_2 y_2 + \Lambda_n^{(1)} {\Lambda_n^{(1)}}^{-1} \left( X_1^\mathrm{T} P_1 y_1 + \Lambda_0 \mu_0 \right) \right) \\
&= {\Lambda_n^{(2)}}^{-1} \left( X_1^\mathrm{T} P_1 y_1 + X_2^\mathrm{T} P_2 y_2 + \Lambda_0 \mu_0 \right) \\
\Lambda_n^{(2)} &= X_2^\mathrm{T} P_2 X_2 + \Lambda_0^{(2)} \\
&= X_2^\mathrm{T} P_2 X_2 + X_1^\mathrm{T} P_1 X_1 + \Lambda_0 \\
&= X_1^\mathrm{T} P_1 X_1 + X_2^\mathrm{T} P_2 X_2 + \Lambda_0 \\
a_n^{(2)} &= a_0^{(2)} + \frac{1}{2} n_2 \\
&= a_0 + \frac{1}{2} n_1 + \frac{1}{2} n_2 \\
&= a_0 + \frac{1}{2} \left( n_1 + n_2 \right) \\
b_n^{(2)} &= b_0^{(2)} + \frac{1}{2} \left( y_2^\mathrm{T} P_2 y_2 + {\mu_0^{(2)}}^\mathrm{T} \Lambda_0^{(2)} \mu_0^{(2)} - {\mu_n^{(2)}}^\mathrm{T} \Lambda_n^{(2)} \mu_n^{(2)} \right) \\
&= b_0 + \frac{1}{2} \left( y_1^\mathrm{T} P_1 y_1 + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - {\mu_n^{(1)}}^\mathrm{T} \Lambda_n^{(1)} \mu_n^{(1)} \right) + \frac{1}{2} \left( y_2^\mathrm{T} P_2 y_2 + {\mu_n^{(1)}}^\mathrm{T} \Lambda_n^{(1)} \mu_n^{(1)} - {\mu_n^{(2)}}^\mathrm{T} \Lambda_n^{(2)} \mu_n^{(2)} \right) \\
&= b_0 + \frac{1}{2} \left( y_1^\mathrm{T} P_1 y_1 + y_2^\mathrm{T} P_2 y_2 + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - {\mu_n^{(2)}}^\mathrm{T} \Lambda_n^{(2)} \mu_n^{(2)} \right) \; .
\end{split}
\end{equation}

These are the prior hyperparameters before seeing the third data set:

\begin{equation} \label{eq:blr-postind-GLM-NG-S-prior-y3}
\begin{split}
\mu_0^{(3)} &= \mu_n^{(2)} \\
\Lambda_0^{(3)} &= \Lambda_n^{(2)} \\
a_0^{(3)} &= a_n^{(2)} \\
b_0^{(3)} &= b_n^{(2)} \; .
\end{split}
\end{equation}

Generalizing this, we have after observing the $j$-th data set:

\begin{equation} \label{eq:blr-postind-GLM-NG-S-post-yi}
\begin{split}
\mu_n^{(j)} &= {\Lambda_n^{(j)}}^{-1} \left( \sum_{i=1}^j X_i^\mathrm{T} P_i y_i + \Lambda_0 \mu_0 \right) \\
\Lambda_n^{(j)} &= \sum_{i=1}^j X_i^\mathrm{T} P_i X_i + \Lambda_0 \\
a_n^{(j)} &= a_0 + \frac{1}{2} \sum_{i=1}^j n_i \\
b_n^{(j)} &= b_0 + \frac{1}{2} \left( \sum_{i=1}^j y_i^\mathrm{T} P_i y_i + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - {\mu_n^{(j)}}^\mathrm{T} \Lambda_n^{(j)} \mu_n^{(j)} \right) \; .
\end{split}
\end{equation}

Plugging in $j = S$, we obtain the final posterior distribution:

\begin{equation} \label{eq:blr-postind-GLM-NG-S-post-par-qed}
\begin{split}
\mu_n = \mu_n^{(S)} &= {\Lambda_n^{(S)}}^{-1} \left( \sum_{i=1}^S X_i^\mathrm{T} P_i y_i + \Lambda_0 \mu_0 \right) = \Lambda_n^{-1} \left( \sum_{i=1}^S X_i^\mathrm{T} P_i y_i + \Lambda_0 \mu_0 \right) \\
\Lambda_n = \Lambda_n^{(S)} &= \sum_{i=1}^S X_i^\mathrm{T} P_i X_i + \Lambda_0 \\
a_n = a_n^{(S)} &= a_0 + \frac{1}{2} \sum_{i=1}^S n_i \\
b_n = b_n^{(S)} &= b_0 + \frac{1}{2} \left( \sum_{i=1}^S y_i^\mathrm{T} P_i y_i + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - {\mu_n^{(S)}}^\mathrm{T} \Lambda_n^{(S)} \mu_n^{(S)} \right) \\
&= b_0 + \frac{1}{2} \left( \sum_{i=1}^S y_i^\mathrm{T} P_i y_i + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n \right) \; .
\end{split}
\end{equation}

This result is also compatible with the general theorem about combined posterior distributions in terms of individual posterior distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-ind}) when analyzing independent data sets.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Log Bayes factor for comparison of two regression models}]{Log Bayes factor for comparison of two regression models} \label{sec:blr-lbf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = \left[ y_1, \ldots, y_n \right]^\mathrm{T}$ be an $n \times 1$ vector of a measured univariate signal and consider two linear regression models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with design matrices ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $X_1, X_2$ and precision matrices ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-prior}) $P_1, P_2$, entailing potentially different regression coefficients ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $\beta_1, \beta_2$ and noise precisions $\tau_1, \tau_2$:

\begin{equation} \label{eq:blr-lbf-GLM-NG-12}
\begin{split}
m_1: \; y &= X_1 \beta_1 + \varepsilon_1, \; \varepsilon_1 \sim \mathcal{N}(0, \sigma_1^2 V_1), \; \sigma_1^2 V_1 = (\tau_1 P_1)^{-1} \\
m_2: \; y &= X_2 \beta_2 + \varepsilon_2, \; \varepsilon_2 \sim \mathcal{N}(0, \sigma_2^2 V_2), \; \sigma_2^2 V_2 = (\tau_2 P_2)^{-1} \; .
\end{split}
\end{equation}

Moreover, assume normal-gamma prior distributions ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-prior}) over the model parameters $\beta_1$ and $\tau_1 = 1/\sigma_1^2$ as well as $\beta_2$ and $\tau_2 = 1/\sigma_2^2$:

\begin{equation} \label{eq:blr-lbf-GLM-NG-prior-12}
\begin{split}
p(\beta_1,\tau_1) &= \mathcal{N}\left( \beta_1; \mu_0^{(1)}, \left( \tau_1 \Lambda_0^{(1)} \right)^{-1} \right) \cdot \mathrm{Gam}\left( \tau_1; a_0^{(1)}, b_0^{(1)} \right) \\
p(\beta_2,\tau_2) &= \mathcal{N}\left( \beta_2; \mu_0^{(2)}, \left( \tau_2 \Lambda_0^{(2)} \right)^{-1} \right) \cdot \mathrm{Gam}\left( \tau_2; a_0^{(2)}, b_0^{(2)} \right) \; .
\end{split}
\end{equation}

Then, the log Bayes factor ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf}) in favor of $m_1$ against $m_2$ is

\begin{equation} \label{eq:blr-lbf-GLN-NG-LBF-12}
\begin{split}
\mathrm{LBF}_{12} &= \frac{1}{2} \log \frac{|P_1|}{|P_2|}  + \frac{1}{2} \log \frac{|\Lambda_0^{(1)}|}{|\Lambda_0^{(2)}|} - \frac{1}{2} \log \frac{|\Lambda_n^{(1)}|}{|\Lambda_n^{(2)}|} \\
&+ \log \frac{\Gamma\left( a_n^{(1)} \right)}{\Gamma\left( a_0^{(1)} \right)} + a_0^{(1)} \log b_0^{(1)} - a_n^{(1)} \log b_n^{(1)} \\
&+ \log \frac{\Gamma\left( a_n^{(2)} \right)}{\Gamma\left( a_0^{(2)} \right)} - a_0^{(2)} \log b_0^{(2)} + a_n^{(2)} \log b_n^{(2)}
\end{split}
\end{equation}

where $\mu_n^{(1)}, \Lambda_n^{(1)}, a_n^{(1)}, b_n^{(1)}$ and $\mu_n^{(2)}, \Lambda_n^{(2)}, a_n^{(2)}, b_n^{(2)}$ are the posterior hyperparameters for Bayesian linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post}) for each of the two models which are functions of the design matrices, the precision matrices and the data vector.


\vspace{1em}
\textbf{Proof:} For Bayesian linear regression with data vector $y$, design matrix $X$, precision matrix $P$ and a normal-gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-prior}) over $\beta$ and $\tau$, the log model evidence is given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-lme})

\begin{equation} \label{eq:blr-lbf-GLM-NG-LME}
\begin{split}
\log p(y|m) = \frac{1}{2} & \log |P| - \frac{n}{2} \log (2 \pi) + \frac{1}{2} \log |\Lambda_0| - \frac{1}{2} \log |\Lambda_n| + \\
& \log \Gamma(a_n) - \log \Gamma(a_0) + a_0 \log b_0 - a_n \log b_n
\end{split}
\end{equation}

where the posterior hyperparameters are equal to ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blr-post})

\begin{equation} \label{eq:blr-lbf-GLM-NG-post-par}
\begin{split}
\mu_n &= \Lambda_n^{-1} (X^\mathrm{T} P y + \Lambda_0 \mu_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \\
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2} (y^\mathrm{T} P y + \mu_0^\mathrm{T} \Lambda_0 \mu_0 - \mu_n^\mathrm{T} \Lambda_n \mu_n) \; .
\end{split}
\end{equation}

Thus, the log model evidences ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) for $m_1$ and $m_2$ are given by:

\begin{equation} \label{eq:blr-lbf-GLM-NG-LME-12}
\begin{split}
\mathrm{LME}(m_1) = \frac{1}{2} & \log |P_1| - \frac{n}{2} \log (2 \pi) + \frac{1}{2} \log |\Lambda_0^{(1)}| - \frac{1}{2} \log |\Lambda_n^{(1)}| + \\
& \log \Gamma(a_n^{(1)}) - \log \Gamma(a_0^{(1)}) + a_0^{(1)} \log b_0^{(1)} - a_n^{(1)} \log b_n^{(1)} \\
\mathrm{LME}(m_2) = \frac{1}{2} & \log |P_2| - \frac{n}{2} \log (2 \pi) + \frac{1}{2} \log |\Lambda_0^{(2)}| - \frac{1}{2} \log |\Lambda_n^{(2)}| + \\
& \log \Gamma(a_n^{(2)}) - \log \Gamma(a_0^{(2)}) + a_0^{(2)} \log b_0^{(2)} - a_n^{(2)} \log b_n^{(2)} \; .
\end{split}
\end{equation}

The log Bayes factor is equal to the difference of two log model evidences ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf-lme}):

\begin{equation} \label{eq:blr-lbf-LBF-LME}
\mathrm{LBF}_{12} = \mathrm{LME}(m_1) - \mathrm{LME}(m_2) \; .
\end{equation}

Plugging \eqref{eq:blr-lbf-GLM-NG-LME-12} into \eqref{eq:blr-lbf-LBF-LME}, this gives:

\begin{equation} \label{eq:blr-lbf-GLN-NG-LBF-12-s1}
\begin{split}
\mathrm{LBF}_{12} &= \frac{1}{2} \log |P_1| - \frac{1}{2} \log |P_2| \\
&+ \frac{1}{2} \log |\Lambda_0^{(1)}| - \frac{1}{2} \log |\Lambda_0^{(2)}| \\
&- \frac{1}{2} \log |\Lambda_n^{(1)}| + \frac{1}{2} \log |\Lambda_n^{(2)}| \\
&+ \log \Gamma\left( a_n^{(1)} \right) - \log \Gamma\left( a_0^{(1)} \right) + a_0^{(1)} \log b_0^{(1)} - a_n^{(1)} \log b_n^{(1)} \\
&- \log \Gamma\left( a_n^{(2)} \right) + \log \Gamma\left( a_0^{(2)} \right) - a_0^{(2)} \log b_0^{(2)} + a_n^{(2)} \log b_n^{(2)} \; .
\end{split}
\end{equation}

Applying $\log a - \log b = \log(a/b)$, we obtain:

\begin{equation} \label{eq:blr-lbf-GLN-NG-LBF-12-s2}
\begin{split}
\mathrm{LBF}_{12} &= \frac{1}{2} \log \frac{|P_1|}{|P_2|}  + \frac{1}{2} \log \frac{|\Lambda_0^{(1)}|}{|\Lambda_0^{(2)}|} - \frac{1}{2} \log \frac{|\Lambda_n^{(1)}|}{|\Lambda_n^{(2)}|} \\
&+ \log \frac{\Gamma\left( a_n^{(1)} \right)}{\Gamma\left( a_0^{(1)} \right)} + a_0^{(1)} \log b_0^{(1)} - a_n^{(1)} \log b_n^{(1)} \\
&- \log \frac{\Gamma\left( a_n^{(2)} \right)}{\Gamma\left( a_0^{(2)} \right)} - a_0^{(2)} \log b_0^{(2)} + a_n^{(2)} \log b_n^{(2)} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{Bayesian linear regression with known covariance}

\subsubsection[\textbf{Conjugate prior distribution}]{Conjugate prior distribution} \label{sec:blrkc-prior}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:blrkc-prior-GLM}
y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \Sigma)
\end{equation}

be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with measured $n \times 1$ data vector $y$, known $n \times p$ design matrix $X$ and known $n \times n$ covariance matrix $\Sigma$ as well as unknown $p \times 1$ regression coefficients $\beta$.

Then, the conjugate prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-conj}) for this model is a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn})

\begin{equation} \label{eq:blrkc-prior-GLM-N-prior}
p(\beta) = \mathcal{N}(\beta; \mu_0, \Sigma_0) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} By definition, a conjugate prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-conj}) is a prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) that, when combined with the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}), leads to a posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) that belongs to the same family of probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}). This is fulfilled when the prior density and the likelihood function are proportional to the model parameters in the same way, i.e. the model parameters appear in the same functional form in both.

Equation \eqref{eq:blrkc-prior-GLM} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}):

\begin{equation} \label{eq:blrkc-prior-GLM-LF}
p(y|\beta) = \mathcal{N}(y; X \beta, \Sigma) = \sqrt{\frac{1}{(2 \pi)^n |\Sigma|}} \, \exp\left[ -\frac{1}{2} (y-X\beta)^\mathrm{T} \Sigma^{-1} (y-X\beta) \right] \; .
\end{equation}

Expanding the product in the exponent, we have:

\begin{equation} \label{eq:blrkc-prior-GLM-LF-s1}
p(y|\beta) = \sqrt{\frac{1}{(2 \pi)^n |\Sigma|}} \cdot \exp\left[ -\frac{1}{2} \left( y^\mathrm{T} \Sigma^{-1} y - y^\mathrm{T} \Sigma^{-1} X \beta - \beta^\mathrm{T} X^\mathrm{T} \Sigma^{-1} y + \beta^\mathrm{T} X^\mathrm{T} \Sigma^{-1} X \beta \right) \right] \; .
\end{equation}

Completing the square over $\beta$, one obtains

\begin{equation} \label{eq:blrkc-prior-GLM-LF-s2}
p(y|\beta) = \sqrt{\frac{1}{(2 \pi)^n |\Sigma|}} \cdot \exp\left[ -\frac{1}{2} \left( (\beta - \tilde{X}y)^\mathrm{T} X^\mathrm{T} \Sigma^{-1} X (\beta - \tilde{X}y) - y^\mathrm{T} Q y + y^\mathrm{T} \Sigma^{-1} y \right) \right]
\end{equation}

where $\tilde{X} = \left( X^\mathrm{T} \Sigma^{-1} X \right)^{-1} X^\mathrm{T} \Sigma^{-1}$ and $Q = \tilde{X}^\mathrm{T} \left( X^\mathrm{T} \Sigma^{-1} X \right) \tilde{X}$.

\vspace{1em}
Separating constant and variable terms, we get:

\begin{equation} \label{eq:blrkc-prior-GLM-LF-s3}
p(y|\beta) = \sqrt{\frac{1}{(2 \pi)^n |\Sigma|}} \cdot \exp\left[ -\frac{1}{2} \left( y^\mathrm{T} Q y + y^\mathrm{T} \Sigma^{-1} y \right) \right] \cdot \exp\left[ -\frac{1}{2} (\beta - \tilde{X}y)^\mathrm{T} X^\mathrm{T} \Sigma^{-1} X (\beta - \tilde{X}y) \right] \; .
\end{equation}

In other words, the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) is proportional to an exponential of a squared form of $\beta$:

\begin{equation} \label{eq:blrkc-prior-GLM-LF-s4}
p(y|\beta) \propto \exp\left[ -\frac{1}{2} (\beta - \tilde{X}y)^\mathrm{T} X^\mathrm{T} \Sigma^{-1} X (\beta - \tilde{X}y) \right] \; .
\end{equation}

The same is true for a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn}) over $\beta$

\begin{equation} \label{eq:blrkc-prior-GLM-N-prior-s1}
p(\beta) = \mathcal{N}(\beta; \mu_0, \Sigma_0)
\end{equation}

the probability density function of which ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf})

\begin{equation} \label{eq:blrkc-prior-GLM-N-prior-s2}
p(\beta) = \sqrt{\frac{1}{(2 \pi)^p |\Sigma_0|}} \cdot \exp\left[ -\frac{1}{2} (\beta-\mu_0)^\mathrm{T} \Sigma_0^{-1} (\beta-\mu_0) \right]
\end{equation}

exhibits the same proportionality

\begin{equation} \label{eq:blrkc-prior-GLM-N-prior-s3}
p(\beta) \propto \exp\left[ -\frac{1}{2} (\beta-\mu_0)^\mathrm{T} \Sigma_0^{-1} (\beta-\mu_0) \right]
\end{equation}

and is therefore conjugate relative to the likelihood.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop CM (2006): "Bayesian linear regression"; in: \textit{Pattern Recognition for Machine Learning}, pp. 152-161, eq. 3.48; URL: \url{https://www.springer.com/gp/book/9780387310732}.
\item Penny WD (2012): "Comparing Dynamic Causal Models using AIC, BIC and Free Energy"; in: \textit{NeuroImage}, vol. 59, iss. 2, pp. 319-330, eq. 9; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811911008160}; DOI: 10.1016/j.neuroimage.2011.07.039.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Posterior distribution}]{Posterior distribution} \label{sec:blrkc-post}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:blrkc-post-GLM}
y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \Sigma)
\end{equation}

be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with measured $n \times 1$ data vector $y$, known $n \times p$ design matrix $X$ and known $n \times n$ covariance matrix $\Sigma$ as well as unknown $p \times 1$ regression coefficients $\beta$. Moreover, assume a multivariate normal distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blrkc-prior}) over the model parameter $\beta$:

\begin{equation} \label{eq:blrkc-post-GLM-N-prior}
p(\beta) = \mathcal{N}(\beta; \mu_0, \Sigma_0) \; .
\end{equation}

Then, the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) is also a multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn})

\begin{equation} \label{eq:blrkc-post-GLM-N-post}
p(\beta|y) = \mathcal{N}(\beta; \mu_n, \Sigma_n)
\end{equation}

and the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:blrkc-post-GLM-N-post-par}
\begin{split}
\mu_n &= \Sigma_n (X^\mathrm{T} \Sigma^{-1} y + \Sigma_0^{-1} \mu_0) \\
\Sigma_n &= \left( X^\mathrm{T} \Sigma^{-1} X + \Sigma_0^{-1} \right)^{-1} \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} According to Bayes' theorem ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:bayes-th}), the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) is given by

\begin{equation} \label{eq:blrkc-post-GLM-N-BT}
p(\beta|y) = \frac{p(y|\beta) \, p(\beta)}{p(y)} \; .
\end{equation}

Since $p(y)$ is just a normalization factor, the posterior is proportional ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl}) to the numerator:

\begin{equation} \label{eq:blrkc-post-GLM-N-post-JL}
p(\beta|y) \propto p(y|\beta) \, p(\beta) = p(y,\beta) \; .
\end{equation}

Equation \eqref{eq:blrkc-post-GLM} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}):

\begin{equation} \label{eq:blrkc-post-GLM-LF}
p(y|\beta) = \mathcal{N}(y; X \beta, \Sigma) = \sqrt{\frac{1}{(2 \pi)^n |\Sigma|}} \, \exp\left[ -\frac{1}{2} (y-X\beta)^\mathrm{T} \Sigma^{-1} (y-X\beta) \right] \; .
\end{equation}

Combining the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) \eqref{eq:blrkc-post-GLM-LF} with the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) \eqref{eq:blrkc-post-GLM-N-prior} using the probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}), the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) of the model is given by

\begin{equation} \label{eq:blrkc-post-GLM-N-JL-s1}
\begin{split}
p(y,\beta) = \; & p(y|\beta) \, p(\beta) \\
= \; & \sqrt{\frac{1}{(2 \pi)^n |\Sigma|}} \, \exp\left[ -\frac{1}{2} (y-X\beta)^\mathrm{T} \Sigma^{-1} (y-X\beta) \right] \cdot \\
\; & \sqrt{\frac{1}{(2 \pi)^p |\Sigma_0|}} \, \exp\left[ -\frac{1}{2} (\beta-\mu_0)^\mathrm{T} \Sigma_0^{-1} (\beta-\mu_0) \right] \; .
\end{split}
\end{equation}

Collecting identical variables gives:

\begin{equation} \label{eq:blrkc-post-GLM-N-JL-s2}
\begin{split}
p(y,\beta) = \; & \sqrt{\frac{1}{(2 \pi)^{n+p} |\Sigma| |\Sigma_0|}} \cdot \\
& \exp\left[ -\frac{1}{2} \left( (y-X\beta)^\mathrm{T} \Sigma^{-1} (y-X\beta) + (\beta-\mu_0)^\mathrm{T} \Sigma_0^{-1} (\beta-\mu_0) \right) \right] \; .
\end{split}
\end{equation}

Expanding the products in the exponent gives:

\begin{equation} \label{eq:blrkc-post-GLM-N-JL-s3}
\begin{split}
p(y,\beta) = \; & \sqrt{\frac{1}{(2 \pi)^{n+p} |\Sigma| |\Sigma_0|}} \cdot \\
& \exp\left[ -\frac{1}{2} \left( y^\mathrm{T} \Sigma^{-1} y - y^\mathrm{T} \Sigma^{-1} X \beta - \beta^\mathrm{T} X^\mathrm{T} \Sigma^{-1} y + \beta^\mathrm{T} X^\mathrm{T} \Sigma^{-1} X \beta + \right. \right. \\
& \hphantom{\exp \left[ -\frac{1}{2} \right.} \; \left. \left. \beta^\mathrm{T} \Sigma_0^{-1} \beta - \beta^\mathrm{T} \Sigma_0^{-1} \mu_0 - \mu_0^\mathrm{T} \Sigma_0^{-1} \beta + \mu_0^\mathrm{T} \Sigma_0^{-1} \mu_0 \right) \right] \; .
\end{split}
\end{equation}

Regrouping the terms in the exponent gives:

\begin{equation} \label{eq:blrkc-post-GLM-N-JL-s4}
\begin{split}
p(y,\beta) = \; & \sqrt{\frac{1}{(2 \pi)^{n+p} |\Sigma| |\Sigma_0|}} \cdot \\
& \exp\left[ -\frac{1}{2} \left( \beta^\mathrm{T} [ X^\mathrm{T} \Sigma^{-1} X + \Sigma_0^{-1} ] \beta - 2 \beta^\mathrm{T} [X^\mathrm{T} \Sigma^{-1} y + \Sigma_0^{-1} \mu_0] + \right. \right. \\
& \hphantom{\exp \left[ -\frac{1}{2} \right.} \; \left. \left. y^\mathrm{T} \Sigma^{-1} y + \mu_0^\mathrm{T} \Sigma_0^{-1} \mu_0 \right) \right] \; .
\end{split}
\end{equation}

Completing the square over $\beta$, we finally have

\begin{equation} \label{eq:blrkc-post-GLM-N-JL-s5}
\begin{split}
p(y,\beta) = \; & \sqrt{\frac{1}{(2 \pi)^{n+p} |\Sigma| |\Sigma_0|}} \cdot \\
& \exp\left[ -\frac{1}{2} \left( (\beta-\mu_n)^\mathrm{T} \Sigma_n^{-1} (\beta-\mu_n) + (y^\mathrm{T} \Sigma^{-1} y + \mu_0^\mathrm{T} \Sigma_0^{-1} \mu_0 - \mu_n^\mathrm{T} \Sigma_n^{-1} \mu_n) \right) \right]
\end{split}
\end{equation}

with the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post})

\begin{equation} \label{eq:blrkc-post-GLM-N-post-par-qed}
\begin{split}
\mu_n &= \Sigma_n (X^\mathrm{T} \Sigma^{-1} y + \Sigma_0^{-1} \mu_0) \\
\Sigma_n &= \left( X^\mathrm{T} \Sigma^{-1} X + \Sigma_0^{-1} \right)^{-1} \; .
\end{split}
\end{equation}

Ergo, the joint likelihood is proportional to

\begin{equation} \label{eq:blrkc-post-GLM-N-JL-s6}
p(y,\beta) \propto \exp\left[ -\frac{1}{2} (\beta-\mu_n)^\mathrm{T} \Sigma_n^{-1} (\beta-\mu_n) \right] \; ,
\end{equation}

such that the posterior distribution over $\beta$ is given by

\begin{equation} \label{eq:blrkc-post-GLM-N-post-qed}
p(\beta|y) = \mathcal{N}(\beta; \mu_n, \Sigma_n)
\end{equation}

with the posterior hyperparameters given in \eqref{eq:blrkc-post-GLM-N-post-par-qed}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop CM (2006): "Bayesian linear regression"; in: \textit{Pattern Recognition for Machine Learning}, pp. 152-161, eqs. 3.49-3.51, ex. 3.7; URL: \url{https://www.springer.com/gp/book/9780387310732}.
\item Penny WD (2012): "Comparing Dynamic Causal Models using AIC, BIC and Free Energy"; in: \textit{NeuroImage}, vol. 59, iss. 2, pp. 319-330, eq. 27; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811911008160}; DOI: 10.1016/j.neuroimage.2011.07.039.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Log model evidence}]{Log model evidence} \label{sec:blrkc-lme}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:blrkc-lme-GLM}
m: y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \Sigma)
\end{equation}

be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with measured $n \times 1$ data vector $y$, known $n \times p$ design matrix $X$ and known $n \times n$ covariance matrix $\Sigma$ as well as unknown $p \times 1$ regression coefficients $\beta$. Moreover, assume a multivariate normal distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blrkc-prior}) over the model parameter $\beta$:

\begin{equation} \label{eq:blrkc-lme-GLM-N-prior}
p(\beta) = \mathcal{N}(\beta; \mu_0, \Sigma_0) \; .
\end{equation}

Then, the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) for this model is

\begin{equation} \label{eq:blrkc-lme-GLM-N-LME}
\begin{split}
\log p(y|m) = &- \frac{1}{2} e_y^\mathrm{T} \Sigma^{-1} e_y - \frac{1}{2} \log |\Sigma| - \frac{n}{2} \log (2 \pi) \\
&- \frac{1}{2} e_\beta^\mathrm{T} \Sigma_0^{-1} e_\beta - \frac{1}{2} \log |\Sigma_0| + \frac{1}{2} \log |\Sigma_n| \; .
\end{split}
\end{equation}

with the "prediction error" and "parameter error" terms

\begin{equation} \label{eq:blrkc-lme-GLM-N-err}
\begin{split}
e_y &= y - X \mu_n \\
e_\beta &= \mu_0 - \mu_n
\end{split}
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:blrkc-lme-GLM-N-post-par}
\begin{split}
\mu_n &= \Sigma_n (X^\mathrm{T} \Sigma^{-1} y + \Sigma_0^{-1} \mu_0) \\
\Sigma_n &= \left( X^\mathrm{T} \Sigma^{-1} X + \Sigma_0^{-1} \right)^{-1} \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} According to the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), the model evidence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml}) for this model is:

\begin{equation} \label{eq:blrkc-lme-GLM-N-ME-s1}
p(y|m) = \int p(y|\beta) \, p(\beta) \, \mathrm{d}\beta \; .
\end{equation}

According to the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), the integrand is equivalent to the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}):

\begin{equation} \label{eq:blrkc-lme-GLM-N-ME-s2}
p(y|m) = \int p(y,\beta) \, \mathrm{d}\beta \; .
\end{equation}

Equation \eqref{eq:blrkc-lme-GLM} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}):

\begin{equation} \label{eq:blrkc-lme-GLM-LF}
p(y|\beta) = \mathcal{N}(y; X \beta, \Sigma) = \sqrt{\frac{1}{(2 \pi)^n |\Sigma|}} \, \exp\left[ -\frac{1}{2} (y-X\beta)^\mathrm{T} \Sigma^{-1} (y-X\beta) \right] \; .
\end{equation}

When deriving the posterior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blrkc-post}) $p(\beta \vert y)$, the joint likelihood $p(y,\beta)$ is obtained as

\begin{equation} \label{eq:blrkc-lme-GLM-N-LME-s1}
\begin{split}
p(y,\beta) = \; & \sqrt{\frac{1}{(2 \pi)^{n+p} |\Sigma| |\Sigma_0|}} \cdot \\
& \exp\left[ -\frac{1}{2} \left( (\beta-\mu_n)^\mathrm{T} \Sigma_n^{-1} (\beta-\mu_n) + (y^\mathrm{T} \Sigma^{-1} y + \mu_0^\mathrm{T} \Sigma_0^{-1} \mu_0 - \mu_n^\mathrm{T} \Sigma_n^{-1} \mu_n) \right) \right] \; .
\end{split}
\end{equation}

Using the probability density function of the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-pdf}), we can rewrite this as

\begin{equation} \label{eq:blrkc-lme-GLM-N-LME-s2}
\begin{split}
p(y,\beta) = \; & \sqrt{\frac{1}{(2 \pi)^n |\Sigma|}} \, \sqrt{\frac{1}{(2 \pi)^p |\Sigma_0|}} \, \sqrt{\frac{(2 \pi)^p |\Sigma_n|}{1}} \cdot \mathcal{N}(\beta; \mu_n, \Sigma_n) \cdot \\
& \exp\left[ -\frac{1}{2} \left( y^\mathrm{T} \Sigma^{-1} y + \mu_0^\mathrm{T} \Sigma_0^{-1} \mu_0 - \mu_n^\mathrm{T} \Sigma_n^{-1} \mu_n \right) \right] \; .
\end{split}
\end{equation}

With that, $\beta$ can be integrated out easily:

\begin{equation} \label{eq:blrkc-lme-GLM-N-LME-s3}
\int p(y,\beta) \, \mathrm{d}\beta = \sqrt{\frac{1}{(2 \pi)^n |\Sigma|}} \, \sqrt{\frac{|\Sigma_n|}{|\Sigma_0|}} \cdot \exp\left[ -\frac{1}{2} \left( y^\mathrm{T} \Sigma^{-1} y + \mu_0^\mathrm{T} \Sigma_0^{-1} \mu_0 - \mu_n^\mathrm{T} \Sigma_n^{-1} \mu_n \right) \right] \; .
\end{equation}

Now we turn to the intra-exponent term

\begin{equation} \label{eq:blrkc-lme-GLM-N-LME-s4a}
y^\mathrm{T} \Sigma^{-1} y + \mu_0^\mathrm{T} \Sigma_0^{-1} \mu_0 - \mu_n^\mathrm{T} \Sigma_n^{-1} \mu_n
\end{equation}

and plug in the posterior covariance

\begin{equation} \label{eq:blrkc-lme-GLM-N-post-par-Sigma}
\Sigma_n = \left( X^\mathrm{T} \Sigma^{-1} X + \Sigma_0^{-1} \right)^{-1} \; .
\end{equation}

This gives

\begin{equation} \label{eq:blrkc-lme-GLM-N-LME-s4b}
\begin{split}
& \; y^\mathrm{T} \Sigma^{-1} y + \mu_0^\mathrm{T} \Sigma_0^{-1} \mu_0 - \mu_n^\mathrm{T} \Sigma_n^{-1} \mu_n \\
= & \; y^\mathrm{T} \Sigma^{-1} y + \mu_0^\mathrm{T} \Sigma_0^{-1} \mu_0 - \mu_n^\mathrm{T} \left( X^\mathrm{T} \Sigma^{-1} X + \Sigma_0^{-1} \right) \mu_n \\
= & \; y^\mathrm{T} \Sigma^{-1} y + \mu_0^\mathrm{T} \Sigma_0^{-1} \mu_0 - \mu_n^\mathrm{T} X^\mathrm{T} \Sigma^{-1} X \mu_n - \mu_n^\mathrm{T} \Sigma_0^{-1} \mu_n \\
= & \; (y - X \mu_n)^\mathrm{T} \Sigma^{-1} (y - X \mu_n) + (\mu_0 - \mu_n)^\mathrm{T} \Sigma_0^{-1} (\mu_0 - \mu_n) \\
\overset{\eqref{eq:blrkc-lme-GLM-N-err}}{=} & \; e_y^\mathrm{T} \Sigma^{-1} e_y + e_\beta^\mathrm{T} \Sigma_0^{-1} e_\beta \; .
\end{split}
\end{equation}

Thus, the marginal likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml}) becomes

\begin{equation} \label{eq:blrkc-lme-GLM-N-LME-s5}
p(y|m) = \int p(y,\beta) \, \mathrm{d}\beta \overset{\eqref{eq:blrkc-lme-GLM-N-LME-s3}}{=} \sqrt{\frac{1}{(2 \pi)^n |\Sigma|}} \, \sqrt{\frac{|\Sigma_n|}{|\Sigma_0|}} \cdot \exp\left[ -\frac{1}{2} \left( e_y^\mathrm{T} \Sigma^{-1} e_y + e_\beta^\mathrm{T} \Sigma_0^{-1} e_\beta \right) \right]
\end{equation}

and the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) of this model is given by

\begin{equation} \label{eq:blrkc-lme-GLM-N-LME-s6}
\begin{split}
\log p(y|m) = &- \frac{1}{2} e_y^\mathrm{T} \Sigma^{-1} e_y - \frac{1}{2} \log |\Sigma| - \frac{n}{2} \log (2 \pi) \\
&- \frac{1}{2} e_\beta^\mathrm{T} \Sigma_0^{-1} e_\beta - \frac{1}{2} \log |\Sigma_0| + \frac{1}{2} \log |\Sigma_n| \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Penny WD (2012): "Comparing Dynamic Causal Models using AIC, BIC and Free Energy"; in: \textit{NeuroImage}, vol. 59, iss. 2, pp. 319-330, eqs. 19-23; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811911008160}; DOI: 10.1016/j.neuroimage.2011.07.039.
\item Bishop CM (2006): "Bayesian linear regression"; in: \textit{Pattern Recognition for Machine Learning}, pp. 152-161; URL: \url{https://www.springer.com/gp/book/9780387310732}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Accuracy and complexity}]{Accuracy and complexity} \label{sec:blrkc-anc}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:blrkc-anc-GLM}
m: y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \Sigma)
\end{equation}

be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with measured $n \times 1$ data vector $y$, known $n \times p$ design matrix $X$ and known $n \times n$ covariance matrix $\Sigma$ as well as unknown $p \times 1$ regression coefficients $\beta$. Moreover, assume a multivariate normal distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blrkc-prior}) over the model parameter $\beta$:

\begin{equation} \label{eq:blrkc-anc-GLM-N-prior}
p(\beta) = \mathcal{N}(\beta; \mu_0, \Sigma_0) \; .
\end{equation}

Then, accuracy and complexity ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme-anc}) of this model are

\begin{equation} \label{eq:blrkc-anc-GLM-N-AnC}
\begin{split}
\mathrm{Acc}(m) &= - \frac{1}{2} e_y^\mathrm{T} \Sigma^{-1} e_y - \frac{1}{2} \log |\Sigma| - \frac{n}{2} \log (2 \pi) - \frac{1}{2} \mathrm{tr}(X^\mathrm{T} \Sigma^{-1} X \Sigma_n) \\
\mathrm{Com}(m) &= \hphantom{+} \frac{1}{2} e_\beta^\mathrm{T} \Sigma_0^{-1} e_\beta + \frac{1}{2} \log |\Sigma_0| - \frac{1}{2} \log |\Sigma_n| + \frac{1}{2} \mathrm{tr}(\Sigma_0^{-1} \Sigma_n) - \frac{p}{2}
\end{split}
\end{equation}

with the "prediction error" and "parameter error" terms

\begin{equation} \label{eq:blrkc-anc-GLM-N-err}
\begin{split}
e_y &= y - X \mu_n \\
e_\beta &= \mu_0 - \mu_n
\end{split}
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:blrkc-anc-GLM-N-post-par}
\begin{split}
\mu_n &= \Sigma_n (X^\mathrm{T} \Sigma^{-1} y + \Sigma_0^{-1} \mu_0) \\
\Sigma_n &= \left( X^\mathrm{T} \Sigma^{-1} X + \Sigma_0^{-1} \right)^{-1} \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} Model accuracy and complexity are defined as ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme-anc})

\begin{equation} \label{eq:blrkc-anc-lme-anc}
\begin{split}
\mathrm{LME}(m) &= \mathrm{Acc}(m) - \mathrm{Com}(m) \\
\mathrm{Acc}(m) &= \left\langle \log p(y|\beta,m) \right\rangle_{p(\beta|y,m)} \\
\mathrm{Com}(m) &= \mathrm{KL} \left[ p(\beta|y,m) \, || \, p(\beta|m) \right] \; .
\end{split}
\end{equation}

\vspace{1em}
1) The accuracy term is the expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) $\log p(y|\beta)$ with respect to the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) $p(\beta|y)$:

\begin{equation} \label{eq:blrkc-anc-GLM-N-Acc-s1}
\mathrm{Acc}(m) = \left\langle \log p(y|\beta) \right\rangle_{p(\beta|y)} \; .
\end{equation}

With the likelihood function for Bayesian linear regression with known covariance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blrkc-prior}), we have:

\begin{equation} \label{eq:blrkc-anc-GLM-N-Acc-s2}
\begin{split}
\mathrm{Acc}(m) &= \left\langle \log \left( \sqrt{\frac{1}{(2 \pi)^n |\Sigma|}} \, \exp\left[ -\frac{1}{2} (y-X\beta)^\mathrm{T} \Sigma^{-1} (y-X\beta) \right] \right) \right\rangle_{p(\beta|y)} \\
&= \left\langle - \frac{n}{2} \log (2 \pi) - \frac{1}{2} \log |\Sigma| - \frac{1}{2} (y-X\beta)^\mathrm{T} \Sigma^{-1} (y-X\beta) \right\rangle_{p(\beta|y)} \\
&= \left\langle - \frac{n}{2} \log (2 \pi) - \frac{1}{2} \log |\Sigma| - \frac{1}{2} \left[ y^\mathrm{T} \Sigma^{-1} y - 2 y^\mathrm{T} \Sigma^{-1} X \beta + \beta^\mathrm{T} X^\mathrm{T} \Sigma^{-1} X \beta \right] \right\rangle_{p(\beta|y)} \; .
\end{split}
\end{equation}

With the posterior distribution for Bayesian linear regression with known covariance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blrkc-post}), this becomes:

\begin{equation} \label{eq:blrkc-anc-GLM-N-Acc-s3}
\mathrm{Acc}(m) = \left\langle - \frac{n}{2} \log (2 \pi) - \frac{1}{2} \log |\Sigma| - \frac{1}{2} \left[ y^\mathrm{T} \Sigma^{-1} y - 2 y^\mathrm{T} \Sigma^{-1} X \beta + \beta^\mathrm{T} X^\mathrm{T} \Sigma^{-1} X \beta \right] \right\rangle_{\mathcal{N}(\beta; \mu_n, \Sigma_n)} \; .
\end{equation}

If $x \sim \mathrm{N}(\mu, \Sigma)$, then its expected value is ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-mean})

\begin{equation} \label{eq:blrkc-anc-mvn-mean}
\left\langle x \right\rangle = \mu
\end{equation}

and the expectation of a quadratic form is given by ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-qf})

\begin{equation} \label{eq:blrkc-anc-mvn-meansqr}
\left\langle x^\mathrm{T} A x \right\rangle = \mu^\mathrm{T} A \mu + \mathrm{tr}(A \Sigma) \; .
\end{equation}

Thus, the model accuracy of $m$ evaluates to

\begin{equation} \label{eq:blrkc-anc-GLM-N-Acc-s4}
\begin{split}
\mathrm{Acc}(m) = - &\frac{n}{2} \log (2 \pi) - \frac{1}{2} \log |\Sigma| - \\
&\frac{1}{2} \left[ y^\mathrm{T} \Sigma^{-1} y - 2 y^\mathrm{T} \Sigma^{-1} X \mu_n + \mu_n^\mathrm{T} X^\mathrm{T} \Sigma^{-1} X \mu_n + \mathrm{tr}(X^\mathrm{T} \Sigma^{-1} X \Sigma_n) \right] \\
= - &\frac{1}{2} (y - X \mu_n)^\mathrm{T} \Sigma^{-1} (y - X \mu_n) - \frac{1}{2} \log |\Sigma| - \frac{n}{2} \log (2 \pi) - \frac{1}{2} \mathrm{tr}(X^\mathrm{T} \Sigma^{-1} X \Sigma_n) \\
\overset{\eqref{eq:blrkc-anc-GLM-N-err}}{=} - &\frac{1}{2} e_y^\mathrm{T} \Sigma^{-1} e_y - \frac{1}{2} \log |\Sigma| - \frac{n}{2} \log (2 \pi) - \frac{1}{2} \mathrm{tr}(X^\mathrm{T} \Sigma^{-1} X \Sigma_n)
\end{split}
\end{equation}

which proofs the first part of \eqref{eq:blrkc-anc-GLM-N-AnC}.

\vspace{1em}
2) The complexity penalty is the Kullback-Leibler divergence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:kl}) of the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) $p(\beta|y)$ from the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) $p(\beta)$:

\begin{equation} \label{eq:blrkc-anc-GLM-N-Com-s1}
\mathrm{Com}(m) = \mathrm{KL} \left[ p(\beta|y) \, || \, p(\beta) \right] \; .
\end{equation}

With the prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blrkc-prior}) given by \eqref{eq:blrkc-anc-GLM-N-prior} and the posterior distribution for Bayesian linear regression with known covariance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blrkc-post}), this becomes:

\begin{equation} \label{eq:blrkc-anc-GLM-N-Com-s2}
\mathrm{Com}(m) = \mathrm{KL} \left[ \mathcal{N}(\beta; \mu_n, \Sigma_n)\,||\,\mathcal{N}(\beta; \mu_0, \Sigma_0) \right] \; .
\end{equation}

With the Kullback-Leibler divergence for the multivariate normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mvn-kl})

\begin{equation} \label{eq:blrkc-anc-mvn-kl}
\mathrm{KL}[\mathcal{N}(\mu_1, \Sigma_1)\,||\,\mathcal{N}(\mu_2, \Sigma_2)] = \frac{1}{2} \left[ (\mu_2 - \mu_1)^\mathrm{T} \Sigma_2^{-1} (\mu_2 - \mu_1) + \mathrm{tr}(\Sigma_2^{-1} \Sigma_1) - \ln \frac{|\Sigma_1|}{|\Sigma_2|} - n \right]
\end{equation}

the model complexity of $m$ evaluates to

\begin{equation} \label{eq:blrkc-anc-GLM-N-Com-s3}
\begin{split}
\mathrm{Com}(m) &= \frac{1}{2} \left[ (\mu_0 - \mu_n)^\mathrm{T} \Sigma_0^{-1} (\mu_0 - \mu_n) + \mathrm{tr}(\Sigma_0^{-1} \Sigma_n) - \log \frac{|\Sigma_n|}{|\Sigma_0|} - p \right] \\
&= \frac{1}{2} (\mu_0 - \mu_n)^\mathrm{T} \Sigma_0^{-1} (\mu_0 - \mu_n) + \frac{1}{2} \log |\Sigma_0| - \frac{1}{2} \log |\Sigma_n| + \frac{1}{2} \mathrm{tr}(\Sigma_0^{-1} \Sigma_n) - \frac{p}{2} \\
&\overset{\eqref{eq:blrkc-anc-GLM-N-err}}{=} \frac{1}{2} e_\beta^\mathrm{T} \Sigma_0^{-1} e_\beta + \frac{1}{2} \log |\Sigma_0| - \frac{1}{2} \log |\Sigma_n| + \frac{1}{2} \mathrm{tr}(\Sigma_0^{-1} \Sigma_n) - \frac{p}{2}
\end{split}
\end{equation}

which proofs the second part of \eqref{eq:blrkc-anc-GLM-N-AnC}.

\vspace{1em}
3) A control calculation confirms that

\begin{equation} \label{eq:blrkc-anc-GLM-N-AnC-LME}
\mathrm{Acc}(m) - \mathrm{Com}(m) = \mathrm{LME}(m)
\end{equation}

where $\mathrm{LME}(m)$ is the log model evidence for Bayesian linear regression with known covariance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:blrkc-lme}):

\begin{equation} \label{eq:blrkc-anc-GLM-N-LME}
\begin{split}
\log p(y|m) = &- \frac{1}{2} e_y^\mathrm{T} \Sigma^{-1} e_y - \frac{1}{2} \log |\Sigma| - \frac{n}{2} \log (2 \pi) \\
&- \frac{1}{2} e_\beta^\mathrm{T} \Sigma_0^{-1} e_\beta - \frac{1}{2} \log |\Sigma_0| + \frac{1}{2} \log |\Sigma_n| \; .
\end{split}
\end{equation}

This requires to recognize, based on \eqref{eq:blrkc-anc-GLM-N-post-par}, that

\begin{equation} \label{eq:blrkc-anc-GLM-N-AnC-LME-a1}
\begin{split}
& \; - \frac{1}{2} \mathrm{tr}(X^\mathrm{T} \Sigma^{-1} X \Sigma_n) - \frac{1}{2} \mathrm{tr}(\Sigma_0^{-1} \Sigma_n) + \frac{p}{2} \\
= & \; - \frac{1}{2} \mathrm{tr}\left( [X^\mathrm{T} \Sigma^{-1} X + \Sigma_0^{-1}] \Sigma_n \right) + \frac{p}{2} \\
= & \; - \frac{1}{2} \mathrm{tr}\left( \Sigma_n^{-1} \Sigma_n \right) + \frac{p}{2} \\
= & \; - \frac{1}{2} \mathrm{tr}\left( I_p \right) + \frac{p}{2} \\
= & \; - \frac{p}{2} + \frac{p}{2} \\
= & \;\; 0 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Penny WD (2012): "Comparing Dynamic Causal Models using AIC, BIC and Free Energy"; in: \textit{NeuroImage}, vol. 59, iss. 2, pp. 319-330, eqs. 20-21; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811911008160}; DOI: 10.1016/j.neuroimage.2011.07.039.
\item Bishop CM (2006): "Bayesian linear regression"; in: \textit{Pattern Recognition for Machine Learning}, pp. 152-161; URL: \url{https://www.springer.com/gp/book/9780387310732}.
\end{itemize}
\vspace{1em}



\pagebreak
\section{Multivariate normal data}

\subsection{General linear model}

\subsubsection[\textit{Definition}]{Definition} \label{sec:glm}
\setcounter{equation}{0}

\textbf{Definition:} Let $Y$ be an $n \times v$ matrix and let $X$ be an $n \times p$ matrix. Then, a statement asserting a linear mapping from $X$ to $Y$ with parameters $B$ and matrix-normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) errors $E$

\begin{equation} \label{eq:glm-glm}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma)
\end{equation}

is called a multivariate linear regression model or simply, "general linear model".

\begin{itemize}

\item $Y$ is called "data matrix", "set of dependent variables" or "measurements";

\item $X$ is called "design matrix", "set of independent variables" or "predictors";

\item $B$ are called "regression coefficients" or "weights";

\item $E$ is called "noise matrix" or "error terms";

\item $V$ is called "covariance across rows";

\item $\Sigma$ is called "covariance across columns";

\item $n$ is the number of observations;

\item $v$ is the number of measurements;

\item $p$ is the number of predictors.

\end{itemize}

When rows of $Y$ correspond to units of time, e.g. subsequent measurements, $V$ is called "temporal covariance". When columns of $Y$ correspond to units of space, e.g. measurement channels, $\Sigma$ is called "spatial covariance".

When the covariance matrix $V$ is a scalar multiple of the $n \times n$ identity matrix, this is called a general linear model with independent and identically distributed (i.i.d.) observations:

\begin{equation} \label{eq:glm-glm-iid}
V = \lambda I_n \quad \Rightarrow \quad E \sim \mathcal{MN}(0, \lambda I_n, \Sigma) \quad \Rightarrow \quad \varepsilon_i \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0, \lambda \Sigma) \; .
\end{equation}

Otherwise, it is called a general linear model with correlated observations.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "General linear model"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-21; URL: \url{https://en.wikipedia.org/wiki/General_linear_model}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Ordinary least squares}]{Ordinary least squares} \label{sec:glm-ols}
\setcounter{equation}{0}

\textbf{Theorem:} Given a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) with independent observations

\begin{equation} \label{eq:glm-ols-GLM}
Y = X B + E, \; E \sim \mathcal{MN}(0, \sigma^2 I_n, \Sigma) \; ,
\end{equation}

the ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}) parameters estimates are given by

\begin{equation} \label{eq:glm-ols-OLS}
\hat{B} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} Y \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Let $\hat{B}$ be the ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}) (OLS) solution and let $\hat{E} = Y - X\hat{B}$ be the resulting matrix of residuals. According to the exogeneity assumption of OLS, the errors have conditional mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) zero

\begin{equation} \label{eq:glm-ols-OLS-exo}
\mathrm{E}(E|X) = 0 \; ,
\end{equation}

a direct consequence of which is that the regressors are uncorrelated with the errors

\begin{equation} \label{eq:glm-ols-OLS-uncorr}
\mathrm{E}(X^\mathrm{T} E) = 0 \; ,
\end{equation}

which, in the finite sample, means that the residual matrix must be orthogonal to the design matrix:

\begin{equation} \label{eq:glm-ols-X-E-orth}
X^\mathrm{T} \hat{E} = 0 \; .
\end{equation}

From \eqref{eq:glm-ols-X-E-orth}, the OLS formula can be directly derived:

\begin{equation} \label{eq:glm-ols-OLS-qed}
\begin{split}
X^\mathrm{T} \hat{E} &= 0 \\
X^\mathrm{T} \left( Y - X\hat{B} \right) &= 0 \\
X^\mathrm{T} Y - X^\mathrm{T} X\hat{B} &= 0 \\
X^\mathrm{T} X\hat{B} &= X^\mathrm{T} Y \\
\hat{B} &= (X^\mathrm{T} X)^{-1} X^\mathrm{T} Y \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Weighted least squares}]{Weighted least squares} \label{sec:glm-wls}
\setcounter{equation}{0}

\textbf{Theorem:} Given a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) with correlated observations

\begin{equation} \label{eq:glm-wls-GLM}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma) \; ,
\end{equation}

the weighted least sqaures ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wls}) parameter estimates are given by

\begin{equation} \label{eq:glm-wls-WLS}
\hat{B} = (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} Y \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Let there be an $n \times n$ square matrix $W$, such that

\begin{equation} \label{eq:glm-wls-W-def}
W V W^\mathrm{T} = I_n \; .
\end{equation}

Since $V$ is a covariance matrix and thus symmetric, $W$ is also symmetric and can be expressed as the matrix square root of the inverse of $V$:

\begin{equation} \label{eq:glm-wls-W-V}
W W = V^{-1} \quad \Leftrightarrow \quad W = V^{-1/2} \; .
\end{equation}

Left-multiplying the linear regression equation \eqref{eq:glm-wls-GLM} with $W$, the linear transformation theorem ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-ltt}) implies that

\begin{equation} \label{eq:glm-wls-GLM-W}
WY = WXB + WE, \; WE \sim \mathcal{MN}(0, W V W^\mathrm{T}, \Sigma) \; .
\end{equation}

Applying \eqref{eq:glm-wls-W-def}, we see that \eqref{eq:glm-wls-GLM-W} is actually a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) with independent observations

\begin{equation} \label{eq:glm-wls-GLM-W-dev}
\tilde{Y} = \tilde{X}B + \tilde{E}, \; \tilde{E} \sim \mathcal{N}(0, I_n, \Sigma)
\end{equation}

where $\tilde{Y} = WY$, $\tilde{X} = WX$ and $\tilde{E} = WE$, such that we can apply the ordinary least squares solution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-ols}) giving

\begin{equation} \label{eq:glm-wls-WLS-qed}
\begin{split}
\hat{B} &= (\tilde{X}^\mathrm{T} \tilde{X})^{-1} \tilde{X}^\mathrm{T} \tilde{Y} \\
&= \left( (WX)^\mathrm{T} WX \right)^{-1} (WX)^\mathrm{T} WY \\
&= \left( X^\mathrm{T} W^\mathrm{T} W X \right)^{-1} X^\mathrm{T} W^\mathrm{T} W Y \\
&= \left( X^\mathrm{T} W W X \right)^{-1} X^\mathrm{T} W W Y \\
&\overset{\eqref{eq:glm-wls-W-V}}{=} \left( X^\mathrm{T} V^{-1} X \right)^{-1} X^\mathrm{T} V^{-1} Y
\end{split}
\end{equation}

which corresponds to the weighted least squares solution \eqref{eq:glm-wls-WLS}.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Maximum likelihood estimation}]{Maximum likelihood estimation} \label{sec:glm-mle}
\setcounter{equation}{0}

\textbf{Theorem:} Given a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) with matrix-normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) errors

\begin{equation} \label{eq:glm-mle-GLM}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma) \; ,
\end{equation}

maximum likelihood estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) for the unknown parameters $B$ and $\Sigma$ are given by

\begin{equation} \label{eq:glm-mle-GLM-MLE}
\begin{split}
\hat{B} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} Y \\
\hat{\Sigma} &= \frac{1}{n} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} In \eqref{eq:glm-mle-GLM}, $Y$ is an $n \times v$ matrix of measurements ($n$ observations, $v$ dependent variables), $X$ is an $n \times p$ design matrix ($n$ observations, $p$ independent variables) and $V$ is an $n \times n$ covariance matrix across observations. This multivariate GLM implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:glm-mle-GLM-LF}
\begin{split}
p(Y|B,\Sigma) &= \mathcal{MN}(Y; XB, V, \Sigma) \\
&= \sqrt{\frac{1}{(2\pi)^{nv} |\Sigma|^n |V|^v}} \cdot \exp\left[ -\frac{1}{2} \, \mathrm{tr}\left( \Sigma^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \right) \right]
\end{split}
\end{equation}

and the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf})

\begin{equation} \label{eq:glm-mle-GLM-LL1}
\begin{split}
\mathrm{LL}(B,\Sigma) = &\log p(Y|B,\Sigma) \\
= &- \frac{nv}{2} \log(2\pi) - \frac{n}{2} \log |\Sigma| - \frac{v}{2} \log |V| \\
&- \frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \right] \; .
\end{split}
\end{equation}

Substituting $V^{-1}$ by the precision matrix $P$ to ease notation, we have:

\begin{equation} \label{eq:glm-mle-GLM-LL2}
\begin{split}
\mathrm{LL}(B,\Sigma) = &- \frac{nv}{2} \log(2\pi) - \frac{n}{2} \log |\Sigma|  + \frac{v}{2} \log |P| \\
&- \frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} \left( Y^\mathrm{T} P Y - Y^\mathrm{T} P X B - B^\mathrm{T} X^\mathrm{T} P Y + B^\mathrm{T} X^\mathrm{T} P X B \right) \right] \; .
\end{split}
\end{equation}

\vspace{1em}
The derivative of the log-likelihood function \eqref{eq:glm-mle-GLM-LL2} with respect to $B$ is

\begin{equation} \label{eq:glm-mle-dLL-dB}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(B,\Sigma)}{\mathrm{d}B} &= \frac{\mathrm{d}}{\mathrm{d}B} \left( - \frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} \left( Y^\mathrm{T} P Y - Y^\mathrm{T} P X B - B^\mathrm{T} X^\mathrm{T} P Y + B^\mathrm{T} X^\mathrm{T} P X B \right) \right] \right) \\
&= \frac{\mathrm{d}}{\mathrm{d}B} \left( -\frac{1}{2} \, \mathrm{tr}\left[ -2 \Sigma^{-1} Y^\mathrm{T} P X B \right] \right) + \frac{\mathrm{d}}{\mathrm{d}B} \left( -\frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} B^\mathrm{T} X^\mathrm{T} P X B \right] \right) \\
&= - \frac{1}{2} \left( -2 X^\mathrm{T} P Y \Sigma^{-1} \right) - \frac{1}{2} \left( X^\mathrm{T} P X B \Sigma^{-1} + (X^\mathrm{T} P X)^\mathrm{T} B (\Sigma^{-1})^\mathrm{T} \right) \\
&= X^\mathrm{T} P Y \Sigma^{-1} - X^\mathrm{T} P X B \Sigma^{-1}
\end{split}
\end{equation}

and setting this derivative to zero gives the MLE for $B$:

\begin{equation} \label{eq:glm-mle-B-MLE}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{B},\Sigma)}{\mathrm{d}B} &= 0 \\
0 &= X^\mathrm{T} P Y \Sigma^{-1} - X^\mathrm{T} P X \hat{B} \Sigma^{-1} \\
0 &= X^\mathrm{T} P Y - X^\mathrm{T} P X \hat{B} \\
X^\mathrm{T} P X \hat{B} &= X^\mathrm{T} P Y \\
\hat{B} &= \left( X^\mathrm{T} P X \right)^{-1} X^\mathrm{T} P Y \; .
\end{split}
\end{equation}

\vspace{1em}
The derivative of the log-likelihood function \eqref{eq:glm-mle-GLM-LL1} at $\hat{B}$ with respect to $\Sigma$ is

\begin{equation} \label{eq:glm-mle-dLL-dS}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{B},\Sigma)}{\mathrm{d}\Sigma} &= \frac{\mathrm{d}}{\mathrm{d}\Sigma} \left( - \frac{n}{2} \log |\Sigma| - \frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \right] \right) \\
&= - \frac{n}{2} \left( \Sigma^{-1} \right)^\mathrm{T} + \frac{1}{2} \left( \Sigma^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \, \Sigma^{-1} \right)^\mathrm{T} \\
&= - \frac{n}{2} \, \Sigma^{-1} + \frac{1}{2} \, \Sigma^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \, \Sigma^{-1}
\end{split}
\end{equation}

and setting this derivative to zero gives the MLE for $\Sigma$:

\begin{equation} \label{eq:glm-mle-S-MLE}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{B},\hat{\Sigma})}{\mathrm{d}\Sigma} &= 0 \\
0 &= - \frac{n}{2} \, \hat{\Sigma}^{-1} + \frac{1}{2} \, \hat{\Sigma}^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \, \hat{\Sigma}^{-1} \\
\frac{n}{2} \, \hat{\Sigma}^{-1} &= \frac{1}{2} \, \hat{\Sigma}^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \, \hat{\Sigma}^{-1} \\
\hat{\Sigma}^{-1} &= \frac{1}{n} \, \hat{\Sigma}^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \, \hat{\Sigma}^{-1} \\
I_v &= \frac{1}{n} \, (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \, \hat{\Sigma}^{-1} \\
\hat{\Sigma} &= \frac{1}{n} \, (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \; .
\end{split}
\end{equation}

\vspace{1em}
Together, \eqref{eq:glm-mle-B-MLE} and \eqref{eq:glm-mle-S-MLE} constitute the MLE for the GLM.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Maximum log-likelihood}]{Maximum log-likelihood} \label{sec:glm-mll}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) $m$ with $n \times v$ data matrix $Y$, $n \times p$ design matrix $X$ and $n \times n$ covariance across rows ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) $V$

\begin{equation} \label{eq:glm-mll-glm}
m: \; Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma) \; .
\end{equation}

Then, the maximum log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}) for this model is

\begin{equation} \label{eq:glm-mll-glm-mll-v1}
\mathrm{MLL}(m) = - \frac{nv}{2} \ln(2\pi) - \frac{n}{2} \ln |\hat{\Sigma}| - \frac{nv}{2}
\end{equation}

under uncorrelated observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}), i.e. if $V = I_n$, and

\begin{equation} \label{eq:glm-mll-glm-mll-v2}
\mathrm{MLL}(m) = - \frac{nv}{2} \ln(2\pi) - \frac{n}{2} \ln |\hat{\Sigma}| - \frac{v}{2} \ln |V| - \frac{nv}{2} \; ,
\end{equation}

in the general case, i.e. if $V \neq I_n$, where $\hat{\Sigma}$ is the maximum likelihood estimate ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) of the $v \times v$ covariance across columns ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}).


\vspace{1em}
\textbf{Proof:} The likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) for the general linear model is given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-mle})

\begin{equation} \label{eq:glm-mll-glm-lf}
\begin{split}
p(Y|B,\Sigma) &= \mathcal{MN}(Y; XB, V, \Sigma) \\
&= \sqrt{\frac{1}{(2\pi)^{nv} |\Sigma|^n |V|^v}} \cdot \exp\left[ -\frac{1}{2} \, \mathrm{tr}\left( \Sigma^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \right) \right] \; ,
\end{split}
\end{equation}

such that the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) for this model becomes ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-mle})

\begin{equation} \label{eq:glm-mll-glm-llf}
\begin{split}
\mathrm{LL}(B,\Sigma) = - \frac{nv}{2} \log(2\pi) - \frac{n}{2} \log |\Sigma| - \frac{v}{2} \log |V| - \frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \right] \; .
\end{split}
\end{equation}

The maximum likelihood estimate for the noise covariance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-mle}) is

\begin{equation} \label{eq:glm-mll-glm-mle-Si}
\hat{\Sigma} = \frac{1}{n} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B})
\end{equation}

Plugging \eqref{eq:glm-mll-glm-mle-Si} into \eqref{eq:glm-mll-glm-llf}, we obtain the maximum log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}) as

\begin{equation} \label{eq:glm-mll-glm-mll-v2-qed}
\begin{split}
\mathrm{MLL}(m) = &\;\mathrm{LL}(\hat{B},\hat{\Sigma}) \\
= &- \frac{nv}{2} \log(2\pi) - \frac{n}{2} \log |\hat{\Sigma}| - \frac{v}{2} \log |V| - \frac{1}{2} \, \mathrm{tr}\left[ \hat{\Sigma}^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \right] \\
= &- \frac{nv}{2} \log(2\pi) - \frac{n}{2} \log |\hat{\Sigma}| - \frac{v}{2} \log |V| \\
&- \frac{1}{2} \, \mathrm{tr}\left[ \left( \frac{1}{n} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \right)^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \right] \\
= &- \frac{nv}{2} \log(2\pi) - \frac{n}{2} \log |\hat{\Sigma}| - \frac{v}{2} \log |V| - \frac{n}{2} \, \mathrm{tr}\left[ I_v \right] \\
= &- \frac{nv}{2} \log(2\pi) - \frac{n}{2} \log |\hat{\Sigma}| - \frac{v}{2} \log |V| - \frac{nv}{2}
\end{split}
\end{equation}

which proves the result in \eqref{eq:glm-mll-glm-mll-v2}. Assuming $V = I_n$, we have

\begin{equation} \label{eq:glm-mll-glm-mle-Si-iid}
\hat{\Sigma} = \frac{1}{n} (Y - X\hat{B})^\mathrm{T} (Y - X\hat{B})
\end{equation}

and

\begin{equation} \label{eq:glm-mll-glm-logdet-V-iid}
\frac{v}{2} \log|V| = \frac{v}{2} \log|I_n| = \frac{v}{2} \log 1 = 0 \; ,
\end{equation}

such that

\begin{equation} \label{eq:glm-mll-glm-mll-v1-qed}
\mathrm{MLL}(m) = - \frac{nv}{2} \ln(2\pi) - \frac{n}{2} \ln |\hat{\Sigma}| - \frac{nv}{2}
\end{equation}

which proves the result in \eqref{eq:glm-mll-glm-mll-v1}. This completes the proof.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Log-likelihood ratio}]{Log-likelihood ratio} \label{sec:glm-llr}
\setcounter{equation}{0}

\textbf{Theorem:} Let $Y = \left[ y_1, \ldots, y_v \right]$ be an $n \times v$ data matrix and consider two general linear models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) with design matrices ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) $X_1, X_2$ and row-by-row covariance matrices ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) $V_1, V_2$, entailing potentially different regression coefficients ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) $B_1, B_2$ and column-by-column covariance matrices ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) $\Sigma_1, \Sigma_2$:

\begin{equation} \label{eq:glm-llr-m1-m2}
\begin{split}
m_1: \; Y &= X_1 B_1 + E_1, \; E_1 \sim \mathcal{N}(0, V_1, \Sigma_1) \\
m_2: \; Y &= X_2 B_2 + E_2, \; E_2 \sim \mathcal{N}(0, V_2, \Sigma_2) \; .
\end{split}
\end{equation}

Then, if the models assume the same covariance matrix across observations, i.e. if $V_1 = V_2$, the log-likelihood ratio ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llr}) for comparing $m_1$ vs. $m_2$ is given by

\begin{equation} \label{eq:glm-llr-glm-llr}
\ln \Lambda_{12} = \frac{n}{2} \ln \frac{|\hat{\Sigma}_2|}{|\hat{\Sigma}_1|}
\end{equation}

where $\hat{\Sigma}_1$ and $\hat{\Sigma}_2$ are the maximum likelihood estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) of $\Sigma_1$ and $\Sigma_2$.


\vspace{1em}
\textbf{Proof:} The likelihood ratio ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lr}) between two models $m_1$ and $m_2$ with model parameters $\theta_1$ and $\theta_2$ and parameter spaces $\Theta_1$ and $\Theta_2$ is defined as the quotient of their maximized ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) likelihood functions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}):

\begin{equation} \label{eq:glm-llr-lr}
\Lambda_{12} = \frac{\operatorname*{max}_{\theta_1 \in \Theta_1} p(y|\theta_1,m_1)}{\operatorname*{max}_{\theta_2 \in \Theta_2} p(y|\theta_2,m_2)} \; .
\end{equation}

Thus, the log-likelihood ratio ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llr}) is equal to the difference of the maximum log-likelihoods ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}) of the two models:

\begin{equation} \label{eq:glm-llr-llr}
\ln \Lambda_{12} = \ln p(y|\hat{\theta}_1,m_1) - \ln p(y|\hat{\theta}_2,m_2) \; .
\end{equation}

The likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) of the general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) is a matrix-normal probability density function ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-pdf}):

\begin{equation} \label{eq:glm-llr-glm-lf}
\begin{split}
p(Y|B,\Sigma)
&= \mathcal{MN}(Y; XB, V, \Sigma) \\
&= \sqrt{\frac{1}{(2\pi)^{nv} |\Sigma|^n |V|^v}} \cdot \exp\left[ -\frac{1}{2} \, \mathrm{tr}\left( \Sigma^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \right) \right] \; .
\end{split}
\end{equation}

Thus, the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) is equal to a logarithmized matrix-normal ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) density ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pdf}):

\begin{equation} \label{eq:glm-llr-glm-llf}
\begin{split}
\ln p(Y|B,\Sigma)
&= \ln \mathcal{MN}(Y; XB, V, \Sigma) \\
&= - \frac{nv}{2} \ln(2\pi) - \frac{n}{2} \ln |\Sigma| - \frac{v}{2} \ln |V| - \frac{1}{2} \, \mathrm{tr}\left[ \Sigma^{-1} (Y - XB)^\mathrm{T} V^{-1} (Y - XB) \right] \; .
\end{split}
\end{equation}

The maximum likelihood estimates for the general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-mle}) are given by

\begin{equation} \label{eq:glm-llr-glm-mle}
\begin{split}
\hat{B}      &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} Y \\
\hat{\Sigma} &= \frac{1}{n} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \; .
\end{split}
\end{equation}

such that the last term in the maximum log-likelihood function \eqref{eq:glm-llr-glm-llf} becomes

\begin{equation} \label{eq:glm-llr-glm-mll-tr}
\begin{split}
&\quad\; \frac{1}{2} \, \mathrm{tr}\left[ \hat{\Sigma}^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \right] \\
&= \frac{1}{2} \, \mathrm{tr}\left[ \left( \frac{1}{n} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \right)^{-1} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \right] \\
&= \frac{1}{2} \, \mathrm{tr}\left[ n \left( (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \right)^{-1} \left( (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \right) \right] \\
&= \frac{n}{2} \, \mathrm{tr}\left[ I_v \right] \\
&= \frac{nv}{2} \; .
\end{split}
\end{equation}

Thus, the maximum log-likelihood for the general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-mll}) is equal to

\begin{equation} \label{eq:glm-llr-glm-mll}
\ln p(Y|\hat{B},\hat{\Sigma}) = - \frac{nv}{2} \ln(2\pi) - \frac{n}{2} \ln |\hat{\Sigma}| - \frac{v}{2} \ln |V| - \frac{nv}{2} \; .
\end{equation}

Evaluating \eqref{eq:glm-llr-glm-mll} for $m_1$ and $m_2$ and plugging into \eqref{eq:glm-llr-llr}, we obtain:

\begin{equation} \label{eq:glm-llr-glm-llr-m1-m2}
\begin{split}
\ln \Lambda_{12}
&= \ln p(Y|\hat{B}_1,\hat{\Sigma}_1,m_1) - \ln p(Y|\hat{B}_2,\hat{\Sigma}_2,m_2) \\
&= \left( - \frac{nv}{2} \ln(2\pi) - \frac{n}{2} \ln |\hat{\Sigma}_1| - \frac{v}{2} \ln |V_1| - \frac{nv}{2} \right) \\
&- \left( - \frac{nv}{2} \ln(2\pi) - \frac{n}{2} \ln |\hat{\Sigma}_2| - \frac{v}{2} \ln |V_2| - \frac{nv}{2} \right) \\
&= - \frac{n}{2} \ln \frac{|\hat{\Sigma}_1|}{|\hat{\Sigma}_2|} - \frac{v}{2} \ln \frac{|V_1|}{|V_2|} \; .
\end{split}
\end{equation}

Thus, if $V_1 = V_2$, such that $\ln(\vert V_2 \vert / \vert V_1 \vert) = \ln(1) = 0$, the log-likelihood ratio is equal to

\begin{equation} \label{eq:glm-llr-glm-llr-qed}
\ln \Lambda_{12} = - \frac{n}{2} \ln \frac{|\hat{\Sigma}_1|}{|\hat{\Sigma}_2|} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Mutual information}]{Mutual information} \label{sec:glm-mi}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) $m_1$ with $n \times v$ data matrix $Y$, $n \times p$ design matrix $X$ and uncorrelated observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}), i.e. $V = I_n$,

\begin{equation} \label{eq:glm-mi-m1}
m_1: \; Y = X B + E_1, \; E_1 \sim \mathcal{MN}(0, I_n, \Sigma_1) \; ,
\end{equation}

as well as another model $m_0$ in which $X$ has no influence on $Y$:

\begin{equation} \label{eq:glm-mi-m0}
m_0: \; Y = E_0, \; E_0 \sim \mathcal{MN}(0, I_n, \Sigma_0) \; .
\end{equation}

Then, the mutual information ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mi}) of $Y$ and $X$ is equal to

\begin{equation} \label{eq:glm-mi-glm-mi}
I(X,Y) = - \frac{n}{2} \ln \frac{|\Sigma_1|}{|\Sigma_0|} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The continuous mutual information can be written in terms of marginal and conditional differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cmi-mcde}) as follows:

\begin{equation} \label{eq:glm-mi-cmi-mcde}
\mathrm{I}(X,Y) = \mathrm{h}(Y) - \mathrm{h}(Y|X) \; .
\end{equation}

The marginal distribution of $Y$, unconditional on $X$, is given by model $m_0$

\begin{equation} \label{eq:glm-mi-m0-dist}
Y \sim \mathcal{MN}(0, I_n, \Sigma_0)
\end{equation}

and the conditional distribution of $Y$ given $X$ is given by model $m_1$

\begin{equation} \label{eq:glm-mi-m1-dist}
Y \sim \mathcal{MN}(XB, I_n, \Sigma_1) \; .
\end{equation}

Since $X$ is constant ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:const}) and thus only has one possible value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:samp-spc}), the conditional differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent-cond}) of $Y$ given $X$ is obtained by simply entering $X$ into the probability distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}) for which the differential entropy ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dent-cond}) is calculated:

\begin{equation} \label{eq:glm-mi-dent-cond-const}
\begin{split}
\mathrm{h}(Y|X)
&= \int_{z \in \mathcal{X}} p(z) \cdot \mathrm{h}(Y|z) \, \mathrm{d}z \\
&= p(X) \cdot \mathrm{h}(Y|X) \\
&= \mathrm{h}\left[ p(Y|X,B,\Sigma_1) \right] \; .
\end{split}
\end{equation}

The differential entropy of the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-dent}) is

\begin{equation} \label{eq:glm-mi-matn-dent}
\begin{split}
&X \sim \mathcal{MN}(M, U, V) \quad \text{where} \quad X \in \mathbb{R}^{n \times p} \\
\Rightarrow \quad 
&\mathrm{h}(X) = \frac{np}{2} \ln(2\pi) + \frac{n}{2} \ln|V| + \frac{p}{2} \ln|U| + \frac{np}{2}
\end{split}
\end{equation}

such that the mutual information of $Y$ and $X$ becomes

\begin{equation} \label{eq:glm-mi-dent-cond}
\begin{split}
\mathrm{I}(X,Y)
&= \mathrm{h}\left[ p(Y|\Sigma_0) \right] - \mathrm{h}\left[ p(Y|X,B,\Sigma_1) \right] \\
&= \mathrm{h}\left[ \mathcal{MN}(0, I_n, \Sigma_0) \right] - \mathrm{h}\left[ \mathcal{MN}(XB, I_n, \Sigma_1) \right] \\
&= \left( \frac{nv}{2} \ln(2\pi) + \frac{n}{2} \ln|\Sigma_0| + \frac{v}{2} \ln|I_n| + \frac{nv}{2} \right) \\
&- \left( \frac{nv}{2} \ln(2\pi) + \frac{n}{2} \ln|\Sigma_1| + \frac{v}{2} \ln|I_n| + \frac{nv}{2} \right) \\
&= \frac{n}{2} \ln|\Sigma_0| - \frac{n}{2} \ln|\Sigma_1| \\
&= \frac{n}{2} \ln \frac{|\Sigma_0|}{|\Sigma_1|} \\
&= - \frac{n}{2} \ln \frac{|\Sigma_1|}{|\Sigma_0|} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Log-likelihood ratio and estimated mutual information}]{Log-likelihood ratio and estimated mutual information} \label{sec:glm-llrmi}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) $m_1$ with $n \times v$ data matrix $Y$, $n \times p$ design matrix $X$ and uncorrelated observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}), i.e. $V = I_n$,

\begin{equation} \label{eq:glm-llrmi-m1}
m_1: \; Y = X B + E_1, \; E_1 \sim \mathcal{MN}(0, I_n, \Sigma_1) \; ,
\end{equation}

as well as another model $m_0$ in which $X$ has no influence on $Y$:

\begin{equation} \label{eq:glm-llrmi-m0}
m_0: \; Y = E_0, \; E_0 \sim \mathcal{MN}(0, I_n, \Sigma_0) \; .
\end{equation}

Then, the log-likelihood ratio ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llr}) of $m_1$ vs. $m_0$ is equal to the estimated mutual information ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mi}) of $X$ and $Y$:

\begin{equation} \label{eq:glm-llrmi-glm-llrmi}
\ln \Lambda_{10} = \hat{I}(X,Y) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The maximum likelihood estimates for a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-mle}) are

\begin{equation} \label{eq:glm-llrmi-glm-mle}
\begin{split}
\hat{B} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} Y \\
\hat{\Sigma} &= \frac{1}{n} (Y - X\hat{B})^\mathrm{T} V^{-1} (Y - X\hat{B}) \; ,
\end{split}
\end{equation}

such that, for the two models, the maximum likelihood estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) are:

\begin{equation} \label{eq:glm-llrmi-m1-m0-mle}
\begin{split}
\hat{\Sigma}_1 &= \frac{1}{n} (Y - X\hat{B})^\mathrm{T} (Y - X\hat{B}) \quad \text{with} \quad
\hat{B} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} Y \quad \text{and} \quad \\
\hat{\Sigma}_0 &= \frac{1}{n} Y^\mathrm{T} Y \; .
\end{split}
\end{equation}

The log-likelihood ratio for two general linear models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-llr}) is

\begin{equation} \label{eq:glm-llrmi-glm-llr}
\ln \Lambda_{12} = - \frac{n}{2} \ln \frac{|\hat{\Sigma}_1|}{|\hat{\Sigma}_2|} \; ,
\end{equation}

such that in the present case, we have:

\begin{equation} \label{eq:glm-llrmi-m1-m0-llr}
\ln \Lambda_{10} = - \frac{n}{2} \ln \frac{|\hat{\Sigma}_1|}{|\hat{\Sigma}_0|} \; .
\end{equation}

The mutual information for the general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-mi}) is

\begin{equation} \label{eq:glm-llrmi-glm-mi}
I(X,Y) = - \frac{n}{2} \ln \frac{|\Sigma_1|}{|\Sigma_0|} \; ,
\end{equation}

such that with \eqref{eq:glm-llrmi-m1-m0-mle}, the estimated mutual information is:

\begin{equation} \label{eq:glm-llrmi-Y-X-mi}
\hat{I}(X,Y) = - \frac{n}{2} \ln \frac{|\hat{\Sigma}_1|}{|\hat{\Sigma}_0|} \; ,
\end{equation}

Together, \eqref{eq:glm-llrmi-m1-m0-llr} and \eqref{eq:glm-llrmi-Y-X-mi} show that

\begin{equation} \label{eq:glm-llrmi-glm-llrmi-qed}
\ln \Lambda_{10} = \hat{I}(X,Y) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Friston K, Chu C, MourÃ£o-Miranda J, Hulme O, Rees G, Penny W, Ashburner J (2008): "Bayesian decoding of brain images"; in: \textit{NeuroImage}, vol. 39, pp. 181-205, eq. 6; URL: \url{https://www.sciencedirect.com/science/article/abs/pii/S1053811907007203}; DOI: 10.1016/j.neuroimage.2007.08.013.
\end{itemize}
\vspace{1em}



\subsection{Transformed general linear model}

\subsubsection[\textit{Definition}]{Definition} \label{sec:tglm}
\setcounter{equation}{0}

\textbf{Definition:} Let there be two general linear models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) of measured data $Y \in \mathbb{R}^{n \times v}$ using design matrices ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) $X \in \mathbb{R}^{n \times p}$ and $X_t \in \mathbb{R}^{n \times t}$

\begin{equation} \label{eq:tglm-glm1}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma)
\end{equation}

\begin{equation} \label{eq:tglm-glm2}
Y = X_t \Gamma + E_t, \; E_t \sim \mathcal{MN}(0, V, \Sigma_t)
\end{equation}

and assume that $X_t$ can be transformed into $X$ using a transformation matrix $T \in \mathbb{R}^{t \times p}$

\begin{equation} \label{eq:tglm-X-Xt-T}
X = X_t \, T
\end{equation}

where $p < t$ and $X$, $X_t$ and $T$ have full ranks $\mathrm{rk}(X) = p$, $\mathrm{rk}(X_t) = t$ and $\mathrm{rk}(T) = p$.

Then, a linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) of the parameter estimates from \eqref{eq:tglm-glm2}, under the assumption of \eqref{eq:tglm-glm1}, is called a transformed general linear model.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Soch J, Allefeld C, Haynes JD (2020): "Inverse transformed encoding models â€“ a solution to the problem of correlated trial-by-trial parameter estimates in fMRI decoding"; in: \textit{NeuroImage}, vol. 209, art. 116449, Appendix A; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811919310407}; DOI: 10.1016/j.neuroimage.2019.116449.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Derivation of the distribution}]{Derivation of the distribution} \label{sec:tglm-dist}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be two general linear models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) of measured data $Y$

\begin{equation} \label{eq:tglm-dist-glm1}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma)
\end{equation}

\begin{equation} \label{eq:tglm-dist-glm2}
Y = X_t \Gamma + E_t, \; E_t \sim \mathcal{MN}(0, V, \Sigma_t)
\end{equation}

and a matrix $T$ transforming $X_t$ into $X$:

\begin{equation} \label{eq:tglm-dist-X-Xt-T}
X = X_t \, T \; .
\end{equation}

Then, the transformed general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tglm}) is given by

\begin{equation} \label{eq:tglm-dist-tglm}
\hat{\Gamma} = T B + H, \; H \sim \mathcal{MN}(0, U, \Sigma)
\end{equation}

where the covariance across rows ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) is $U = ( X_t^\mathrm{T} V^{-1} X_t )^{-1}$.


\vspace{1em}
\textbf{Proof:} The linear transformation theorem for the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-ltt}) states:

\begin{equation} \label{eq:tglm-dist-matn-ltt}
X \sim \mathcal{MN}(M, U, V) \quad \Rightarrow \quad Y = AXB + C \sim \mathcal{MN}(AMB+C, AUA^\mathrm{T}, B^\mathrm{T}VB) \; .
\end{equation}

The weighted least squares parameter estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-wls}) for \eqref{eq:tglm-dist-glm2} are given by

\begin{equation} \label{eq:tglm-dist-glm2-wls}
\hat{\Gamma} = ( X_t^\mathrm{T} V^{-1} X_t )^{-1} X_t^\mathrm{T} V^{-1} Y \; .
\end{equation}

Using \eqref{eq:tglm-dist-glm1} and \eqref{eq:tglm-dist-matn-ltt}, the distribution of $Y$ is

\begin{equation} \label{eq:tglm-dist-Y-dist}
Y \sim \mathcal{MN}(X B, V, \Sigma)
\end{equation}

Combining \eqref{eq:tglm-dist-glm2-wls} with \eqref{eq:tglm-dist-Y-dist}, the distribution of $\hat{\Gamma}$ is

\begin{equation} \label{eq:tglm-dist-G-dist}
\begin{split}
\hat{\Gamma} &\sim \mathcal{MN}\left( \left[ ( X_t^\mathrm{T} V^{-1} X_t )^{-1} X_t^\mathrm{T} V^{-1} \right] X B, \left[ ( X_t^\mathrm{T} V^{-1} X_t )^{-1} X_t^\mathrm{T} V^{-1} \right] V \left[ V^{-1} X_t ( X_t^\mathrm{T} V^{-1} X_t )^{-1} \right], \Sigma \right) \\
&\sim \mathcal{MN}\left( ( X_t^\mathrm{T} V^{-1} X_t )^{-1} X_t^\mathrm{T} V^{-1} X_t \, T B, ( X_t^\mathrm{T} V^{-1} X_t )^{-1} X_t^\mathrm{T} V^{-1} X_t ( X_t^\mathrm{T} V^{-1} X_t )^{-1}, \Sigma \right) \\
&\sim \mathcal{MN}\left( T B, ( X_t^\mathrm{T} V^{-1} X_t )^{-1}, \Sigma \right) \; .
\end{split}
\end{equation}

This can also be written as

\begin{equation} \label{eq:tglm-dist-tglm-qed}
\hat{\Gamma} = T B + H, \; H \sim \mathcal{MN}\left( 0, ( X_t^\mathrm{T} V^{-1} X_t )^{-1}, \Sigma \right)
\end{equation}

which is equivalent to \eqref{eq:tglm-dist-tglm}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Soch J, Allefeld C, Haynes JD (2020): "Inverse transformed encoding models â€“ a solution to the problem of correlated trial-by-trial parameter estimates in fMRI decoding"; in: \textit{NeuroImage}, vol. 209, art. 116449, Appendix A, Theorem 1; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811919310407}; DOI: 10.1016/j.neuroimage.2019.116449.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Equivalence of parameter estimates}]{Equivalence of parameter estimates} \label{sec:tglm-para}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm})

\begin{equation} \label{eq:tglm-para-glm1}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma)
\end{equation}

and the transformed general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tglm})

\begin{equation} \label{eq:tglm-para-tglm}
\hat{\Gamma} = T B + H, \; H \sim \mathcal{MN}(0, U, \Sigma)
\end{equation}

which are linked to each other ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tglm-dist}) via

\begin{equation} \label{eq:tglm-para-glm2-wls}
\hat{\Gamma} = ( X_t^\mathrm{T} V^{-1} X_t )^{-1} X_t^\mathrm{T} V^{-1} Y
\end{equation}

and

\begin{equation} \label{eq:tglm-para-X-Xt-T}
X = X_t \, T \; .
\end{equation}

Then, the parameter estimates for $B$ from \eqref{eq:tglm-para-glm1} and \eqref{eq:tglm-para-tglm} are equivalent.


\vspace{1em}
\textbf{Proof:} The weighted least squares parameter estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-wls}) for \eqref{eq:tglm-para-glm1} are given by

\begin{equation} \label{eq:tglm-para-glm1-wls}
\hat{B} = (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} Y
\end{equation}

and the weighted least squares parameter estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-wls}) for \eqref{eq:tglm-para-tglm} are given by

\begin{equation} \label{eq:tglm-para-tglm-wls}
\hat{B} = (T^\mathrm{T} U^{-1} T)^{-1} T^\mathrm{T} U^{-1} \hat{\Gamma} \; .
\end{equation}

The covariance across rows for the transformed general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tglm-dist}) is equal to

\begin{equation} \label{eq:tglm-para-U}
U = ( X_t^\mathrm{T} V^{-1} X_t )^{-1} \; .
\end{equation}

Applying \eqref{eq:tglm-para-U}, \eqref{eq:tglm-para-X-Xt-T} and \eqref{eq:tglm-para-glm2-wls}, the estimates in \eqref{eq:tglm-para-tglm-wls} can be developed into

\begin{equation} \label{eq:tglm-para-tglm-wls-dev}
\begin{split}
\hat{B} \; &\overset{\eqref{eq:tglm-para-tglm-wls}}{=} ( T^\mathrm{T} \, U^{-1} \, T )^{-1} \, T^\mathrm{T} \, U^{-1} \, \hat{\Gamma} \\
&\overset{\eqref{eq:tglm-para-U}}{=} ( T^\mathrm{T} \left[ X_t^\mathrm{T} V^{-1} X_t \right] T )^{-1} \, T^\mathrm{T} \left[ X_t^\mathrm{T} V^{-1} X_t \right] \hat{\Gamma} \\
&\overset{\eqref{eq:tglm-para-X-Xt-T}}{=} ( X^\mathrm{T} V^{-1} X )^{-1} \, T^\mathrm{T} \, X_t^\mathrm{T} V^{-1} X_t \, \hat{\Gamma} \\
&\overset{\eqref{eq:tglm-para-glm2-wls}}{=} ( X^\mathrm{T} V^{-1} X )^{-1} \, T^\mathrm{T} \, X_t^\mathrm{T} V^{-1} X_t \left[ ( X_t^\mathrm{T} V^{-1} X_t )^{-1} X_t^\mathrm{T} V^{-1} Y \right] \\
&= ( X^\mathrm{T} V^{-1} X )^{-1} \, T^\mathrm{T} \, X_t^\mathrm{T} V^{-1} Y \\
&\overset{\eqref{eq:tglm-para-X-Xt-T}}{=} ( X^\mathrm{T} V^{-1} X )^{-1} X^\mathrm{T} V^{-1} Y
\end{split}
\end{equation}

which is equivalent to the estimates in \eqref{eq:tglm-para-glm1-wls}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Soch J, Allefeld C, Haynes JD (2020): "Inverse transformed encoding models â€“ a solution to the problem of correlated trial-by-trial parameter estimates in fMRI decoding"; in: \textit{NeuroImage}, vol. 209, art. 116449, Appendix A, Theorem 2; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811919310407}; DOI: 10.1016/j.neuroimage.2019.116449.
\end{itemize}
\vspace{1em}



\subsection{Inverse general linear model}

\subsubsection[\textit{Definition}]{Definition} \label{sec:iglm}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) of measured data $Y \in \mathbb{R}^{n \times v}$ in terms of the design matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) $X \in \mathbb{R}^{n \times p}$:

\begin{equation} \label{eq:iglm-glm}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma) \; .
\end{equation}

Then, a linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) of $X$ in terms of $Y$, under the assumption of \eqref{eq:iglm-glm}, is called an inverse general linear model.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Soch J, Allefeld C, Haynes JD (2020): "Inverse transformed encoding models â€“ a solution to the problem of correlated trial-by-trial parameter estimates in fMRI decoding"; in: \textit{NeuroImage}, vol. 209, art. 116449, Appendix C; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811919310407}; DOI: 10.1016/j.neuroimage.2019.116449.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Derivation of the distribution}]{Derivation of the distribution} \label{sec:iglm-dist}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) of $Y \in \mathbb{R}^{n \times v}$

\begin{equation} \label{eq:iglm-dist-glm}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma) \; .
\end{equation}

Then, the inverse general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:iglm}) of $X \in \mathbb{R}^{n \times p}$ is given by

\begin{equation} \label{eq:iglm-dist-iglm}
X = Y W + N, \; N \sim \mathcal{MN}(0, V, \Sigma_x)
\end{equation}

where $W \in \mathbb{R}^{v \times p}$ is a matrix, such that $B \, W = I_p$, and the covariance across columns ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) is $\Sigma_x = W^\mathrm{T} \Sigma W$.


\vspace{1em}
\textbf{Proof:} The linear transformation theorem for the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-ltt}) states:

\begin{equation} \label{eq:iglm-dist-matn-ltt}
X \sim \mathcal{MN}(M, U, V) \quad \Rightarrow \quad Y = AXB + C \sim \mathcal{MN}(AMB+C, AUA^\mathrm{T}, B^\mathrm{T}VB) \; .
\end{equation}

The matrix $W$ exists, if the rows of $B \in \mathbb{R}^{p \times v}$ are linearly independent, such that $\mathrm{rk}(B) = p$. Then, right-multiplying the model \eqref{eq:iglm-dist-glm} with $W$ and applying \eqref{eq:iglm-dist-matn-ltt} yields

\begin{equation} \label{eq:iglm-dist-iglm-s1}
Y W = X B W + E W, \; E W \sim \mathcal{MN}(0, V, W^\mathrm{T} \Sigma W) \; .
\end{equation}

Employing $B \, W = I_p$ and rearranging, we have

\begin{equation} \label{eq:iglm-dist-iglm-s2}
X = Y W - E W, \; E W \sim \mathcal{MN}(0, V, W^\mathrm{T} \Sigma W) \; .
\end{equation}

Substituting $N = - E W$, we get

\begin{equation} \label{eq:iglm-dist-iglm-s3}
X = Y W + N, \; N \sim \mathcal{MN}(0, V, W^\mathrm{T} \Sigma W)
\end{equation}

which is equivalent to \eqref{eq:iglm-dist-iglm}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Soch J, Allefeld C, Haynes JD (2020): "Inverse transformed encoding models â€“ a solution to the problem of correlated trial-by-trial parameter estimates in fMRI decoding"; in: \textit{NeuroImage}, vol. 209, art. 116449, Appendix C, Theorem 4; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811919310407}; DOI: 10.1016/j.neuroimage.2019.116449.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Best linear unbiased estimator}]{Best linear unbiased estimator} \label{sec:iglm-blue}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) of $Y \in \mathbb{R}^{n \times v}$

\begin{equation} \label{eq:iglm-blue-glm}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma)
\end{equation}

implying the inverse general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:iglm-dist}) of $X \in \mathbb{R}^{n \times p}$

\begin{equation} \label{eq:iglm-blue-iglm}
X = Y W + N, \; N \sim \mathcal{MN}(0, V, \Sigma_x) \; .
\end{equation}

where 

\begin{equation} \label{eq:iglm-blue-BW-Sx}
B \, W = I_p \quad \text{and} \quad \Sigma_x = W^\mathrm{T} \Sigma W \; .
\end{equation}

Then, the weighted least squares solution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-wls}) for $W$ is the best linear unbiased estimator of $W$.


\vspace{1em}
\textbf{Proof:} The linear transformation theorem for the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-ltt}) states:

\begin{equation} \label{eq:iglm-blue-matn-ltt}
X \sim \mathcal{MN}(M, U, V) \quad \Rightarrow \quad Y = AXB + C \sim \mathcal{MN}(AMB+C, AUA^\mathrm{T}, B^\mathrm{T}VB) \; .
\end{equation}

The weighted least squares parameter estimates ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-wls}) for \eqref{eq:iglm-blue-iglm} are given by

\begin{equation} \label{eq:iglm-blue-iglm-wls}
\hat{W} = (Y^\mathrm{T} V^{-1} Y)^{-1} Y^\mathrm{T} V^{-1} X \; .
\end{equation}

The best linear unbiased estimator $\hat{\theta}$ of a certain quantity $\theta$ estimated from measured data $y$ is 1) an estimator resulting from a linear operation $f(y)$, 2) whose expected value is equal to $\theta$ and 3) which has, among those satisfying 1) and 2), the minimum variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}).

\vspace{1em}
1) First, $\hat{W}$ is a linear estimator, because it is of the form $\tilde{W} = M X$ where $M$ is an arbitrary $v \times n$ matrix.

\vspace{1em}
2) Second, $\hat{W}$ is an unbiased estimator, if $\left\langle \hat{W} \right\rangle = W$. By applying \eqref{eq:iglm-blue-matn-ltt} to \eqref{eq:iglm-blue-iglm}, the distribution of $\tilde{W}$ is

\begin{equation} \label{eq:iglm-blue-W-hat-dist}
\tilde{W} = M X \sim \mathcal{MN}(M Y W, M V M^T, \Sigma_x) \;
\end{equation}

which requires ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-mean}) that $M Y = I_v$. This is fulfilled by any matrix

\begin{equation} \label{eq:iglm-blue-M-D}
M = (Y^\mathrm{T} V^{-1} Y)^{-1} Y^\mathrm{T} V^{-1} + D
\end{equation}

where $D$ is a $v \times n$ matrix which satisfies $D Y = 0$.

\vspace{1em}
3) Third, the best linear unbiased estimator is the one with minimum variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}), i.e. the one that minimizes the expected Frobenius norm

\begin{equation} \label{eq:iglm-blue-Var-W}
\mathrm{Var}\left( \tilde{W} \right) = \left\langle \mathrm{tr}\left[ (\tilde{W} - W)^\mathrm{T} (\tilde{W} - W) \right] \right\rangle \; .
\end{equation}

Using the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) of $\tilde{W}$ from \eqref{eq:iglm-blue-W-hat-dist}

\begin{equation} \label{eq:iglm-blue-W-hat-W-dist}
\left( \tilde{W} - W \right) \sim \mathcal{MN}(0, M V M^\mathrm{T}, \Sigma_x)
\end{equation}

and the property of the Wishart distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:wish})

\begin{equation} \label{eq:iglm-blue-E-XX}
X \sim \mathcal{MN}(0, U, V) \quad \Rightarrow \quad \left\langle X X^\mathrm{T} \right\rangle = \mathrm{tr}(V) \, U \; ,
\end{equation}

this variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) can be evaluated as a function of $M$:

\begin{equation} \label{eq:iglm-blue-Var-M}
\begin{split}
\mathrm{Var}\left[ \tilde{W}(M) \right] &\overset{\eqref{eq:iglm-blue-Var-W}}{=} \left\langle \mathrm{tr}\left[ (\tilde{W} - W)^\mathrm{T} (\tilde{W} - W) \right] \right\rangle \\
&= \left\langle \mathrm{tr}\left[ (\tilde{W} - W) (\tilde{W} - W)^\mathrm{T} \right] \right\rangle \\
&= \mathrm{tr}\left[ \left\langle (\tilde{W} - W) (\tilde{W} - W)^\mathrm{T} \right\rangle \right] \\
&\overset{\eqref{eq:iglm-blue-E-XX}}{=} \mathrm{tr}\left[ \mathrm{tr}(\Sigma_x) \, M V M^\mathrm{T} \right] \\
&= \mathrm{tr}(\Sigma_x) \; \mathrm{tr}(M V M^\mathrm{T}) \; .
\end{split}
\end{equation}

As a function of $D$ and using $D Y = 0$, it becomes:

\begin{equation} \label{eq:iglm-blue-Var-D}
\begin{split}
\mathrm{Var}\left[ \tilde{W}(D) \right] &\overset{\eqref{eq:iglm-blue-M-D}}{=} \mathrm{tr}(\Sigma_x) \; \mathrm{tr}\!\left[ \left( (Y^\mathrm{T} V^{-1} Y)^{-1} Y^\mathrm{T} V^{-1} + D \right) V \left( (Y^\mathrm{T} V^{-1} Y)^{-1} Y^\mathrm{T} V^{-1} + D \right)^\mathrm{T} \right] \\
&= \mathrm{tr}(\Sigma_x) \; \mathrm{tr}\!\left[ (Y^\mathrm{T} V^{-1} Y)^{-1} \, Y^\mathrm{T} V^{-1} V V^{-1} Y \; (Y^\mathrm{T} V^{-1} Y)^{-1} + \right. \\
&\hphantom{=\mathrm{tr}(\Sigma_x) \; \mathrm{tr}\!\left[\right.} \left. \, (Y^\mathrm{T} V^{-1} Y)^{-1} Y^\mathrm{T} V^{-1} V D^\mathrm{T} + D V V^{-1} Y (Y^\mathrm{T} V^{-1} Y)^{-1} + D V D^\mathrm{T} \right] \\
&= \mathrm{tr}(\Sigma_x) \left[ \mathrm{tr}\!\left( (Y^\mathrm{T} V^{-1} Y)^{-1} \right) + \mathrm{tr}\!\left( D V D^\mathrm{T} \right) \right] \; .
\end{split}
\end{equation}

Since $D V D^\mathrm{T}$ is a positive-semidefinite matrix, all its eigenvalues are non-negative. Because the trace of a square matrix is the sum of its eigenvalues, the mimimum variance is achieved by $D = 0$, thus producing $\hat{W}$ as in \eqref{eq:iglm-blue-iglm-wls}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Soch J, Allefeld C, Haynes JD (2020): "Inverse transformed encoding models â€“ a solution to the problem of correlated trial-by-trial parameter estimates in fMRI decoding"; in: \textit{NeuroImage}, vol. 209, art. 116449, Appendix C, Theorem 5; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811919310407}; DOI: 10.1016/j.neuroimage.2019.116449.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Equivalence of log-likelihood ratios}]{Equivalence of log-likelihood ratios} \label{sec:iglm-llrs}
\setcounter{equation}{0}

\textbf{Theorem:} Consider two general linear models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm})

\begin{equation} \label{eq:iglm-llrs-glms}
\begin{split}
m_1^{(Y)}: \; & Y = X B + E_1, \; E_1 \sim \mathcal{MN}(0, I_n, \Sigma_1^{(Y)}) \\
m_0^{(Y)}: \; & Y =       E_0, \; E_0 \sim \mathcal{MN}(0, I_n, \Sigma_0^{(Y)})
\end{split}
\end{equation}

and two inverse general linear models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:iglm})

\begin{equation} \label{eq:iglm-llrs-iglms}
\begin{split}
m_1^{(X)}: \; & X = Y W + N_1, \; N_1 \sim \mathcal{MN}(0, I_n, \Sigma_1^{(X)}) \\
m_0^{(X)}: \; & X =       N_0, \; N_0 \sim \mathcal{MN}(0, I_n, \Sigma_0^{(X)})
\end{split}
\end{equation}

where $Y \in \mathbb{R}^{n \times v}$ and $X \in \mathbb{R}^{n \times p}$ are data matrices, such that $n > v$ and $n > p$. Then, the log-likelihood ratio ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llr}) comparing the forward models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) is equivalent to the log-likelihood ratio ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llr}) comparing the backward models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:iglm}):

\begin{equation} \label{eq:iglm-llrs-iglm-llreq}
\ln \Lambda_{10}^{(Y)} = \ln \Lambda_{10}^{(X)} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The maximum likelihood estimates for the general linear models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-mle}) are

\begin{equation} \label{eq:iglm-llrs-glms-mle}
\begin{split}
\hat{\Sigma}_1^{(Y)} &= \frac{1}{n} (Y - X\hat{B})^\mathrm{T} (Y - X\hat{B}) \quad \text{with} \quad
\hat{B}               = (X^\mathrm{T} X)^{-1} X^\mathrm{T} Y \quad \text{and} \quad \\
\hat{\Sigma}_0^{(Y)} &= \frac{1}{n} Y^\mathrm{T} Y
\end{split}
\end{equation}

as well as

\begin{equation} \label{eq:iglm-llrs-iglms-mle}
\begin{split}
\hat{\Sigma}_1^{(X)} &= \frac{1}{n} (X - Y\hat{W})^\mathrm{T} (X - Y\hat{W}) \quad \text{with} \quad
\hat{W}               = (Y^\mathrm{T} Y)^{-1} Y^\mathrm{T} X \quad \text{and} \quad \\
\hat{\Sigma}_0^{(X)} &= \frac{1}{n} X^\mathrm{T} X \; .
\end{split}
\end{equation}

The likelihood ratio for two general linear models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-llr}) $m_1$ and $m_2$ is:

\begin{equation} \label{eq:iglm-llrs-glm-llr}
\begin{split}
\ln \Lambda_{12}
&= - \frac{n}{2} \ln \frac{|\hat{\Sigma}_1|}{|\hat{\Sigma}_2|} \\
&= - \frac{n}{2} \ln \left( |\hat{\Sigma}_2^{-1}| |\hat{\Sigma}_1| \right) \\
&= - \frac{n}{2} \ln |\hat{\Sigma}_2^{-1} \hat{\Sigma}_1| \; .
\end{split}
\end{equation}

Thus, with \eqref{eq:iglm-llrs-glms-mle}, the log-likelihood ratio ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llr}) of $m_1^{(Y)}$ vs. $m_0^{(Y)}$ is given as

\begin{equation} \label{eq:iglm-llrs-glms-llr}
\begin{split}
\ln \Lambda_Y = \ln \Lambda_{10}^{(Y)}
&\overset{\eqref{eq:iglm-llrs-glm-llr}}{=} - \frac{n}{2} \ln \left| \left( \hat{\Sigma}_0^{(Y)} \right)^{-1} \hat{\Sigma}_1^{(Y)} \right| \\
&\overset{\eqref{eq:iglm-llrs-glms-mle}}{=} - \frac{n}{2} \ln \left| \left( \frac{1}{n} Y^\mathrm{T} Y \right)^{-1} \frac{1}{n} (Y - X \hat{B})^\mathrm{T} (Y - X \hat{B}) \right| \\
&= - \frac{n}{2} \ln \left| \left( Y^\mathrm{T} Y \right)^{-1} \left( Y^\mathrm{T} Y - 2 Y^\mathrm{T} X \hat{B} + \hat{B}^\mathrm{T} X^\mathrm{T} X \hat{B} \right) \right| \\
&= - \frac{n}{2} \ln \left| \left( (Y^\mathrm{T} Y)^{-1} Y^\mathrm{T} Y - 2 (Y^\mathrm{T} Y)^{-1} Y^\mathrm{T} X \hat{B} + (Y^\mathrm{T} Y)^{-1} \hat{B}^\mathrm{T} X^\mathrm{T} X \hat{B} \right) \right| \\
&\overset{\eqref{eq:iglm-llrs-glms-mle}}{=} - \frac{n}{2} \ln \left| I_v - 2 \hat{W} \hat{B} + (Y^\mathrm{T} Y)^{-1} Y^\mathrm{T} X (X^\mathrm{T} X)^{-1} X^\mathrm{T} X (X^\mathrm{T} X)^{-1} X^\mathrm{T} Y \right| \\
&= - \frac{n}{2} \ln \left| I_v - 2 \hat{W} \hat{B} + (Y^\mathrm{T} Y)^{-1} Y^\mathrm{T} X (X^\mathrm{T} X)^{-1} X^\mathrm{T} Y \right| \\
&= - \frac{n}{2} \ln \left| I_v - 2 \hat{W} \hat{B} + \hat{W} \hat{B} \right| \\
&= - \frac{n}{2} \ln \left| I_v - \hat{W} \hat{B} \right| \; .
\end{split}
\end{equation}

Similarly, with \eqref{eq:iglm-llrs-iglms-mle}, the log-likelihood ratio ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llr}) of $m_1^{(X)}$ vs. $m_0^{(X)}$ becomes

\begin{equation} \label{eq:iglm-llrs-iglms-llr}
\begin{split}
\ln \Lambda_X = \ln \Lambda_{10}^{(X)}
&\overset{\eqref{eq:iglm-llrs-glm-llr}}{=} - \frac{n}{2} \ln \left| \left( \hat{\Sigma}_0^{(X)} \right)^{-1} \hat{\Sigma}_1^{(X)} \right| \\
&\overset{\eqref{eq:iglm-llrs-iglms-mle}}{=} - \frac{n}{2} \ln \left| \left( \frac{1}{n} X^\mathrm{T} X \right)^{-1} \frac{1}{n} (X - Y \hat{W})^\mathrm{T} (X - Y \hat{W}) \right| \\
&= - \frac{n}{2} \ln \left| \left( X^\mathrm{T} X \right)^{-1} \left( X^\mathrm{T} X - 2 X^\mathrm{T} Y \hat{W} + \hat{W}^\mathrm{T} Y^\mathrm{T} Y \hat{W} \right) \right| \\
&= - \frac{n}{2} \ln \left| \left( (X^\mathrm{T} X)^{-1} X^\mathrm{T} X - 2 (X^\mathrm{T} X)^{-1} X^\mathrm{T} Y \hat{W} + (X^\mathrm{T} X)^{-1} \hat{W}^\mathrm{T} Y^\mathrm{T} Y \hat{W} \right) \right| \\
&\overset{\eqref{eq:iglm-llrs-iglms-mle}}{=} - \frac{n}{2} \ln \left| I_p - 2 \hat{B} \hat{W} + (X^\mathrm{T} X)^{-1} X^\mathrm{T} Y (Y^\mathrm{T} Y)^{-1} Y^\mathrm{T} Y (Y^\mathrm{T} Y)^{-1} Y^\mathrm{T} X \right| \\
&= - \frac{n}{2} \ln \left| I_p - 2 \hat{B} \hat{W} + (X^\mathrm{T} X)^{-1} X^\mathrm{T} Y (Y^\mathrm{T} Y)^{-1} Y^\mathrm{T} X \right| \\
&= - \frac{n}{2} \ln \left| I_p - 2 \hat{B} \hat{W} + \hat{B} \hat{W} \right| \\
&= - \frac{n}{2} \ln \left| I_p - \hat{B} \hat{W} \right| \; .
\end{split}
\end{equation}

Sylvester's determinant theorem (also known as the "Weinsteinâ€“Aronszajn identity") states that, for two matrices $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times m}$, the following identity holds:

\begin{equation} \label{eq:iglm-llrs-sdt}
\left| I_m + AB \right| = \left| I_n + BA \right| \; .
\end{equation}

Since $\hat{B} \in \mathbb{R}^{p \times v}$ and $(-\hat{W}) \in \mathbb{R}^{v \times p}$, it follows that

\begin{equation} \label{eq:iglm-llrs-sdt-BW}
\left| I_p - \hat{B} \hat{W} \right| = \left| I_v - \hat{W} \hat{B} \right|
\end{equation}

and thus, we finally have:

\begin{equation} \label{eq:iglm-llrs-iglm-llreq-qed}
  \ln \Lambda_Y
= - \frac{n}{2} \ln \left| I_v - \hat{W} \hat{B} \right|
= - \frac{n}{2} \ln \left| I_p - \hat{B} \hat{W} \right|
= \ln \Lambda_X \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Friston K, Chu C, MourÃ£o-Miranda J, Hulme O, Rees G, Penny W, Ashburner J (2008): "Bayesian decoding of brain images"; in: \textit{NeuroImage}, vol. 39, pp. 181-205, p. 183; URL: \url{https://www.sciencedirect.com/science/article/abs/pii/S1053811907007203}; DOI: 10.1016/j.neuroimage.2007.08.013.
\item Wikipedia (2024): "Weinsteinâ€“Aronszajn identity"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2024-06-28; URL: \url{https://en.wikipedia.org/wiki/Weinstein%E2%80%93Aronszajn_identity}.
\end{itemize}
\vspace{1em}



\subsubsection[\textit{Corresponding forward model}]{Corresponding forward model} \label{sec:cfm}
\setcounter{equation}{0}

\textbf{Definition:} Let there be observations $Y \in \mathbb{R}^{n \times v}$ and $X \in \mathbb{R}^{n \times p}$ and consider a weight matrix $W = f(Y,X) \in \mathbb{R}^{v \times p}$ estimated from $Y$ and $X$, such that right-multiplying $Y$ with the weight matrix gives an estimate or prediction of $X$:

\begin{equation} \label{eq:cfm-bda}
\hat{X} = Y W \; .
\end{equation}

Given that the columns of $\hat{X}$ are linearly independent, then

\begin{equation} \label{eq:cfm-cfm}
Y = \hat{X} A^\mathrm{T} + E \quad \text{with} \quad \hat{X}^\mathrm{T} E = 0
\end{equation}

is called the corresponding forward model relative to the weight matrix $W$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Haufe S, Meinecke F, GÃ¶rgen K, DÃ¤hne S, Haynes JD, Blankertz B, BieÃŸmann F (2014): "On the interpretation of weight vectors of linear models in multivariate neuroimaging"; in: \textit{NeuroImage}, vol. 87, pp. 96â€“110, eq. 3; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811913010914}; DOI: 10.1016/j.neuroimage.2013.10.067.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Derivation of parameters}]{Derivation of parameters} \label{sec:cfm-para}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be observations $Y \in \mathbb{R}^{n \times v}$ and $X \in \mathbb{R}^{n \times p}$ and consider a weight matrix $W = f(Y,X) \in \mathbb{R}^{v \times p}$ predicting $X$ from $Y$:

\begin{equation} \label{eq:cfm-para-bda}
\hat{X} = Y W \; .
\end{equation}

Then, the parameter matrix of the corresponding forward model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:cfm}) is equal to

\begin{equation} \label{eq:cfm-para-cfm-para}
A = \Sigma_y W \Sigma_x^{-1}
\end{equation}

with the "sample covariances ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cov-samp})"

\begin{equation} \label{eq:cfm-para-Sx-Sy}
\begin{split}
\Sigma_x &= \hat{X}^\mathrm{T} \hat{X} \\
\Sigma_y &= Y^\mathrm{T} Y \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} The corresponding forward model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:cfm}) is given by

\begin{equation} \label{eq:cfm-para-cfm}
Y = \hat{X} A^\mathrm{T} + E \; ,
\end{equation}

subject to the constraint that predicted $X$ and errors $E$ are uncorrelated:

\begin{equation} \label{eq:cfm-para-cfm-con}
\hat{X}^\mathrm{T} E = 0 \; .
\end{equation}

With that, we can directly derive the parameter matrix $A$:

\begin{equation} \label{eq:cfm-para-cfm-para-qed}
\begin{split}
Y &\overset{\eqref{eq:cfm-para-cfm}}{=} \hat{X} A^\mathrm{T} + E \\
\hat{X} A^\mathrm{T} &= Y - E \\
\hat{X}^\mathrm{T} \hat{X} A^\mathrm{T} &= \hat{X}^\mathrm{T} (Y - E) \\
\hat{X}^\mathrm{T} \hat{X} A^\mathrm{T} &= \hat{X}^\mathrm{T} Y - \hat{X}^\mathrm{T} E \\
\hat{X}^\mathrm{T} \hat{X} A^\mathrm{T} &\overset{\eqref{eq:cfm-para-cfm-con}}{=} \hat{X}^\mathrm{T} Y \\
\hat{X}^\mathrm{T} \hat{X} A^\mathrm{T} &\overset{\eqref{eq:cfm-para-bda}}{=} W^\mathrm{T} Y^\mathrm{T} Y \\
\Sigma_x A^\mathrm{T} &\overset{\eqref{eq:cfm-para-Sx-Sy}}{=} W^\mathrm{T} \Sigma_y \\
A^\mathrm{T} &= \Sigma_x^{-1} W^\mathrm{T} \Sigma_y \\
A &= \Sigma_y W \Sigma_x^{-1} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Haufe S, Meinecke F, GÃ¶rgen K, DÃ¤hne S, Haynes JD, Blankertz B, BieÃŸmann F (2014): "On the interpretation of weight vectors of linear models in multivariate neuroimaging"; in: \textit{NeuroImage}, vol. 87, pp. 96â€“110, Theorem 1; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811913010914}; DOI: 10.1016/j.neuroimage.2013.10.067.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Proof of existence}]{Proof of existence} \label{sec:cfm-exist}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be observations $Y \in \mathbb{R}^{n \times v}$ and $X \in \mathbb{R}^{n \times p}$ and consider a weight matrix $W = f(Y,X) \in \mathbb{R}^{v \times p}$ predicting $X$ from $Y$:

\begin{equation} \label{eq:cfm-exist-bda}
\hat{X} = Y W \; .
\end{equation}

Then, there exists a corresponding forward model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:cfm}).


\vspace{1em}
\textbf{Proof:} The corresponding forward model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:cfm}) is defined as

\begin{equation} \label{eq:cfm-exist-cfm}
Y = \hat{X} A^\mathrm{T} + E \quad \text{with} \quad \hat{X}^\mathrm{T} E = 0
\end{equation}

and the parameters of the corresponding forward model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:cfm-para}) are equal to

\begin{equation} \label{eq:cfm-exist-cfm-para}
A = \Sigma_y W \Sigma_x^{-1} \quad \text{where} \quad \Sigma_x = \hat{X}^\mathrm{T} \hat{X} \quad \text{and} \quad \Sigma_y = Y^\mathrm{T} Y \; .
\end{equation}

\vspace{1em}
1) Because the columns of $\hat{X}$ are assumed to be linearly independent by definition of the corresponding forward model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:cfm}), the matrix $\Sigma_x = \hat{X}^\mathrm{T} \hat{X}$ is invertible, such that $A$ in \eqref{eq:cfm-exist-cfm-para} is well-defined.

\vspace{1em}
2) Moreover, the solution for the matrix $A$ satisfies the constraint of the corresponding forward model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:cfm}) for predicted $X$ and errors $E$ to be uncorrelated which can be shown as follows:

\begin{equation} \label{eq:cfm-exist-X-E-0}
\begin{split}
\hat{X}^\mathrm{T} E &\overset{\eqref{eq:cfm-exist-cfm}}{=} \hat{X}^\mathrm{T} \left( Y - \hat{X} A^\mathrm{T} \right) \\
&\overset{\eqref{eq:cfm-exist-cfm-para}}{=} \hat{X}^\mathrm{T} \left( Y - \hat{X} \, \Sigma_x^{-1} W^\mathrm{T} \Sigma_y \right) \\
&= \hat{X}^\mathrm{T} Y - \hat{X}^\mathrm{T} \hat{X} \, \Sigma_x^{-1} W^\mathrm{T} \Sigma_y \\
&\overset{\eqref{eq:cfm-exist-cfm-para}}{=} \hat{X}^\mathrm{T} Y - \hat{X}^\mathrm{T} \hat{X} \left( \hat{X}^\mathrm{T} \hat{X} \right)^{-1} W^\mathrm{T} \left( Y^\mathrm{T} Y \right) \\
% &= \hat{X}^\mathrm{T} Y - W^\mathrm{T} \left( Y^\mathrm{T} Y \right) \\
&\overset{\eqref{eq:cfm-exist-bda}}{=} (Y W)^\mathrm{T} Y - W^\mathrm{T} \left( Y^\mathrm{T} Y \right) \\
&= W^\mathrm{T} Y^\mathrm{T} Y - W^\mathrm{T} Y^\mathrm{T} Y \\
&= 0 \; .
\end{split}
\end{equation}

This completes the proof.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Haufe S, Meinecke F, GÃ¶rgen K, DÃ¤hne S, Haynes JD, Blankertz B, BieÃŸmann F (2014): "On the interpretation of weight vectors of linear models in multivariate neuroimaging"; in: \textit{NeuroImage}, vol. 87, pp. 96â€“110, Appendix B; URL: \url{https://www.sciencedirect.com/science/article/pii/S1053811913010914}; DOI: 10.1016/j.neuroimage.2013.10.067.
\end{itemize}
\vspace{1em}



\subsection{Multivariate Bayesian linear regression}

\subsubsection[\textbf{Conjugate prior distribution}]{Conjugate prior distribution} \label{sec:mblr-prior}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:mblr-prior-GLM}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma)
\end{equation}

be a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) with measured $n \times v$ data matrix $Y$, known $n \times p$ design matrix $X$, known $n \times n$ covariance structure ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) $V$ as well as unknown $p \times v$ regression coefficients $B$ and unknown $v \times v$ noise covariance ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) $\Sigma$.

Then, the conjugate prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-conj}) for this model is a normal-Wishart distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:nw})

\begin{equation} \label{eq:mblr-prior-GLM-NW-prior}
p(B,T) = \mathcal{MN}(B; M_0, \Lambda_0^{-1}, T^{-1}) \cdot \mathcal{W}(T; \Omega_0^{-1}, \nu_0)
\end{equation}

where $T = \Sigma^{-1}$ is the inverse noise covariance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:covmat}) or noise precision matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:precmat}).


\vspace{1em}
\textbf{Proof:} By definition, a conjugate prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-conj}) is a prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) that, when combined with the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}), leads to a posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) that belongs to the same family of probability distributions ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist}). This is fulfilled when the prior density and the likelihood function are proportional to the model parameters in the same way, i.e. the model parameters appear in the same functional form in both.

Equation \eqref{eq:mblr-prior-GLM} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:mblr-prior-GLM-LF-Class}
p(Y|B,\Sigma) = \mathcal{MN}(Y; X B, V, \Sigma) = \sqrt{\frac{1}{(2 \pi)^{nv} |\Sigma|^n |V|^v}} \, \exp\left[ -\frac{1}{2} \mathrm{tr}\left( \Sigma^{-1} (Y-XB)^\mathrm{T} V^{-1} (Y-XB) \right) \right]
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:mblr-prior-GLM-LF-Bayes}
p(Y|B,T) = \mathcal{MN}(Y; X B, P^{-1}, T^{-1}) = \sqrt{\frac{|T|^n |P|^v}{(2 \pi)^{nv}}} \, \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T (Y-XB)^\mathrm{T} P (Y-XB) \right) \right]
\end{equation}

using the $v \times v$ precision matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:precmat}) $T = \Sigma^{-1}$ and the $n \times n$ precision matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:precmat}) $P = V^{-1}$.

\vspace{1em}
Seperating constant and variable terms, we have:

\begin{equation} \label{eq:mblr-prior-GLM-LF-s1}
p(Y|B,T) = \sqrt{\frac{|P|^v}{(2 \pi)^{nv}}} \cdot |T|^{n/2} \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T (Y-XB)^\mathrm{T} P (Y-XB) \right) \right] \; .
\end{equation}

Expanding the product in the exponent, we have:

\begin{equation} \label{eq:mblr-prior-GLM-LF-s2}
p(Y|B,T) = \sqrt{\frac{|P|^v}{(2 \pi)^{nv}}} \cdot |T|^{n/2} \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T \left[ Y^\mathrm{T} P Y - Y^\mathrm{T} P X B - B^\mathrm{T} X^\mathrm{T} P Y + B^\mathrm{T} X^\mathrm{T} P X B \right] \right) \right] \; .
\end{equation}

Completing the square over $B$, finally gives

\begin{equation} \label{eq:mblr-prior-GLM-LF-s3}
p(Y|B,T) = \sqrt{\frac{|P|^v}{(2 \pi)^{nv}}} \cdot |T|^{n/2} \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T \left[ (B - \tilde{X}Y)^\mathrm{T} X^\mathrm{T} P X (B - \tilde{X}Y) - Y^\mathrm{T} Q Y + Y^\mathrm{T} P Y \right] \right) \right]
\end{equation}

where $\tilde{X} = \left( X^\mathrm{T} P X \right)^{-1} X^\mathrm{T} P$ and $Q = \tilde{X}^\mathrm{T} \left( X^\mathrm{T} P X \right) \tilde{X}$.

\vspace{1em}
In other words, the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) is proportional to a power of the determinant of $T$, times an exponential of the trace of $T$ and an exponential of the trace of a squared form of $B$, weighted by $T$:

\begin{equation} \label{eq:mblr-prior-GLM-LF-s4}
p(Y|B,T) \propto |T|^{n/2} \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T \left[ Y^\mathrm{T} P Y - Y^\mathrm{T} Q Y \right] \right) \right] \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T \left[ (B - \tilde{X}Y)^\mathrm{T} X^\mathrm{T} P X (B - \tilde{X}Y) \right] \right) \right] \; .
\end{equation}

The same is true for a normal-Wishart distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:nw}) over $B$ and $T$

\begin{equation} \label{eq:mblr-prior-MBLR-prior-s1}
p(B,T) = \mathcal{MN}(B; M_0, \Lambda_0^{-1}, T^{-1}) \cdot \mathcal{W}(T; \Omega_0^{-1}, \nu_0)
\end{equation}

the probability density function of which ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:nw-pdf})

\begin{equation} \label{eq:mblr-prior-MBLR-prior-s2}
p(B,T) = \sqrt{\frac{|T|^p |\Lambda_0|^v}{(2 \pi)^{pv}}} \, \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T (B-M_0)^\mathrm{T} \Lambda_0 (B-M_0) \right) \right] \cdot \frac{1}{\Gamma_v \left( \frac{\nu_0}{2} \right)} \sqrt{\frac{|\Omega_0|^{\nu_0}}{2^{\nu_0 v}}} |T|^{(\nu_0-v-1)/2} \exp\left[ -\frac{1}{2} \mathrm{tr}\left( \Omega_0 T \right) \right]
\end{equation}

exhibits the same proportionality

\begin{equation} \label{eq:mblr-prior-MBLR-prior-s3}
p(B,T) \propto |T|^{(\nu_0+p-v-1)/2} \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T \Omega_0 \right) \right] \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T \left[ (B-M_0)^\mathrm{T} \Lambda_0 (B-M_0) \right] \right) \right]
\end{equation}

and is therefore conjugate relative to the likelihood.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Bayesian multivariate linear regression"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-09-03; URL: \url{https://en.wikipedia.org/wiki/Bayesian_multivariate_linear_regression#Conjugate_prior_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Posterior distribution}]{Posterior distribution} \label{sec:mblr-post}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:mblr-post-GLM}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma)
\end{equation}

be a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) with measured $n \times v$ data matrix $Y$, known $n \times p$ design matrix $X$, known $n \times n$ covariance structure ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) $V$ as well as unknown $p \times v$ regression coefficients $B$ and unknown $v \times v$ noise covariance ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) $\Sigma$. Moreover, assume a normal-Wishart prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mblr-prior}) over the model parameters $B$ and $T = \Sigma^{-1}$:

\begin{equation} \label{eq:mblr-post-GLM-NW-prior}
p(B,T) = \mathcal{MN}(B; M_0, \Lambda_0^{-1}, T^{-1}) \cdot \mathcal{W}(T; \Omega_0^{-1}, \nu_0) \; .
\end{equation}

Then, the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) is also a normal-Wishart distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:nw})

\begin{equation} \label{eq:mblr-post-GLM-NW-post}
p(B,T|Y) = \mathcal{MN}(B; M_n, \Lambda_n^{-1}, T^{-1}) \cdot \mathcal{W}(T; \Omega_n^{-1}, \nu_n)
\end{equation}

and the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:mblr-post-GLM-NW-post-par}
\begin{split}
M_n &= \Lambda_n^{-1} (X^\mathrm{T} P Y + \Lambda_0 M_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \\
\Omega_n &= \Omega_0 + Y^\mathrm{T} P Y + M_0^\mathrm{T} \Lambda_0 M_0 - M_n^\mathrm{T} \Lambda_n M_n \\
\nu_n &= \nu_0 + n \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} According to Bayes' theorem ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:bayes-th}), the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) is given by

\begin{equation} \label{eq:mblr-post-GLM-NG-BT}
p(B,T|Y) = \frac{p(Y|B,T) \, p(B,T)}{p(Y)} \; .
\end{equation}

Since $p(Y)$ is just a normalization factor, the posterior is proportional ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl}) to the numerator:

\begin{equation} \label{eq:mblr-post-GLM-NG-post-JL}
p(B,T|Y) \propto p(Y|B,T) \, p(B,T) = p(Y,B,T) \; .
\end{equation}

Equation \eqref{eq:mblr-post-GLM} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:mblr-post-GLM-LF-Class}
p(Y|B,\Sigma) = \mathcal{MN}(Y; X B, V, \Sigma) = \sqrt{\frac{1}{(2 \pi)^{nv} |\Sigma|^n |V|^v}} \, \exp\left[ -\frac{1}{2} \mathrm{tr}\left( \Sigma^{-1} (Y-XB)^\mathrm{T} V^{-1} (Y-XB) \right) \right]
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:mblr-post-GLM-LF-Bayes}
p(Y|B,T) = \mathcal{MN}(Y; X B, P, T^{-1}) = \sqrt{\frac{|T|^n |P|^v}{(2 \pi)^{nv}}} \, \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T (Y-XB)^\mathrm{T} P (Y-XB) \right) \right]
\end{equation}

using the $v \times v$ precision matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:precmat}) $T = \Sigma^{-1}$ and the $n \times n$ precision matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:precmat}) $P = V^{-1}$.

\vspace{1em}
Combining the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) \eqref{eq:mblr-post-GLM-LF-Bayes} with the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) \eqref{eq:mblr-post-GLM-NW-prior}, the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) of the model is given by

\begin{equation} \label{eq:mblr-post-GLM-NW-JL-s1}
\begin{split}
p(Y,B,T) = \; & p(Y|B,T) \, p(B,T) \\
= \; & \sqrt{\frac{|T|^n |P|^v}{(2 \pi)^{nv}}} \, \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T (Y-XB)^\mathrm{T} P (Y-XB) \right) \right] \cdot \\
& \sqrt{\frac{|T|^p |\Lambda_0|^v}{(2 \pi)^{pv}}} \, \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T (B-M_0)^\mathrm{T} \Lambda_0 (B-M_0) \right) \right] \cdot \\
& \frac{1}{\Gamma_v \left( \frac{\nu_0}{2} \right)} \sqrt{\frac{|\Omega_0|^{\nu_0}}{2^{\nu_0 v}}} |T|^{(\nu_0-v-1)/2} \exp\left[ -\frac{1}{2} \mathrm{tr}\left( \Omega_0 T \right) \right] \; .
\end{split}
\end{equation}

Collecting identical variables gives:

\begin{equation} \label{eq:mblr-post-GLM-NW-JL-s2}
\begin{split}
p(Y,B,T) = \; & \sqrt{\frac{|T|^n |P|^v}{(2 \pi)^{nv}}} \sqrt{\frac{|T|^p |\Lambda_0|^v}{(2 \pi)^{pv}}} \sqrt{\frac{|\Omega_0|^{\nu_0}}{2^{\nu_0 v}}} \frac{1}{\Gamma_v \left( \frac{\nu_0}{2} \right)} \cdot |T|^{(\nu_0-v-1)/2} \exp\left[ -\frac{1}{2} \mathrm{tr}\left( \Omega_0 T \right) \right] \cdot \\
& \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T \left[ (Y-XB)^\mathrm{T} P (Y-XB) + (B-M_0)^\mathrm{T} \Lambda_0 (B-M_0) \right] \right) \right] \; .
\end{split}
\end{equation}

Expanding the products in the exponent gives:

\begin{equation} \label{eq:mblr-post-GLM-NW-JL-s3}
\begin{split}
p(Y,B,T) = \; & \sqrt{\frac{|T|^n |P|^v}{(2 \pi)^{nv}}} \sqrt{\frac{|T|^p |\Lambda_0|^v}{(2 \pi)^{pv}}} \sqrt{\frac{|\Omega_0|^{\nu_0}}{2^{\nu_0 v}}} \frac{1}{\Gamma_v \left( \frac{\nu_0}{2} \right)} \cdot |T|^{(\nu_0-v-1)/2} \exp\left[ -\frac{1}{2} \mathrm{tr}\left( \Omega_0 T \right) \right] \cdot \\
& \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T \left[ Y^\mathrm{T} P Y - Y^\mathrm{T} P X B - B^\mathrm{T} X^\mathrm{T} P Y + B^\mathrm{T} X^\mathrm{T} P X B + \right. \right. \right. \\
& \hphantom{\exp\left[ -\frac{1}{2} \mathrm{tr}\left( T \left[ \right. \right. \right. \!\!\!} \; \left. \left. \left. B^\mathrm{T} \Lambda_0 B - B^\mathrm{T} \Lambda_0 M_0 - M_0^\mathrm{T} \Lambda_0 B + M_0^\mathrm{T} \Lambda_0 \mu_0 \right] \right) \right] \; .
\end{split}
\end{equation}

Completing the square over $B$, we finally have

\begin{equation} \label{eq:mblr-post-GLM-NW-JL-s4}
\begin{split}
p(Y,B,T) = \; & \sqrt{\frac{|T|^n |P|^v}{(2 \pi)^{nv}}} \sqrt{\frac{|T|^p |\Lambda_0|^v}{(2 \pi)^{pv}}} \sqrt{\frac{|\Omega_0|^{\nu_0}}{2^{\nu_0 v}}} \frac{1}{\Gamma_v \left( \frac{\nu_0}{2} \right)} \cdot |T|^{(\nu_0-v-1)/2} \exp\left[ -\frac{1}{2} \mathrm{tr}\left( \Omega_0 T \right) \right] \cdot \\
& \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T \left[ (B-M_n)^\mathrm{T} \Lambda_n (B-M_n) + (Y^\mathrm{T} P Y + M_0^\mathrm{T} \Lambda_0 M_0 - M_n^\mathrm{T} \Lambda_n M_n) \right] \right) \right] \; .
\end{split}
\end{equation}

with the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post})

\begin{equation} \label{eq:mblr-post-GLM-NW-post-B-par}
\begin{split}
M_n &= \Lambda_n^{-1} (X^\mathrm{T} P Y + \Lambda_0 M_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \; .
\end{split}
\end{equation}

Ergo, the joint likelihood is proportional to

\begin{equation} \label{eq:mblr-post-GLM-NW-JL-s5}
p(Y,B,T) \propto |T|^{p/2} \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T \left[ (B-M_n)^\mathrm{T} \Lambda_n (B-M_n) \right] \right) \right] \cdot |T|^{(\nu_n-v-1)/2} \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( \Omega_n T \right) \right]
\end{equation}

with the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post})

\begin{equation} \label{eq:mblr-post-GLM-NW-post-T-par}
\begin{split}
\Omega_n &= \Omega_0 + Y^\mathrm{T} P Y + M_0^\mathrm{T} \Lambda_0 M_0 - M_n^\mathrm{T} \Lambda_n M_n \\
\nu_n &= \nu_0 + n \; .
\end{split}
\end{equation}

From the term in \eqref{eq:mblr-post-GLM-NW-JL-s5}, we can isolate the posterior distribution over $B$ given $T$:

\begin{equation} \label{eq:mblr-post-GLM-NW-post-B}
p(B|T,Y) = \mathcal{MN}(B; M_n, \Lambda_n^{-1}, T^{-1}) \; .
\end{equation}

From the remaining term, we can isolate the posterior distribution over $T$:

\begin{equation} \label{eq:mblr-post-GLM-NW-post-T}
p(T|Y) = \mathcal{W}(T; \Omega_n^{-1}, \nu_n) \; .
\end{equation}

Together, \eqref{eq:mblr-post-GLM-NW-post-B} and \eqref{eq:mblr-post-GLM-NW-post-T} constitute the joint ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-joint}) posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) of $B$ and $T$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Bayesian multivariate linear regression"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-09-03; URL: \url{https://en.wikipedia.org/wiki/Bayesian_multivariate_linear_regression#Posterior_distribution}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Log model evidence}]{Log model evidence} \label{sec:mblr-lme}
\setcounter{equation}{0}

\textbf{Theorem:} Let

\begin{equation} \label{eq:mblr-lme-GLM}
Y = X B + E, \; E \sim \mathcal{MN}(0, V, \Sigma)
\end{equation}

be a general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm}) with measured $n \times v$ data matrix $Y$, known $n \times p$ design matrix $X$, known $n \times n$ covariance structure ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) $V$ as well as unknown $p \times v$ regression coefficients $B$ and unknown $v \times v$ noise covariance ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn}) $\Sigma$. Moreover, assume a normal-Wishart prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mblr-prior}) over the model parameters $B$ and $T = \Sigma^{-1}$:

\begin{equation} \label{eq:mblr-lme-GLM-NW-prior}
p(B,T) = \mathcal{MN}(B; M_0, \Lambda_0^{-1}, T^{-1}) \cdot \mathcal{W}(T; \Omega_0^{-1}, \nu_0) \; .
\end{equation}

Then, the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) for this model is

\begin{equation} \label{eq:mblr-lme-GLM-NW-LME}
\begin{split}
\log p(Y|m) = & \frac{v}{2} \log |P| - \frac{nv}{2} \log (2 \pi)  + \frac{v}{2} \log |\Lambda_0| - \frac{v}{2} \log |\Lambda_n| + \\
& \frac{\nu_0}{2} \log\left| \frac{1}{2} \Omega_0 \right| - \frac{\nu_n}{2} \log\left| \frac{1}{2} \Omega_n \right| + \log \Gamma_v \left( \frac{\nu_n}{2} \right) - \log \Gamma_v \left( \frac{\nu_0}{2} \right)
\end{split}
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:mblr-lme-GLM-NW-post-par}
\begin{split}
M_n &= \Lambda_n^{-1} (X^\mathrm{T} P Y + \Lambda_0 M_0) \\
\Lambda_n &= X^\mathrm{T} P X + \Lambda_0 \\
\Omega_n &= \Omega_0 + Y^\mathrm{T} P Y + M_0^\mathrm{T} \Lambda_0 M_0 - M_n^\mathrm{T} \Lambda_n M_n \\
\nu_n &= \nu_0 + n \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} According to the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), the model evidence ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml}) for this model is:

\begin{equation} \label{eq:mblr-lme-GLM-NW-ME-s1}
p(Y|m) = \iint p(Y|B,T) \, p(B,T) \, \mathrm{d}B \, \mathrm{d}T \; .
\end{equation}

According to the law of conditional probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-cond}), the integrand is equivalent to the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}):

\begin{equation} \label{eq:mblr-lme-GLM-NW-ME-s2}
p(Y|m) = \iint p(Y,B,T) \, \mathrm{d}B \, \mathrm{d}T \; .
\end{equation}

Equation \eqref{eq:mblr-lme-GLM} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf})

\begin{equation} \label{eq:mblr-lme-GLM-LF-Class}
p(Y|B,\Sigma) = \mathcal{MN}(Y; X B, V, \Sigma) = \sqrt{\frac{1}{(2 \pi)^{nv} |\Sigma|^n |V|^v}} \, \exp\left[ -\frac{1}{2} \mathrm{tr}\left( \Sigma^{-1} (Y-XB)^\mathrm{T} V^{-1} (Y-XB) \right) \right]
\end{equation}

which, for mathematical convenience, can also be parametrized as

\begin{equation} \label{eq:mblr-lme-GLM-LF-Bayes}
p(Y|B,T) = \mathcal{MN}(Y; X B, P, T^{-1}) = \sqrt{\frac{|T|^n |P|^v}{(2 \pi)^{nv}}} \, \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T (Y-XB)^\mathrm{T} P (Y-XB) \right) \right]
\end{equation}

using the $v \times v$ precision matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:precmat}) $T = \Sigma^{-1}$ and the $n \times n$ precision matrix ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:precmat}) $P = V^{-1}$.

\vspace{1em}
When deriving the posterior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mblr-post}) $p(B,T|Y)$, the joint likelihood $p(Y,B,T)$ is obtained as

\begin{equation} \label{eq:mblr-lme-GLM-NW-LME-s1}
\begin{split}
p(Y,B,T) = \; & \sqrt{\frac{|T|^n |P|^v}{(2 \pi)^{nv}}} \sqrt{\frac{|T|^p |\Lambda_0|^v}{(2 \pi)^{pv}}} \sqrt{\frac{|\Omega_0|^{\nu_0}}{2^{\nu_0 v}}} \frac{1}{\Gamma_v \left( \frac{\nu_0}{2} \right)} \cdot |T|^{(\nu_0-v-1)/2} \exp\left[ -\frac{1}{2} \mathrm{tr}\left( \Omega_0 T \right) \right] \cdot \\
& \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T \left[ (B-M_n)^\mathrm{T} \Lambda_n (B-M_n) + (Y^\mathrm{T} P Y + M_0^\mathrm{T} \Lambda_0 M_0 - M_n^\mathrm{T} \Lambda_n M_n) \right] \right) \right] \; .
\end{split}
\end{equation}

Using the probability density function of the matrix-normal distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:matn-pdf}), we can rewrite this as

\begin{equation} \label{eq:mblr-lme-GLM-NW-LME-s2}
\begin{split}
p(Y,B,T) = \; & \sqrt{\frac{|T|^n |P|^v}{(2 \pi)^{nv}}} \sqrt{\frac{|T|^p |\Lambda_0|^v}{(2 \pi)^{pv}}} \sqrt{\frac{(2 \pi)^{pv}}{|T|^p |\Lambda_n|^v}} \sqrt{\frac{|\Omega_0|^{\nu_0}}{2^{\nu_0 v}}} \frac{1}{\Gamma_v \left( \frac{\nu_0}{2} \right)} \cdot |T|^{(\nu_0-v-1)/2} \exp\left[ -\frac{1}{2} \mathrm{tr}\left( \Omega_0 T \right) \right] \cdot \\
& \mathcal{MN}(B; M_n, \Lambda_n^{-1}, T^{-1}) \cdot \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T \left[ Y^\mathrm{T} P Y + M_0^\mathrm{T} \Lambda_0 M_0 - M_n^\mathrm{T} \Lambda_n M_n \right] \right) \right] \; .
\end{split}
\end{equation}

Now, $B$ can be integrated out easily:

\begin{equation} \label{eq:mblr-lme-GLM-NW-LME-s3}
\begin{split}
\int p(Y,B,T) \, \mathrm{d}B = \; & \sqrt{\frac{|T|^n |P|^v}{(2 \pi)^{nv}}} \sqrt{\frac{|\Lambda_0|^v}{|\Lambda_n|^v}} \sqrt{\frac{|\Omega_0|^{\nu_0}}{2^{\nu_0 v}}} \frac{1}{\Gamma_v \left( \frac{\nu_0}{2} \right)} \cdot |T|^{(\nu_0-v-1)/2} \cdot \\
& \exp\left[ -\frac{1}{2} \mathrm{tr}\left( T \left[ \Omega_0 + Y^\mathrm{T} P Y + M_0^\mathrm{T} \Lambda_0 M_0 - M_n^\mathrm{T} \Lambda_n M_n \right] \right) \right] \; .
\end{split}
\end{equation}

Using the probability density function of the Wishart distribution, we can rewrite this as

\begin{equation} \label{eq:mblr-lme-GLM-NW-LME-s4}
\int p(Y,B,T) \, \mathrm{d}B = \sqrt{\frac{|P|^v}{(2 \pi)^{nv}}} \sqrt{\frac{|\Lambda_0|^v}{|\Lambda_n|^v}} \sqrt{\frac{|\Omega_0|^{\nu_0}}{2^{\nu_0 v}}} \sqrt{\frac{2^{\nu_n v}}{|\Omega_n|^{\nu_n}}} \, \frac{\Gamma_v \left( \frac{\nu_n}{2} \right)}{\Gamma_v \left( \frac{\nu_0}{2} \right)} \cdot \mathcal{W}(T; \Omega_n^{-1}, \nu_n) \; .
\end{equation}

Finally, $T$ can also be integrated out:

\begin{equation} \label{eq:mblr-lme-GLM-NW-LME-s5}
\iint p(Y,B,T) \, \mathrm{d}B \, \mathrm{d}T = \sqrt{\frac{|P|^v}{(2 \pi)^{nv}}} \sqrt{\frac{|\Lambda_0|^v}{|\Lambda_n|^v}} \sqrt{\frac{\left| \frac{1}{2} \Omega_0 \right|^{\nu_0}}{\left| \frac{1}{2} \Omega_n \right|^{\nu_n}}} \, \frac{\Gamma_v \left( \frac{\nu_n}{2} \right)}{\Gamma_v \left( \frac{\nu_0}{2} \right)} = p(y|m) \; .
\end{equation}

Thus, the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) of this model is given by

\begin{equation} \label{eq:mblr-lme-GLM-NW-LME-s6}
\begin{split}
\log p(Y|m) = & \frac{v}{2} \log |P| - \frac{nv}{2} \log (2 \pi)  + \frac{v}{2} \log |\Lambda_0| - \frac{v}{2} \log |\Lambda_n| + \\
& \frac{\nu_0}{2} \log\left| \frac{1}{2} \Omega_0 \right| - \frac{\nu_n}{2} \log\left| \frac{1}{2} \Omega_n \right| + \log \Gamma_v \left( \frac{\nu_n}{2} \right) - \log \Gamma_v \left( \frac{\nu_0}{2} \right) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\pagebreak
\section{Count data}

\subsection{Binomial observations}

\subsubsection[\textit{Definition}]{Definition} \label{sec:bin-data}
\setcounter{equation}{0}

\textbf{Definition:} An ordered pair $(n,y)$ with $n \in \mathbb{N}$ and $y \in \mathbb{N}_0$, where $y$ is the number of successes in $n$ trials, constitutes a set of binomial observations.


\subsubsection[\textbf{Binomial test}]{Binomial test} \label{sec:bin-test}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y$ be the number of successes resulting from $n$ independent trials with unknown success probability $p$, such that $y$ follows a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-test-Bin}
y \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Then, the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})

\begin{equation} \label{eq:bin-test-bin-test-h0}
H_0: \; p = p_0
\end{equation}

is rejected ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}) at significance level ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:alpha}) $\alpha$, if

\begin{equation} \label{eq:bin-test-bin-test-rej}
y \leq c_1 \quad \text{or} \quad y \geq c_2
\end{equation}

where $c_1$ is the largest integer value, such that

\begin{equation} \label{eq:bin-test-bin-test-c1}
\sum_{x=0}^{c_1} \mathrm{Bin}(x; n, p_0) \leq \frac{\alpha}{2} \; ,
\end{equation}

and $c_2$ is the smallest integer value, such that

\begin{equation} \label{eq:bin-test-bin-test-c2}
\sum_{x=c_2}^{n} \mathrm{Bin}(x; n, p_0) \leq \frac{\alpha}{2} \; ,
\end{equation}

where $\mathrm{Bin}(x; n, p)$ is the probability mass function of the binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-pmf}):

\begin{equation} \label{eq:bin-test-bin-pmf}
\mathrm{Bin}(x; n, p) = {n \choose x} \, p^x \, (1-p)^{n-x} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The alternative hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}) relative to $H_0$ for a two-sided test ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test-tail}) is

\begin{equation} \label{eq:bin-test-bin-test-h1}
H_1: \; p \neq p_0 \; .
\end{equation}

We can use $y$ as a test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat}). Its sampling distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-samp}) is given by \eqref{eq:bin-test-Bin}. The cumulative distribution function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cdf}) (CDF) of the test statistic under the null hypothesis is thus equal to the cumulative distribution function of a binomial distribution with success probability ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) $p_0$:

\begin{equation} \label{eq:bin-test-y-cdf}
\mathrm{Pr}(y \leq z \vert H_0) = \sum_{x=0}^{z} \mathrm{Bin}(x; n, p_0) = \sum_{x=0}^{z} {n \choose x} \, p_0^x \, (1-p_0)^{n-x} \; .
\end{equation}

The critical value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cval}) is the value of $y$, such that the probability of observing this or more extreme values of the test statistic is equal to or smaller than $\alpha$. Since $H_0$ and $H_1$ define a two-tailed test, we need two critical values $y_1$ and $y_2$ that satisfy

\begin{equation} \label{eq:bin-test-y-cvals}
\begin{split}
\alpha &\geq \mathrm{Pr}(y \in \left\lbrace 0, \ldots, y_1 \right\rbrace \cup \left\lbrace y_2, \ldots, n \right\rbrace \vert H_0) \\
&= \mathrm{Pr}(y \leq y_1 \vert H_0) + \mathrm{Pr}(y \geq y_2 \vert H_0) \\
&= \mathrm{Pr}(y \leq y_1 \vert H_0) + (1-\mathrm{Pr}(y \leq (y_2-1) \vert H_0) \; .
\end{split}
\end{equation}

Given the test statistic's CDF in \eqref{eq:bin-test-y-cdf}, this is fulfilled by the values $c_1$ and $c_2$ defined in \eqref{eq:bin-test-bin-test-c1} and \eqref{eq:bin-test-bin-test-c2}. Thus, the null hypothesis $H_0$ can be rejected ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cval}), if the observed test statistic is inside the rejection region ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}):

\begin{equation} \label{eq:bin-test-bin-test-rej-qed}
y \in \left\lbrace 0, \ldots, c_1 \right\rbrace \cup \left\lbrace c_2, \ldots, n \right\rbrace \; .
\end{equation}

This is equivalent to \eqref{eq:bin-test-bin-test-rej} and thus completes the proof.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2023): "Binomial test"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2023-12-16; URL: \url{https://en.wikipedia.org/wiki/Binomial_test#Usage}.
\item Wikipedia (2023): "Binomialtest"; in: \textit{Wikipedia â€“ Die freie EnzyklopÃ¤die}, retrieved on 2023-12-16; URL: \url{https://de.wikipedia.org/wiki/Binomialtest#Signifikanzniveau_und_kritische_Werte}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Maximum likelihood estimation}]{Maximum likelihood estimation} \label{sec:bin-mle}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y$ be the number of successes resulting from $n$ independent trials with unknown success probability $p$, such that $y$ follows a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-mle-Bin}
y \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Then, the maximum likelihood estimator ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) of $p$ is

\begin{equation} \label{eq:bin-mle-Bin-MLE}
\hat{p} = \frac{y}{n} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-pmf}), equation \eqref{eq:bin-mle-Bin} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}):

\begin{equation} \label{eq:bin-mle-Bin-LF}
\begin{split}
\mathrm{p}(y|p) &= \mathrm{Bin}(y; n, p) \\
&= {n \choose y} \, p^y \, (1-p)^{n-y} \; .
\end{split}
\end{equation}

Thus, the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) is given by

\begin{equation} \label{eq:bin-mle-Bin-LL}
\begin{split}
\mathrm{LL}(p) &= \log \mathrm{p}(y|p) \\
&= \log {n \choose y} + y \log p + (n-y) \log (1-p) \; .
\end{split}
\end{equation}

The derivative of the log-likelihood function \eqref{eq:bin-mle-Bin-LL} with respect to $p$ is

\begin{equation} \label{eq:bin-mle-dLL-dp}
\frac{\mathrm{d}\mathrm{LL}(p)}{\mathrm{d}p} = \frac{y}{p} - \frac{n-y}{1-p}
\end{equation}

and setting this derivative to zero gives the MLE for $p$:

\begin{equation} \label{eq:bin-mle-p-MLE}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(p)}{\mathrm{d}\hat{p}} &= 0 \\
0 &= \frac{y}{\hat{p}} - \frac{n-y}{1-\hat{p}} \\
\frac{n-y}{1-\hat{p}} &= \frac{y}{\hat{p}} \\
(n-y) \, \hat{p} &= y \, (1-\hat{p}) \\
n \, \hat{p} - y \, \hat{p} &= y - y \, \hat{p} \\
n \, \hat{p} &= y \\
\hat{p} &= \frac{y}{n} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2022): "Binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-11-23; URL: \url{https://en.wikipedia.org/wiki/Binomial_distribution#Statistical_inference}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Maximum log-likelihood}]{Maximum log-likelihood} \label{sec:bin-mll}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y$ be the number of successes resulting from $n$ independent trials with unknown success probability $p$, such that $y$ follows a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-mll-Bin}
y \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Then, the maximum log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}) for this model is

\begin{equation} \label{eq:bin-mll-Bin-MLL}
\begin{split}
\mathrm{MLL} &= \log \Gamma(n+1) - \log \Gamma(y+1) - \log \Gamma(n-y+1) \\
&- n \log (n) + y \log (y) + (n-y) \log (n-y) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} The log-likelihood function for binomial data ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-mle}) is given by

\begin{equation} \label{eq:bin-mll-Bin-LL}
\mathrm{LL}(p) = \log {n \choose y} + y \log p + (n-y) \log (1-p)
\end{equation}

and the maximum likelihood estimate of the success probability ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-mle}) $p$ is

\begin{equation} \label{eq:bin-mll-Bin-MLE}
\hat{p} = \frac{y}{n} \; .
\end{equation}

Plugging \eqref{eq:bin-mll-Bin-MLE} into \eqref{eq:bin-mll-Bin-LL}, we obtain the maximum log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}) of the binomial observation model in \eqref{eq:bin-mll-Bin} as

\begin{equation} \label{eq:bin-mll-Bin-MLL-s1}
\begin{split}
\mathrm{MLL} &= \mathrm{LL}(\hat{p}) \\
&= \log {n \choose y} + y \log \left( \frac{y}{n} \right) + (n-y) \log \left( 1 - \frac{y}{n} \right) \\
&= \log {n \choose y} + y \log \left( \frac{y}{n} \right) + (n-y) \log \left( \frac{n-y}{n} \right) \\
&= \log {n \choose y} + y \log (y) + (n-y) \log (n-y) - n \log (n) \; .
\end{split}
\end{equation}

With the definition of the binomial coefficient

\begin{equation} \label{eq:bin-mll-bin-coeff}
{n \choose k} = \frac{n!}{k! \, (n-k)!}
\end{equation}

and the definition of the gamma function

\begin{equation} \label{eq:bin-mll-gam-fct}
\Gamma(n) = (n-1)! \; ,
\end{equation}

the MLL finally becomes

\begin{equation} \label{eq:bin-mll-Bin-MLL-s2}
\begin{split}
\mathrm{MLL} &= \log \Gamma(n+1) - \log \Gamma(y+1) - \log \Gamma(n-y+1) \\
&- n \log (n) + y \log (y) + (n-y) \log (n-y) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Maximum-a-posteriori estimation}]{Maximum-a-posteriori estimation} \label{sec:bin-map}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y$ be the number of successes resulting from $n$ independent trials with unknown success probability $p$, such that $y$ follows a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-map-Bin}
y \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Moreover, assume a beta prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-prior}) over the model parameter $p$:

\begin{equation} \label{eq:bin-map-Bin-prior}
\mathrm{p}(p) = \mathrm{Bet}(p; \alpha_0, \beta_0) \; .
\end{equation}

Then, the maximum-a-posteriori estimate ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:map}) of $p$ is

\begin{equation} \label{eq:bin-map-Bin-MAP}
\hat{p}_\mathrm{MAP} = \frac{\alpha_0+y-1}{\alpha_0+\beta_0+n-2} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Given the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) in \eqref{eq:bin-map-Bin-prior}, the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) for binomial observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-data}) is also a beta distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-post})

\begin{equation} \label{eq:bin-map-Bin-post}
\mathrm{p}(p|y) = \mathrm{Bet}(p; \alpha_n, \beta_n)
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are equal to

\begin{equation} \label{eq:bin-map-Bin-post-par}
\begin{split}
\alpha_n &= \alpha_0 + y \\
\beta_n &= \beta_0 + (n-y) \; .
\end{split}
\end{equation}

The mode of the beta distribution is given by:

\begin{equation} \label{eq:bin-map-Beta-mode}
X \sim \mathrm{Bet}(\alpha, \beta) \quad \Rightarrow \quad \mathrm{mode}(X) = \frac{\alpha-1}{\alpha+\beta-2} \; .
\end{equation}

Applying \eqref{eq:bin-map-Beta-mode} to \eqref{eq:bin-map-Bin-post} with \eqref{eq:bin-map-Bin-post-par}, the maximum-a-posteriori estimate ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:map}) of $p$ follows as:

\begin{equation} \label{eq:bin-map-Bin-MAP-qed}
\begin{split}
\hat{p}_\mathrm{MAP} &= \frac{\alpha_n-1}{\alpha_n+\beta_n-2} \\
&\overset{\eqref{eq:bin-map-Bin-post-par}}{=} \frac{\alpha_0+y-1}{\alpha_0+y+\beta_0+(n-y)-2} \\
&= \frac{\alpha_0+y-1}{\alpha_0+\beta_0+n-2} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Conjugate prior distribution}]{Conjugate prior distribution} \label{sec:bin-prior}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y$ be the number of successes resulting from $n$ independent trials with unknown success probability $p$, such that $y$ follows a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-prior-Bin}
y \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Then, the conjugate prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-conj}) for the model parameter $p$ is a beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta}):

\begin{equation} \label{eq:bin-prior-Beta}
\mathrm{p}(p) = \mathrm{Bet}(p; \alpha_0, \beta_0) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-pmf}), the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) implied by \eqref{eq:bin-prior-Bin} is given by

\begin{equation} \label{eq:bin-prior-Bin-LF}
\mathrm{p}(y|p) = {n \choose y} \, p^y \, (1-p)^{n-y} \; .
\end{equation}

In other words, the likelihood function is proportional to a power of $p$ times a power of $(1-p)$:

\begin{equation} \label{eq:bin-prior-Bin-LF-prop}
\mathrm{p}(y|p) \propto p^y \, (1-p)^{n-y} \; .
\end{equation}

The same is true for a beta distribution over $p$

\begin{equation} \label{eq:bin-prior-Bin-prior-s1}
\mathrm{p}(p) = \mathrm{Bet}(p; \alpha_0, \beta_0)
\end{equation}

the probability density function of which ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-pdf})

\begin{equation} \label{eq:bin-prior-Bin-prior-s2}
\mathrm{p}(p) = \frac{1}{B(\alpha_0,\beta_0)} \, p^{\alpha_0-1} \, (1-p)^{\beta_0-1}
\end{equation}

exhibits the same proportionality

\begin{equation} \label{eq:bin-prior-Bin-prior-s3}
\mathrm{p}(p) \propto p^{\alpha_0-1} \, (1-p)^{\beta_0-1}
\end{equation}

and is therefore conjugate relative to the likelihood.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-23; URL: \url{https://en.wikipedia.org/wiki/Binomial_distribution#Estimation_of_parameters}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Posterior distribution}]{Posterior distribution} \label{sec:bin-post}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y$ be the number of successes resulting from $n$ independent trials with unknown success probability $p$, such that $y$ follows a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-post-Bin}
y \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Moreover, assume a beta prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-prior}) over the model parameter $p$:

\begin{equation} \label{eq:bin-post-Bin-prior}
\mathrm{p}(p) = \mathrm{Bet}(p; \alpha_0, \beta_0) \; .
\end{equation}

Then, the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) is also a beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta})

\begin{equation} \label{eq:bin-post-Bin-post}
\mathrm{p}(p|y) = \mathrm{Bet}(p; \alpha_n, \beta_n) \; .
\end{equation}

and the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:bin-post-Bin-post-par}
\begin{split}
\alpha_n &= \alpha_0 + y \\
\beta_n &= \beta_0 + (n-y) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-pmf}), the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) implied by \eqref{eq:bin-post-Bin} is given by

\begin{equation} \label{eq:bin-post-Bin-LF}
\mathrm{p}(y|p) = {n \choose y} \, p^y \, (1-p)^{n-y} \; .
\end{equation}

Combining the likelihood function \eqref{eq:bin-post-Bin-LF} with the prior distribution \eqref{eq:bin-post-Bin-prior}, the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) of the model is given by

\begin{equation} \label{eq:bin-post-Bin-JL}
\begin{split}
\mathrm{p}(y,p) &= \mathrm{p}(y|p) \, \mathrm{p}(p) \\
&= {n \choose y} \, p^y \, (1-p)^{n-y} \cdot \frac{1}{B(\alpha_0,\beta_0)} \, p^{\alpha_0-1} \, (1-p)^{\beta_0-1} \\
&= \frac{1}{B(\alpha_0,\beta_0)} {n \choose y} \, p^{\alpha_0+y-1} \, (1-p)^{\beta_0+(n-y)-1} \; .
\end{split}
\end{equation}

Note that the posterior distribution is proportional to the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl}):

\begin{equation} \label{eq:bin-post-Bin-post-s1}
\mathrm{p}(p|y) \propto \mathrm{p}(y,p) \; .
\end{equation}

Setting $\alpha_n = \alpha_0 + y$ and $\beta_n = \beta_0 + (n-y)$, the posterior distribution is therefore proportional to

\begin{equation} \label{eq:bin-post-Bin-post-s2}
\mathrm{p}(p|y) \propto p^{\alpha_n-1} \, (1-p)^{\beta_n-1}
\end{equation}

which, when normalized to one, results in the probability density function of the beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-pdf}):

\begin{equation} \label{eq:bin-post-Bin-post-qed}
\mathrm{p}(p|y) = \frac{1}{B(\alpha_n,\beta_n)} \, p^{\alpha_n-1} \, (1-p)^{\beta_n-1} = \mathrm{Bet}(p; \alpha_n, \beta_n) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-23; URL: \url{https://en.wikipedia.org/wiki/Binomial_distribution#Estimation_of_parameters}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Log model evidence}]{Log model evidence} \label{sec:bin-lme}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y$ be the number of successes resulting from $n$ independent trials with unknown success probability $p$, such that $y$ follows a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-lme-Bin}
y \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Moreover, assume a beta prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-prior}) over the model parameter $p$:

\begin{equation} \label{eq:bin-lme-Bin-prior}
\mathrm{p}(p) = \mathrm{Bet}(p; \alpha_0, \beta_0) \; .
\end{equation}

Then, the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) for this model is

\begin{equation} \label{eq:bin-lme-Bin-LME}
\begin{split}
\log \mathrm{p}(y|m) = \log \Gamma(n+1) &- \log \Gamma(k+1) - \log \Gamma(n-k+1) \\
&+ \log B(\alpha_n,\beta_n) - \log B(\alpha_0,\beta_0) \; .
\end{split}
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:bin-lme-Bin-post-par}
\begin{split}
\alpha_n &= \alpha_0 + y \\
\beta_n &= \beta_0 + (n-y) \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin-pmf}), the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) implied by \eqref{eq:bin-lme-Bin} is given by

\begin{equation} \label{eq:bin-lme-Bin-LF}
\mathrm{p}(y|p) = {n \choose y} \, p^y \, (1-p)^{n-y} \; .
\end{equation}

Combining the likelihood function \eqref{eq:bin-lme-Bin-LF} with the prior distribution \eqref{eq:bin-lme-Bin-prior}, the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) of the model is given by

\begin{equation} \label{eq:bin-lme-Bin-JL-s1}
\begin{split}
\mathrm{p}(y,p) &= \mathrm{p}(y|p) \, \mathrm{p}(p) \\
&= {n \choose y} \, p^y \, (1-p)^{n-y} \cdot \frac{1}{B(\alpha_0,\beta_0)} \, p^{\alpha_0-1} \, (1-p)^{\beta_0-1} \\
&= {n \choose y} \, \frac{1}{B(\alpha_0,\beta_0)} \, p^{\alpha_0+y-1} \, (1-p)^{\beta_0+(n-y)-1} \; .
\end{split}
\end{equation}

Note that the model evidence is the marginal density of the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml}):

\begin{equation} \label{eq:bin-lme-Bin-ME-s1}
\mathrm{p}(y) = \int \mathrm{p}(y,p) \, \mathrm{d}p \; .
\end{equation}

Setting $\alpha_n = \alpha_0 + y$ and $\beta_n = \beta_0 + (n-y)$, the joint likelihood can also be written as

\begin{equation} \label{eq:bin-lme-Bin-JL-s2}
\mathrm{p}(y,p) = {n \choose y} \, \frac{1}{B(\alpha_0,\beta_0)} \, \frac{B(\alpha_n,\beta_n)}{1} \, \frac{1}{B(\alpha_n,\beta_n)} \, p^{\alpha_n-1} \, (1-p)^{\beta_n-1} \; .
\end{equation}

Using the probability density function of the beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-pdf}), $p$ can now be integrated out easily

\begin{equation} \label{eq:bin-lme-Bin-ME-s2}
\begin{split}
\mathrm{p}(y) &= \int {n \choose y} \, \frac{1}{B(\alpha_0,\beta_0)} \, \frac{B(\alpha_n,\beta_n)}{1} \, \frac{1}{B(\alpha_n,\beta_n)} \, p^{\alpha_n-1} \, (1-p)^{\beta_n-1} \, \mathrm{d}p \\
&= {n \choose y} \, \frac{B(\alpha_n,\beta_n)}{B(\alpha_0,\beta_0)} \int \mathrm{Bet}(p; \alpha_n, \beta_n) \, \mathrm{d}p \\
&= {n \choose y} \, \frac{B(\alpha_n,\beta_n)}{B(\alpha_0,\beta_0)} \; ,
\end{split}
\end{equation}

such that the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) (LME) is shown to be

\begin{equation} \label{eq:bin-lme-Bin-LME-s1}
\log \mathrm{p}(y|m) = \log {n \choose y} + \log B(\alpha_n,\beta_n) - \log B(\alpha_0,\beta_0) \; .
\end{equation}

With the definition of the binomial coefficient

\begin{equation} \label{eq:bin-lme-bin-coeff}
{n \choose k} = \frac{n!}{k! \, (n-k)!}
\end{equation}

and the definition of the gamma function

\begin{equation} \label{eq:bin-lme-gam-fct}
\Gamma(n) = (n-1)! \; ,
\end{equation}

the LME finally becomes

\begin{equation} \label{eq:bin-lme-Bin-LME-s2}
\begin{split}
\log \mathrm{p}(y|m) = \log \Gamma(n+1) &- \log \Gamma(y+1) - \log \Gamma(n-y+1) \\
&+ \log B(\alpha_n,\beta_n) - \log B(\alpha_0,\beta_0) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Beta-binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-24; URL: \url{https://en.wikipedia.org/wiki/Beta-binomial_distribution#Motivation_and_derivation}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Log Bayes factor}]{Log Bayes factor} \label{sec:bin-lbf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y$ be the number of successes resulting from $n$ independent trials with unknown success probability $p$, such that $y$ follows a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-lbf-Bin}
y \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Moreover, assume two statistical models ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fpm}), one assuming that $p$ is 0.5 (null model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})), the other imposing a beta distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-prior}) as the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) on the model parameter $p$ (alternative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1})):

\begin{equation} \label{eq:bin-lbf-Bin-m01}
\begin{split}
m_0&: \; y \sim \mathrm{Bin}(n,p), \; p = 0.5 \\
m_1&: \; y \sim \mathrm{Bin}(n,p), \; p \sim \mathrm{Bet}(\alpha_0, \beta_0) \; .
\end{split}
\end{equation}

Then, the log Bayes factor ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf}) in favor of $m_1$ against $m_0$ is

\begin{equation} \label{eq:bin-lbf-Bin-LBF}
\mathrm{LBF}_{10} = \log B(\alpha_n,\beta_n) - \log B(\alpha_0,\beta_0) - n \log \left( \frac{1}{2} \right)
\end{equation}

where $B(x,y)$ is the beta function and $\alpha_n$ and $\beta_n$ are the posterior hyperparameters for binomial observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-post}) which are functions of the number of trials ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) $n$ and the number of successes ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) $y$.


\vspace{1em}
\textbf{Proof:} The log Bayes factor is equal to the difference of two log model evidences ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf-lme}):

\begin{equation} \label{eq:bin-lbf-LBF-LME}
\mathrm{LBF}_{12} = \mathrm{LME}(m_1) - \mathrm{LME}(m_2) \; .
\end{equation}

The LME of the alternative $m_1$ is equal to the log model evidence for binomial observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-lme}):

\begin{equation} \label{eq:bin-lbf-Bin-LME-m1}
\mathrm{LME}(m_1) = \log p(y|m_1) = \log {n \choose y} + \log B(\alpha_n,\beta_n) - \log B(\alpha_0,\beta_0) \; .
\end{equation}

Because the null model $m_0$ has no free parameter, its log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) (logarithmized marginal likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml})) is equal to the log-likelihood function for binomial observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-mle}) at the value $p = 0.5$:

\begin{equation} \label{eq:bin-lbf-Bin-LME-m0}
\begin{split}
\mathrm{LME}(m_0) = \log p(y|p=0.5) &= \log {n \choose y} + y \log(0.5) + (n-y) \log (1-0.5)  \\
&= \log {n \choose y} + n \log \left( \frac{1}{2} \right) \; .
\end{split}
\end{equation}

Subtracting the two LMEs from each other, the LBF emerges as

\begin{equation} \label{eq:bin-lbf-Bin-LBF-m10}
\mathrm{LBF}_{10} = \log B(\alpha_n,\beta_n) - \log B(\alpha_0,\beta_0) - n \log \left( \frac{1}{2} \right)
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-post})

\begin{equation} \label{eq:bin-lbf-Bin-post-par}
\begin{split}
\alpha_n &= \alpha_0 + y \\
\beta_n &= \beta_0 + (n-y)
\end{split}
\end{equation}

with the number of trials ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) $n$ and the number of successes ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) $y$.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Posterior probability}]{Posterior probability} \label{sec:bin-pp}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y$ be the number of successes resulting from $n$ independent trials with unknown success probability $p$, such that $y$ follows a binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}):

\begin{equation} \label{eq:bin-pp-Bin}
y \sim \mathrm{Bin}(n,p) \; .
\end{equation}

Moreover, assume two statistical models ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fpm}), one assuming that $p$ is 0.5 (null model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})), the other imposing a beta distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-prior}) as the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) on the model parameter $p$ (alternative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1})):

\begin{equation} \label{eq:bin-pp-Bin-m01}
\begin{split}
m_0&: \; y \sim \mathrm{Bin}(n,p), \; p = 0.5 \\
m_1&: \; y \sim \mathrm{Bin}(n,p), \; p \sim \mathrm{Bet}(\alpha_0, \beta_0) \; .
\end{split}
\end{equation}

Then, the posterior probability ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:pmp}) of the alternative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}) is given by

\begin{equation} \label{eq:bin-pp-Bin-PP1}
p(m_1|y) = \frac{1}{1 + 2^{-n} \left[ B(\alpha_0,\beta_0) / B(\alpha_n,\beta_n) \right]}
\end{equation}

where $B(x,y)$ is the beta function and $\alpha_n$ and $\beta_n$ are the posterior hyperparameters for binomial observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-post}) which are functions of the number of trials ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) $n$ and the number of successes ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) $y$.


\vspace{1em}
\textbf{Proof:} The posterior probability for one of two models is a function of the log Bayes factor in favor of this model ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:pmp-lbf}):

\begin{equation} \label{eq:bin-pp-PP-LBF}
p(m_1|y) = \frac{\exp(\mathrm{LBF}_{12})}{\exp(\mathrm{LBF}_{12}) + 1} \; .
\end{equation}

The log Bayes factor in favor of the alternative model for binomial observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-lbf}) is given by

\begin{equation} \label{eq:bin-pp-Bin-LBF10}
\mathrm{LBF}_{10} = \log B(\alpha_n,\beta_n) - \log B(\alpha_0,\beta_0) - n \log \left( \frac{1}{2} \right) \; .
\end{equation}

and the corresponding Bayes factor ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:bf}), i.e. exponentiated log Bayes factor ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf-der}), is equal to

\begin{equation} \label{eq:bin-pp-Bin-BF10}
\mathrm{BF}_{10} = \exp(\mathrm{LBF}_{10}) = 2^n \cdot \frac{B(\alpha_n,\beta_n)}{B(\alpha_0,\beta_0)} \; .
\end{equation}

Thus, the posterior probability of the alternative, assuming a prior distribution over the probability $p$, compared to the null model, assuming a fixed probability $p = 0.5$, follows as

\begin{equation} \label{eq:bin-pp-Bin-PP1-qed}
\begin{split}
p(m_1|y) &\overset{\eqref{eq:bin-pp-PP-LBF}}{=} \frac{\exp(\mathrm{LBF}_{10})}{\exp(\mathrm{LBF}_{10}) + 1} \\
&\overset{\eqref{eq:bin-pp-Bin-BF10}}{=} \frac{2^n \cdot \frac{B(\alpha_n,\beta_n)}{B(\alpha_0,\beta_0)}}{2^n \cdot \frac{B(\alpha_n,\beta_n)}{B(\alpha_0,\beta_0)} + 1} \\
&= \frac{2^n \cdot \frac{B(\alpha_n,\beta_n)}{B(\alpha_0,\beta_0)}}{2^n \cdot \frac{B(\alpha_n,\beta_n)}{B(\alpha_0,\beta_0)} \left( 1 + 2^{-n} \frac{B(\alpha_0,\beta_0)}{B(\alpha_n,\beta_n)} \right)} \\
&= \frac{1}{1 + 2^{-n} \left[ B(\alpha_0,\beta_0) / B(\alpha_n,\beta_n) \right]}
\end{split}
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-post})

\begin{equation} \label{eq:bin-pp-Bin-post-par}
\begin{split}
\alpha_n &= \alpha_0 + y \\
\beta_n &= \beta_0 + (n-y)
\end{split}
\end{equation}

with the number of trials ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) $n$ and the number of successes ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:bin}) $y$.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{Multinomial observations}

\subsubsection[\textit{Definition}]{Definition} \label{sec:mult-data}
\setcounter{equation}{0}

\textbf{Definition:} An ordered pair $(n,y)$ with $n \in \mathbb{N}$ and $y = \left[ y_1, \ldots, y_k \right] \in \mathbb{N}_0^{1 \times k}$, where $y_i$ is the number of observations for the $i$-th out of $k$ categories obtained in $n$ trials, $i = 1, \ldots, k$, constitutes a set of multinomial observations.


\subsubsection[\textbf{Multinomial test}]{Multinomial test} \label{sec:mult-test}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = [y_1, \ldots, y_k]$ be the number of observations in $k$ categories resulting from $n$ independent trials with unknown category probabilities $p = [p_1, \ldots, p_k]$, such that $y$ follows a multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}):

\begin{equation} \label{eq:mult-test-Mult}
y \sim \mathrm{Mult}(n,p) \; .
\end{equation}

Then, the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})

\begin{equation} \label{eq:mult-test-mult-test-h0}
H_0: \; p = p_0 = [p_{01}, \ldots, p_{0k}]
\end{equation}

is rejected ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}) at significance level ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:alpha}) $\alpha$, if

\begin{equation} \label{eq:mult-test-mult-test-rej}
\mathrm{Pr}_\mathrm{sig} = \sum_{x: \; \mathrm{Pr}_0(x) \leq \mathrm{Pr}_0(y)} \mathrm{Pr}_0(x) < \alpha
\end{equation}

where $\mathrm{Pr}_0(x)$ is the probability of observing the numbers of occurences $x = [x_1, \ldots, x_k]$ under the null hypothesis:

\begin{equation} \label{eq:mult-test-mult-test-prob}
\mathrm{Pr}_0(x) = n! \prod_{j=1}^k \frac{p_{0j}^{x_j}}{x_j!} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The alternative hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}) relative to $H_0$ is

\begin{equation} \label{eq:mult-test-bin-test-h1}
H_1: \; p_j \neq p_{0j} \quad \text{for at least one} \quad j = 1, \ldots, k \; .
\end{equation}

We can use $y$ as a test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat}). Its sampling distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:dist-samp}) is given by \eqref{eq:mult-test-Mult}. The probability mass function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pmf}) (PMF) of the test statistic under the null hypothesis is thus equal to the probability mass function of the multionomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult-pmf}) with category probabilities ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}) $p_0$:

\begin{equation} \label{eq:mult-test-y-pmf}
\mathrm{Pr}(y = x \vert H_0) = \mathrm{Mult}(x; n, p_0) = {n \choose {x_1, \ldots, x_k}} \, \prod_{j=1}^k {p_j}^{x_j} \; .
\end{equation}

The multinomial coefficient in this equation is equal to

\begin{equation} \label{eq:mult-test-mult-coeff}
{n \choose {k_1, \ldots, k_m}} = \frac{n!}{k_1! \cdot \ldots \cdot k_m!} \; ,
\end{equation}

such that the probability of observing the counts $y$, given $H_0$, is

\begin{equation} \label{eq:mult-test-Pr0-y}
\mathrm{Pr}(y \vert H_0) = n! \prod_{j=1}^k \frac{p_{0i}^{y_j}}{y_j!} \; .
\end{equation}

The probability of observing any other set of counts $x$, given $H_0$, is

\begin{equation} \label{eq:mult-test-Pr0-x}
\mathrm{Pr}(x \vert H_0) = n! \prod_{j=1}^k \frac{p_{0i}^{x_j}}{x_j!} \; .
\end{equation}

The p-value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pval}) is the probability of observing a value of the test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat}) that is as extreme or more extreme then the actually observed test statistic. Any set of counts $x$ might be considered as extreme or more extreme than the actually observed counts $y$, if the former is equally probable or less probably than the latter:

\begin{equation} \label{eq:mult-test-mult-test-cond}
\mathrm{Pr}_0(x) \leq \mathrm{Pr}_0(y) \; .
\end{equation}

Thus, the p-value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:pval}) for the data in \eqref{eq:mult-test-Mult} is equal to

\begin{equation} \label{eq:mult-test-mult-test-p}
p = \sum_{x: \; \mathrm{Pr}_0(x) \leq \mathrm{Pr}_0(y)} \mathrm{Pr}_0(x)
\end{equation}

and the null hypothesis in \eqref{eq:mult-test-mult-test-h0} is rejected ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:test}), if

\begin{equation} \label{eq:mult-test-mult-test-rej-qed}
p < \alpha \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2023): "Multinomial test"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2023-12-23; URL: \url{https://en.wikipedia.org/wiki/Multinomial_test}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Maximum likelihood estimation}]{Maximum likelihood estimation} \label{sec:mult-mle}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = [y_1, \ldots, y_k]$ be the number of observations in $k$ categories resulting from $n$ independent trials with unknown category probabilities $p = [p_1, \ldots, p_k]$, such that $y$ follows a multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}):

\begin{equation} \label{eq:mult-mle-Mult}
y \sim \mathrm{Mult}(n,p) \; .
\end{equation}

Then, the maximum likelihood estimator ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) of $p$ is

\begin{equation} \label{eq:mult-mle-Mult-MLE}
\hat{p} = \frac{1}{n} y , \quad \text{i.e.} \quad \hat{p}_j = \frac{y_j}{n} \quad \text{for all} \quad j = 1, \ldots, k \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Note that the marginal distribution of each element in a multinomial random vector is a binomial distribution

\begin{equation} \label{eq:mult-mle-Mult-marg}
X \sim \mathrm{Mult}(n,p) \quad \Rightarrow \quad X_j \sim \mathrm{Bin}(n, p_j) \quad \text{for all} \quad j = 1, \ldots, k \; .
\end{equation}

Thus, combining \eqref{eq:mult-mle-Mult} with \eqref{eq:mult-mle-Mult-marg}, we have

\begin{equation} \label{eq:mult-mle-Mult-Bin}
y_j \sim \mathrm{Bin}(n,p_j)
\end{equation}

which implies the likelihood function ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-mle})

\begin{equation} \label{eq:mult-mle-Bin-LF}
\mathrm{p}(y|p_j) = \mathrm{Bin}(y_j; n, p_j) = {n \choose y_j} \, p_j^{y_j} \, (1-p_j)^{n-y_j} \; .
\end{equation}

To this, we can apply maximum likelihood estimation for binomial observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:bin-mle}), such that the MLE for each $p_j$ is

\begin{equation} \label{eq:mult-mle-Mult-MLE-qed}
\hat{p}_j = \frac{y_j}{n} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Maximum log-likelihood}]{Maximum log-likelihood} \label{sec:mult-mll}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = [y_1, \ldots, y_k]$ be the number of observations in $k$ categories resulting from $n$ independent trials with unknown category probabilities $p = [p_1, \ldots, p_k]$, such that $y$ follows a multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}):

\begin{equation} \label{eq:mult-mll-Mult}
y \sim \mathrm{Mult}(n,p) \; .
\end{equation}

Then, the maximum log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}) for this model is

\begin{equation} \label{eq:mult-mll-Mult-MLL}
\mathrm{MLL} = \log \Gamma(n+1) - \sum_{j=1}^{k} \log \Gamma(y_j+1) - n \log (n) + \sum_{j=1}^{k} y_j \log (y_j) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult-pmf}), equation \eqref{eq:mult-mll-Mult} implies the following likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}):

\begin{equation} \label{eq:mult-mll-Mult-LF}
\begin{split}
\mathrm{p}(y|p) &= \mathrm{Mult}(y; n, p) \\
&= {n \choose {y_1, \ldots, y_k}} \prod_{j=1}^{k} {p_j}^{y_j} \; .
\end{split}
\end{equation}

Thus, the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) is given by

\begin{equation} \label{eq:mult-mll-Mult-LL}
\begin{split}
\mathrm{LL}(p) &= \log \mathrm{p}(y|p) \\
&= \log {n \choose {y_1, \ldots, y_k}} + \sum_{j=1}^{k} y_j \log (p_j) \; .
\end{split}
\end{equation}

The maximum likelihood estimates of the category probabilities ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-mle}) $p$ are

\begin{equation} \label{eq:mult-mll-Mult-MLE}
\hat{p} = \left[ \hat{p}_1, \ldots, \hat{p}_k \right] \quad \text{with} \quad \hat{p}_j = \frac{y_j}{n} \quad \text{for all} \quad j = 1, \ldots, k \; .
\end{equation}

Plugging \eqref{eq:mult-mll-Mult-MLE} into \eqref{eq:mult-mll-Mult-LL}, we obtain the maximum log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}) of the multinomial observation model in \eqref{eq:mult-mll-Mult} as

\begin{equation} \label{eq:mult-mll-Mult-MLL-s1}
\begin{split}
\mathrm{MLL} &= \mathrm{LL}(\hat{p}) \\
&= \log {n \choose {y_1, \ldots, y_k}} + \sum_{j=1}^{k} y_j \log \left( \frac{y_j}{n} \right) \\
&= \log {n \choose {y_1, \ldots, y_k}} + \sum_{j=1}^{k} \left[ y_j \log (y_j) - y_j \log (n) \right] \\
&= \log {n \choose {y_1, \ldots, y_k}} + \sum_{j=1}^{k}  y_j \log (y_j) - \sum_{j=1}^{k} y_j \log (n) \\
&= \log {n \choose {y_1, \ldots, y_k}} + \sum_{j=1}^{k}  y_j \log (y_j) - n \log (n) \; .
\end{split}
\end{equation}

With the definition of the multinomial coefficient

\begin{equation} \label{eq:mult-mll-mult-coeff}
{n \choose {k_1, \ldots, k_m}} = \frac{n!}{k_1! \cdot \ldots \cdot k_m!}
\end{equation}

and the definition of the gamma function

\begin{equation} \label{eq:mult-mll-gam-fct}
\Gamma(n) = (n-1)! \; ,
\end{equation}

the MLL finally becomes

\begin{equation} \label{eq:mult-mll-Mult-MLL-s2}
\mathrm{MLL} = \log \Gamma(n+1) - \sum_{j=1}^{k} \log \Gamma(y_j+1) - n \log (n) + \sum_{j=1}^{k} y_j \log (y_j) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Maximum-a-posteriori estimation}]{Maximum-a-posteriori estimation} \label{sec:mult-map}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = [y_1, \ldots, y_k]$ be the number of observations in $k$ categories resulting from $n$ independent trials with unknown category probabilities $p = [p_1, \ldots, p_k]$, such that $y$ follows a multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}):

\begin{equation} \label{eq:mult-map-Mult}
y \sim \mathrm{Mult}(n,p) \; .
\end{equation}

Moreover, assume a Dirichlet prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-prior}) over the model parameter $p$:

\begin{equation} \label{eq:mult-map-Mult-prior}
\mathrm{p}(p) = \mathrm{Dir}(p; \alpha_0) \; .
\end{equation}

Then, the maximum-a-posteriori estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:map}) of $p$ are

\begin{equation} \label{eq:mult-map-Mult-MAP}
\hat{p}_\mathrm{MAP} = \frac{\alpha_0+y-1}{\sum_{j=1}^k \alpha_{0j} + n - k} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Given the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) in \eqref{eq:mult-map-Mult-prior}, the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) for multinomial observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-data}) is also a Dirichlet distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-post})

\begin{equation} \label{eq:mult-map-Mult-post}
\mathrm{p}(p|y) = \mathrm{Dir}(p; \alpha_n)
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are equal to

\begin{equation} \label{eq:mult-map-Mult-post-par}
\alpha_{nj} = \alpha_{0j} + y_j, \; j = 1,\ldots,k \; .
\end{equation}

The mode of the Dirichlet distribution is given by:

\begin{equation} \label{eq:mult-map-Dir-mode}
X \sim \mathrm{Dir}(\alpha) \quad \Rightarrow \quad \mathrm{mode}(X_i) = \frac{\alpha_i-1}{\sum_j \alpha_j - k} \; .
\end{equation}

Applying \eqref{eq:mult-map-Dir-mode} to \eqref{eq:mult-map-Mult-post} with \eqref{eq:mult-map-Mult-post-par}, the maximum-a-posteriori estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:map}) of $p$ follow as

\begin{equation} \label{eq:mult-map-Mult-MAP-s1}
\begin{split}
\hat{p}_{i,\mathrm{MAP}} &= \frac{\alpha_{ni} - 1}{\sum_j \alpha_{nj} - k} \\
&\overset{\eqref{eq:mult-map-Mult-post-par}}{=} \frac{\alpha_{0i} + y_i - 1}{\sum_j (\alpha_{0j} + y_j) - k} \\
&= \frac{\alpha_{0i} + y_i - 1}{\sum_j \alpha_{0j} + \sum_j y_j - k} \; .
\end{split}
\end{equation}

Since $y_1 + \ldots + y_k = n$ by definition ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-data}), this becomes

\begin{equation} \label{eq:mult-map-Mult-MAP-s2}
\hat{p}_{i,\mathrm{MAP}} = \frac{\alpha_{0i} + y_i - 1}{\sum_j \alpha_{0j} + n - k}
\end{equation}

which, using the $1 \times k$ vectors ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-data}) $y$, $p$ and $\alpha_0$, can be written as:

\begin{equation} \label{eq:mult-map-Mult-MAP-qed}
\hat{p}_\mathrm{MAP} = \frac{\alpha_0+y-1}{\sum_{j=1}^k \alpha_{0j} + n - k} \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Conjugate prior distribution}]{Conjugate prior distribution} \label{sec:mult-prior}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = [y_1, \ldots, y_k]$ be the number of observations in $k$ categories resulting from $n$ independent trials with unknown category probabilities $p = [p_1, \ldots, p_k]$, such that $y$ follows a multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}):

\begin{equation} \label{eq:mult-prior-Mult}
y \sim \mathrm{Mult}(n,p) \; .
\end{equation}

Then, the conjugate prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-conj}) for the model parameter $p$ is a Dirichlet distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir}):

\begin{equation} \label{eq:mult-prior-Dir}
\mathrm{p}(p) = \mathrm{Dir}(p; \alpha_0) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult-pmf}), the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) implied by \eqref{eq:mult-prior-Mult} is given by

\begin{equation} \label{eq:mult-prior-Mult-LF}
\mathrm{p}(y|p) = {n \choose {y_1, \ldots, y_k}} \prod_{j=1}^{k} {p_j}^{y_j} \; .
\end{equation}

In other words, the likelihood function is proportional to a product of powers of the entries of the vector $p$:

\begin{equation} \label{eq:mult-prior-Mult-LF-prop}
\mathrm{p}(y|p) \propto \prod_{j=1}^{k} {p_j}^{y_j} \; .
\end{equation}

The same is true for a Dirichlet distribution over $p$

\begin{equation} \label{eq:mult-prior-Mult-prior-s1}
\mathrm{p}(p) = \mathrm{Dir}(p; \alpha_0)
\end{equation}

the probability density function of which ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir-pdf})

\begin{equation} \label{eq:mult-prior-Mult-prior-s2}
\mathrm{p}(p) = \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)}{\prod_{j=1}^k \Gamma(\alpha_{0j})} \prod_{j=1}^{k} {p_j}^{\alpha_{0j}-1}
\end{equation}

exhibits the same proportionality

\begin{equation} \label{eq:mult-prior-Mult-prior-s3}
\mathrm{p}(p) \propto \prod_{j=1}^{k} {p_j}^{\alpha_{0j}-1}
\end{equation}

and is therefore conjugate relative to the likelihood.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Dirichlet distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-11; URL: \url{https://en.wikipedia.org/wiki/Dirichlet_distribution#Conjugate_to_categorical/multinomial}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Posterior distribution}]{Posterior distribution} \label{sec:mult-post}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = [y_1, \ldots, y_k]$ be the number of observations in $k$ categories resulting from $n$ independent trials with unknown category probabilities $p = [p_1, \ldots, p_k]$, such that $y$ follows a multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}):

\begin{equation} \label{eq:mult-post-Mult}
y \sim \mathrm{Mult}(n,p) \; .
\end{equation}

Moreover, assume a Dirichlet prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-prior}) over the model parameter $p$:

\begin{equation} \label{eq:mult-post-Mult-prior}
\mathrm{p}(p) = \mathrm{Dir}(p; \alpha_0) \; .
\end{equation}

Then, the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) is also a Dirichlet distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir})

\begin{equation} \label{eq:mult-post-Mult-post}
\mathrm{p}(p|y) = \mathrm{Dir}(p; \alpha_n) \; .
\end{equation}

and the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:mult-post-Mult-post-par}
\alpha_{nj} = \alpha_{0j} + y_j, \; j = 1,\ldots,k \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult-pmf}), the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) implied by \eqref{eq:mult-post-Mult} is given by

\begin{equation} \label{eq:mult-post-Mult-LF}
\mathrm{p}(y|p) = {n \choose {y_1, \ldots, y_k}} \prod_{j=1}^{k} {p_j}^{y_j} \; .
\end{equation}

Combining the likelihood function \eqref{eq:mult-post-Mult-LF} with the prior distribution \eqref{eq:mult-post-Mult-prior}, the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) of the model is given by

\begin{equation} \label{eq:mult-post-Mult-JL}
\begin{split}
\mathrm{p}(y,p) &= \mathrm{p}(y|p) \, \mathrm{p}(p) \\
&= {n \choose {y_1, \ldots, y_k}} \prod_{j=1}^{k} {p_j}^{y_j} \cdot \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)}{\prod_{j=1}^k \Gamma(\alpha_{0j})} \prod_{j=1}^{k} {p_j}^{\alpha_{0j}-1} \\
&= \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)}{\prod_{j=1}^k \Gamma(\alpha_{0j})} {n \choose {y_1, \ldots, y_k}} \prod_{j=1}^{k} {p_j}^{\alpha_{0j}+y_j-1} \; .
\end{split}
\end{equation}

Note that the posterior distribution is proportional to the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl}):

\begin{equation} \label{eq:mult-post-Mult-post-s1}
\mathrm{p}(p|y) \propto \mathrm{p}(y,p) \; .
\end{equation}

Setting $\alpha_{nj} = \alpha_{0j} + y_j$, the posterior distribution is therefore proportional to

\begin{equation} \label{eq:mult-post-Mult-post-s2}
\mathrm{p}(p|y) \propto \prod_{j=1}^{k} {p_j}^{\alpha_{nj}-1}
\end{equation}

which, when normalized to one, results in the probability density function of the Dirichlet distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir-pdf}):

\begin{equation} \label{eq:mult-post-Mult-post-qed}
\mathrm{p}(p|y) = \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)}{\prod_{j=1}^k \Gamma(\alpha_{nj})} \prod_{j=1}^{k} {p_j}^{\alpha_{nj}-1} = \mathrm{Dir}(p; \alpha_n) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Dirichlet distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-11; URL: \url{https://en.wikipedia.org/wiki/Dirichlet_distribution#Conjugate_to_categorical/multinomial}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Log model evidence}]{Log model evidence} \label{sec:mult-lme}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = [y_1, \ldots, y_k]$ be the number of observations in $k$ categories resulting from $n$ independent trials with unknown category probabilities $p = [p_1, \ldots, p_k]$, such that $y$ follows a multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}):

\begin{equation} \label{eq:mult-lme-Mult}
y \sim \mathrm{Mult}(n,p) \; .
\end{equation}

Moreover, assume a Dirichlet prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-prior}) over the model parameter $p$:

\begin{equation} \label{eq:mult-lme-Mult-prior}
\mathrm{p}(p) = \mathrm{Dir}(p; \alpha_0) \; .
\end{equation}

Then, the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) for this model is

\begin{equation} \label{eq:mult-lme-Mult-LME}
\begin{split}
\log \mathrm{p}(y|m) &= \log \Gamma(n+1) - \sum_{j=1}^{k} \log \Gamma(y_j+1) \\
&+ \log \Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right) - \log \Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right) \\
&+ \sum_{j=1}^k \log \Gamma(\alpha_{nj}) - \sum_{j=1}^k \log \Gamma(\alpha_{0j}) \; .
\end{split}
\end{equation}

and the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:mult-lme-Mult-post-par}
\alpha_{nj} = \alpha_{0j} + y_j, \; j = 1,\ldots,k \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult-pmf}), the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) implied by \eqref{eq:mult-lme-Mult} is given by

\begin{equation} \label{eq:mult-lme-Mult-LF}
\mathrm{p}(y|p) = {n \choose {y_1, \ldots, y_k}} \prod_{j=1}^{k} {p_j}^{y_j} \; .
\end{equation}

Combining the likelihood function \eqref{eq:mult-lme-Mult-LF} with the prior distribution \eqref{eq:mult-lme-Mult-prior}, the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) of the model is given by

\begin{equation} \label{eq:mult-lme-Mult-JL}
\begin{split}
\mathrm{p}(y,p) &= \mathrm{p}(y|p) \, \mathrm{p}(p) \\
&= {n \choose {y_1, \ldots, y_k}} \prod_{j=1}^{k} {p_j}^{y_j} \cdot \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)}{\prod_{j=1}^k \Gamma(\alpha_{0j})} \prod_{j=1}^{k} {p_j}^{\alpha_{0j}-1} \\
&= {n \choose {y_1, \ldots, y_k}} \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)}{\prod_{j=1}^k \Gamma(\alpha_{0j})} \prod_{j=1}^{k} {p_j}^{\alpha_{0j}+y_j-1} \; .
\end{split}
\end{equation}

Note that the model evidence is the marginal density of the joint likelihood:

\begin{equation} \label{eq:mult-lme-Mult-ME-s1}
\mathrm{p}(y) = \int \mathrm{p}(y,p) \, \mathrm{d}p \; .
\end{equation}

Setting $\alpha_{nj} = \alpha_{0j} + y_j$, the joint likelihood can also be written as

\begin{equation} \label{eq:mult-lme-Mult-JL-s2}
\mathrm{p}(y,p) = {n \choose {y_1, \ldots, y_k}} \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)}{\prod_{j=1}^k \Gamma(\alpha_{0j})} \, \frac{\prod_{j=1}^k \Gamma(\alpha_{nj})} {\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)} \, \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)}{\prod_{j=1}^k \Gamma(\alpha_{nj})} \prod_{j=1}^{k} {p_j}^{\alpha_{nj}-1} \; .
\end{equation}

Using the probability density function of the Dirichlet distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir-pdf}), $p$ can now be integrated out easily

\begin{equation} \label{eq:mult-lme-Mult-ME-s2}
\begin{split}
\mathrm{p}(y) &= \int {n \choose {y_1, \ldots, y_k}} \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)}{\prod_{j=1}^k \Gamma(\alpha_{0j})} \, \frac{\prod_{j=1}^k \Gamma(\alpha_{nj})}{\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)} \, \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)}{\prod_{j=1}^k \Gamma(\alpha_{nj})} \prod_{j=1}^{k} {p_j}^{\alpha_{nj}-1} \, \mathrm{d}p \\
&= {n \choose {y_1, \ldots, y_k}} \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)}{\prod_{j=1}^k \Gamma(\alpha_{0j})} \, \frac{\prod_{j=1}^k \Gamma(\alpha_{nj})}{\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)} \int \mathrm{Dir}(p; \alpha_n) \, \mathrm{d}p \\
&= {n \choose {y_1, \ldots, y_k}} \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)}{\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)} \, \frac{\prod_{j=1}^k \Gamma(\alpha_{nj})}{\prod_{j=1}^k \Gamma(\alpha_{0j})} \; ,
\end{split}
\end{equation}

such that the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) (LME) is shown to be

\begin{equation} \label{eq:mult-lme-Mult-LME-s1}
\begin{split}
\log \mathrm{p}(y|m) = \log {n \choose {y_1, \ldots, y_k}} &+ \log \Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right) - \log \Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right) \\
&+ \sum_{j=1}^k \log \Gamma(\alpha_{nj}) - \sum_{j=1}^k \log \Gamma(\alpha_{0j}) \; .
\end{split}
\end{equation}

With the definition of the multinomial coefficient

\begin{equation} \label{eq:mult-lme-mult-coeff}
{n \choose {k_1, \ldots, k_m}} = \frac{n!}{k_1! \cdot \ldots \cdot k_m!}
\end{equation}

and the definition of the gamma function

\begin{equation} \label{eq:mult-lme-gam-fct}
\Gamma(n) = (n-1)! \; ,
\end{equation}

the LME finally becomes

\begin{equation} \label{eq:mult-lme-Mult-LME-s2}
\begin{split}
\log \mathrm{p}(y|m) &= \log \Gamma(n+1) - \sum_{j=1}^{k} \log \Gamma(y_j+1) \\
&+ \log \Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right) - \log \Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right) \\
&+ \sum_{j=1}^k \log \Gamma(\alpha_{nj}) - \sum_{j=1}^k \log \Gamma(\alpha_{0j}) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Log Bayes factor}]{Log Bayes factor} \label{sec:mult-lbf}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = [y_1, \ldots, y_k]$ be the number of observations in $k$ categories resulting from $n$ independent trials with unknown category probabilities $p = [p_1, \ldots, p_k]$, such that $y$ follows a multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}):

\begin{equation} \label{eq:mult-lbf-Mult}
y \sim \mathrm{Mult}(n,p) \; .
\end{equation}

Moreover, assume two statistical models ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fpm}), one assuming that each $p_j$ is $1/k$ (null model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})), the other imposing a Dirichlet distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-prior}) as the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) on the model parameters $p_1, \ldots, p_k$ (alternative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1})):

\begin{equation} \label{eq:mult-lbf-Mult-m01}
\begin{split}
m_0&: \; y \sim \mathrm{Mult}(n,p), \; p = [1/k, \ldots, 1/k] \\
m_1&: \; y \sim \mathrm{Mult}(n,p), \; p \sim \mathrm{Dir}(\alpha_0) \; .
\end{split}
\end{equation}

Then, the log Bayes factor ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf}) in favor of $m_1$ against $m_0$ is

\begin{equation} \label{eq:mult-lbf-Mult-LBF}
\begin{split}
\mathrm{LBF}_{10} &= \log \Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right) - \log \Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right) \\
&+ \sum_{j=1}^k \log \Gamma(\alpha_{nj}) - \sum_{j=1}^k \log \Gamma(\alpha_{0j}) - n \log \left( \frac{1}{k} \right)
\end{split}
\end{equation}

where $\Gamma(x)$ is the gamma function and $\alpha_n$ are the posterior hyperparameters for multinomial observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-post}) which are functions of the numbers of observations ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}) $y_1, \ldots, y_k$.


\vspace{1em}
\textbf{Proof:} The log Bayes factor is equal to the difference of two log model evidences ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf-lme}):

\begin{equation} \label{eq:mult-lbf-LBF-LME}
\mathrm{LBF}_{12} = \mathrm{LME}(m_1) - \mathrm{LME}(m_2) \; .
\end{equation}

The LME of the alternative $m_1$ is equal to the log model evidence for multinomial observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-lme}):

\begin{equation} \label{eq:mult-lbf-Mult-LME-m1}
\begin{split}
\mathrm{LME}(m_1) = \log p(y|m_1) &= \log \Gamma(n+1) - \sum_{j=1}^{k} \log \Gamma(y_j+1) \\
&+ \log \Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right) - \log \Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right) \\
&+ \sum_{j=1}^k \log \Gamma(\alpha_{nj}) - \sum_{j=1}^k \log \Gamma(\alpha_{0j}) \; .
\end{split}
\end{equation}

Because the null model $m_0$ has no free parameter, its log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) (logarithmized marginal likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml})) is equal to the log-likelihood function for multinomial observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-mle}) at the value $p_0 = [1/k, \ldots, 1/k]$:

\begin{equation} \label{eq:mult-lbf-Mult-LME-m0}
\begin{split}
\mathrm{LME}(m_0) = \log p(y|p = p_0) &= \log {n \choose {y_1, \ldots, y_k}} + \sum_{j=1}^{k} y_j \log \left( \frac{1}{k} \right) \\
&= \log {n \choose {y_1, \ldots, y_k}} + n \log \left( \frac{1}{k} \right) \; .
\end{split}
\end{equation}

Subtracting the two LMEs from each other, the LBF emerges as

\begin{equation} \label{eq:mult-lbf-Mult-LBF-m10}
\begin{split}
\mathrm{LBF}_{10} &= \log \Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right) - \log \Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right) \\
&+ \sum_{j=1}^k \log \Gamma(\alpha_{nj}) - \sum_{j=1}^k \log \Gamma(\alpha_{0j}) - n \log \left( \frac{1}{k} \right)
\end{split}
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-post})

\begin{equation} \label{eq:mult-lbf-Mult-post-par}
\begin{split}
\alpha_n &= \alpha_0 + y \\
&= [\alpha_{01}, \ldots, \alpha_{0k}] + [y_1, \ldots, y_k] \\
&= [\alpha_{01} + y_1, \ldots, \alpha_{0k} + y_k] \\
\text{i.e.} \quad \alpha_{nj} &= \alpha_{0j} + y_j \quad \text{for all} \quad j = 1, \ldots, k
\end{split}
\end{equation}

with the numbers of observations ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}) $y_1, \ldots, y_k$.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Posterior probability}]{Posterior probability} \label{sec:mult-pp}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = [y_1, \ldots, y_k]$ be the number of observations in $k$ categories resulting from $n$ independent trials with unknown category probabilities $p = [p_1, \ldots, p_k]$, such that $y$ follows a multinomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}):

\begin{equation} \label{eq:mult-pp-Mult}
y \sim \mathrm{Mult}(n,p) \; .
\end{equation}

Moreover, assume two statistical models ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:fpm}), one assuming that each $p_j$ is $1/k$ (null model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0})), the other imposing a Dirichlet distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-prior}) as the prior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) on the model parameters $p_1, \ldots, p_k$ (alternative ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1})):

\begin{equation} \label{eq:mult-pp-Mult-m01}
\begin{split}
m_0&: \; y \sim \mathrm{Mult}(n,p), \; p = [1/k, \ldots, 1/k] \\
m_1&: \; y \sim \mathrm{Mult}(n,p), \; p \sim \mathrm{Dir}(\alpha_0) \; .
\end{split}
\end{equation}

Then, the posterior probability ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:pmp}) of the alternative model ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h1}) is given by

\begin{equation} \label{eq:mult-pp-Mult-PP1}
p(m_1|y) = \frac{1}{1 + k^{-n} \cdot \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)}{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)} \cdot \frac{\prod_{j=1}^k \Gamma(\alpha_{0j})}{\prod_{j=1}^k \Gamma(\alpha_{nj})}}
\end{equation}

where $\Gamma(x)$ is the gamma function and $\alpha_n$ are the posterior hyperparameters for multinomial observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-post}) which are functions of the numbers of observations ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}) $y_1, \ldots, y_k$.


\vspace{1em}
\textbf{Proof:} The posterior probability for one of two models is a function of the log Bayes factor in favor of this model ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:pmp-lbf}):

\begin{equation} \label{eq:mult-pp-PP-LBF}
p(m_1|y) = \frac{\exp(\mathrm{LBF}_{12})}{\exp(\mathrm{LBF}_{12}) + 1} \; .
\end{equation}

The log Bayes factor in favor of the alternative model for multinomial observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-lbf}) is given by

\begin{equation} \label{eq:mult-pp-Mult-LBF10}
\begin{split}
\mathrm{LBF}_{10} &= \log \Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right) - \log \Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right) \\
&+ \sum_{j=1}^k \log \Gamma(\alpha_{nj}) - \sum_{j=1}^k \log \Gamma(\alpha_{0j}) - n \log \left( \frac{1}{k} \right)
\end{split}
\end{equation}

and the corresponding Bayes factor ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:bf}), i.e. exponentiated log Bayes factor ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lbf-der}), is equal to

\begin{equation} \label{eq:mult-pp-Mult-BF10}
\mathrm{BF}_{10} = \exp(\mathrm{LBF}_{10}) = k^n \cdot \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)}{\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)} \cdot \frac{\prod_{j=1}^k \Gamma(\alpha_{nj})}{\prod_{j=1}^k \Gamma(\alpha_{0j})} \; .
\end{equation}

Thus, the posterior probability of the alternative, assuming a prior distribution over the probabilities $p_1, \ldots, p_k$, compared to the null model, assuming fixed probabilities $p = [1/k, \ldots, 1/k]$, follows as

\begin{equation} \label{eq:mult-pp-Mult-PP1-qed}
\begin{split}
p(m_1|y) &\overset{\eqref{eq:mult-pp-PP-LBF}}{=} \frac{\exp(\mathrm{LBF}_{10})}{\exp(\mathrm{LBF}_{10}) + 1} \\
&\overset{\eqref{eq:mult-pp-Mult-BF10}}{=} \frac{k^n \cdot \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)}{\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)} \cdot \frac{\prod_{j=1}^k \Gamma(\alpha_{nj})}{\prod_{j=1}^k \Gamma(\alpha_{0j})}}{k^n \cdot \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)}{\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)} \cdot \frac{\prod_{j=1}^k \Gamma(\alpha_{nj})}{\prod_{j=1}^k \Gamma(\alpha_{0j})} + 1} \\
&= \frac{k^n \cdot \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)}{\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)} \cdot \frac{\prod_{j=1}^k \Gamma(\alpha_{nj})}{\prod_{j=1}^k \Gamma(\alpha_{0j})}}{k^n \cdot \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)}{\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)} \cdot \frac{\prod_{j=1}^k \Gamma(\alpha_{nj})}{\prod_{j=1}^k \Gamma(\alpha_{0j})} \left( 1 + k^{-n} \cdot \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)}{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)} \cdot \frac{\prod_{j=1}^k \Gamma(\alpha_{0j})}{\prod_{j=1}^k \Gamma(\alpha_{nj})} \right)} \\
&= \frac{1}{1 + k^{-n} \cdot \frac{\Gamma \left( \sum_{j=1}^{k} \alpha_{nj} \right)}{\Gamma \left( \sum_{j=1}^{k} \alpha_{0j} \right)} \cdot \frac{\prod_{j=1}^k \Gamma(\alpha_{0j})}{\prod_{j=1}^k \Gamma(\alpha_{nj})}}
\end{split}
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mult-post})

\begin{equation} \label{eq:mult-pp-Mult-post-par}
\alpha_n = \alpha_0 + y, \quad \text{i.e.} \quad \alpha_{nj} = \alpha_{0j} + y_j
\end{equation}

with the numbers of observations ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:mult}) $y_1, \ldots, y_k$.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{Poisson-distributed data}

\subsubsection[\textit{Definition}]{Definition} \label{sec:poiss-data}
\setcounter{equation}{0}

\textbf{Definition:} Poisson-distributed data are defined as a set of observed counts $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$, independent and identically distributed according to a Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss}) with rate $\lambda$:

\begin{equation} \label{eq:poiss-data-Poiss}
y_i \sim \mathrm{Poiss}(\lambda), \quad i = 1, \ldots, n \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Poisson distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-22; URL: \url{https://en.wikipedia.org/wiki/Poisson_distribution#Parameter_estimation}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Maximum likelihood estimation}]{Maximum likelihood estimation} \label{sec:poiss-mle}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a Poisson-distributed data ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:poiss-data}) set $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$:

\begin{equation} \label{eq:poiss-mle-Poiss}
y_i \sim \mathrm{Poiss}(\lambda), \quad i = 1, \ldots, n \; .
\end{equation}

Then, the maximum likelihood estimate ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) for the rate parameter $\lambda$ is given by

\begin{equation} \label{eq:poiss-mle-Poiss-MLE}
\hat{\lambda} = \bar{y}
\end{equation}

where $\bar{y}$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp})

\begin{equation} \label{eq:poiss-mle-y-mean}
\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) for each observation is given by the probability mass function of the Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss-pmf})

\begin{equation} \label{eq:poiss-mle-Poiss-yi}
p(y_i|\lambda) = \mathrm{Poiss}(y_i; \lambda) = \frac{\lambda^{y_i} \cdot \exp(-\lambda)}{y_i !}
\end{equation}

and because observations are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:poiss-mle-Poiss-LF}
p(y|\lambda) = \prod_{i=1}^n p(y_i|\lambda) = \prod_{i=1}^n \frac{\lambda^{y_i} \cdot \exp(-\lambda)}{y_i !} \; .
\end{equation}

Thus, the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) is

\begin{equation} \label{eq:poiss-mle-Poiss-LL}
\mathrm{LL}(\lambda) = \log p(y|\lambda) = \log \left[ \prod_{i=1}^n \frac{\lambda^{y_i} \cdot \exp(-\lambda)}{y_i !} \right]
\end{equation}

which can be developed into

\begin{equation} \label{eq:poiss-mle-Poiss-LL-der}
\begin{split}
\mathrm{LL}(\lambda) &= \sum_{i=1}^n \log \left[ \frac{\lambda^{y_i} \cdot \exp(-\lambda)}{y_i !} \right] \\
&= \sum_{i=1}^n \left[ y_i \cdot \log(\lambda) - \lambda - \log(y_i !) \right] \\
&= - \sum_{i=1}^n \lambda + \sum_{i=1}^n y_i \cdot \log(\lambda) - \sum_{i=1}^n \log(y_i !) \\
&= - n \lambda + \log(\lambda) \sum_{i=1}^n y_i - \sum_{i=1}^n \log(y_i !) \\
\end{split}
\end{equation}

The derivatives of the log-likelihood with respect to $\lambda$ are

\begin{equation} \label{eq:poiss-mle-Poiss-dLLdl-d2LLdl2}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\lambda)}{\mathrm{d}\lambda} &= \frac{1}{\lambda} \sum_{i=1}^n y_i - n \\
\frac{\mathrm{d}^2\mathrm{LL}(\lambda)}{\mathrm{d}\lambda^2} &= -\frac{1}{\lambda^2} \sum_{i=1}^n y_i \; . \\
\end{split}
\end{equation}

Setting the first derivative to zero, we obtain:

\begin{equation} \label{eq:poiss-mle-Poiss-dLLdl}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{\lambda})}{\mathrm{d}\lambda} &= 0 \\
0 &= \frac{1}{\hat{\lambda}} \sum_{i=1}^n y_i - n \\
\hat{\lambda} &= \frac{1}{n} \sum_{i=1}^n y_i = \bar{y} \; .
\end{split}
\end{equation}

Plugging this value into the second derivative, we confirm:

\begin{equation} \label{eq:poiss-mle-Poiss-d2LLdl2}
\begin{split}
\frac{\mathrm{d}^2\mathrm{LL}(\hat{\lambda})}{\mathrm{d}\lambda^2} &= -\frac{1}{\bar{y}^2} \sum_{i=1}^n y_i \\
&= -\frac{n \cdot \bar{y}}{\bar{y}^2} \\
&= -\frac{n}{\bar{y}} < 0 \; .
\end{split}
\end{equation}

This demonstrates that the estimate $\hat{\lambda} = \bar{y}$ maximizes the likelihood $p(y \vert \lambda)$.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Conjugate prior distribution}]{Conjugate prior distribution} \label{sec:poiss-prior}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a Poisson-distributed data ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:poiss-data}) set $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$:

\begin{equation} \label{eq:poiss-prior-Poiss}
y_i \sim \mathrm{Poiss}(\lambda), \quad i = 1, \ldots, n \; .
\end{equation}

Then, the conjugate prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-conj}) for the model parameter $\lambda$ is a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:poiss-prior-Poiss-prior}
p(\lambda) = \mathrm{Gam}(\lambda; a_0, b_0) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss-pmf}), the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) for each observation implied by \eqref{eq:poiss-prior-Poiss} is given by

\begin{equation} \label{eq:poiss-prior-Poiss-LF-s1}
p(y_i|\lambda) = \mathrm{Poiss}(y_i; \lambda) = \frac{\lambda^{y_i} \cdot \exp\left[-\lambda\right]}{y_i !}
\end{equation}

and because observations are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:poiss-prior-Poiss-LF-s2}
p(y|\lambda) = \prod_{i=1}^n p(y_i|\lambda) = \prod_{i=1}^n \frac{\lambda^{y_i} \cdot \exp\left[-\lambda\right]}{y_i !} \; .
\end{equation}

Resolving the product in the likelihood function, we have

\begin{equation} \label{eq:poiss-prior-Poiss-LF-s3}
\begin{split}
p(y|\lambda) &= \prod_{i=1}^n \frac{1}{y_i !} \cdot \prod_{i=1}^n \lambda^{y_i} \cdot \prod_{i=1}^n \exp\left[-\lambda\right] \\
&= \prod_{i=1}^n \left(\frac{1}{y_i !}\right) \cdot \lambda^{n \bar{y}} \cdot \exp\left[-n \lambda\right]
\end{split}
\end{equation}

where $\bar{y}$ is the mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) of $y$:

\begin{equation} \label{eq:poiss-prior-y-mean}
\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i \; .
\end{equation}

In other words, the likelihood function is proportional to a power of $\lambda$ times an exponential of $\lambda$:

\begin{equation} \label{eq:poiss-prior-Poiss-LF-prop}
p(y|\lambda) \propto \lambda^{n \bar{y}} \cdot \exp\left[-n \lambda\right] \; .
\end{equation}

The same is true for a gamma distribution over $\lambda$

\begin{equation} \label{eq:poiss-prior-Poiss-prior-s1}
p(\lambda) = \mathrm{Gam}(\lambda; a_0, b_0)
\end{equation}

the probability density function of which ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf})

\begin{equation} \label{eq:poiss-prior-Poiss-prior-s2}
p(\lambda) = \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda]
\end{equation}

exhibits the same proportionality

\begin{equation} \label{eq:poiss-prior-Poiss-prior-s3}
p(\lambda) \propto \lambda^{a_0-1} \cdot \exp[-b_0 \lambda]
\end{equation}

and is therefore conjugate relative to the likelihood.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Gelman A, Carlin JB, Stern HS, Dunson DB, Vehtari A, Rubin DB (2014): "Other standard single-parameter models"; in: \textit{Bayesian Data Analysis}, 3rd edition, ch. 2.6, p. 45, eq. 2.14ff.; URL: \url{http://www.stat.columbia.edu/~gelman/book/}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Posterior distribution}]{Posterior distribution} \label{sec:poiss-post}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a Poisson-distributed data ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:poiss-data}) set $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$:

\begin{equation} \label{eq:poiss-post-Poiss}
y_i \sim \mathrm{Poiss}(\lambda), \quad i = 1, \ldots, n \; .
\end{equation}

Moreover, assume a gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:poiss-prior}) over the model parameter $\lambda$:

\begin{equation} \label{eq:poiss-post-Poiss-prior}
p(\lambda) = \mathrm{Gam}(\lambda; a_0, b_0) \; .
\end{equation}

Then, the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) is also a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam})

\begin{equation} \label{eq:poiss-post-Poiss-post}
p(\lambda|y) = \mathrm{Gam}(\lambda; a_n, b_n)
\end{equation}

and the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:poiss-post-Poiss-post-par}
\begin{split}
a_n &= a_0 + n \bar{y} \\
b_n &= b_0 + n \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss-pmf}), the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) for each observation implied by \eqref{eq:poiss-post-Poiss} is given by

\begin{equation} \label{eq:poiss-post-Poiss-LF-s1}
p(y_i|\lambda) = \mathrm{Poiss}(y_i; \lambda) = \frac{\lambda^{y_i} \cdot \exp\left[-\lambda\right]}{y_i !}
\end{equation}

and because observations are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:poiss-post-Poiss-LF-s2}
p(y|\lambda) = \prod_{i=1}^n p(y_i|\lambda) = \prod_{i=1}^n \frac{\lambda^{y_i} \cdot \exp\left[-\lambda\right]}{y_i !} \; .
\end{equation}

Combining the likelihood function \eqref{eq:poiss-post-Poiss-LF-s2} with the prior distribution \eqref{eq:poiss-post-Poiss-prior}, the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) of the model is given by

\begin{equation} \label{eq:poiss-post-Poiss-JL-s1}
\begin{split}
p(y,\lambda) &= p(y|\lambda) \, p(\lambda) \\
&= \prod_{i=1}^n \frac{\lambda^{y_i} \cdot \exp\left[-\lambda\right]}{y_i !} \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \; .
\end{split}
\end{equation}

Resolving the product in the joint likelihood, we have

\begin{equation} \label{eq:poiss-post-Poiss-JL-s2}
\begin{split}
p(y,\lambda) &= \prod_{i=1}^n \frac{1}{y_i !} \prod_{i=1}^n \lambda^{y_i} \prod_{i=1}^n \exp\left[-\lambda\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{1}{y_i !}\right) \lambda^{n \bar{y}} \exp\left[-n \lambda\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{1}{y_i !}\right) \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)}  \cdot \lambda^{a_0 + n \bar{y} - 1} \cdot \exp\left[-(b_0 + n \lambda)\right] \\
\end{split}
\end{equation}

where $\bar{y}$ is the mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) of $y$:

\begin{equation} \label{eq:poiss-post-y-mean}
\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i \; .
\end{equation}

Note that the posterior distribution is proportional to the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl}):

\begin{equation} \label{eq:poiss-post-Poiss-post-s1}
p(\lambda|y) \propto p(y,\lambda) \; .
\end{equation}

Setting $a_n = a_0 + n \bar{y}$ and $b_n = b_0 + n$, the posterior distribution is therefore proportional to

\begin{equation} \label{eq:poiss-post-Poiss-post-s2}
p(\lambda|y) \propto \lambda^{a_n-1} \cdot \exp\left[-b_n \lambda\right]
\end{equation}

which, when normalized to one, results in the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}):

\begin{equation} \label{eq:poiss-post-Poiss-post-s3}
p(\lambda|y) = \frac{ {b_n}^{a_n}}{\Gamma(a_0)} \lambda^{a_n-1} \exp\left[-b_n \lambda\right] = \mathrm{Gam}(\lambda; a_n, b_n) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Gelman A, Carlin JB, Stern HS, Dunson DB, Vehtari A, Rubin DB (2014): "Other standard single-parameter models"; in: \textit{Bayesian Data Analysis}, 3rd edition, ch. 2.6, p. 45, eq. 2.15; URL: \url{http://www.stat.columbia.edu/~gelman/book/}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Log model evidence}]{Log model evidence} \label{sec:poiss-lme}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a Poisson-distributed data ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:poiss-data}) set $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$:

\begin{equation} \label{eq:poiss-lme-Poiss}
y_i \sim \mathrm{Poiss}(\lambda), \quad i = 1, \ldots, n \; .
\end{equation}

Moreover, assume a gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:poiss-prior}) over the model parameter $\lambda$:

\begin{equation} \label{eq:poiss-lme-Poiss-prior}
p(\lambda) = \mathrm{Gam}(\lambda; a_0, b_0) \; .
\end{equation}

Then, the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) for this model is

\begin{equation} \label{eq:poiss-lme-Poiss-LME}
\log p(y|m) = - \sum_{i=1}^n \log y_i ! + \log \Gamma(a_n) - \log \Gamma(a_0) + a_0 \log b_0 - a_n \log b_n \; .
\end{equation}

and the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:poiss-lme-Poiss-post-par}
\begin{split}
a_n &= a_0 + n \bar{y} \\
b_n &= b_0 + n \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss-pmf}), the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) for each observation implied by \eqref{eq:poiss-lme-Poiss} is given by

\begin{equation} \label{eq:poiss-lme-Poiss-LF-s1}
p(y_i|\lambda) = \mathrm{Poiss}(y_i; \lambda) = \frac{\lambda^{y_i} \cdot \exp\left[-\lambda\right]}{y_i !}
\end{equation}

and because observations are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:poiss-lme-Poiss-LF-s2}
p(y|\lambda) = \prod_{i=1}^n p(y_i|\lambda) = \prod_{i=1}^n \frac{\lambda^{y_i} \cdot \exp\left[-\lambda\right]}{y_i !} \; .
\end{equation}

Combining the likelihood function \eqref{eq:poiss-lme-Poiss-LF-s2} with the prior distribution \eqref{eq:poiss-lme-Poiss-prior}, the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) of the model is given by

\begin{equation} \label{eq:poiss-lme-Poiss-JL-s1}
\begin{split}
p(y,\lambda) &= p(y|\lambda) \, p(\lambda) \\
&= \prod_{i=1}^n \frac{\lambda^{y_i} \cdot \exp\left[-\lambda\right]}{y_i !} \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \; .
\end{split}
\end{equation}

Resolving the product in the joint likelihood, we have

\begin{equation} \label{eq:poiss-lme-Poiss-JL-s2}
\begin{split}
p(y,\lambda) &= \prod_{i=1}^n \frac{1}{y_i !} \prod_{i=1}^n \lambda^{y_i} \prod_{i=1}^n \exp\left[-\lambda\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{1}{y_i !}\right) \lambda^{n \bar{y}} \exp\left[-n \lambda\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{1}{y_i !}\right) \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)}  \cdot \lambda^{a_0 + n \bar{y} - 1} \cdot \exp\left[-(b_0 + n \lambda)\right] \\
\end{split}
\end{equation}

where $\bar{y}$ is the mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) of $y$:

\begin{equation} \label{eq:poiss-lme-y-mean}
\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i \; .
\end{equation}

Note that the model evidence is the marginal density of the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml}):

\begin{equation} \label{eq:poiss-lme-Poiss-ME}
p(y) = \int p(y,\lambda) \, \mathrm{d}\lambda \; .
\end{equation}

Setting $a_n = a_0 + n \bar{y}$ and $b_n = b_0 + n$, the joint likelihood can also be written as

\begin{equation} \label{eq:poiss-lme-Poiss-JL-s3}
p(y,\lambda) = \prod_{i=1}^n \left(\frac{1}{y_i !}\right) \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \frac{\Gamma(a_n)}{ {b_n}^{a_n}} \cdot \frac{ {b_n}^{a_n}}{\Gamma(a_n)} \lambda^{a_n-1} \exp\left[-b_n \lambda\right] \; .
\end{equation}

Using the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), $\lambda$ can now be integrated out easily

\begin{equation} \label{eq:poiss-lme-Poiss-ME-qed}
\begin{split}
\mathrm{p}(y) &= \int \prod_{i=1}^n \left(\frac{1}{y_i !}\right) \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \frac{\Gamma(a_n)}{ {b_n}^{a_n}} \cdot \frac{ {b_n}^{a_n}}{\Gamma(a_n)} \lambda^{a_n-1} \exp\left[-b_n \lambda\right] \, \mathrm{d}\lambda \\
&= \prod_{i=1}^n \left(\frac{1}{y_i !}\right) \frac{\Gamma(a_n)}{\Gamma(a_0)} \frac{ {b_0}^{a_0}}{ {b_n}^{a_n}} \int \mathrm{Gam}(\lambda; a_n, b_n) \, \mathrm{d}\lambda \\
&= \prod_{i=1}^n \left(\frac{1}{y_i !}\right) \frac{\Gamma(a_n)}{\Gamma(a_0)} \frac{ {b_0}^{a_0}}{ {b_n}^{a_n}} \; ,
\end{split}
\end{equation}

such that the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) is shown to be

\begin{equation} \label{eq:poiss-lme-Poiss-LME-qed}
\log p(y|m) = - \sum_{i=1}^n \log y_i ! + \log \Gamma(a_n) - \log \Gamma(a_0) + a_0 \log b_0 - a_n \log b_n \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{Poisson distribution with exposure values}

\subsubsection[\textit{Definition}]{Definition} \label{sec:poissexp}
\setcounter{equation}{0}

\textbf{Definition:} A Poisson distribution with exposure values is defined as a set of observed counts $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$, independently distributed according to a Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss}) with common rate $\lambda$ and a set of concurrent exposures $x = \left\lbrace x_1, \ldots, x_n \right\rbrace$:

\begin{equation} \label{eq:poissexp-Poiss-exp}
y_i \sim \mathrm{Poiss}(\lambda x_i), \quad i = 1, \ldots, n \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Gelman A, Carlin JB, Stern HS, Dunson DB, Vehtari A, Rubin DB (2014): "Other standard single-parameter models"; in: \textit{Bayesian Data Analysis}, 3rd edition, ch. 2.6, p. 45, eq. 2.14; URL: \url{http://www.stat.columbia.edu/~gelman/book/}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Maximum likelihood estimation}]{Maximum likelihood estimation} \label{sec:poissexp-mle}
\setcounter{equation}{0}

\textbf{Theorem:} Consider data $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ following a Poisson distribution with exposure values ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:poissexp}):

\begin{equation} \label{eq:poissexp-mle-Poiss-exp}
y_i \sim \mathrm{Poiss}(\lambda x_i), \quad i = 1, \ldots, n \; .
\end{equation}

Then, the maximum likelihood estimate ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) for the rate parameter $\lambda$ is given by

\begin{equation} \label{eq:poissexp-mle-Poiss-exp-MLE}
\hat{\lambda} = \frac{\bar{y}}{\bar{x}}
\end{equation}

where $\bar{y}$ and $\bar{x}$ are the sample means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp})

\begin{equation} \label{eq:poissexp-mle-xy-mean}
\begin{split}
\bar{y} &= \frac{1}{n} \sum_{i=1}^n y_i \\
\bar{x} &= \frac{1}{n} \sum_{i=1}^n x_i \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss-pmf}), the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) for each observation implied by \eqref{eq:poissexp-mle-Poiss-exp} is given by

\begin{equation} \label{eq:poissexp-mle-Poiss-exp-LF-s1}
p(y_i|\lambda) = \mathrm{Poiss}(y_i; \lambda x_i) = \frac{(\lambda x_i)^{y_i} \cdot \exp[-\lambda x_i]}{y_i !}
\end{equation}

and because observations are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:poissexp-mle-Poiss-exp-LF-s2}
p(y|\lambda) = \prod_{i=1}^n p(y_i|\lambda) = \prod_{i=1}^n \frac{(\lambda x_i)^{y_i} \cdot \exp[-\lambda x_i]}{y_i !} \; .
\end{equation}

Thus, the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) is

\begin{equation} \label{eq:poissexp-mle-Poiss-LL}
\mathrm{LL}(\lambda) = \log p(y|\lambda) = \log \left[ \prod_{i=1}^n \frac{(\lambda x_i)^{y_i} \cdot \exp[-\lambda x_i]}{y_i !} \right]
\end{equation}

which can be developed into

\begin{equation} \label{eq:poissexp-mle-Poiss-LL-der}
\begin{split}
\mathrm{LL}(\lambda) &= \sum_{i=1}^n \log \left[ \frac{(\lambda x_i)^{y_i} \cdot \exp[-\lambda x_i]}{y_i !} \right] \\
&= \sum_{i=1}^n \left[ y_i \cdot \log(\lambda x_i) - \lambda x_i - \log(y_i !) \right] \\
&= - \sum_{i=1}^n \lambda x_i + \sum_{i=1}^n y_i \cdot \left[ \log(\lambda) + \log(x_i) \right] - \sum_{i=1}^n \log(y_i !) \\
&= - \lambda \sum_{i=1}^n x_i + \log(\lambda) \sum_{i=1}^n y_i + \sum_{i=1}^n y_i \log(x_i) - \sum_{i=1}^n \log(y_i !) \\
&= - n \bar{x} \lambda + n \bar{y} \log(\lambda) + \sum_{i=1}^n y_i \log(x_i) - \sum_{i=1}^n \log(y_i !) \\
\end{split}
\end{equation}

where $\bar{x}$ and $\bar{y}$ are the sample means from equation \eqref{eq:poissexp-mle-xy-mean}.

\vspace{1em}
The derivatives of the log-likelihood with respect to $\lambda$ are

\begin{equation} \label{eq:poissexp-mle-Poiss-dLLdl-d2LLdl2}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\lambda)}{\mathrm{d}\lambda} &= - n \bar{x} + \frac{n \bar{y}}{\lambda} \\
\frac{\mathrm{d}^2\mathrm{LL}(\lambda)}{\mathrm{d}\lambda^2} &= -\frac{n \bar{y}}{\lambda^2} \; . \\
\end{split}
\end{equation}

Setting the first derivative to zero, we obtain:

\begin{equation} \label{eq:poissexp-mle-Poiss-dLLdl}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\hat{\lambda})}{\mathrm{d}\lambda} &= 0 \\
0 &= - n \bar{x} + \frac{n \bar{y}}{\hat{\lambda}} \\
\hat{\lambda} &= \frac{n \bar{y}}{n \bar{x}} = \frac{\bar{y}}{\bar{x}} \; .
\end{split}
\end{equation}

Plugging this value into the second derivative, we confirm:

\begin{equation} \label{eq:poissexp-mle-Poiss-d2LLdl2}
\begin{split}
\frac{\mathrm{d}^2\mathrm{LL}(\hat{\lambda})}{\mathrm{d}\lambda^2} &= -\frac{n \bar{y}}{\hat{\lambda}^2} \\
&= -\frac{n \cdot \bar{y}}{(\bar{y}/\bar{x})^2} \\
&= -\frac{n \cdot \bar{x}^2}{\bar{y}} < 0 \; .
\end{split}
\end{equation}

This demonstrates that the estimate $\hat{\lambda} = \bar{y}/\bar{x}$ maximizes the likelihood $p(y \vert \lambda)$.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Conjugate prior distribution}]{Conjugate prior distribution} \label{sec:poissexp-prior}
\setcounter{equation}{0}

\textbf{Theorem:} Consider data $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ following a Poisson distribution with exposure values ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:poissexp}):

\begin{equation} \label{eq:poissexp-prior-Poiss-exp}
y_i \sim \mathrm{Poiss}(\lambda x_i), \quad i = 1, \ldots, n \; .
\end{equation}

Then, the conjugate prior ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior-conj}) for the model parameter $\lambda$ is a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam}):

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-prior}
p(\lambda) = \mathrm{Gam}(\lambda; a_0, b_0) \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss-pmf}), the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) for each observation implied by \eqref{eq:poissexp-prior-Poiss-exp} is given by

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-LF-s1}
p(y_i|\lambda) = \mathrm{Poiss}(y_i; \lambda x_i) = \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !}
\end{equation}

and because observations are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-LF-s2}
p(y|\lambda) = \prod_{i=1}^n p(y_i|\lambda) = \prod_{i=1}^n \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !} \; .
\end{equation}

Resolving the product in the likelihood function, we have

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-LF-s3}
\begin{split}
p(y|\lambda) &= \prod_{i=1}^n \frac{ {x_i}^{y_i}}{y_i !} \cdot \prod_{i=1}^n \lambda^{y_i} \cdot \prod_{i=1}^n \exp\left[-\lambda x_i\right] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \cdot \lambda^{\sum_{i=1}^n y_i} \cdot \exp\left[-\lambda \sum_{i=1}^n x_i\right] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \cdot \lambda^{n \bar{y}} \cdot \exp\left[-n \bar{x} \lambda\right]
\end{split}
\end{equation}

where $\bar{y}$ and $\bar{x}$ are the means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) of $y$ and $x$ respectively:

\begin{equation} \label{eq:poissexp-prior-xy-mean}
\begin{split}
\bar{y} &= \frac{1}{n} \sum_{i=1}^n y_i \\
\bar{x} &= \frac{1}{n} \sum_{i=1}^n x_i \; .
\end{split}
\end{equation}

In other words, the likelihood function is proportional to a power of $\lambda$ times an exponential of $\lambda$:

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-LF-prop}
p(y|\lambda) \propto \lambda^{n \bar{y}} \cdot \exp\left[-n \bar{x} \lambda\right] \; .
\end{equation}

The same is true for a gamma distribution over $\lambda$

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-prior-s1}
p(\lambda) = \mathrm{Gam}(\lambda; a_0, b_0)
\end{equation}

the probability density function of which ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf})

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-prior-s2}
p(\lambda) = \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda]
\end{equation}

exhibits the same proportionality

\begin{equation} \label{eq:poissexp-prior-Poiss-exp-prior-s3}
p(\lambda) \propto \lambda^{a_0-1} \cdot \exp[-b_0 \lambda]
\end{equation}

and is therefore conjugate relative to the likelihood.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Gelman A, Carlin JB, Stern HS, Dunson DB, Vehtari A, Rubin DB (2014): "Other standard single-parameter models"; in: \textit{Bayesian Data Analysis}, 3rd edition, ch. 2.6, p. 45, eq. 2.14ff.; URL: \url{http://www.stat.columbia.edu/~gelman/book/}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Posterior distribution}]{Posterior distribution} \label{sec:poissexp-post}
\setcounter{equation}{0}

\textbf{Theorem:} Consider data $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ following a Poisson distribution with exposure values ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:poissexp}):

\begin{equation} \label{eq:poissexp-post-Poiss-exp}
y_i \sim \mathrm{Poiss}(\lambda x_i), \quad i = 1, \ldots, n \; .
\end{equation}

Moreover, assume a gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:poissexp-prior}) over the model parameter $\lambda$:

\begin{equation} \label{eq:poissexp-post-Poiss-exp-prior}
p(\lambda) = \mathrm{Gam}(\lambda; a_0, b_0) \; .
\end{equation}

Then, the posterior distribution ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) is also a gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam})

\begin{equation} \label{eq:poissexp-post-Poiss-exp-post}
p(\lambda|y) = \mathrm{Gam}(\lambda; a_n, b_n)
\end{equation}

and the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:poissexp-post-Poiss-exp-post-par}
\begin{split}
a_n &= a_0 + n \bar{y} \\
b_n &= b_0 + n \bar{x} \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss-pmf}), the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) for each observation implied by \eqref{eq:poissexp-post-Poiss-exp} is given by

\begin{equation} \label{eq:poissexp-post-Poiss-exp-LF-s1}
p(y_i|\lambda) = \mathrm{Poiss}(y_i; \lambda x_i) = \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !}
\end{equation}

and because observations are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:poissexp-post-Poiss-exp-LF-s2}
p(y|\lambda) = \prod_{i=1}^n p(y_i|\lambda) = \prod_{i=1}^n \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !} \; .
\end{equation}

Combining the likelihood function \eqref{eq:poissexp-post-Poiss-exp-LF-s2} with the prior distribution \eqref{eq:poissexp-post-Poiss-exp-prior}, the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) of the model is given by

\begin{equation} \label{eq:poissexp-post-Poiss-exp-JL-s1}
\begin{split}
p(y,\lambda) &= p(y|\lambda) \, p(\lambda) \\
&= \prod_{i=1}^n \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !} \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \; .
\end{split}
\end{equation}

Resolving the product in the joint likelihood, we have

\begin{equation} \label{eq:poissexp-post-Poiss-JL-s2}
\begin{split}
p(y,\lambda) &= \prod_{i=1}^n \frac{ {x_i}^{y_i}}{y_i !} \prod_{i=1}^n \lambda^{y_i} \prod_{i=1}^n \exp\left[-\lambda x_i\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \lambda^{\sum_{i=1}^n y_i} \exp\left[-\lambda \sum_{i=1}^n x_i\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \lambda^{n \bar{y}} \exp\left[-n \bar{x} \lambda\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)}  \cdot \lambda^{a_0 + n \bar{y} - 1} \cdot \exp\left[-(b_0 + n \bar{x}) \lambda\right] \\
\end{split}
\end{equation}

where $\bar{y}$ and $\bar{x}$ are the means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) of $y$ and $x$ respectively:

\begin{equation} \label{eq:poissexp-post-xy-mean}
\begin{split}
\bar{y} &= \frac{1}{n} \sum_{i=1}^n y_i \\
\bar{x} &= \frac{1}{n} \sum_{i=1}^n x_i \; .
\end{split}
\end{equation}

Note that the posterior distribution is proportional to the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post-jl}):

\begin{equation} \label{eq:poissexp-post-Poiss-exp-post-s1}
p(\lambda|y) \propto p(y,\lambda) \; .
\end{equation}

Setting $a_n = a_0 + n \bar{y}$ and $b_n = b_0 + n \bar{x}$, the posterior distribution is therefore proportional to

\begin{equation} \label{eq:poissexp-post-Poiss-exp-post-s2}
p(\lambda|y) \propto \lambda^{a_n-1} \cdot \exp\left[-b_n \lambda\right]
\end{equation}

which, when normalized to one, results in the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}):

\begin{equation} \label{eq:poissexp-post-Poiss-exp-post-s3}
p(\lambda|y) = \frac{ {b_n}^{a_n}}{\Gamma(a_0)} \lambda^{a_n-1} \exp\left[-b_n \lambda\right] = \mathrm{Gam}(\lambda; a_n, b_n) \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Gelman A, Carlin JB, Stern HS, Dunson DB, Vehtari A, Rubin DB (2014): "Other standard single-parameter models"; in: \textit{Bayesian Data Analysis}, 3rd edition, ch. 2.6, p. 45, eq. 2.15; URL: \url{http://www.stat.columbia.edu/~gelman/book/}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Log model evidence}]{Log model evidence} \label{sec:poissexp-lme}
\setcounter{equation}{0}

\textbf{Theorem:} Consider data $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ following a Poisson distribution with exposure values ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:poissexp}):

\begin{equation} \label{eq:poissexp-lme-Poiss-exp}
y_i \sim \mathrm{Poiss}(\lambda x_i), \quad i = 1, \ldots, n \; .
\end{equation}

Moreover, assume a gamma prior distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:poissexp-prior}) over the model parameter $\lambda$:

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-prior}
p(\lambda) = \mathrm{Gam}(\lambda; a_0, b_0) \; .
\end{equation}

Then, the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) for this model is

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-LME}
\begin{split}
\log p(y|m) = &\sum_{i=1}^n y_i \log(x_i) - \sum_{i=1}^n \log y_i ! + \\ 
&\log \Gamma(a_n) - \log \Gamma(a_0) + a_0 \log b_0 - a_n \log b_n \; .
\end{split}
\end{equation}

where the posterior hyperparameters ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) are given by

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-post-par}
\begin{split}
a_n &= a_0 + n \bar{y} \\
a_n &= a_0 + n \bar{x} \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} With the probability mass function of the Poisson distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:poiss-pmf}), the likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) for each observation implied by \eqref{eq:poissexp-lme-Poiss-exp} is given by

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-LF-s1}
p(y_i|\lambda) = \mathrm{Poiss}(y_i; \lambda x_i) = \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !}
\end{equation}

and because observations are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-LF-s2}
p(y|\lambda) = \prod_{i=1}^n p(y_i|\lambda) = \prod_{i=1}^n \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !} \; .
\end{equation}

Combining the likelihood function \eqref{eq:poissexp-lme-Poiss-exp-LF-s2} with the prior distribution \eqref{eq:poissexp-lme-Poiss-exp-prior}, the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:jl}) of the model is given by

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-JL-s1}
\begin{split}
p(y,\lambda) &= p(y|\lambda) \, p(\lambda) \\
&= \prod_{i=1}^n \frac{(\lambda x_i)^{y_i} \cdot \exp\left[-\lambda x_i\right]}{y_i !} \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \; .
\end{split}
\end{equation}

Resolving the product in the joint likelihood, we have

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-JL-s2}
\begin{split}
p(y,\lambda) &= \prod_{i=1}^n \frac{ {x_i}^{y_i}}{y_i !} \prod_{i=1}^n \lambda^{y_i} \prod_{i=1}^n \exp\left[-\lambda x_i\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \lambda^{\sum_{i=1}^n y_i} \exp\left[-\lambda \sum_{i=1}^n x_i\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \lambda^{n \bar{y}} \exp\left[-n \bar{x} \lambda\right] \cdot \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \lambda^{a_0-1} \exp[-b_0 \lambda] \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \frac{ {b_0}^{a_0}}{\Gamma(a_0)}  \cdot \lambda^{a_0 + n \bar{y} - 1} \cdot \exp\left[-(b_0 + n \bar{x}) \lambda\right] \\
\end{split}
\end{equation}

where $\bar{y}$ and $\bar{x}$ are the means ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) of $y$ and $x$ respectively:

\begin{equation} \label{eq:poissexp-lme-xy-mean}
\begin{split}
\bar{y} &= \frac{1}{n} \sum_{i=1}^n y_i \\
\bar{x} &= \frac{1}{n} \sum_{i=1}^n x_i \; .
\end{split}
\end{equation}

Note that the model evidence is the marginal density of the joint likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ml}):

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-ME}
p(y) = \int p(y,\lambda) \, \mathrm{d}\lambda \; .
\end{equation}

Setting $a_n = a_0 + n \bar{y}$ and $b_n = b_0 + n \bar{x}$, the joint likelihood can also be written as

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-JL-s3}
p(y,\lambda) = \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \frac{\Gamma(a_n)}{ {b_n}^{a_n}} \cdot \frac{ {b_n}^{a_n}}{\Gamma(a_n)} \lambda^{a_n-1} \exp\left[-b_n \lambda\right] \; .
\end{equation}

Using the probability density function of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-pdf}), $\lambda$ can now be integrated out easily

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-ME-qed}
\begin{split}
\mathrm{p}(y) &= \int \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \frac{ {b_0}^{a_0}}{\Gamma(a_0)} \frac{\Gamma(a_n)}{ {b_n}^{a_n}} \cdot \frac{ {b_n}^{a_n}}{\Gamma(a_n)} \lambda^{a_n-1} \exp\left[-b_n \lambda\right] \, \mathrm{d}\lambda \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \frac{\Gamma(a_n)}{\Gamma(a_0)} \frac{ {b_0}^{a_0}}{ {b_n}^{a_n}} \int \mathrm{Gam}(\lambda; a_n, b_n) \, \mathrm{d}\lambda \\
&= \prod_{i=1}^n \left(\frac{ {x_i}^{y_i}}{y_i !}\right) \frac{\Gamma(a_n)}{\Gamma(a_0)} \frac{ {b_0}^{a_0}}{ {b_n}^{a_n}} \; ,
\end{split}
\end{equation}

such that the log model evidence ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:lme}) is shown to be

\begin{equation} \label{eq:poissexp-lme-Poiss-exp-LME-qed}
\begin{split}
\log p(y|m) = &\sum_{i=1}^n y_i \log(x_i) - \sum_{i=1}^n \log y_i ! + \\ 
&\log \Gamma(a_n) - \log \Gamma(a_0) + a_0 \log b_0 - a_n \log b_n \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\pagebreak
\section{Frequency data}

\subsection{Beta-distributed data}

\subsubsection[\textit{Definition}]{Definition} \label{sec:beta-data}
\setcounter{equation}{0}

\textbf{Definition:} Beta-distributed data are defined as a set of proportions $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ with $y_i \in [0,1], \; i = 1,\ldots,n$, independent and identically distributed according to a beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta}) with shapes $\alpha$ and $\beta$:

\begin{equation} \label{eq:beta-data-beta-data}
y_i \sim \mathrm{Bet}(\alpha,\beta), \quad i = 1, \ldots, n \; .
\end{equation}


\subsubsection[\textbf{Method of moments}]{Method of moments} \label{sec:beta-mome}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ be a set of observed counts independent and identically distributed according to a beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta}) with shapes $\alpha$ and $\beta$:

\begin{equation} \label{eq:beta-mome-Beta}
y_i \sim \mathrm{Bet}(\alpha,\beta), \quad i = 1, \ldots, n \; .
\end{equation}

Then, the method-of-moments estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mome}) for the shape parameters $\alpha$ and $\beta$ are given by

\begin{equation} \label{eq:beta-mome-Beta-MoM}
\begin{split}
\hat{\alpha} &= \bar{y} \left( \frac{\bar{y} (1-\bar{y})}{\bar{v}} - 1  \right) \\
\hat{\beta} &= (1-\bar{y}) \left( \frac{\bar{y} (1-\bar{y})}{\bar{v}} - 1  \right)
\end{split}
\end{equation}

where $\bar{y}$ is the sample mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean-samp}) and $\bar{v}$ is the unbiased sample variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}):

\begin{equation} \label{eq:beta-mome-y-mean-var}
\begin{split}
\bar{y} &= \frac{1}{n} \sum_{i=1}^n y_i \\
\bar{v} &= \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2 \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} Mean ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-mean}) and variance ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta-var}) of the beta distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:beta}) in terms of the parameters $\alpha$ and $\beta$ are given by

\begin{equation} \label{eq:beta-mome-Beta-E-Var}
\begin{split}
\mathrm{E}(X) &= \frac{\alpha}{\alpha+\beta} \\
\mathrm{Var}(X) &= \frac{\alpha\beta}{(\alpha+\beta)^2 (\alpha+\beta+1)} \; .
\end{split}
\end{equation}

Thus, matching the moments ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mome}) requires us to solve the following equation system for $\alpha$ and $\beta$:

\begin{equation} \label{eq:beta-mome-Beta-mean-var}
\begin{split}
\bar{y} &= \frac{\alpha}{\alpha+\beta} \\
\bar{v} &= \frac{\alpha\beta}{(\alpha+\beta)^2 (\alpha+\beta+1)} \; .
\end{split}
\end{equation}

From the first equation, we can deduce:

\begin{equation} \label{eq:beta-mome-beta-as-alpha}
\begin{split}
\bar{y}(\alpha+\beta) &= \alpha \\
\alpha \bar{y} + \beta \bar{y} &= \alpha \\
\beta \bar{y} &= \alpha - \alpha \bar{y} \\
\beta &= \frac{\alpha}{\bar{y}} - \alpha \\
\beta &= \alpha \left( \frac{1}{\bar{y}} - 1 \right) \; .
\end{split}
\end{equation}

If we define $q = 1/\bar{y} - 1$ and plug \eqref{eq:beta-mome-beta-as-alpha} into the second equation, we have:

\begin{equation} \label{eq:beta-mome-alpha-as-q}
\begin{split}
\bar{v} &= \frac{\alpha \cdot \alpha q}{(\alpha + \alpha q)^2 (\alpha + \alpha q + 1)} \\
&= \frac{\alpha^2 q}{(\alpha (1+q))^2 (\alpha (1+q) + 1)} \\
&= \frac{q}{(1+q)^2 (\alpha (1+q) + 1)} \\
&= \frac{q}{\alpha (1+q)^3 + (1+q)^2} \\
q &= \bar{v} \left[ \alpha (1+q)^3 + (1+q)^2 \right] \; .
\end{split}
\end{equation}

Noting that $1+q = 1/\bar{y}$ and $q = (1-\bar{y})/\bar{y}$, one obtains for $\alpha$:

\begin{equation} \label{eq:beta-mome-Beta-MoM-alpha}
\begin{split}
\frac{1-\bar{y}}{\bar{y}} &= \bar{v} \left[ \frac{\alpha}{\bar{y}^3} + \frac{1}{\bar{y}^2} \right] \\
\frac{1-\bar{y}}{\bar{y} \, \bar{v}} &= \frac{\alpha}{\bar{y}^3} + \frac{1}{\bar{y}^2} \\
\frac{\bar{y}^3(1-\bar{y})}{\bar{y} \, \bar{v}} &= \alpha + \bar{y} \\
\alpha &= \frac{\bar{y}^2(1-\bar{y})}{\bar{v}} - \bar{y} \\
&= \bar{y} \left( \frac{\bar{y} (1-\bar{y})}{\bar{v}} - 1 \right) \; .
\end{split}
\end{equation}

Plugging this into equation \eqref{eq:beta-mome-beta-as-alpha}, one obtains for $\beta$:

\begin{equation} \label{eq:beta-mome-Beta-MoM-beta}
\begin{split}
\beta &= \bar{y} \left( \frac{\bar{y} (1-\bar{y})}{\bar{v}} - 1 \right) \cdot \left( \frac{1-\bar{y}}{\bar{y}} \right) \\
&= (1-\bar{y}) \left( \frac{\bar{y} (1-\bar{y})}{\bar{v}} - 1 \right) \; .
\end{split}
\end{equation}

Together, \eqref{eq:beta-mome-Beta-MoM-alpha} and \eqref{eq:beta-mome-Beta-MoM-beta} constitute the method-of-moment estimates of $\alpha$ and $\beta$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Beta distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-01-20; URL: \url{https://en.wikipedia.org/wiki/Beta_distribution#Method_of_moments}.
\end{itemize}
\vspace{1em}



\subsection{Dirichlet-distributed data}

\subsubsection[\textit{Definition}]{Definition} \label{sec:dir-data}
\setcounter{equation}{0}

\textbf{Definition:} Dirichlet-distributed data are defined as a set of vectors of proportions $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ where

\begin{equation} \label{eq:dir-data-dir-def}
\begin{split}
y_i &= [y_{i1}, \ldots, y_{ik}], \\
y_{ij} &\in [0,1] \quad \text{and} \\
\sum_{j=1}^k y_{ij} &= 1
\end{split}
\end{equation}

for all $i = 1,\ldots,n$ (and $j = 1,\ldots,k$) and each $y_i$ is independent and identically distributed according to a Dirichlet distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir}) with concentration parameters $\alpha = [\alpha_1, \ldots, \alpha_k]$:

\begin{equation} \label{eq:dir-data-dir-data}
y_i \sim \mathrm{Dir}(\alpha), \quad i = 1, \ldots, n \; .
\end{equation}


\subsubsection[\textbf{Maximum likelihood estimation}]{Maximum likelihood estimation} \label{sec:dir-mle}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a Dirichlet-distributed data ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:dir-data}) set $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$:

\begin{equation} \label{eq:dir-mle-Dir}
y_i \sim \mathrm{Dir}(\alpha), \quad i = 1, \ldots, n \; .
\end{equation}

Then, the maximum likelihood estimate ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) for the concentration parameters $\alpha$ can be obtained by iteratively computing

\begin{equation} \label{eq:dir-mle-Dir-MLE}
\alpha_j^{\text{(new)}} = \psi^{-1}\left[ \psi\left( \sum_{j=1}^k \alpha_j^{\text{(old)}} \right) + \log \bar{y}_j \right]
\end{equation}

where $\psi(x)$ is the digamma function and $\log \bar{y}_j$ is given by:

\begin{equation} \label{eq:dir-mle-log-pi}
\log \bar{y}_j = \frac{1}{n} \sum_{i=1}^n \log y_{ij} \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} The likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) for each observation is given by the probability density function of the Dirichlet distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir-pdf})

\begin{equation} \label{eq:dir-mle-Dir-yi}
p(y_i|\alpha) = \frac{\Gamma\left( \sum_{j=1}^k \alpha_j \right)}{\prod_{j=1}^k \Gamma(\alpha_j)} \, \prod_{j=1}^k {y_{ij}}^{\alpha_j-1}
\end{equation}

and because observations are independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}), the likelihood function for all observations is the product of the individual ones:

\begin{equation} \label{eq:dir-mle-Dir-LF}
p(y|\alpha) = \prod_{i=1}^n p(y_i|\alpha) = \prod_{i=1}^n \left[ \frac{\Gamma\left( \sum_{j=1}^k \alpha_j \right)}{\prod_{j=1}^k \Gamma(\alpha_j)} \, \prod_{j=1}^k {y_{ij}}^{\alpha_j-1} \right] \; .
\end{equation}

Thus, the log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf}) is

\begin{equation} \label{eq:dir-mle-Dir-LL}
\mathrm{LL}(\alpha) = \log p(y|\alpha) = \log \prod_{i=1}^n \left[ \frac{\Gamma\left( \sum_{j=1}^k \alpha_j \right)}{\prod_{j=1}^k \Gamma(\alpha_j)} \, \prod_{j=1}^k {y_{ij}}^{\alpha_j-1} \right]
\end{equation}

which can be developed into

\begin{equation} \label{eq:dir-mle-Dir-LL-der}
\begin{split}
\mathrm{LL}(\alpha) &= \sum_{i=1}^n \log \Gamma\left( \sum_{j=1}^k \alpha_j \right) - \sum_{i=1}^n \sum_{j=1}^k \log \Gamma(\alpha_j) + \sum_{i=1}^n \sum_{j=1}^k (\alpha_j-1) \log y_{ij} \\
&= n \log \Gamma\left( \sum_{j=1}^k \alpha_j \right) - n \sum_{j=1}^k \log \Gamma(\alpha_j) + n \sum_{j=1}^k (\alpha_j-1) \, \frac{1}{n} \sum_{i=1}^n \log y_{ij} \\
&= n \log \Gamma\left( \sum_{j=1}^k \alpha_j \right) - n \sum_{j=1}^k \log \Gamma(\alpha_j) + n \sum_{j=1}^k (\alpha_j-1) \, \log \bar{y}_j
\end{split}
\end{equation}

where we have specified

\begin{equation} \label{eq:dir-mle-log-pi-reit}
\log \bar{y}_j = \frac{1}{n} \sum_{i=1}^n \log y_{ij} \; .
\end{equation}

The derivative of the log-likelihood with respect to a particular parameter $\alpha_j$ is

\begin{equation} \label{eq:dir-mle-Dir-dLLdaj}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\alpha)}{\mathrm{d}\alpha_j} &= \frac{\mathrm{d}}{\mathrm{d}\alpha_j} \left[ n \log \Gamma\left( \sum_{j=1}^k \alpha_j \right) - n \sum_{j=1}^k \log \Gamma(\alpha_j) + n \sum_{j=1}^k (\alpha_j-1) \, \log \bar{y}_j \right] \\
&= \frac{\mathrm{d}}{\mathrm{d}\alpha_j} \left[ n \log \Gamma\left( \sum_{j=1}^k \alpha_j \right) \right] - \frac{\mathrm{d}}{\mathrm{d}\alpha_j} \left[ n \log \Gamma(\alpha_j) \right] + \frac{\mathrm{d}}{\mathrm{d}\alpha_j} \left[ n (\alpha_j-1) \, \log \bar{y}_j \right] \\
&= n \psi\left( \sum_{j=1}^k \alpha_j \right) - n \psi(\alpha_j) + n \log \bar{y}_j
\end{split}
\end{equation}

where we have used the digamma function

\begin{equation} \label{eq:dir-mle-digamma-fct}
\psi(x) = \frac{\mathrm{d}\log \Gamma(x)}{\mathrm{d}x} \; .
\end{equation}

Setting this derivative to zero, we obtain:

\begin{equation} \label{eq:dir-mle-Dir-dLLdaj-0}
\begin{split}
\frac{\mathrm{d}\mathrm{LL}(\alpha)}{\mathrm{d}\alpha_j} &= 0 \\
0 &= n \psi\left( \sum_{j=1}^k \alpha_j \right) - n \psi(\alpha_j) + n \log \bar{y}_j \\
0 &= \psi\left( \sum_{j=1}^k \alpha_j \right) - \psi(\alpha_j) + \log \bar{y}_j \\
\psi(\alpha_j) &= \psi\left( \sum_{j=1}^k \alpha_j \right) + \log \bar{y}_j \\
\alpha_j &= \psi^{-1} \left[ \psi\left( \sum_{j=1}^k \alpha_j \right) + \log \bar{y}_j \right] \; .
\end{split}
\end{equation}

In the following, we will use a fixed-point iteration to maximize $\mathrm{LL}(\alpha)$. Given an initial guess for $\alpha$, we construct a lower bound on the likelihood function \eqref{eq:dir-mle-Dir-LL-der} which is tight at $\alpha$. The maximum of this bound is computed and it becomes the new guess. Because the Dirichlet distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:dir}) belongs to the exponential family, the log-likelihood function is convex in $\alpha$ Ã¡nd the maximum is the only stationary point, such that the procedure is guaranteed to converge to the maximum.

In our case, we use a bound on the gamma function

\begin{equation} \label{eq:dir-mle-gamma-fct-bound}
\begin{split}
\Gamma(x) &\geq \Gamma(\hat{x}) \cdot \mathrm{exp}\left[(x-\hat{x}) \, \psi(\hat{x}) \right] \\
\log \Gamma(x) &\geq \log \Gamma(\hat{x}) + (x-\hat{x}) \, \psi(\hat{x})
\end{split}
\end{equation}

and apply it to $\Gamma\left( \sum_{j=1}^k \alpha_j \right)$ in \eqref{eq:dir-mle-Dir-LL-der} to yield

\begin{equation} \label{eq:dir-mle-Dir-LL-bound}
\begin{split}
\frac{1}{n} \mathrm{LL}(\alpha) &= \log \Gamma\left( \sum_{j=1}^k \alpha_j \right) - \sum_{j=1}^k \log \Gamma(\alpha_j) + \sum_{j=1}^k (\alpha_j-1) \, \log \bar{y}_j \\
\frac{1}{n} \mathrm{LL}(\alpha) &\geq \log \Gamma\left(\sum_{j=1}^k \hat{\alpha}_j\right) + \left(\sum_{j=1}^k \alpha_j - \sum_{j=1}^k \hat{\alpha}_j\right) \psi\left(\sum_{j=1}^k \hat{\alpha}_j\right) - \sum_{j=1}^k \log \Gamma(\alpha_j) + \sum_{j=1}^k (\alpha_j-1) \, \log \bar{y}_j \\
\frac{1}{n} \mathrm{LL}(\alpha) &\geq \left(\sum_{j=1}^k \alpha_j\right) \psi\left(\sum_{j=1}^k \hat{\alpha}_j\right) - \sum_{j=1}^k \log \Gamma(\alpha_j) + \sum_{j=1}^k (\alpha_j-1) \, \log \bar{y}_j + \mathrm{const.}
\end{split}
\end{equation}

which leads to the following fixed-point iteration using \eqref{eq:dir-mle-Dir-dLLdaj-0}:

\begin{equation} \label{eq:dir-mle-Dir-MLE-qed}
\alpha_j^{\text{(new)}} = \psi^{-1}\left[ \psi\left( \sum_{j=1}^k \alpha_j^{\text{(old)}} \right) + \log \bar{y}_j \right] \; .
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Minka TP (2012): "Estimating a Dirichlet distribution"; in: \textit{Papers by Tom Minka}, retrieved on 2020-10-22; URL: \url{https://tminka.github.io/papers/dirichlet/minka-dirichlet.pdf}.
\end{itemize}
\vspace{1em}



\subsection{Beta-binomial data}

\subsubsection[\textit{Definition}]{Definition} \label{sec:betabin-data}
\setcounter{equation}{0}

\textbf{Definition:} Beta-binomial data are defined as a set of counts $y = \left\lbrace y_1, \ldots, y_N \right\rbrace$ with $y_i \in \mathbb{N}, \; i = 1,\ldots,N$, independent and identically distributed according to a beta-binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:betabin}) with number of trials $n$ as well as shapes $\alpha$ and $\beta$:

\begin{equation} \label{eq:betabin-data-betabin-data}
y_i \sim \mathrm{BetBin}(n,\alpha,\beta), \quad i = 1, \ldots, N \; .
\end{equation}


\subsubsection[\textbf{Method of moments}]{Method of moments} \label{sec:betabin-mome}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = \left\lbrace y_1, \ldots, y_N \right\rbrace$ be a set of observed counts independent and identically distributed according to a beta-binomial distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:betabin}) with number of trials $n$ as well as parameters $\alpha$ and $\beta$:

\begin{equation} \label{eq:betabin-mome-binbeta}
y_i \sim \mathrm{BetBin}(n, \alpha, \beta), \quad i = 1, \ldots, N \; .
\end{equation}

Then, the method-of-moments estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mome}) for the parameters $\alpha$ and $\beta$ are given by

\begin{equation} \label{eq:betabin-mome-binbeta-mome}
\begin{split}
\hat{\alpha} &= \frac{n m_1 - m_2}{n \left( \frac{m_2}{m_1} - m_1 - 1 \right) + m_1} \\
\hat{\beta} &= \frac{\left( n - m_1 \right)\left( n - \frac{m_2}{m_1} \right)}{n \left( \frac{m_2}{m_1} - m_1 - 1 \right) + m_1}
\end{split}
\end{equation}

where $m_1$ and $m_2$ are the first two raw sample moments ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mom-raw}):

\begin{equation} \label{eq:betabin-mome-y-m1-m2}
\begin{split}
m_1 &= \frac{1}{N} \sum_{i=1}^N y_i \\
m_2 &= \frac{1}{N} \sum_{i=1}^N y_i^2 \; .
\end{split}
\end{equation}


\vspace{1em}
\textbf{Proof:} The first two raw moments of the beta-binomial distribution in terms of the parameters $\alpha$ and $\beta$ are given by

\begin{equation} \label{eq:betabin-mome-binbeta-mu1-mu2}
\begin{split}
\mu_1 &= \frac{n \alpha}{\alpha + \beta} \\
\mu_2 &= \frac{n \alpha (n \alpha + \beta + n)}{(\alpha + \beta)(n \alpha + \beta + 1)}
\end{split}
\end{equation}

Thus, matching the moments ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mome}) requires us to solve the following equation system for $\alpha$ and $\beta$:

\begin{equation} \label{eq:betabin-mome-binbeta-m1-m2}
\begin{split}
m_1 &= \frac{n \alpha}{\alpha + \beta} \\
m_2 &= \frac{n \alpha (n \alpha + \beta + n)}{(\alpha + \beta)(n \alpha + \beta + 1)} \; .
\end{split}
\end{equation}

From the first equation, we can deduce:

\begin{equation} \label{eq:betabin-mome-beta-as-alpha}
\begin{split}
m_1(\alpha+\beta) &= n \alpha \\
m_1 \alpha + m_1 \beta &= n \alpha \\
m_1 \beta &= n \alpha - m_1 \alpha \\
\beta &= \frac{n \alpha}{m_1} - \alpha \\
\beta &= \alpha \left( \frac{n}{m_1} - 1 \right) \; .
\end{split}
\end{equation}

If we define $q = n/m_1 - 1$ and plug \eqref{eq:betabin-mome-beta-as-alpha} into the second equation, we have:

\begin{equation} \label{eq:betabin-mome-alpha-as-q}
\begin{split}
m_2 &= \frac{n \alpha (n \alpha + \alpha q + n)}{(\alpha + \alpha q)(\alpha + \alpha q + 1)} \\
&= \frac{n \alpha (\alpha (n + q) + n)}{\alpha (1 + q)(\alpha (1 + q) + 1)} \\
&= \frac{n (\alpha (n + q) + n)}{(1 + q)(\alpha (1 + q) + 1)} \\
&= \frac{n (\alpha (n + q) + n)}{\alpha (1 + q)^2 + (1 + q)} \; .
\end{split}
\end{equation}

Noting that $1+q = n/m_1$ and expanding the fraction with $m_1$, one obtains:

\begin{equation} \label{eq:betabin-mome-binbeta-mome-alpha}
\begin{split}
m_2 &= \frac{n \left(\alpha \left( \frac{n}{m_1} + n - 1 \right) + n \right)}{n \left( \alpha \frac{n}{m_1^2} + \frac{1}{m_1} \right)} \\
m_2 &= \frac{\alpha \left( n + n m_1 - m_1 \right) + n m_1}{\alpha \frac{n}{m_1} + 1} \\
m_2 \left( \frac{\alpha n}{m_1} + 1 \right) &= \alpha \left( n + n m_1 - m_1 \right) + n m_1 \\
\alpha \left( n \frac{m_2}{m_1} - (n + n m_1 - m_1) \right) &= n m_1 - m_2 \\
\alpha \left( n \left( \frac{m_2}{m_1} - m_1 - 1 \right) + m_1 \right) &= n m_1 - m_2 \\
\alpha &= \frac{n m_1 - m_2}{n \left( \frac{m_2}{m_1} - m_1 - 1 \right) + m_1} \; .
\end{split}
\end{equation}

Plugging this into equation \eqref{eq:betabin-mome-beta-as-alpha}, one obtains for $\beta$:

\begin{equation} \label{eq:betabin-mome-binbeta-mome-beta}
\begin{split}
\beta &= \alpha \left( \frac{n}{m_1} - 1 \right) \\
\beta &= \left( \frac{n m_1 - m_2}{n \left( \frac{m_2}{m_1} - m_1 - 1 \right) + m_1} \right) \left( \frac{n}{m_1} - 1 \right) \\
\beta &= \frac{n^2 - n m_1 - n \frac{m_2}{m_1} + m_2}{n \left( \frac{m_2}{m_1} - m_1 - 1 \right) + m_1} \\
\hat{\beta} &= \frac{\left( n - m_1 \right)\left( n - \frac{m_2}{m_1} \right)}{n \left( \frac{m_2}{m_1} - m_1 - 1 \right) + m_1} \; .
\end{split}
\end{equation}

Together, \eqref{eq:betabin-mome-binbeta-mome-alpha} and \eqref{eq:betabin-mome-binbeta-mome-beta} constitute the method-of-moment estimates of $\alpha$ and $\beta$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item statisticsmatt (2022): "Method of Moments Estimation Beta Binomial Distribution"; in: \textit{YouTube}, retrieved on 2022-10-07; URL: \url{https://www.youtube.com/watch?v=18PWnWJsPnA}.
\item Wikipedia (2022): "Beta-binomial distribution"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2022-10-07; URL: \url{https://en.wikipedia.org/wiki/Beta-binomial_distribution#Method_of_moments}.
\end{itemize}
\vspace{1em}



\pagebreak
\section{Categorical data}

\subsection{Logistic regression}

\subsubsection[\textit{Definition}]{Definition} \label{sec:logreg}
\setcounter{equation}{0}

\textbf{Definition:} A logistic regression model is given by a set of binary observations $y_i \in \left\lbrace 0, 1 \right\rbrace, i = 1,\ldots,n$, a set of predictors $x_j \in \mathbb{R}^n, j = 1,\ldots,p$, a base $b$ and the assumption that the log-odds are a linear combination of the predictors:

\begin{equation} \label{eq:logreg-logreg}
l_i = x_i \beta + \varepsilon_i, \; i = 1,\ldots,n
\end{equation}

where $l_i$ are the log-odds that $y_i = 1$

\begin{equation} \label{eq:logreg-logodds}
l_i = \log_b \frac{\mathrm{Pr}(y_i = 1)}{\mathrm{Pr}(y_i = 0)}
\end{equation}

and $x_i$ is the $i$-th row of the $n \times p$ matrix

\begin{equation} \label{eq:logreg-X}
X = \left[ x_1, \ldots, x_p \right] \; .
\end{equation}

Within this model,

\begin{itemize}

\item $y$ are called "categorical observations" or "dependent variable";

\item $X$ is called "design matrix" or "set of independent variables";

\item $\beta$ are called "regression coefficients" or "weights";

\item $\varepsilon_i$ is called "noise" or "error term";

\item $n$ is the number of observations;

\item $p$ is the number of predictors.

\end{itemize}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Logistic regression"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-06-28; URL: \url{https://en.wikipedia.org/wiki/Logistic_regression#Logistic_model}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Probability and log-odds}]{Probability and log-odds} \label{sec:logreg-pnlo}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a logistic regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:logreg})

\begin{equation} \label{eq:logreg-pnlo-logreg}
l_i = x_i \beta + \varepsilon_i, \; i = 1,\ldots,n
\end{equation}

where $x_i$ are the predictors corresponding to the $i$-th observation $y_i$ and $l_i$ are the log-odds that $y_i = 1$.

Then, the log-odds in favor of $y_i = 1$ against $y_i = 0$ can also be expressed as

\begin{equation} \label{eq:logreg-pnlo-lodds}
l_i = \log_b \frac{p(x_i|y_i=1) \, p(y_i=1)}{p(x_i|y_i=0) \, p(y_i=0)}
\end{equation}

where $p(x_i \vert y_i)$ is a likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:lf}) consistent with \eqref{eq:logreg-pnlo-logreg}, $p(y_i)$ are prior probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prior}) for $y_i = 1$ and $y_i = 0$ and where $b$ is the base used to form the log-odds $l_i$.


\vspace{1em}
\textbf{Proof:} Using Bayes' theorem ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:bayes-th}) and the law of marginal probability ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:prob-marg}), the posterior probabilities ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:post}) for $y_i = 1$ and $y_i = 0$ are given by

\begin{equation} \label{eq:logreg-pnlo-prob}
\begin{split}
p(y_i=1|x_i) &= \frac{p(x_i|y_i=1) \, p(y_i=1)}{p(x_i|y_i=1) \, p(y_i=1) + p(x_i|y_i=0) \, p(y_i=0)} \\
p(y_i=0|x_i) &= \frac{p(x_i|y_i=0) \, p(y_i=0)}{p(x_i|y_i=1) \, p(y_i=1) + p(x_i|y_i=0) \, p(y_i=0)} \; .
\end{split}
\end{equation}

Calculating the log-odds from the posterior probabilties, we have

\begin{equation} \label{eq:logreg-pnlo-lodds-qed}
\begin{split}
l_i &= \log_b \frac{p(y_i=1|x_i)}{p(y_i=0|x_i)} \\
&= \log_b \frac{p(x_i|y_i=1) \, p(y_i=1)}{p(x_i|y_i=0) \, p(y_i=0)} \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Bishop, Christopher M. (2006): "Linear Models for Classification"; in: \textit{Pattern Recognition for Machine Learning}, ch. 4, p. 197, eq. 4.58; URL: \url{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Log-odds and probability}]{Log-odds and probability} \label{sec:logreg-lonp}
\setcounter{equation}{0}

\textbf{Theorem:} Assume a logistic regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:logreg})

\begin{equation} \label{eq:logreg-lonp-logreg}
l_i = x_i \beta + \varepsilon_i, \; i = 1,\ldots,n
\end{equation}

where $x_i$ are the predictors corresponding to the $i$-th observation $y_i$ and $l_i$ are the log-odds that $y_i = 1$.

Then, the probability that $y_i = 1$ is given by

\begin{equation} \label{eq:logreg-lonp-prob}
\mathrm{Pr}(y_i = 1) = \frac{1}{1 + b^{-(x_i \beta + \varepsilon_i)}}
\end{equation}

where $b$ is the base used to form the log-odds $l_i$.


\vspace{1em}
\textbf{Proof:} Let us denote $\mathrm{Pr}(y_i = 1)$ as $p_i$. Then, the log-odds are

\begin{equation} \label{eq:logreg-lonp-lodds}
l_i = \log_b \frac{p_i}{1-p_i}
\end{equation}

and using \eqref{eq:logreg-lonp-logreg}, we have

\begin{equation} \label{eq:logreg-lonp-prob-qed}
\begin{split}
\log_b \frac{p_i}{1-p_i} &= x_i \beta + \varepsilon_i \\
\frac{p_i}{1-p_i} &= b^{x_i \beta + \varepsilon_i} \\
p_i &= \left( b^{x_i \beta + \varepsilon_i} \right) (1-p_i) \\
p_i \left( 1 + b^{x_i \beta + \varepsilon_i} \right) &= b^{x_i \beta + \varepsilon_i} \\
p_i &= \frac{b^{x_i \beta + \varepsilon_i}}{1 + b^{x_i \beta + \varepsilon_i}} \\
p_i &= \frac{b^{x_i \beta + \varepsilon_i}}{b^{x_i \beta + \varepsilon_i} \left( 1 + b^{-(x_i \beta + \varepsilon_i)} \right)} \\
p_i &= \frac{1}{1 + b^{-(x_i \beta + \varepsilon_i)}}
\end{split}
\end{equation}

which proves the identity given by \eqref{eq:logreg-lonp-prob}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Logistic regression"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-03-03; URL: \url{https://en.wikipedia.org/wiki/Logistic_regression#Logistic_model}.
\end{itemize}
\vspace{1em}





% Chapter 4 %
\chapter{Model Selection} \label{sec:Model Selection} \newpage

\pagebreak
\section{Goodness-of-fit measures}

\subsection{Residual variance}

\subsubsection[\textit{Definition}]{Definition} \label{sec:resvar}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr})

\begin{equation} \label{eq:resvar-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V)
\end{equation}

with measured data $y$, known design matrix $X$ and covariance structure $V$ as well as unknown regression coefficients $\beta$ and noise variance $\sigma^2$.

Then, an estimate of the noise variance $\sigma^2$ is called the "residual variance" $\hat{\sigma}^2$, e.g. obtained via maximum likelihood estimation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}).


\subsubsection[\textbf{Maximum likelihood estimator is biased (p = 1)}]{Maximum likelihood estimator is biased (p = 1)} \label{sec:resvar-bias}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ be a set of independent normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) observations with unknown mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu$ and variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) $\sigma^2$:

\begin{equation} \label{eq:resvar-bias-ug}
y_i \overset{\text{i.i.d.}}{\sim} \mathcal{N}(\mu, \sigma^2), \quad i = 1,\ldots,n \; .
\end{equation}

Then,

1) the maximum likelihood estimator ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) of $\sigma^2$ is

\begin{equation} \label{eq:resvar-bias-resvar-mle}
\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \bar{y} \right)^2
\end{equation}

where

\begin{equation} \label{eq:resvar-bias-mean-mle}
\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i
\end{equation}

2) and $\hat{\sigma}^2$ is a biased estimator of $\sigma^2$

\begin{equation} \label{eq:resvar-bias-resvar-var}
\mathrm{E}\left[ \hat{\sigma}^2 \right] \neq \sigma^2 \; ,
\end{equation}

more precisely:

\begin{equation} \label{eq:resvar-bias-resvar-bias}
\mathrm{E}\left[ \hat{\sigma}^2 \right] = \frac{n-1}{n} \sigma^2 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:}

1) This is equivalent to the maximum likelihood estimator for the univariate Gaussian with unknown variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ug-mle}) and a special case of the maximum likelihood estimator for multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle}) in which $X = 1_n$ and $\hat{\beta} = \bar{y}$:

\begin{equation} \label{eq:resvar-bias-resvar-mle-qed}
\begin{split}
\hat{\sigma}^2 &= \frac{1}{n} (y-X\hat{\beta})^\mathrm{T} (y-X\hat{\beta}) \\
&= \frac{1}{n} (y - 1_n \bar{y})^\mathrm{T} (y - 1_n \bar{y}) \\
&= \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \bar{y} \right)^2 \; .
\end{split}
\end{equation}

2) The expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of the maximum likelihood estimator ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) can be developed as follows:

\begin{equation} \label{eq:resvar-bias-E-resvar-mle-s1}
\begin{split}
\mathrm{E}\left[ \hat{\sigma}^2 \right] &= \mathrm{E}\left[ \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \bar{y} \right)^2 \right] \\
&= \frac{1}{n} \mathrm{E}\left[ \sum_{i=1}^{n} \left( y_i - \bar{y} \right)^2 \right] \\
&= \frac{1}{n} \mathrm{E}\left[ \sum_{i=1}^{n} \left( y_i^2 - 2 y_i \bar{y} + \bar{y}^2 \right) \right] \\
&= \frac{1}{n} \mathrm{E}\left[ \sum_{i=1}^{n} y_i^2 - 2 \sum_{i=1}^{n} y_i \bar{y} + \sum_{i=1}^{n} \bar{y}^2 \right] \\
&= \frac{1}{n} \mathrm{E}\left[ \sum_{i=1}^{n} y_i^2 - 2 n \bar{y}^2 + n \bar{y}^2 \right] \\
&= \frac{1}{n} \mathrm{E}\left[ \sum_{i=1}^{n} y_i^2 - n \bar{y}^2 \right] \\
&= \frac{1}{n} \left( \sum_{i=1}^{n} \mathrm{E} \left[ y_i^2 \right] - n \mathrm{E}\left[ \bar{y}^2 \right] \right) \\
&= \frac{1}{n} \sum_{i=1}^{n} \mathrm{E} \left[ y_i^2 \right] - \mathrm{E}\left[ \bar{y}^2 \right] \\
\end{split}
\end{equation}

Due to the partition of variance into expected values ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-mean})

\begin{equation} \label{eq:resvar-bias-var-mean}
\mathrm{Var}(X) = \mathrm{E}(X^2) - \mathrm{E}(X)^2 \; ,
\end{equation}

we have

\begin{equation} \label{eq:resvar-bias-Var-yi-yb}
\begin{split}
\mathrm{Var}(y_i) &= \mathrm{E}(y_i^2) - \mathrm{E}(y_i)^2 \\
\mathrm{Var}(\bar{y}) &= \mathrm{E}(\bar{y}^2) - \mathrm{E}(\bar{y})^2 \; ,
\end{split}
\end{equation}

such that \eqref{eq:resvar-bias-E-resvar-mle-s1} becomes

\begin{equation} \label{eq:resvar-bias-E-resvar-mle-s2}
\mathrm{E}\left[ \hat{\sigma}^2 \right] = \frac{1}{n} \sum_{i=1}^{n} \left( \mathrm{Var}(y_i) + \mathrm{E}(y_i)^2 \right) - \left( \mathrm{Var}(\bar{y}) + \mathrm{E}(\bar{y})^2 \right) \; .
\end{equation}

From \eqref{eq:resvar-bias-ug}, it follows that

\begin{equation} \label{eq:resvar-bias-E-Var-yi}
\mathrm{E}(y_i) = \mu \quad \text{and} \quad \mathrm{Var}(y_i) = \sigma^2 \; .
\end{equation}

The expectation ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) of $\bar{y}$ given by \eqref{eq:resvar-bias-mean-mle} is

\begin{equation} \label{eq:resvar-bias-E-mean-mle}
\begin{split}
\mathrm{E}\left[ \bar{y} \right] &= \mathrm{E}\left[ \frac{1}{n} \sum_{i=1}^{n} y_i \right] = \frac{1}{n} \sum_{i=1}^{n} \mathrm{E}\left[ y_i \right] \\
&\overset{\eqref{eq:resvar-bias-E-Var-yi}}{=} \frac{1}{n} \sum_{i=1}^{n} \mu = \frac{1}{n} \cdot n \cdot \mu \\
&= \mu \; .
\end{split}
\end{equation}

The variance of $\bar{y}$ given by \eqref{eq:resvar-bias-mean-mle} is

\begin{equation} \label{eq:resvar-bias-Var-mean-mle}
\begin{split}
\mathrm{Var}\left[ \bar{y} \right] &= \mathrm{Var}\left[ \frac{1}{n} \sum_{i=1}^{n} y_i \right] = \frac{1}{n^2} \sum_{i=1}^{n} \mathrm{Var}\left[ y_i \right] \\
&\overset{\eqref{eq:resvar-bias-E-Var-yi}}{=} \frac{1}{n^2} \sum_{i=1}^{n} \sigma^2 = \frac{1}{n^2} \cdot n \cdot \sigma^2 \\
&= \frac{1}{n} \sigma^2 \; .
\end{split}
\end{equation}

Plugging \eqref{eq:resvar-bias-E-Var-yi}, \eqref{eq:resvar-bias-E-mean-mle} and \eqref{eq:resvar-bias-Var-mean-mle} into \eqref{eq:resvar-bias-E-resvar-mle-s2}, we have

\begin{equation} \label{eq:resvar-bias-E-resvar-mle-s3}
\begin{split}
\mathrm{E}\left[ \hat{\sigma}^2 \right] &= \frac{1}{n} \sum_{i=1}^{n} \left( \sigma^2 + \mu^2 \right) - \left( \frac{1}{n} \sigma^2 + \mu^2 \right) \\
\mathrm{E}\left[ \hat{\sigma}^2 \right] &= \frac{1}{n} \cdot n \cdot \left( \sigma^2 + \mu^2 \right) - \left( \frac{1}{n} \sigma^2 + \mu^2 \right) \\
\mathrm{E}\left[ \hat{\sigma}^2 \right] &= \sigma^2 + \mu^2 - \frac{1}{n} \sigma^2 - \mu^2 \\
\mathrm{E}\left[ \hat{\sigma}^2 \right] &= \frac{n-1}{n} \sigma^2
\end{split}
\end{equation}

which proves the bias given by \eqref{eq:resvar-bias-resvar-bias}.

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Liang, Dawen (????): "Maximum Likelihood Estimator for Variance is Biased: Proof", retrieved on 2020-02-24; URL: \url{https://dawenl.github.io/files/mle_biased.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Maximum likelihood estimator is biased (p > 1)}]{Maximum likelihood estimator is biased (p > 1)} \label{sec:resvar-biasp}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with known design matrix $X$, known covariance structure $V$, unknown regression parameters $\beta$ and unknown noise variance $\sigma^2$:

\begin{equation} \label{eq:resvar-biasp-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; .
\end{equation}

Then,

1) the maximum likelihood estimator ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) of $\sigma^2$ is

\begin{equation} \label{eq:resvar-biasp-sigma-mle}
\hat{\sigma}^2 = \frac{1}{n} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta})
\end{equation}

where

\begin{equation} \label{eq:resvar-biasp-beta-mle}
\hat{\beta} = (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y
\end{equation}

2) and $\hat{\sigma}^2$ is a biased estimator of $\sigma^2$

\begin{equation} \label{eq:resvar-biasp-resvar-var}
\mathrm{E}\left[ \hat{\sigma}^2 \right] \neq \sigma^2 \; ,
\end{equation}

more precisely:

\begin{equation} \label{eq:resvar-biasp-resvar-biasp}
\mathrm{E}\left[ \hat{\sigma}^2 \right] = \frac{n-p}{n} \sigma^2 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} 

1) This follows from maximum likelihood estimation for multiple linear regression ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle}) and is a special case ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-glm}) of maximum likelihood estimation for the general linear model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:glm-mle}) in which $Y = y$, $B = \beta$ and $\Sigma = \sigma^2$:

\begin{equation} \label{eq:resvar-biasp-sigma-mle-qed}
\begin{split}
\hat{\sigma}^2 &= \frac{1}{n} (Y-X\hat{B})^\mathrm{T} V^{-1} (Y-X\hat{B}) \\
&= \frac{1}{n} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) \; .
\end{split}
\end{equation}

2) We know that the residual sum of squares, divided by the true noise variance, is following a chi-squared distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-rssdist}):

\begin{equation} \label{eq:resvar-biasp-rss-dist}
\begin{split}
\frac{\hat{\varepsilon}^\mathrm{T} \hat{\varepsilon}}{\sigma^2} &\sim \chi^2(n-p) \\
\text{where} \quad \hat{\varepsilon}^\mathrm{T} \hat{\varepsilon} &= (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) \; .
\end{split}
\end{equation}

Thus, combining \eqref{eq:resvar-biasp-rss-dist} and \eqref{eq:resvar-biasp-sigma-mle-qed}, we have:

\begin{equation} \label{eq:resvar-biasp-resvar-bias-s1}
\frac{n \hat{\sigma}^2}{\sigma^2} \sim \chi^2(n-p) \; .
\end{equation}

Using the relationship between chi-squared distribution and gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:chi2-gam})

\begin{equation} \label{eq:resvar-biasp-chi2-gam}
X \sim \chi^2(k) \quad \Rightarrow \quad cX \sim \mathrm{Gam}\left( \frac{k}{2}, \frac{1}{2c} \right) \; ,
\end{equation}

we can deduce from \eqref{eq:resvar-biasp-resvar-bias-s1} that

\begin{equation} \label{eq:resvar-biasp-resvar-bias-s2}
\hat{\sigma}^2 = \frac{\sigma^2}{n} \cdot \frac{n \hat{\sigma}^2}{\sigma^2} \sim \mathrm{Gam}\left( \frac{n-p}{2}, \frac{n}{2\sigma^2} \right) \; .
\end{equation}

Using the expected value of the gamma distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:gam-mean})

\begin{equation} \label{eq:resvar-biasp-gam-mean}
X \sim \mathrm{Gam}(a,b) \quad \Rightarrow \quad \mathrm{E}(X) = \frac{a}{b} \; ,
\end{equation}

we can deduce from \eqref{eq:resvar-biasp-resvar-bias-s2} that

\begin{equation} \label{eq:resvar-biasp-resvar-bias-s3}
\mathrm{E}\left[ \hat{\sigma}^2 \right] = \frac{\frac{n-p}{2}}{\frac{n}{2\sigma^2}} = \frac{n-p}{n} \sigma^2
\end{equation}

which proves the relationship given by \eqref{eq:resvar-biasp-resvar-biasp}.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item ocram (2022): "Why is RSS distributed chi square times n-p?"; in: \textit{StackExchange CrossValidated}, retrieved on 2022-12-21; URL: \url{https://stats.stackexchange.com/a/20230}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Construction of unbiased estimator (p = 1)}]{Construction of unbiased estimator (p = 1)} \label{sec:resvar-unb}
\setcounter{equation}{0}

\textbf{Theorem:} Let $y = \left\lbrace y_1, \ldots, y_n \right\rbrace$ be a set of independent normally distributed ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:norm}) observations with unknown mean ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mean}) $\mu$ and variance ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var}) $\sigma^2$:

\begin{equation} \label{eq:resvar-unb-ug}
y_i \overset{\text{i.i.d.}}{\sim} \mathcal{N}(\mu, \sigma^2), \quad i = 1,\ldots,n \; .
\end{equation}

An unbiased estimator of $\sigma^2$ is given by

\begin{equation} \label{eq:resvar-unb-resvar-unb}
\hat{\sigma}^2_{\mathrm{unb}} = \frac{1}{n-1} \sum_{i=1}^{n} \left( y_i - \bar{y} \right)^2 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} It can be shown that ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:resvar-bias}) the maximum likelihood estimator ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) of $\sigma^2$

\begin{equation} \label{eq:resvar-unb-resvar-mle}
\hat{\sigma}^2_{\mathrm{MLE}} = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \bar{y} \right)^2
\end{equation}

is a biased estimator in the sense that

\begin{equation} \label{eq:resvar-unb-resvar-bias}
\mathbb{E}\left[ \hat{\sigma}^2_{\mathrm{MLE}} \right] = \frac{n-1}{n} \sigma^2 \; .
\end{equation}

From \eqref{eq:resvar-unb-resvar-bias}, it follows that

\begin{equation} \label{eq:resvar-unb-resvar-bias-adj}
\begin{split}
\mathbb{E}\left[ \frac{n}{n-1} \hat{\sigma}^2_{\mathrm{MLE}} \right] &= \frac{n}{n-1} \mathbb{E}\left[ \hat{\sigma}^2_{\mathrm{MLE}} \right] \\
&\overset{\eqref{eq:resvar-unb-resvar-bias}}{=} \frac{n}{n-1} \cdot \frac{n-1}{n} \sigma^2 \\
&= \sigma^2 \; ,
\end{split}
\end{equation}

such that an unbiased estimator can be constructed as

\begin{equation} \label{eq:resvar-unb-resvar-unb-qed}
\begin{split}
\hat{\sigma}^2_{\mathrm{unb}} &= \frac{n}{n-1} \hat{\sigma}^2_{\mathrm{MLE}} \\
&\overset{\eqref{eq:resvar-unb-resvar-mle}}{=} \frac{n}{n-1} \cdot \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \bar{y} \right)^2 \\
&= \frac{1}{n-1} \sum_{i=1}^{n} \left( y_i - \bar{y} \right)^2 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Liang, Dawen (????): "Maximum Likelihood Estimator for Variance is Biased: Proof", retrieved on 2020-02-25; URL: \url{https://dawenl.github.io/files/mle_biased.pdf}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Construction of unbiased estimator (p > 1)}]{Construction of unbiased estimator (p > 1)} \label{sec:resvar-unbp}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with known design matrix $X$, known covariance structure $V$, unknown regression parameters $\beta$ and unknown noise variance $\sigma^2$:

\begin{equation} \label{eq:resvar-unbp-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; .
\end{equation}

An unbiased estimator of $\sigma^2$ is given by

\begin{equation} \label{eq:resvar-unbp-sigma-unb}
\hat{\sigma}^2 = \frac{1}{n-p} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta})
\end{equation}

where

\begin{equation} \label{eq:resvar-unbp-beta-mle}
\hat{\beta} = (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} It can be shown that ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:resvar-biasp}) the maximum likelihood estimator ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}) of $\sigma^2$

\begin{equation} \label{eq:resvar-unbp-resvar-mle}
\hat{\sigma}^2_{\mathrm{MLE}} = \frac{1}{n} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta})
\end{equation}

is a biased estimator in the sense that

\begin{equation} \label{eq:resvar-unbp-resvar-bias}
\mathbb{E}\left[ \hat{\sigma}^2_{\mathrm{MLE}} \right] = \frac{n-p}{n} \sigma^2 \; .
\end{equation}

From \eqref{eq:resvar-unbp-resvar-bias}, it follows that

\begin{equation} \label{eq:resvar-unbp-resvar-bias-adj}
\begin{split}
\mathbb{E}\left[ \frac{n}{n-p} \hat{\sigma}^2_{\mathrm{MLE}} \right] &= \frac{n}{n-p} \mathbb{E}\left[ \hat{\sigma}^2_{\mathrm{MLE}} \right] \\
&\overset{\eqref{eq:resvar-unbp-resvar-bias}}{=} \frac{n}{n-p} \cdot \frac{n-p}{n} \sigma^2 \\
&= \sigma^2 \; ,
\end{split}
\end{equation}

such that an unbiased estimator can be constructed as

\begin{equation} \label{eq:resvar-unbp-resvar-unb-qed}
\begin{split}
\hat{\sigma}^2_{\mathrm{unb}} &= \frac{n}{n-p} \hat{\sigma}^2_{\mathrm{MLE}} \\
&\overset{\eqref{eq:resvar-unbp-resvar-mle}}{=} \frac{n}{n-p} \cdot \frac{1}{n} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) \\
&= \frac{1}{n-p} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{R-squared}

\subsubsection[\textit{Definition}]{Definition} \label{sec:rsq}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) observations

\begin{equation} \label{eq:rsq-mlr}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)
\end{equation}

with measured data $y$, known design matrix $X$ as well as unknown regression coefficients $\beta$ and noise variance $\sigma^2$.

Then, the proportion of the variance of the dependent variable $y$ ("total variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss})") that can be predicted from the independent variables $X$ ("explained variance ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess})") is called "coefficient of determination", "R-squared" or $R^2$.

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2020): "Coefficient of determination"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2020-02-25; URL: \url{https://en.wikipedia.org/wiki/Mean_squared_error#Proof_of_variance_and_bias_relationship}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Derivation of RÂ² and adjusted RÂ²}]{Derivation of RÂ² and adjusted RÂ²} \label{sec:rsq-der}
\setcounter{equation}{0}

\textbf{Theorem:} Given a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr})

\begin{equation} \label{eq:rsq-der-rsq-mlr}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)
\end{equation}

with $n$ independent observations and $p$ independent variables,

1) the coefficient of determination ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq}) is given by

\begin{equation} \label{eq:rsq-der-R2}
R^2 = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}}
\end{equation}

2) the adjusted coefficient of determination is

\begin{equation} \label{eq:rsq-der-R2-adj}
R^2_{\mathrm{adj}} = 1 - \frac{\mathrm{RSS}/(n-p)}{\mathrm{TSS}/(n-1)}
\end{equation}

where the residual ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) and total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}) are

\begin{equation} \label{eq:rsq-der-SS}
\begin{split}
\mathrm{RSS} &= \sum_{i=1}^{n} (y_i - \hat{y}_i)^2, \quad \hat{y} = X\hat{\beta} \\
\mathrm{TSS} &= \sum_{i=1}^{n} (y_i - \bar{y})^2\;, \quad \bar{y} = \frac{1}{n} \sum_{i=1}^n y_i \\
\end{split}
\end{equation}

where $X$ is the $n \times p$ design matrix and $\hat{\beta}$ are the ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}) estimates.


\vspace{1em}
\textbf{Proof:} The coefficient of determination ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq}) $R^2$ is defined as the proportion of the variance explained by the independent variables, relative to the total variance in the data.

\vspace{1em}
1) If we define the explained sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess}) as

\begin{equation} \label{eq:rsq-der-ESS}
\mathrm{ESS} = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 \; ,
\end{equation}

then $R^2$ is given by

\begin{equation} \label{eq:rsq-der-R2-s1}
R^2 = \frac{\mathrm{ESS}}{\mathrm{TSS}} \; .
\end{equation}

which is equal to

\begin{equation} \label{eq:rsq-der-R2-s2}
R^2 = \frac{\mathrm{TSS}-\mathrm{RSS}}{\mathrm{TSS}} = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} \; ,
\end{equation}

because ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-pss}) $\mathrm{TSS} = \mathrm{ESS} + \mathrm{RSS}$.

\vspace{1em}
2) Using \eqref{eq:rsq-der-SS}, the coefficient of determination can be also written as:

\begin{equation} \label{eq:rsq-der-R2'}
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} = 1 - \frac{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\frac{1}{n} \sum_{i=1}^{n} (y_i - \bar{y})^2} \; .
\end{equation}

If we replace the variance estimates by their unbiased estimators ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:resvar-unbp}), we obtain

\begin{equation} \label{eq:rsq-der-R2-adj'}
R^2_{\mathrm{adj}} = 1 - \frac{\frac{1}{n-p} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\frac{1}{n-1} \sum_{i=1}^{n} (y_i - \bar{y})^2} = 1 - \frac{\mathrm{RSS}/\mathrm{df}_r}{\mathrm{TSS}/\mathrm{df}_t}
\end{equation}

where $\mathrm{df}_r = n-p$ and $\mathrm{df}_t = n-1$ are the residual and total degrees of freedom.

\vspace{1em}
This gives the adjusted $R^2$ which adjusts $R^2$ for the number of explanatory variables.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2019): "Coefficient of determination"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2019-12-06; URL: \url{https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relationship to residual variance}]{Relationship to residual variance} \label{sec:rsq-resvar}
\setcounter{equation}{0}

\textbf{Theorem:} Given a linear regression model with independent observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr})

\begin{equation} \label{eq:rsq-resvar-mlr}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; ,
\end{equation}

the coefficient of determination ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq}) can be expressed in terms of residual variances ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:resvar}) as

\begin{equation} \label{eq:rsq-resvar-rsq-resvar}
R^2 = 1 - \frac{(n-p) \cdot \hat{\sigma}^2}{(n-1) \cdot s^2}
\end{equation}

where $n$ is the number of observations, $p$ is the number of predictors, $\hat{\sigma}^2$ is an unbiased estimate of the noise variance ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:resvar-unbp}) $\sigma^2$ and $s^2$ is the unbiased sample variance of $y$.


\vspace{1em}
\textbf{Proof:} The coefficient of determination ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq-der}) is given by

\begin{equation} \label{eq:rsq-resvar-rsq}
R^2 = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}}
\end{equation}

where $\mathrm{RSS}$ is the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss})

\begin{equation} \label{eq:rsq-resvar-rss}
\mathrm{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 \quad \text{where} \quad \hat{y} = X \hat{\beta}
\end{equation}

and $\mathrm{TSS}$ is the total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss})

\begin{equation} \label{eq:rsq-resvar-tss}
\mathrm{TSS} = \sum_{i=1}^n (y_i - \bar{y})^2 \quad \text{where} \quad \bar{y} = \frac{1}{n} \sum_{i=1}^n y_i \; .
\end{equation}

Note that the residual sum of squares can be written as:

\begin{equation} \label{eq:rsq-resvar-rss-dev}
\mathrm{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n (y_i - (X \hat{\beta})_i)^2 = (y-X\hat{\beta})^\mathrm{T} (y-X\hat{\beta}) \; .
\end{equation}

The unbiased estimate of the noise variance ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:resvar-unbp}) is

\begin{equation} \label{eq:rsq-resvar-sigma-unb}
\hat{\sigma}^2 = \frac{1}{n-p} (y-X\hat{\beta})^\mathrm{T} (y-X\hat{\beta})
\end{equation}

and the unbiased sample variance of the dependent variable is

\begin{equation} \label{eq:rsq-resvar-var-samp-unb}
s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (y_i - \bar{y})^2 \; ,
\end{equation}

Combining \eqref{eq:rsq-resvar-rsq}, \eqref{eq:rsq-resvar-rss} and \eqref{eq:rsq-resvar-tss}, the coefficient of determination can be rewritten as follows:

\begin{equation} \label{eq:rsq-resvar-rsq-resvar-qed}
\begin{split}
R^2 &\overset{\eqref{eq:rsq-resvar-rsq}}{=} 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} \\
&\overset{\eqref{eq:rsq-resvar-tss}}{=} 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} \\
&\overset{\eqref{eq:rsq-resvar-rss-dev}}{=} 1 - \frac{(y-X\hat{\beta})^\mathrm{T} (y-X\hat{\beta})}{\sum_{i=1}^{n} (y_i - \bar{y})^2} \\
&= 1 - \frac{(n-p) \cdot \frac{1}{n-p} (y-X\hat{\beta})^\mathrm{T} (y-X\hat{\beta})}{(n-1) \cdot \frac{1}{n-1} \sum_{i=1}^{n} (y_i - \bar{y})^2} \\
&\overset{\eqref{eq:rsq-resvar-sigma-unb}}{=} 1 - \frac{(n-p) \cdot \hat{\sigma}^2}{(n-1) \cdot \frac{1}{n-1} \sum_{i=1}^{n} (y_i - \bar{y})^2} \\
&\overset{\eqref{eq:rsq-resvar-var-samp-unb}}{=} 1 - \frac{(n-p) \cdot \hat{\sigma}^2}{(n-1) \cdot s^2} \; .
\end{split}
\end{equation}

This completes the proof.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Relationship to maximum log-likelihood}]{Relationship to maximum log-likelihood} \label{sec:rsq-mll}
\setcounter{equation}{0}

\textbf{Theorem:} Given a linear regression model with independent observations ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr})

\begin{equation} \label{eq:rsq-mll-MLR}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; ,
\end{equation}

the coefficient of determination ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq}) can be expressed in terms of the maximum log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}) as

\begin{equation} \label{eq:rsq-mll-R2-MLL}
R^2 = 1 - \left( \exp[\Delta\mathrm{MLL}] \right)^{-2/n}
\end{equation}

where $n$ is the number of observations and $\Delta\mathrm{MLL}$ is the difference in maximum log-likelihood between the model given by \eqref{eq:rsq-mll-MLR} and a linear regression model with only a constant regressor.


\vspace{1em}
\textbf{Proof:} First, we express the maximum log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}) (MLL) of a linear regression model in terms of its residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) (RSS). The model in \eqref{eq:rsq-mll-MLR} implies the following log-likelihood function ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:llf})

\begin{equation} \label{eq:rsq-mll-MLR-LL}
\mathrm{LL}(\beta,\sigma^2) = \log p(y|\beta,\sigma^2) = - \frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} (y - X\beta)^\mathrm{T} (y - X\beta) \; ,
\end{equation}

such that maximum likelihood estimates are ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle})

\begin{equation} \label{eq:rsq-mll-MLR-MLE-beta}
\hat{\beta} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y
\end{equation}

\begin{equation} \label{eq:rsq-mll-MLR-MLE-sigma2}
\hat{\sigma}^2 = \frac{1}{n} (y - X\hat{\beta})^\mathrm{T} (y - X\hat{\beta})
\end{equation}

and the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) is

\begin{equation} \label{eq:rsq-mll-RSS}
\mathrm{RSS} = \sum_{i=1}^n \hat{\varepsilon}_i = \hat{\varepsilon}^\mathrm{T} \hat{\varepsilon} = (y - X\hat{\beta})^\mathrm{T} (y - X\hat{\beta}) = n \cdot \hat{\sigma}^2 \; .
\end{equation}

Since $\hat{\beta}$ and $\hat{\sigma}^2$ are maximum likelihood estimates ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mle}), plugging them into the log-likelihood function gives the maximum log-likelihood:

\begin{equation} \label{eq:rsq-mll-MLR-MLL}
\mathrm{MLL} = \mathrm{LL}(\hat{\beta},\hat{\sigma}^2) = - \frac{n}{2} \log(2\pi\hat{\sigma}^2) - \frac{1}{2\hat{\sigma}^2} (y - X\hat{\beta})^\mathrm{T} (y - X\hat{\beta}) \; .
\end{equation}

With \eqref{eq:rsq-mll-RSS} for the first $\hat{\sigma}^2$ and \eqref{eq:rsq-mll-MLR-MLE-sigma2} for the second $\hat{\sigma}^2$, the MLL becomes

\begin{equation} \label{eq:rsq-mll-MLR-MLL-RSS}
\mathrm{MLL} = - \frac{n}{2} \log(\mathrm{RSS}) - \frac{n}{2} \log \left( \frac{2\pi}{n} \right) - \frac{n}{2} \; .
\end{equation}

Second, we establish the relationship between maximum log-likelihood (MLL) and coefficient of determination (RÂ²). Consider the two models

\begin{equation} \label{eq:rsq-mll-m0-m1}
\begin{split}
m_0: \; X_0 &= 1_n \\
m_1: \; X_1 &= X
\end{split}
\end{equation}

For $m_1$, the residual sum of squares is given by \eqref{eq:rsq-mll-RSS}; and for $m_0$, the residual sum of squares is equal to the total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}):

\begin{equation} \label{eq:rsq-mll-TSS}
\mathrm{TSS} = \sum_{i=1}^n (y_i - \bar{y})^2 \; .
\end{equation}

Using \eqref{eq:rsq-mll-MLR-MLL-RSS}, we can therefore write

\begin{equation} \label{eq:rsq-mll-MLR-DMLL}
\Delta\mathrm{MLL} = \mathrm{MLL}(m_1) - \mathrm{MLL}(m_0) = - \frac{n}{2} \log(\mathrm{RSS}) + \frac{n}{2} \log(\mathrm{TSS}) \; .
\end{equation}

Exponentiating both sides of the equation, we have:

\begin{equation} \label{eq:rsq-mll-MLR-DMLL-RTSS}
\begin{split}
\exp[\Delta\mathrm{MLL}] &= \exp\left[ - \frac{n}{2} \log(\mathrm{RSS}) + \frac{n}{2} \log(\mathrm{TSS}) \right] \\
&= \left( \exp\left[ \log(\mathrm{RSS}) - \log(\mathrm{TSS}) \right] \right)^{-n/2} \\
&= \left( \frac{\exp[\log(\mathrm{RSS})]}{\exp[\log(\mathrm{TSS})]} \right)^{-n/2} \\
&= \left( \frac{\mathrm{RSS}}{\mathrm{TSS}} \right)^{-n/2} \; .
\end{split}
\end{equation}

Taking both sides to the power of $-2/n$ and subtracting from 1, we have

\begin{equation} \label{eq:rsq-mll-MLR-DMLL-R2}
\begin{split}
\left( \exp[\Delta\mathrm{MLL}] \right)^{-2/n} &= \frac{\mathrm{RSS}}{\mathrm{TSS}} \\
1 - \left( \exp[\Delta\mathrm{MLL}] \right)^{-2/n} &= 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} = R^2
\end{split}
\end{equation}

which proves the identity given above.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Statistical significance test for RÂ²}]{Statistical significance test for RÂ²} \label{sec:rsq-test}
\setcounter{equation}{0}

\textbf{Theorem:} Consider a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with known design matrix $X$, known covariance structure $V$, unknown regression parameters $\beta$ and unknown noise variance $\sigma^2$:

\begin{equation} \label{eq:rsq-test-mlr}
y = X\beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; .
\end{equation}

Further assume that $X$ contains a constant regressor ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}). Then, the coefficient of determination ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq}) can be used to calculate a test statistic ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:tstat})

\begin{equation} \label{eq:rsq-test-f-rsq}
F = \frac{R^2/(p-1)}{(1-R^2)/(n-p)}
\end{equation}

where $n$ and $p$ are the dimensions of the design matrix ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $X$, and this test statistic follows an F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f})

\begin{equation} \label{eq:rsq-test-f-rsq-dist}
F \sim \mathrm{F}(p-1, n-p)
\end{equation}

under the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) that the true coefficient of determination ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq}) is zero

\begin{equation} \label{eq:rsq-test-rsq-test-h0}
H_0: \; R^2 = 0 \; .
\end{equation}


\vspace{1em}
\textbf{Proof:} Consider two linear regression models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) for the same measured data $y$, with design matrices $X = X_0 \in \mathbb{R}^{n \times p_0}$ and $X = \left[ X_0, X_1 \right] \in \mathbb{R}^{n \times p}$ as well as regression coefficients $\beta = \beta_0 \in \mathbb{R}^{p_0 \times 1}$ and $\beta = \left[ \beta_0^\mathrm{T}, \beta_1^\mathrm{T} \right]^\mathrm{T} \in \mathbb{R}^{p \times 1}$.

Then, under the null hypothesis that all regression coefficients ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $\beta_1$ associated with $X_1$ are zero

\begin{equation} \label{eq:rsq-test-mlr-fomnibus-h0}
H_0: \; \beta_1 = 0_{p-p_0} \quad \Leftrightarrow \quad \beta_i = 0 \quad \text{for all} \quad j = p_0+1,\ldots,p \; ,
\end{equation}

the omnibus F-statistic follows an F-distribution ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-fomnibus})

\begin{equation} \label{eq:rsq-test-mlr-fomnibus}
F = \frac{(\mathrm{RSS}_0-\mathrm{RSS})/(p-p_0)}{\mathrm{RSS}/(n-p)} \sim \mathrm{F}(p-p_0, n-p)
\end{equation}

where $\mathrm{RSS}_0$ and $\mathrm{RSS}$ are the residual sums of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) of the null model with $X_0$ and the full model with $X_0$ nested in $X$, after regression coefficients have been estimated with weighted least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-wls}) or maximum likelihood ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle}).

\vspace{1em}
Since by the requirements of our theorem, $X$ contains a constant regressor, we can assume the following design matrices without loss of generality:

\begin{equation} \label{eq:rsq-test-f-rsq-X}
X_0 = 1_n \in \mathbb{R}^{n \times 1} \quad \text{and} \quad X = \left[ 1_n, X_1 \right] \in \mathbb{R}^{n \times p} \; .
\end{equation}

Thus, since a single constant regressor estimates the mean and considering the definition of the total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}) $\mathrm{TSS}$, we in our case have:

\begin{equation} \label{eq:rsq-test-rss0-p0}
\mathrm{RSS}_0 = \mathrm{TSS} \quad \text{and} \quad p_0 = 1 \; .
\end{equation}

\vspace{1em}
The coefficient of determination is given by ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq-der})

\begin{equation} \label{eq:rsq-test-rsq-rss}
R^2 = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}}
\end{equation}

which can also be written as ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-pss})

\begin{equation} \label{eq:rsq-test-rsq-ess}
R^2 = \frac{\mathrm{ESS}}{\mathrm{TSS}} \; .
\end{equation}

If all regression coefficients $\beta_1$ associated with $X_1$ are zero, then the true $R^2$ is zero, because there is no variance explained beyond the constant regressor, the explained sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess}) $\mathrm{ESS}$ is zero and the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) $\mathrm{RSS}$ is equal to the total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}) $\mathrm{TSS}$.

\vspace{1em}
Then, by virtue of \eqref{eq:rsq-test-mlr-fomnibus}, we get the following F-statistic:

\begin{equation} \label{eq:rsq-test-f-rsq-qed}
\begin{split}
F &\overset{\eqref{eq:rsq-test-mlr-fomnibus}}{=} \frac{(\mathrm{RSS}_0-\mathrm{RSS})/(p-p_0)}{\mathrm{RSS}/(n-p)} \\
&\overset{\eqref{eq:rsq-test-rss0-p0}}{=} \frac{(\mathrm{TSS}-\mathrm{RSS})/(p-1)}{\mathrm{RSS}/(n-p)} \\
&= \frac{\frac{\mathrm{TSS}-\mathrm{RSS}}{\mathrm{TSS}}/(p-1)}{\frac{\mathrm{RSS}}{\mathrm{TSS}}/(n-p)} \\
&= \frac{\left( 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} \right)/(p-1)}{\left( 1 - \left( 1- \frac{\mathrm{RSS}}{\mathrm{TSS}} \right)\right)/(n-p)} \\
&\overset{\eqref{eq:rsq-test-rsq-rss}}{=} \frac{\left(R^2\right)/(p-1)}{\left(1-R^2\right)/(n-p)} \; .
\end{split}
\end{equation}

This means that the null hypothesis ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:h0}) can be rejected when $F$ as a function of $R^2$ is as extreme or more extreme than the critical value ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:cval}) obtained from the F-distribution ($\rightarrow$ \ref{sec:Probability Distributions}/\ref{sec:f}) with $p-1$ denominator and $n-p$ numerator degrees of freedom using a significance level ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:alpha}) $\alpha$.
\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Alecos Papadopoulos (2014): "What is the distribution of RÂ² in linear regression under the null hypothesis?"; in: \textit{StackExchange CrossValidated}, retrieved on 2024-03-15; URL: \url{https://stats.stackexchange.com/a/130082}.
\end{itemize}
\vspace{1em}



\subsection{F-statistic}

\subsubsection[\textit{Definition}]{Definition} \label{sec:fstat}
\setcounter{equation}{0}

\textbf{Definition:} Consider two linear regression models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) observations

\begin{equation} \label{eq:fstat-m0-m1}
\begin{split}
m_1: \; y &= X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \\
m_0: \; y &= X_0\beta_0 + \varepsilon_0, \; \varepsilon_{0i} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma_0^2)
\end{split}
\end{equation}

operating on identical measured data $y$, but with different design matrices $X \in \mathbb{R}^{n \times p}$ and $X_0 \in \mathbb{R}^{n \times p_0}$ and thus different regression coefficients $\beta \in \mathbb{R}^{p \times 1}$ and $\beta_0 \in \mathbb{R}^{p_0 \times 1}$. Furthermore, let the design matrix of the null model be fully contained in the design matrix of the full model:

\begin{equation} \label{eq:fstat-X-X0-X1}
X = \left[ \begin{array}{cc} X_0 & X_1 \end{array} \right] \; .
\end{equation}

Then, the F-statistic for model comparison is defined as the ratio of the difference in residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) between the two models, divided by the difference in number of parameters ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}), to the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) of the full model, divided by the number of degrees of freedom:

\begin{equation} \label{eq:fstat-F}
F = \frac{(\mathrm{RSS}_0-\mathrm{RSS})/(p-p_0)}{\mathrm{RSS}/(n-p)} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Wikipedia (2024): "F-test"; in: \textit{Wikipedia, the free encyclopedia}, retrieved on 2024-03-15; URL: \url{https://en.wikipedia.org/wiki/F-test#Regression_problems}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relationship to coefficient of determination}]{Relationship to coefficient of determination} \label{sec:fstat-rsq}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) observations

\begin{equation} \label{eq:fstat-rsq-mlr}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; .
\end{equation}

Then, the F-statistic ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:fstat}) for comparing this model against a null model containing only a constant regressor ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) $x_0 = 1_n$ can be expressed in terms of the coefficient of determination ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq})

\begin{equation} \label{eq:fstat-rsq-F-R2}
F = \frac{R^2/(p-1)}{(1-R^2)/(n-p)}
\end{equation}

and vice versa

\begin{equation} \label{eq:fstat-rsq-R2-F}
R^2 = 1 - \frac{1}{F \cdot \frac{n-p}{p-1} + 1}
\end{equation}

where $n$ and $p$ are the dimensions of the design matrix $X \in \mathbb{R}^{n \times p}$.


\vspace{1em}
\textbf{Proof:} Consider two linear regression models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) for the same measured data $y$, one using design matrix $X$ from \eqref{eq:fstat-rsq-mlr} and the other with design matrix $X_0 = 1_n \in \mathbb{R}^{n \times 1}$. Then, $\mathrm{RSS}$ is the residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) of the model in \eqref{eq:fstat-rsq-mlr} and the residual sum of squares for the model using $X_0$ is equal to the total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}).

\vspace{1em}
1) Thus, the F-statistic ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:fstat})

\begin{equation} \label{eq:fstat-rsq-F}
F = \frac{(\mathrm{RSS}_0-\mathrm{RSS})/(p-p_0)}{\mathrm{RSS}/(n-p)}
\end{equation}

becomes

\begin{equation} \label{eq:fstat-rsq-F-RSS-TSS}
F = \frac{(\mathrm{TSS}-\mathrm{RSS})/(p-1)}{\mathrm{RSS}/(n-p)} \; .
\end{equation}

From this, we can derive $F$ in terms of $R^2$:

\begin{equation} \label{eq:fstat-rsq-F-R2-qed}
\begin{split}
F &= \frac{(\mathrm{TSS}-\mathrm{RSS})/(p-1)}{\mathrm{RSS}/(n-p)} \\
&= \frac{\frac{\mathrm{TSS} - \mathrm{RSS}}{\mathrm{TSS}} / (p-1)}{\frac{\mathrm{RSS}}{\mathrm{TSS}} / (n-p)} \\
&= \frac{\left( 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} \right) / (p-1)}{\left( 1 - \left( 1- \frac{\mathrm{RSS}}{\mathrm{TSS}} \right)\right) / (n-p)} \\
&= \frac{\left(R^2\right)/(p-1)}{\left(1-R^2\right)/(n-p)} \; .
\end{split}
\end{equation}

\vspace{1em}
2) Rearranging this equation, we can derive $R^2$ in terms of $F$:

\begin{equation} \label{eq:fstat-rsq-R2-F-qed}
\begin{split}
F &= \frac{\left(R^2\right)/(p-1)}{\left(1-R^2\right)/(n-p)} \\
F \cdot \frac{n-p}{p-1} &= \frac{R^2}{\left(1-R^2\right)} \\
F \cdot \frac{n-p}{p-1} \cdot (1-R^2) &= R^2 \\
F \cdot \frac{n-p}{p-1} - F \cdot \frac{n-p}{p-1} \cdot R^2 &= R^2 \\
F \cdot \frac{n-p}{p-1} \cdot R^2 + R^2 &= F \cdot \frac{n-p}{p-1} \\
R^2 \left( F \cdot \frac{n-p}{p-1} \cdot + 1 \right) &= F \cdot \frac{n-p}{p-1} \\
R^2 &= \frac{F \cdot \frac{n-p}{p-1}}{F \cdot \frac{n-p}{p-1} \cdot + 1} \\
R^2 &= \frac{F \cdot \frac{n-p}{p-1} + 1 - 1}{F \cdot \frac{n-p}{p-1} \cdot + 1} \\
R^2 &= \frac{F \cdot \frac{n-p}{p-1} + 1}{F \cdot \frac{n-p}{p-1} \cdot + 1} - \frac{1}{F \cdot \frac{n-p}{p-1} \cdot + 1} \\
R^2 &= 1 - \frac{1}{F \cdot \frac{n-p}{p-1} \cdot + 1} \\
\end{split}
\end{equation}

This completes the proof.

\begin{flushright} $\blacksquare$ \end{flushright}


\vspace{-1em}
\textbf{Sources:}
\begin{itemize}
\item Alecos Papadopoulos (2014): "What is the distribution of RÂ² in linear regression under the null hypothesis?"; in: \textit{StackExchange CrossValidated}, retrieved on 2024-03-15; URL: \url{https://stats.stackexchange.com/a/130082}.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relationship to maximum log-likelihood}]{Relationship to maximum log-likelihood} \label{sec:fstat-mll}
\setcounter{equation}{0}

\textbf{Theorem:} Consider two linear regression models ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) observations

\begin{equation} \label{eq:fstat-mll-m0-m1}
\begin{split}
m_1: \; y &= X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \\
m_0: \; y &= X_0\beta_0 + \varepsilon_0, \; \varepsilon_{0i} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma_0^2) \; .
\end{split}
\end{equation}

Then, the F-statistic ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:fstat}) can be expressed in terms of the maximum log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}) as

\begin{equation} \label{eq:fstat-mll-F-MLL}
F = \left[ \left( \exp[\Delta\mathrm{MLL}] \right)^{2/n} - 1 \right] \cdot \frac{n-p}{p-p_0}
\end{equation}

where $n$, $p$ and $p_0$ are the dimensions of the design matrices $X = \left[ X_0, X_1 \right] \in \mathbb{R}^{n \times p}$ and $X_0 \in \mathbb{R}^{n \times p_0}$ and $\Delta\mathrm{MLL}$ is the difference in maximum log-likelihood between the two models given by \eqref{eq:fstat-mll-m0-m1}


\vspace{1em}
\textbf{Proof:} Under the conditions mentioned in the theorem, the F-statistic is defined in terms of the residual sum of squares ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:fstat}) as

\begin{equation} \label{eq:fstat-mll-F}
F = \frac{(\mathrm{RSS}_0-\mathrm{RSS})/(p-p_0)}{\mathrm{RSS}/(n-p)} \; .
\end{equation}

We also know that the maximum log-likelihood can be expressed in terms of residual sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mll}):

\begin{equation} \label{eq:fstat-mll-mlr-mll}
\mathrm{MLL}(m) = - \frac{n}{2} \log\left( \frac{\mathrm{RSS}}{n} \right) - \frac{n}{2} \left[ 1 + \log(2\pi) \right] \; .
\end{equation}

Based on this, we see that the difference of the maximum log-likelihoods develops into

\begin{equation} \label{eq:fstat-mll-dMLL-m1-m0}
\begin{split}
\Delta\mathrm{MLL}
&= \mathrm{MLL}(m_1) - \mathrm{MLL}(m_0) \\
&= \left( - \frac{n}{2} \log\left( \frac{\mathrm{RSS}}{n} \right) - \frac{n}{2} \left[ 1 + \log(2\pi) \right] \right) \\
&- \left( - \frac{n}{2} \log\left( \frac{\mathrm{RSS}_0}{n} \right) - \frac{n}{2} \left[ 1 + \log(2\pi) \right] \right) \\
&= - \frac{n}{2} \log\left( \frac{\mathrm{RSS}}{n} \right) + \frac{n}{2} \log\left( \frac{\mathrm{RSS}_0}{n} \right) \; .
\end{split}
\end{equation}

Finally, we simply perform algebraic operations on both sides ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq-mll}) until we reach the F-statistic on the right side. We start by exponentiating the MLL difference:

\begin{equation} \label{eq:fstat-mll-F-MLL-qed}
\begin{split}
\exp[\Delta\mathrm{MLL}]
&= \exp\left[ - \frac{n}{2} \log(\mathrm{RSS}/n) + \frac{n}{2} \log(\mathrm{RSS}_0/n) \right] \\
\exp[\Delta\mathrm{MLL}]
&= \left( \exp\left[ \log(\mathrm{RSS}/n) - \log(\mathrm{RSS}_0/n) \right] \right)^{-n/2} \\
\exp[\Delta\mathrm{MLL}]
&= \left( \frac{\exp[\log(\mathrm{RSS}/n)]}{\exp[\log(\mathrm{RSS}_0/n)]} \right)^{-n/2} \\
\exp[\Delta\mathrm{MLL}]
&= \left( \frac{\mathrm{RSS}/n}{\mathrm{RSS}_0/n} \right)^{-n/2} \\
\exp[\Delta\mathrm{MLL}]
&= \left( \frac{\mathrm{RSS}_0}{\mathrm{RSS}} \right)^{n/2} \\
\left( \exp[\Delta\mathrm{MLL}] \right)^{2/n}
&= \frac{\mathrm{RSS}_0}{\mathrm{RSS}} \\
\left( \exp[\Delta\mathrm{MLL}] \right)^{2/n} - 1
&= \frac{\mathrm{RSS}_0}{\mathrm{RSS}} - 1 \\
\left( \exp[\Delta\mathrm{MLL}] \right)^{2/n} - 1
&= \frac{\mathrm{RSS}_0}{\mathrm{RSS}} - \frac{\mathrm{RSS}}{\mathrm{RSS}} \\
\left[ \left( \exp[\Delta\mathrm{MLL}] \right)^{2/n} - 1 \right] \cdot \frac{n-p}{p-p_0}
&= \left[ \frac{\mathrm{RSS}_0 - \mathrm{RSS}}{\mathrm{RSS}} \right] \cdot \frac{n-p}{p-p_0} \\
\left[ \left( \exp[\Delta\mathrm{MLL}] \right)^{2/n} - 1 \right] \cdot \frac{n-p}{p-p_0}
&= \frac{(\mathrm{RSS}_0-\mathrm{RSS})/(p-p_0)}{\mathrm{RSS}/(n-p)} \\
\left[ \left( \exp[\Delta\mathrm{MLL}] \right)^{2/n} - 1 \right] \cdot \frac{n-p}{p-p_0}
&= F \; .
\end{split}
\end{equation}

This completes the proof.
\begin{flushright} $\blacksquare$ \end{flushright}



\subsection{Signal-to-noise ratio}

\subsubsection[\textit{Definition}]{Definition} \label{sec:snr}
\setcounter{equation}{0}

\textbf{Definition:} Let there be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) observations

\begin{equation} \label{eq:snr-mlr}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)
\end{equation}

with measured data $y$, known design matrix $X$ as well as unknown regression coefficients $\beta$ and noise variance $\sigma^2$.

Given estimated regression coefficients ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-mle}) $\hat{\beta}$ and residual variance ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:resvar}) $\hat{\sigma}^2$, the signal-to-noise ratio (SNR) is defined as the ratio of estimated signal variance to estimated noise variance:

\begin{equation} \label{eq:snr-SNR}
\mathrm{SNR} = \frac{\mathrm{Var}(X\hat{\beta})}{\hat{\sigma}^2} \; .
\end{equation}

\vspace{1em}
\textbf{Sources:}
\begin{itemize}
\item Soch J, Allefeld C (2018): "MACS â€“ a new SPM toolbox for model assessment, comparison and selection"; in: \textit{Journal of Neuroscience Methods}, vol. 306, pp. 19-31, eq. 6; URL: \url{https://www.sciencedirect.com/science/article/pii/S0165027018301468}; DOI: 10.1016/j.jneumeth.2018.05.017.
\end{itemize}
\vspace{1em}



\subsubsection[\textbf{Relationship to coefficient of determination}]{Relationship to coefficient of determination} \label{sec:snr-rsq}
\setcounter{equation}{0}

\textbf{Theorem:} Let there be a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) observations

\begin{equation} \label{eq:snr-rsq-mlr}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)
\end{equation}

and parameter estimates obtained with ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols})

\begin{equation} \label{eq:snr-rsq-OLS}
\hat{\beta} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \; .
\end{equation}

Then, the signal-to noise ratio ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:snr}) can be expressed in terms of the coefficient of determination ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq})

\begin{equation} \label{eq:snr-rsq-SNR-R2}
\mathrm{SNR} = \frac{R^2}{\mathrm{1-R^2}}
\end{equation}

and vice versa

\begin{equation} \label{eq:snr-rsq-R2-SNR}
R^2 = \frac{\mathrm{SNR}}{\mathrm{1+\mathrm{SNR}}} \; ,
\end{equation}

if the predicted signal mean is equal to the actual signal mean.


\vspace{1em}
\textbf{Proof:} The signal-to-noise ratio ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:snr}) (SNR) is defined as

\begin{equation} \label{eq:snr-rsq-SNR}
\mathrm{SNR} = \frac{\mathrm{Var}(X\hat{\beta})}{\hat{\sigma}^2} = \frac{\mathrm{Var}(\hat{y})}{\hat{\sigma}^2} \; .
\end{equation}

Writing out the sample variances ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:var-samp}), we have

\begin{equation} \label{eq:snr-rsq-SNR-s1}
\mathrm{SNR} = \frac{\frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - \bar{\hat{y}})^2}{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} = \frac{\sum_{i=1}^{n} (\hat{y}_i - \bar{\hat{y}})^2}{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2} \; .
\end{equation}

Note that it is irrelevant whether we use the biased estimator of the variance ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:resvar-bias}) (dividing by $n$) or the unbiased estimator fo the variance ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:resvar-unb}) (dividing by $n-1$), because the relevant terms cancel out.

If the predicted signal mean is equal to the actual signal mean -- which is the case when variable regressors in $X$ have mean zero, such that they are orthogonal to a constant regressor in $X$ --, this means that $\bar{\hat{y}} = \bar{y}$, such that

\begin{equation} \label{eq:snr-rsq-SNR-s2}
\mathrm{SNR} = \frac{\sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2}{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2} \; .
\end{equation}

Then, the SNR can be written in terms of the explained ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:ess}), residual ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:rss}) and total sum of squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:tss}):

\begin{equation} \label{eq:snr-rsq-SNR-s3}
\mathrm{SNR} = \frac{\mathrm{ESS}}{\mathrm{RSS}} = \frac{\mathrm{ESS}/\mathrm{TSS}}{\mathrm{RSS}/\mathrm{TSS}} \; .
\end{equation}

With the derivation of the coefficient of determination ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq-der}), this becomes

\begin{equation} \label{eq:snr-rsq-SNR-R2-qed}
\mathrm{SNR} = \frac{R^2}{1-R^2} \; .
\end{equation}

Rearranging this equation for the coefficient of determination ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq}), we have

\begin{equation} \label{eq:snr-rsq-R2-SNR-qed}
R^2 = \frac{\mathrm{SNR}}{\mathrm{1+\mathrm{SNR}}} \; ,
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\subsubsection[\textbf{Relationship to maximum log-likelihood}]{Relationship to maximum log-likelihood} \label{sec:snr-mll}
\setcounter{equation}{0}

\textbf{Theorem:} Given a linear regression model ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr}) with independent ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:ind}) observations

\begin{equation} \label{eq:snr-mll-mlr}
y = X\beta + \varepsilon, \; \varepsilon_i \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2) \; ;
\end{equation}

the signal-to-noise ratio ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:snr}) can be expressed in terms of the maximum log-likelihood ($\rightarrow$ \ref{sec:General Theorems}/\ref{sec:mll}) as

\begin{equation} \label{eq:snr-mll-SNR-MLL}
\mathrm{SNR} = \left( \exp[\Delta\mathrm{MLL}] \right)^{2/n} - 1 \; ,
\end{equation}

where $n$ is the number of observations and $\Delta\mathrm{MLL}$ is the difference in maximum log-likelihood between the model given by \eqref{eq:snr-mll-mlr} and a linear regression model with only a constant regressor.

This holds, if the predicted signal mean is equal to the actual signal mean

\begin{equation} \label{eq:snr-mll-y-hat-mean-y-mean}
\bar{\hat{y}} = \frac{1}{n} \sum_{i=1}^{n} (X\hat{\beta})_i = \frac{1}{n} \sum_{i=1}^{n} y_i = \bar{y}
\end{equation}

where $X$ is the $n \times p$ design matrix and $\hat{\beta}$ are the ordinary least squares ($\rightarrow$ \ref{sec:Statistical Models}/\ref{sec:mlr-ols}) estimates.


\vspace{1em}
\textbf{Proof:} Under the conditions mentioned in the theorem, the signal-to-noise ratio can be expressed in terms of the coefficient of determination ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:snr-rsq}) as

\begin{equation} \label{eq:snr-mll-SNR-R2}
\mathrm{SNR} = \frac{R^2}{\mathrm{1-R^2}}
\end{equation}

and R-squared can be expressed in terms of maximum likelihood ($\rightarrow$ \ref{sec:Model Selection}/\ref{sec:rsq-mll}) as 

\begin{equation} \label{eq:snr-mll-R2-MLL}
R^2 = 1 - \left( \exp[\Delta\mathrm{MLL}] \right)^{-2/n} \; .
\end{equation}

Plugging \eqref{eq:snr-mll-R2-MLL} into \eqref{eq:snr-mll-SNR-R2}, we obtain:

\begin{equation} \label{eq:snr-mll-SNR-MLL-qed}
\begin{split}
\mathrm{SNR} &= \frac{1 - \left( \exp[\Delta\mathrm{MLL}] \right)^{-2/n}}{\mathrm{\left( \exp[\Delta\mathrm{MLL}] \right)^{-2/n}}} \\
&= \frac{1}{\left( \exp[\Delta\mathrm{MLL}] \right)^{-2/n}} - \frac{\left( \exp[\Delta\mathrm{MLL}] \right)^{-2/n}}{\mathrm{\left( \exp[\Delta\mathrm{MLL}] \right)^{-2/n}}} \\
&= \left( \exp[\Delta\mathrm{MLL}] \right)^{2/n} - 1 \; .
\end{split}
\end{equation}
\begin{flushright} $\blacksquare$ \end{flushright}



\pagebreak
\section{Bayesian model selection}

% Appendix %
\chapter{Appendix} \label{sec:Appendix} \newpage

\pagebreak
\section{Proof by Number}

\begin{longtable}{|p{1cm}|p{2cm}|p{6.5cm}|p{3cm}|p{2cm}|c|}
\hline
\textbf{ID} & \textbf{Shortcut} & \textbf{Theorem} & \textbf{Author} & \textbf{Date} & \textbf{Page} \\ \hline
P1 & mvn-ltt & Linear transformation theorem for the multivariate normal distribution & JoramSoch & 2019-08-27 & \pageref{sec:mvn-ltt} \\ \hline
P2 & mlr-ols & Ordinary least squares for multiple linear regression & JoramSoch & 2019-09-27 & \pageref{sec:mlr-ols} \\ \hline
P4 & bayes-th & Bayes' theorem & JoramSoch & 2019-09-27 & \pageref{sec:bayes-th} \\ \hline
P5 & mse-bnv & Partition of the mean squared error into bias and variance & JoramSoch & 2019-11-27 & \pageref{sec:mse-bnv} \\ \hline
P6 & ng-kl & Kullback-Leibler divergence for the normal-gamma distribution & JoramSoch & 2019-12-06 & \pageref{sec:ng-kl} \\ \hline
P7 & glm-mle & Maximum likelihood estimation for the general linear model & JoramSoch & 2019-12-06 & \pageref{sec:glm-mle} \\ \hline
P8 & rsq-der & Derivation of RÂ² and adjusted RÂ² & JoramSoch & 2019-12-06 & \pageref{sec:rsq-der} \\ \hline
P9 & blr-prior & Conjugate prior distribution for Bayesian linear regression & JoramSoch & 2020-01-03 & \pageref{sec:blr-prior} \\ \hline
P10 & blr-post & Posterior distribution for Bayesian linear regression & JoramSoch & 2020-01-03 & \pageref{sec:blr-post} \\ \hline
P11 & blr-lme & Log model evidence for Bayesian linear regression & JoramSoch & 2020-01-03 & \pageref{sec:blr-lme} \\ \hline
P12 & bayes-rule & Bayes' rule & JoramSoch & 2020-01-06 & \pageref{sec:bayes-rule} \\ \hline
P14 & rsq-mll & Relationship between RÂ² and maximum log-likelihood & JoramSoch & 2020-01-08 & \pageref{sec:rsq-mll} \\ \hline
P15 & norm-mean & Mean of the normal distribution & JoramSoch & 2020-01-09 & \pageref{sec:norm-mean} \\ \hline
P16 & norm-med & Median of the normal distribution & JoramSoch & 2020-01-09 & \pageref{sec:norm-med} \\ \hline
P17 & norm-mode & Mode of the normal distribution & JoramSoch & 2020-01-09 & \pageref{sec:norm-mode} \\ \hline
P18 & norm-var & Variance of the normal distribution & JoramSoch & 2020-01-09 & \pageref{sec:norm-var} \\ \hline
P19 & dmi-mce & Relation of mutual information to marginal and conditional entropy & JoramSoch & 2020-01-13 & \pageref{sec:dmi-mce} \\ \hline
P20 & dmi-mje & Relation of mutual information to marginal and joint entropy & JoramSoch & 2020-01-13 & \pageref{sec:dmi-mje} \\ \hline
P21 & dmi-jce & Relation of mutual information to joint and conditional entropy & JoramSoch & 2020-01-13 & \pageref{sec:dmi-jce} \\ \hline
P22 & bern-mean & Mean of the Bernoulli distribution & JoramSoch & 2020-01-16 & \pageref{sec:bern-mean} \\ \hline
P23 & bin-mean & Mean of the binomial distribution & JoramSoch & 2020-01-16 & \pageref{sec:bin-mean} \\ \hline
P24 & cat-mean & Mean of the categorical distribution & JoramSoch & 2020-01-16 & \pageref{sec:cat-mean} \\ \hline
P25 & mult-mean & Mean of the multinomial distribution & JoramSoch & 2020-01-16 & \pageref{sec:mult-mean} \\ \hline
P26 & matn-mvn & Equivalence of matrix-normal distribution and multivariate normal distribution & JoramSoch & 2020-01-20 & \pageref{sec:matn-mvn} \\ \hline
P27 & poiss-mle & Maximum likelihood estimation for Poisson-distributed data & JoramSoch & 2020-01-20 & \pageref{sec:poiss-mle} \\ \hline
P28 & beta-mome & Method of moments for beta-distributed data & JoramSoch & 2020-01-22 & \pageref{sec:beta-mome} \\ \hline
P29 & bin-prior & Conjugate prior distribution for binomial observations & JoramSoch & 2020-01-23 & \pageref{sec:bin-prior} \\ \hline
P30 & bin-post & Posterior distribution for binomial observations & JoramSoch & 2020-01-24 & \pageref{sec:bin-post} \\ \hline
P31 & bin-lme & Log model evidence for binomial observations & JoramSoch & 2020-01-24 & \pageref{sec:bin-lme} \\ \hline
P33 & norm-pdf & Probability density function of the normal distribution & JoramSoch & 2020-01-27 & \pageref{sec:norm-pdf} \\ \hline
P34 & mvn-pdf & Probability density function of the multivariate normal distribution & JoramSoch & 2020-01-27 & \pageref{sec:mvn-pdf} \\ \hline
P35 & mvn-marg & Marginal distributions of the multivariate normal distribution & JoramSoch & 2020-01-29 & \pageref{sec:mvn-marg} \\ \hline
P36 & ng-marg & Marginal distributions of the normal-gamma distribution & JoramSoch & 2020-01-29 & \pageref{sec:ng-marg} \\ \hline
P37 & cuni-pdf & Probability density function of the continuous uniform distribution & JoramSoch & 2020-01-31 & \pageref{sec:cuni-pdf} \\ \hline
P38 & cuni-cdf & Cumulative distribution function of the continuous uniform distribution & JoramSoch & 2020-01-02 & \pageref{sec:cuni-cdf} \\ \hline
P39 & cuni-qf & Quantile function of the continuous uniform distribution & JoramSoch & 2020-01-02 & \pageref{sec:cuni-qf} \\ \hline
P40 & mlr-ols2 & Ordinary least squares for multiple linear regression & JoramSoch & 2020-02-03 & \pageref{sec:mlr-ols2} \\ \hline
P41 & poissexp-prior & Conjugate prior distribution for the Poisson distribution with exposure values & JoramSoch & 2020-02-04 & \pageref{sec:poissexp-prior} \\ \hline
P42 & poissexp-post & Posterior distribution for the Poisson distribution with exposure values & JoramSoch & 2020-02-04 & \pageref{sec:poissexp-post} \\ \hline
P43 & poissexp-lme & Log model evidence for the Poisson distribution with exposure values & JoramSoch & 2020-02-04 & \pageref{sec:poissexp-lme} \\ \hline
P44 & ng-pdf & Probability density function of the normal-gamma distribution & JoramSoch & 2020-02-07 & \pageref{sec:ng-pdf} \\ \hline
P45 & gam-pdf & Probability density function of the gamma distribution & JoramSoch & 2020-02-08 & \pageref{sec:gam-pdf} \\ \hline
P46 & exp-pdf & Probability density function of the exponential distribution & JoramSoch & 2020-02-08 & \pageref{sec:exp-pdf} \\ \hline
P47 & exp-mean & Mean of the exponential distribution & JoramSoch & 2020-02-10 & \pageref{sec:exp-mean} \\ \hline
P48 & exp-cdf & Cumulative distribution function of the exponential distribution & JoramSoch & 2020-02-11 & \pageref{sec:exp-cdf} \\ \hline
P49 & exp-med & Median of the exponential distribution & JoramSoch & 2020-02-11 & \pageref{sec:exp-med} \\ \hline
P50 & exp-qf & Quantile function of the exponential distribution & JoramSoch & 2020-02-12 & \pageref{sec:exp-qf} \\ \hline
P51 & exp-mode & Mode of the exponential distribution & kantundpeterpan & 2020-02-12 & \pageref{sec:exp-mode} \\ \hline
P52 & mean-nonneg & Non-negativity of the expected value & JoramSoch & 2020-02-13 & \pageref{sec:mean-nonneg} \\ \hline
P53 & mean-lin & Linearity of the expected value & JoramSoch & 2020-02-13 & \pageref{sec:mean-lin} \\ \hline
P54 & mean-mono & Monotonicity of the expected value & JoramSoch & 2020-02-17 & \pageref{sec:mean-mono} \\ \hline
P55 & mean-mult & (Non-)Multiplicativity of the expected value & JoramSoch & 2020-02-17 & \pageref{sec:mean-mult} \\ \hline
P56 & ci-wilks & Construction of confidence intervals using Wilks' theorem & JoramSoch & 2020-02-19 & \pageref{sec:ci-wilks} \\ \hline
P57 & ent-nonneg & Non-negativity of the Shannon entropy & JoramSoch & 2020-02-19 & \pageref{sec:ent-nonneg} \\ \hline
P58 & cmi-mcde & Relation of continuous mutual information to marginal and conditional differential entropy & JoramSoch & 2020-02-21 & \pageref{sec:cmi-mcde} \\ \hline
P59 & cmi-mjde & Relation of continuous mutual information to marginal and joint differential entropy & JoramSoch & 2020-02-21 & \pageref{sec:cmi-mjde} \\ \hline
P60 & cmi-jcde & Relation of continuous mutual information to joint and conditional differential entropy & JoramSoch & 2020-02-21 & \pageref{sec:cmi-jcde} \\ \hline
P61 & resvar-bias & Maximum likelihood estimator of variance is biased & JoramSoch & 2020-02-24 & \pageref{sec:resvar-bias} \\ \hline
P62 & resvar-unb & Construction of unbiased estimator for variance & JoramSoch & 2020-02-25 & \pageref{sec:resvar-unb} \\ \hline
P63 & snr-rsq & Relationship between signal-to-noise ratio and RÂ² & JoramSoch & 2020-02-26 & \pageref{sec:snr-rsq} \\ \hline
P68 & dent-neg & Differential entropy can be negative & JoramSoch & 2020-03-02 & \pageref{sec:dent-neg} \\ \hline
P69 & exp-gam & Exponential distribution is a special case of gamma distribution & JoramSoch & 2020-03-02 & \pageref{sec:exp-gam} \\ \hline
P70 & matn-pdf & Probability density function of the matrix-normal distribution & JoramSoch & 2020-03-02 & \pageref{sec:matn-pdf} \\ \hline
P71 & norm-mgf & Moment-generating function of the normal distribution & JoramSoch & 2020-03-03 & \pageref{sec:norm-mgf} \\ \hline
P72 & logreg-lonp & Log-odds and probability in logistic regression & JoramSoch & 2020-03-03 & \pageref{sec:logreg-lonp} \\ \hline
P75 & mlr-mat & Transformation matrices for ordinary least squares & JoramSoch & 2020-03-09 & \pageref{sec:mlr-mat} \\ \hline
P76 & mlr-pss & Partition of sums of squares for multiple linear regression & JoramSoch & 2020-03-09 & \pageref{sec:mlr-pss} \\ \hline
P77 & mlr-wls & Weighted least squares for multiple linear regression & JoramSoch & 2020-03-11 & \pageref{sec:mlr-wls} \\ \hline
P78 & mlr-mle & Maximum likelihood estimation for multiple linear regression & JoramSoch & 2020-03-11 & \pageref{sec:mlr-mle} \\ \hline
P79 & mult-prior & Conjugate prior distribution for multinomial observations & JoramSoch & 2020-03-11 & \pageref{sec:mult-prior} \\ \hline
P80 & mult-post & Posterior distribution for multinomial observations & JoramSoch & 2020-03-11 & \pageref{sec:mult-post} \\ \hline
P81 & mult-lme & Log model evidence for multinomial observations & JoramSoch & 2020-03-11 & \pageref{sec:mult-lme} \\ \hline
P82 & cuni-mean & Mean of the continuous uniform distribution & JoramSoch & 2020-03-16 & \pageref{sec:cuni-mean} \\ \hline
P83 & cuni-med & Median of the continuous uniform distribution & JoramSoch & 2020-03-16 & \pageref{sec:cuni-med} \\ \hline
P84 & cuni-med & Mode of the continuous uniform distribution & JoramSoch & 2020-03-16 & \pageref{sec:cuni-med} \\ \hline
P85 & norm-cdf & Cumulative distribution function of the normal distribution & JoramSoch & 2020-03-20 & \pageref{sec:norm-cdf} \\ \hline
P86 & norm-cdfwerf & Expression of the cumulative distribution function of the normal distribution without the error function & JoramSoch & 2020-03-20 & \pageref{sec:norm-cdfwerf} \\ \hline
P87 & norm-qf & Quantile function of the normal distribution & JoramSoch & 2020-03-20 & \pageref{sec:norm-qf} \\ \hline
P88 & mvn-cond & Conditional distributions of the multivariate normal distribution & JoramSoch & 2020-03-20 & \pageref{sec:mvn-cond} \\ \hline
P89 & jl-lfnprior & Joint likelihood is the product of likelihood function and prior density & JoramSoch & 2020-05-05 & \pageref{sec:jl-lfnprior} \\ \hline
P90 & post-jl & Posterior density is proportional to joint likelihood & JoramSoch & 2020-05-05 & \pageref{sec:post-jl} \\ \hline
P91 & ml-jl & Marginal likelihood is a definite integral of joint likelihood & JoramSoch & 2020-05-05 & \pageref{sec:ml-jl} \\ \hline
P92 & mvn-kl & Kullback-Leibler divergence for the multivariate normal distribution & JoramSoch & 2020-05-05 & \pageref{sec:mvn-kl} \\ \hline
P93 & gam-kl & Kullback-Leibler divergence for the gamma distribution & JoramSoch & 2020-05-05 & \pageref{sec:gam-kl} \\ \hline
P94 & beta-pdf & Probability density function of the beta distribution & JoramSoch & 2020-05-05 & \pageref{sec:beta-pdf} \\ \hline
P95 & dir-pdf & Probability density function of the Dirichlet distribution & JoramSoch & 2020-05-05 & \pageref{sec:dir-pdf} \\ \hline
P96 & bern-pmf & Probability mass function of the Bernoulli distribution & JoramSoch & 2020-05-11 & \pageref{sec:bern-pmf} \\ \hline
P97 & bin-pmf & Probability mass function of the binomial distribution & JoramSoch & 2020-05-11 & \pageref{sec:bin-pmf} \\ \hline
P98 & cat-pmf & Probability mass function of the categorical distribution & JoramSoch & 2020-05-11 & \pageref{sec:cat-pmf} \\ \hline
P99 & mult-pmf & Probability mass function of the multinomial distribution & JoramSoch & 2020-05-11 & \pageref{sec:mult-pmf} \\ \hline
P100 & mvn-dent & Differential entropy of the multivariate normal distribution & JoramSoch & 2020-05-14 & \pageref{sec:mvn-dent} \\ \hline
P101 & norm-dent & Differential entropy of the normal distribution & JoramSoch & 2020-05-14 & \pageref{sec:norm-dent} \\ \hline
P102 & poiss-pmf & Probability mass function of the Poisson distribution & JoramSoch & 2020-05-14 & \pageref{sec:poiss-pmf} \\ \hline
P103 & mean-nnrvar & Expected value of a non-negative random variable & JoramSoch & 2020-05-18 & \pageref{sec:mean-nnrvar} \\ \hline
P104 & var-mean & Partition of variance into expected values & JoramSoch & 2020-05-19 & \pageref{sec:var-mean} \\ \hline
P105 & logreg-pnlo & Probability and log-odds in logistic regression & JoramSoch & 2020-05-19 & \pageref{sec:logreg-pnlo} \\ \hline
P106 & glm-ols & Ordinary least squares for the general linear model & JoramSoch & 2020-05-19 & \pageref{sec:glm-ols} \\ \hline
P107 & glm-wls & Weighted least squares for the general linear model & JoramSoch & 2020-05-19 & \pageref{sec:glm-wls} \\ \hline
P108 & gam-mean & Mean of the gamma distribution & JoramSoch & 2020-05-19 & \pageref{sec:gam-mean} \\ \hline
P109 & gam-var & Variance of the gamma distribution & JoramSoch & 2020-05-19 & \pageref{sec:gam-var} \\ \hline
P110 & gam-logmean & Logarithmic expectation of the gamma distribution & JoramSoch & 2020-05-25 & \pageref{sec:gam-logmean} \\ \hline
P111 & norm-snorm & Relationship between normal distribution and standard normal distribution & JoramSoch & 2020-05-26 & \pageref{sec:norm-snorm} \\ \hline
P112 & gam-sgam & Relationship between gamma distribution and standard gamma distribution & JoramSoch & 2020-05-26 & \pageref{sec:gam-sgam} \\ \hline
P113 & kl-ent & Relation of discrete Kullback-Leibler divergence to Shannon entropy & JoramSoch & 2020-05-27 & \pageref{sec:kl-ent} \\ \hline
P114 & kl-dent & Relation of continuous Kullback-Leibler divergence to differential entropy & JoramSoch & 2020-05-27 & \pageref{sec:kl-dent} \\ \hline
P115 & kl-inv & Invariance of the Kullback-Leibler divergence under parameter transformation & JoramSoch & 2020-05-28 & \pageref{sec:kl-inv} \\ \hline
P116 & kl-add & Additivity of the Kullback-Leibler divergence for independent distributions & JoramSoch & 2020-05-31 & \pageref{sec:kl-add} \\ \hline
P117 & kl-nonneg & Non-negativity of the Kullback-Leibler divergence & JoramSoch & 2020-05-31 & \pageref{sec:kl-nonneg} \\ \hline
P118 & cov-mean & Partition of covariance into expected values & JoramSoch & 2020-06-02 & \pageref{sec:cov-mean} \\ \hline
P119 & cov-corr & Relationship between covariance and correlation & JoramSoch & 2020-06-02 & \pageref{sec:cov-corr} \\ \hline
P120 & covmat-mean & Partition of a covariance matrix into expected values & JoramSoch & 2020-06-06 & \pageref{sec:covmat-mean} \\ \hline
P121 & covmat-corrmat & Relationship between covariance matrix and correlation matrix & JoramSoch & 2020-06-06 & \pageref{sec:covmat-corrmat} \\ \hline
P122 & precmat-corrmat & Relationship between precision matrix and correlation matrix & JoramSoch & 2020-06-06 & \pageref{sec:precmat-corrmat} \\ \hline
P123 & var-nonneg & Non-negativity of the variance & JoramSoch & 2020-06-06 & \pageref{sec:var-nonneg} \\ \hline
P124 & var-const & Variance of constant is zero & JoramSoch & 2020-06-27 & \pageref{sec:var-const} \\ \hline
P126 & var-inv & Invariance of the variance under addition of a constant & JoramSoch & 2020-07-07 & \pageref{sec:var-inv} \\ \hline
P127 & var-scal & Scaling of the variance upon multiplication with a constant & JoramSoch & 2020-07-07 & \pageref{sec:var-scal} \\ \hline
P128 & var-sum & Variance of the sum of two random variables & JoramSoch & 2020-07-07 & \pageref{sec:var-sum} \\ \hline
P129 & var-lincomb & Variance of the linear combination of two random variables & JoramSoch & 2020-07-07 & \pageref{sec:var-lincomb} \\ \hline
P130 & var-add & Additivity of the variance for independent random variables & JoramSoch & 2020-07-07 & \pageref{sec:var-add} \\ \hline
P131 & mean-qf & Expectation of a quadratic form & JoramSoch & 2020-07-13 & \pageref{sec:mean-qf} \\ \hline
P133 & blr-pp & Posterior probability of the alternative hypothesis for Bayesian linear regression & JoramSoch & 2020-07-17 & \pageref{sec:blr-pp} \\ \hline
P134 & blr-pcr & Posterior credibility region against the omnibus null hypothesis for Bayesian linear regression & JoramSoch & 2020-07-17 & \pageref{sec:blr-pcr} \\ \hline
P135 & mlr-idem & Projection matrix and residual-forming matrix are idempotent & JoramSoch & 2020-07-22 & \pageref{sec:mlr-idem} \\ \hline
P136 & mlr-wls2 & Weighted least squares for multiple linear regression & JoramSoch & 2020-07-22 & \pageref{sec:mlr-wls2} \\ \hline
P138 & mean-lotus & Law of the unconscious statistician & JoramSoch & 2020-07-22 & \pageref{sec:mean-lotus} \\ \hline
P140 & duni-pmf & Probability mass function of the discrete uniform distribution & JoramSoch & 2020-07-28 & \pageref{sec:duni-pmf} \\ \hline
P141 & duni-cdf & Cumulative distribution function of the discrete uniform distribution & JoramSoch & 2020-07-28 & \pageref{sec:duni-cdf} \\ \hline
P142 & duni-qf & Quantile function of the discrete uniform distribution & JoramSoch & 2020-07-28 & \pageref{sec:duni-qf} \\ \hline
P144 & matn-trans & Transposition of a matrix-normal random variable & JoramSoch & 2020-08-03 & \pageref{sec:matn-trans} \\ \hline
P145 & matn-ltt & Linear transformation theorem for the matrix-normal distribution & JoramSoch & 2020-08-03 & \pageref{sec:matn-ltt} \\ \hline
P146 & ng-cond & Conditional distributions of the normal-gamma distribution & JoramSoch & 2020-08-05 & \pageref{sec:ng-cond} \\ \hline
P147 & kl-nonsymm & Non-symmetry of the Kullback-Leibler divergence & JoramSoch & 2020-08-11 & \pageref{sec:kl-nonsymm} \\ \hline
P148 & kl-conv & Convexity of the Kullback-Leibler divergence & JoramSoch & 2020-08-11 & \pageref{sec:kl-conv} \\ \hline
P149 & ent-conc & Concavity of the Shannon entropy & JoramSoch & 2020-08-11 & \pageref{sec:ent-conc} \\ \hline
P150 & entcross-conv & Convexity of the cross-entropy & JoramSoch & 2020-08-11 & \pageref{sec:entcross-conv} \\ \hline
P151 & poiss-mean & Mean of the Poisson distribution & JoramSoch & 2020-08-19 & \pageref{sec:poiss-mean} \\ \hline
P152 & norm-fwhm & Full width at half maximum for the normal distribution & JoramSoch & 2020-08-19 & \pageref{sec:norm-fwhm} \\ \hline
P153 & mom-mgf & Moment in terms of moment-generating function & JoramSoch & 2020-08-19 & \pageref{sec:mom-mgf} \\ \hline
P154 & mgf-ltt & Linear transformation theorem for the moment-generating function & JoramSoch & 2020-08-19 & \pageref{sec:mgf-ltt} \\ \hline
P155 & mgf-lincomb & Moment-generating function of linear combination of independent random variables & JoramSoch & 2020-08-19 & \pageref{sec:mgf-lincomb} \\ \hline
P158 & cov-ind & Covariance of independent random variables & JoramSoch & 2020-09-03 & \pageref{sec:cov-ind} \\ \hline
P159 & mblr-prior & Conjugate prior distribution for multivariate Bayesian linear regression & JoramSoch & 2020-09-03 & \pageref{sec:mblr-prior} \\ \hline
P160 & mblr-post & Posterior distribution for multivariate Bayesian linear regression & JoramSoch & 2020-09-03 & \pageref{sec:mblr-post} \\ \hline
P161 & mblr-lme & Log model evidence for multivariate Bayesian linear regression & JoramSoch & 2020-09-03 & \pageref{sec:mblr-lme} \\ \hline
P162 & wald-pdf & Probability density function of the Wald distribution & tomfaulkenberry & 2020-09-04 & \pageref{sec:wald-pdf} \\ \hline
P164 & gibbs-ineq & Gibbs' inequality & JoramSoch & 2020-09-09 & \pageref{sec:gibbs-ineq} \\ \hline
P165 & logsum-ineq & Log sum inequality & JoramSoch & 2020-09-09 & \pageref{sec:logsum-ineq} \\ \hline
P166 & kl-nonneg2 & Non-negativity of the Kullback-Leibler divergence & JoramSoch & 2020-09-09 & \pageref{sec:kl-nonneg2} \\ \hline
P167 & momcent-1st & First central moment is zero & JoramSoch & 2020-09-09 & \pageref{sec:momcent-1st} \\ \hline
P168 & wald-mgf & Moment-generating function of the Wald distribution & tomfaulkenberry & 2020-09-13 & \pageref{sec:wald-mgf} \\ \hline
P169 & wald-mean & Mean of the Wald distribution & tomfaulkenberry & 2020-09-13 & \pageref{sec:wald-mean} \\ \hline
P170 & wald-var & Variance of the Wald distribution & tomfaulkenberry & 2020-09-13 & \pageref{sec:wald-var} \\ \hline
P171 & momraw-1st & First raw moment is mean & JoramSoch & 2020-10-08 & \pageref{sec:momraw-1st} \\ \hline
P172 & momraw-2nd & Relationship between second raw moment, variance and mean & JoramSoch & 2020-10-08 & \pageref{sec:momraw-2nd} \\ \hline
P173 & momcent-2nd & Second central moment is variance & JoramSoch & 2020-10-08 & \pageref{sec:momcent-2nd} \\ \hline
P174 & chi2-gam & Chi-squared distribution is a special case of gamma distribution & kjpetrykowski & 2020-10-12 & \pageref{sec:chi2-gam} \\ \hline
P175 & chi2-mom & Moments of the chi-squared distribution & kjpetrykowski & 2020-10-13 & \pageref{sec:chi2-mom} \\ \hline
P176 & norm-snorm2 & Relationship between normal distribution and standard normal distribution & JoramSoch & 2020-10-15 & \pageref{sec:norm-snorm2} \\ \hline
P177 & gam-sgam2 & Relationship between gamma distribution and standard gamma distribution & JoramSoch & 2020-10-15 & \pageref{sec:gam-sgam2} \\ \hline
P178 & gam-cdf & Cumulative distribution function of the gamma distribution & JoramSoch & 2020-10-15 & \pageref{sec:gam-cdf} \\ \hline
P179 & gam-xlogx & Expected value of x times ln(x) for a gamma distribution & JoramSoch & 2020-10-15 & \pageref{sec:gam-xlogx} \\ \hline
P180 & norm-snorm3 & Relationship between normal distribution and standard normal distribution & JoramSoch & 2020-10-22 & \pageref{sec:norm-snorm3} \\ \hline
P181 & dir-ep & Exceedance probabilities for the Dirichlet distribution & JoramSoch & 2020-10-22 & \pageref{sec:dir-ep} \\ \hline
P182 & dir-mle & Maximum likelihood estimation for Dirichlet-distributed data & JoramSoch & 2020-10-22 & \pageref{sec:dir-mle} \\ \hline
P183 & cdf-sifct & Cumulative distribution function of a strictly increasing function of a random variable & JoramSoch & 2020-10-29 & \pageref{sec:cdf-sifct} \\ \hline
P184 & pmf-sifct & Probability mass function of a strictly increasing function of a discrete random variable & JoramSoch & 2020-10-29 & \pageref{sec:pmf-sifct} \\ \hline
P185 & pdf-sifct & Probability density function of a strictly increasing function of a continuous random variable & JoramSoch & 2020-10-29 & \pageref{sec:pdf-sifct} \\ \hline
P186 & cdf-sdfct & Cumulative distribution function of a strictly decreasing function of a random variable & JoramSoch & 2020-11-06 & \pageref{sec:cdf-sdfct} \\ \hline
P187 & pmf-sdfct & Probability mass function of a strictly decreasing function of a discrete random variable & JoramSoch & 2020-11-06 & \pageref{sec:pmf-sdfct} \\ \hline
P188 & pdf-sdfct & Probability density function of a strictly decreasing function of a continuous random variable & JoramSoch & 2020-11-06 & \pageref{sec:pdf-sdfct} \\ \hline
P189 & cdf-pmf & Cumulative distribution function in terms of probability mass function of a discrete random variable & JoramSoch & 2020-11-12 & \pageref{sec:cdf-pmf} \\ \hline
P190 & cdf-pdf & Cumulative distribution function in terms of probability density function of a continuous random variable & JoramSoch & 2020-11-12 & \pageref{sec:cdf-pdf} \\ \hline
P191 & pdf-cdf & Probability density function is first derivative of cumulative distribution function & JoramSoch & 2020-11-12 & \pageref{sec:pdf-cdf} \\ \hline
P192 & qf-cdf & Quantile function is inverse of strictly monotonically increasing cumulative distribution function & JoramSoch & 2020-11-12 & \pageref{sec:qf-cdf} \\ \hline
P193 & norm-kl & Kullback-Leibler divergence for the normal distribution & JoramSoch & 2020-11-19 & \pageref{sec:norm-kl} \\ \hline
P194 & gam-qf & Quantile function of the gamma distribution & JoramSoch & 2020-11-19 & \pageref{sec:gam-qf} \\ \hline
P195 & beta-cdf & Cumulative distribution function of the beta distribution & JoramSoch & 2020-11-19 & \pageref{sec:beta-cdf} \\ \hline
P196 & norm-gi & Gaussian integral & JoramSoch & 2020-11-25 & \pageref{sec:norm-gi} \\ \hline
P197 & chi2-pdf & Probability density function of the chi-squared distribution & JoramSoch & 2020-11-25 & \pageref{sec:chi2-pdf} \\ \hline
P198 & beta-mgf & Moment-generating function of the beta distribution & JoramSoch & 2020-11-25 & \pageref{sec:beta-mgf} \\ \hline
P199 & dent-inv & Invariance of the differential entropy under addition of a constant & JoramSoch & 2020-12-02 & \pageref{sec:dent-inv} \\ \hline
P200 & dent-add & Addition of the differential entropy upon multiplication with a constant & JoramSoch & 2020-12-02 & \pageref{sec:dent-add} \\ \hline
P201 & ug-prior & Conjugate prior distribution for the univariate Gaussian & JoramSoch & 2021-03-03 & \pageref{sec:ug-prior} \\ \hline
P202 & ug-post & Posterior distribution for the univariate Gaussian & JoramSoch & 2021-03-03 & \pageref{sec:ug-post} \\ \hline
P203 & ug-lme & Log model evidence for the univariate Gaussian & JoramSoch & 2021-03-03 & \pageref{sec:ug-lme} \\ \hline
P204 & ug-ttest1 & One-sample t-test for independent observations & JoramSoch & 2021-03-12 & \pageref{sec:ug-ttest1} \\ \hline
P205 & ug-ttest2 & Two-sample t-test for independent observations & JoramSoch & 2021-03-12 & \pageref{sec:ug-ttest2} \\ \hline
P206 & ug-ttestp & Paired t-test for dependent observations & JoramSoch & 2021-03-12 & \pageref{sec:ug-ttestp} \\ \hline
P207 & ugkv-mle & Maximum likelihood estimation for the univariate Gaussian with known variance & JoramSoch & 2021-03-24 & \pageref{sec:ugkv-mle} \\ \hline
P208 & ugkv-ztest1 & One-sample z-test for independent observations & JoramSoch & 2021-03-24 & \pageref{sec:ugkv-ztest1} \\ \hline
P209 & ugkv-ztest2 & Two-sample z-test for independent observations & JoramSoch & 2021-03-24 & \pageref{sec:ugkv-ztest2} \\ \hline
P210 & ugkv-ztestp & Paired z-test for dependent observations & JoramSoch & 2021-03-24 & \pageref{sec:ugkv-ztestp} \\ \hline
P211 & ugkv-prior & Conjugate prior distribution for the univariate Gaussian with known variance & JoramSoch & 2021-03-24 & \pageref{sec:ugkv-prior} \\ \hline
P212 & ugkv-post & Posterior distribution for the univariate Gaussian with known variance & JoramSoch & 2021-03-24 & \pageref{sec:ugkv-post} \\ \hline
P213 & ugkv-lme & Log model evidence for the univariate Gaussian with known variance & JoramSoch & 2021-03-24 & \pageref{sec:ugkv-lme} \\ \hline
P214 & ugkv-anc & Accuracy and complexity for the univariate Gaussian with known variance & JoramSoch & 2021-03-24 & \pageref{sec:ugkv-anc} \\ \hline
P215 & ugkv-lbf & Log Bayes factor for the univariate Gaussian with known variance & JoramSoch & 2021-03-24 & \pageref{sec:ugkv-lbf} \\ \hline
P216 & ugkv-lbfmean & Expectation of the log Bayes factor for the univariate Gaussian with known variance & JoramSoch & 2021-03-24 & \pageref{sec:ugkv-lbfmean} \\ \hline
P217 & ugkv-cvlme & Cross-validated log model evidence for the univariate Gaussian with known variance & JoramSoch & 2021-03-24 & \pageref{sec:ugkv-cvlme} \\ \hline
P218 & ugkv-cvlbf & Cross-validated log Bayes factor for the univariate Gaussian with known variance & JoramSoch & 2021-03-24 & \pageref{sec:ugkv-cvlbf} \\ \hline
P219 & ugkv-cvlbfmean & Expectation of the cross-validated log Bayes factor for the univariate Gaussian with known variance & JoramSoch & 2021-03-24 & \pageref{sec:ugkv-cvlbfmean} \\ \hline
P220 & cdf-pit & Probability integral transform using cumulative distribution function & JoramSoch & 2021-04-07 & \pageref{sec:cdf-pit} \\ \hline
P221 & cdf-itm & Inverse transformation method using cumulative distribution function & JoramSoch & 2021-04-07 & \pageref{sec:cdf-itm} \\ \hline
P222 & cdf-dt & Distributional transformation using cumulative distribution function & JoramSoch & 2021-04-07 & \pageref{sec:cdf-dt} \\ \hline
P223 & ug-mle & Maximum likelihood estimation for the univariate Gaussian & JoramSoch & 2021-04-16 & \pageref{sec:ug-mle} \\ \hline
P224 & poissexp-mle & Maximum likelihood estimation for the Poisson distribution with exposure values & JoramSoch & 2021-04-16 & \pageref{sec:poissexp-mle} \\ \hline
P225 & poiss-prior & Conjugate prior distribution for Poisson-distributed data & JoramSoch & 2020-04-21 & \pageref{sec:poiss-prior} \\ \hline
P226 & poiss-post & Posterior distribution for Poisson-distributed data & JoramSoch & 2020-04-21 & \pageref{sec:poiss-post} \\ \hline
P227 & poiss-lme & Log model evidence for Poisson-distributed data & JoramSoch & 2020-04-21 & \pageref{sec:poiss-lme} \\ \hline
P228 & beta-mean & Mean of the beta distribution & JoramSoch & 2021-04-29 & \pageref{sec:beta-mean} \\ \hline
P229 & beta-var & Variance of the beta distribution & JoramSoch & 2021-04-29 & \pageref{sec:beta-var} \\ \hline
P230 & poiss-var & Variance of the Poisson distribution & JoramSoch & 2021-04-29 & \pageref{sec:poiss-var} \\ \hline
P231 & mvt-f & Relationship between multivariate t-distribution and F-distribution & JoramSoch & 2021-05-04 & \pageref{sec:mvt-f} \\ \hline
P232 & nst-t & Relationship between non-standardized t-distribution and t-distribution & JoramSoch & 2021-05-11 & \pageref{sec:nst-t} \\ \hline
P233 & norm-chi2 & Relationship between normal distribution and chi-squared distribution & JoramSoch & 2021-05-20 & \pageref{sec:norm-chi2} \\ \hline
P234 & norm-t & Relationship between normal distribution and t-distribution & JoramSoch & 2021-05-27 & \pageref{sec:norm-t} \\ \hline
P235 & norm-lincomb & Linear combination of independent normal random variables & JoramSoch & 2021-06-02 & \pageref{sec:norm-lincomb} \\ \hline
P236 & mvn-ind & Necessary and sufficient condition for independence of multivariate normal random variables & JoramSoch & 2021-06-02 & \pageref{sec:mvn-ind} \\ \hline
P237 & ng-mean & Mean of the normal-gamma distribution & JoramSoch & 2021-07-08 & \pageref{sec:ng-mean} \\ \hline
P238 & ng-dent & Differential entropy of the normal-gamma distribution & JoramSoch & 2021-07-08 & \pageref{sec:ng-dent} \\ \hline
P239 & gam-dent & Differential entropy of the gamma distribution & JoramSoch & 2021-07-14 & \pageref{sec:gam-dent} \\ \hline
P240 & ug-anc & Accuracy and complexity for the univariate Gaussian & JoramSoch & 2021-07-14 & \pageref{sec:ug-anc} \\ \hline
P241 & prob-ind & Probability under statistical independence & JoramSoch & 2021-07-23 & \pageref{sec:prob-ind} \\ \hline
P242 & prob-exc & Probability under mutual exclusivity & JoramSoch & 2021-07-23 & \pageref{sec:prob-exc} \\ \hline
P243 & prob-mon & Monotonicity of probability & JoramSoch & 2021-07-30 & \pageref{sec:prob-mon} \\ \hline
P244 & prob-emp & Probability of the empty set & JoramSoch & 2021-07-30 & \pageref{sec:prob-emp} \\ \hline
P245 & prob-comp & Probability of the complement & JoramSoch & 2021-07-30 & \pageref{sec:prob-comp} \\ \hline
P246 & prob-range & Range of probability & JoramSoch & 2021-07-30 & \pageref{sec:prob-range} \\ \hline
P247 & prob-add & Addition law of probability & JoramSoch & 2021-07-30 & \pageref{sec:prob-add} \\ \hline
P248 & prob-tot & Law of total probability & JoramSoch & 2021-08-08 & \pageref{sec:prob-tot} \\ \hline
P249 & prob-exh & Probability of exhaustive events & JoramSoch & 2021-08-08 & \pageref{sec:prob-exh} \\ \hline
P250 & norm-maxent & Normal distribution maximizes differential entropy for fixed variance & JoramSoch & 2020-08-25 & \pageref{sec:norm-maxent} \\ \hline
P251 & norm-extr & Extreme points of the probability density function of the normal distribution & JoramSoch & 2020-08-25 & \pageref{sec:norm-extr} \\ \hline
P252 & norm-infl & Inflection points of the probability density function of the normal distribution & JoramSoch & 2020-08-26 & \pageref{sec:norm-infl} \\ \hline
P253 & pmf-invfct & Probability mass function of an invertible function of a random vector & JoramSoch & 2021-08-30 & \pageref{sec:pmf-invfct} \\ \hline
P254 & pdf-invfct & Probability density function of an invertible function of a continuous random vector & JoramSoch & 2021-08-30 & \pageref{sec:pdf-invfct} \\ \hline
P255 & pdf-linfct & Probability density function of a linear function of a continuous random vector & JoramSoch & 2021-08-30 & \pageref{sec:pdf-linfct} \\ \hline
P256 & cdf-sumind & Cumulative distribution function of a sum of independent random variables & JoramSoch & 2021-08-30 & \pageref{sec:cdf-sumind} \\ \hline
P257 & pmf-sumind & Probability mass function of a sum of independent discrete random variables & JoramSoch & 2021-08-30 & \pageref{sec:pmf-sumind} \\ \hline
P258 & pdf-sumind & Probability density function of a sum of independent discrete random variables & JoramSoch & 2021-08-30 & \pageref{sec:pdf-sumind} \\ \hline
P259 & cf-fct & Characteristic function of a function of a random variable & JoramSoch & 2021-09-22 & \pageref{sec:cf-fct} \\ \hline
P260 & mgf-fct & Moment-generating function of a function of a random variable & JoramSoch & 2021-09-22 & \pageref{sec:mgf-fct} \\ \hline
P261 & dent-addvec & Addition of the differential entropy upon multiplication with invertible matrix & JoramSoch & 2021-10-07 & \pageref{sec:dent-addvec} \\ \hline
P262 & dent-noninv & Non-invariance of the differential entropy under change of variables & JoramSoch & 2021-10-07 & \pageref{sec:dent-noninv} \\ \hline
P263 & t-pdf & Probability density function of the t-distribution & JoramSoch & 2021-10-12 & \pageref{sec:t-pdf} \\ \hline
P264 & f-pdf & Probability density function of the F-distribution & JoramSoch & 2021-10-12 & \pageref{sec:f-pdf} \\ \hline
P265 & tglm-dist & Distribution of the transformed general linear model & JoramSoch & 2021-10-21 & \pageref{sec:tglm-dist} \\ \hline
P266 & tglm-para & Equivalence of parameter estimates from the transformed general linear model & JoramSoch & 2021-10-21 & \pageref{sec:tglm-para} \\ \hline
P267 & iglm-dist & Distribution of the inverse general linear model & JoramSoch & 2021-10-21 & \pageref{sec:iglm-dist} \\ \hline
P268 & iglm-blue & Best linear unbiased estimator for the inverse general linear model & JoramSoch & 2021-10-21 & \pageref{sec:iglm-blue} \\ \hline
P269 & cfm-para & Parameters of the corresponding forward model & JoramSoch & 2021-10-21 & \pageref{sec:cfm-para} \\ \hline
P270 & cfm-exist & Existence of a corresponding forward model & JoramSoch & 2021-10-21 & \pageref{sec:cfm-exist} \\ \hline
P271 & slr-ols & Ordinary least squares for simple linear regression & JoramSoch & 2021-10-27 & \pageref{sec:slr-ols} \\ \hline
P272 & slr-olsmean & Expectation of parameter estimates for simple linear regression & JoramSoch & 2021-10-27 & \pageref{sec:slr-olsmean} \\ \hline
P273 & slr-olsvar & Variance of parameter estimates for simple linear regression & JoramSoch & 2021-10-27 & \pageref{sec:slr-olsvar} \\ \hline
P274 & slr-meancent & Effects of mean-centering on parameter estimates for simple linear regression & JoramSoch & 2021-10-27 & \pageref{sec:slr-meancent} \\ \hline
P275 & slr-comp & The regression line goes through the center of mass point & JoramSoch & 2021-10-27 & \pageref{sec:slr-comp} \\ \hline
P276 & slr-ressum & The sum of residuals is zero in simple linear regression & JoramSoch & 2021-10-27 & \pageref{sec:slr-ressum} \\ \hline
P277 & slr-rescorr & The residuals and the covariate are uncorrelated in simple linear regression & JoramSoch & 2021-10-27 & \pageref{sec:slr-rescorr} \\ \hline
P278 & slr-resvar & Relationship between residual variance and sample variance in simple linear regression & JoramSoch & 2021-10-27 & \pageref{sec:slr-resvar} \\ \hline
P279 & slr-corr & Relationship between correlation coefficient and slope estimate in simple linear regression & JoramSoch & 2021-10-27 & \pageref{sec:slr-corr} \\ \hline
P280 & slr-rsq & Relationship between coefficient of determination and correlation coefficient in simple linear regression & JoramSoch & 2021-10-27 & \pageref{sec:slr-rsq} \\ \hline
P281 & slr-mlr & Simple linear regression is a special case of multiple linear regression & JoramSoch & 2021-11-09 & \pageref{sec:slr-mlr} \\ \hline
P282 & slr-olsdist & Distribution of parameter estimates for simple linear regression & JoramSoch & 2021-11-09 & \pageref{sec:slr-olsdist} \\ \hline
P283 & slr-proj & Projection of a data point to the regression line & JoramSoch & 2021-11-09 & \pageref{sec:slr-proj} \\ \hline
P284 & slr-sss & Sums of squares for simple linear regression & JoramSoch & 2021-11-09 & \pageref{sec:slr-sss} \\ \hline
P285 & slr-mat & Transformation matrices for simple linear regression & JoramSoch & 2021-11-09 & \pageref{sec:slr-mat} \\ \hline
P286 & slr-wls & Weighted least squares for simple linear regression & JoramSoch & 2021-11-16 & \pageref{sec:slr-wls} \\ \hline
P287 & slr-mle & Maximum likelihood estimation for simple linear regression & JoramSoch & 2021-11-16 & \pageref{sec:slr-mle} \\ \hline
P288 & slr-ols2 & Ordinary least squares for simple linear regression & JoramSoch & 2021-11-16 & \pageref{sec:slr-ols2} \\ \hline
P289 & slr-wls2 & Weighted least squares for simple linear regression & JoramSoch & 2021-11-16 & \pageref{sec:slr-wls2} \\ \hline
P290 & slr-mle2 & Maximum likelihood estimation for simple linear regression & JoramSoch & 2021-11-16 & \pageref{sec:slr-mle2} \\ \hline
P291 & mean-tot & Law of total expectation & JoramSoch & 2021-11-26 & \pageref{sec:mean-tot} \\ \hline
P292 & var-tot & Law of total variance & JoramSoch & 2021-11-26 & \pageref{sec:var-tot} \\ \hline
P293 & cov-tot & Law of total covariance & JoramSoch & 2021-11-26 & \pageref{sec:cov-tot} \\ \hline
P294 & dir-kl & Kullback-Leibler divergence for the Dirichlet distribution & JoramSoch & 2021-12-02 & \pageref{sec:dir-kl} \\ \hline
P295 & wish-kl & Kullback-Leibler divergence for the Wishart distribution & JoramSoch & 2021-12-02 & \pageref{sec:wish-kl} \\ \hline
P296 & matn-kl & Kullback-Leibler divergence for the matrix-normal distribution & JoramSoch & 2021-12-02 & \pageref{sec:matn-kl} \\ \hline
P297 & matn-samp & Sampling from the matrix-normal distribution & JoramSoch & 2021-12-07 & \pageref{sec:matn-samp} \\ \hline
P298 & mean-tr & Expected value of the trace of a matrix & JoramSoch & 2021-12-07 & \pageref{sec:mean-tr} \\ \hline
P299 & corr-z & Correlation coefficient in terms of standard scores & JoramSoch & 2021-12-14 & \pageref{sec:corr-z} \\ \hline
P300 & corr-range & Correlation always falls between -1 and +1 & JoramSoch & 2021-12-14 & \pageref{sec:corr-range} \\ \hline
P301 & bern-var & Variance of the Bernoulli distribution & JoramSoch & 2022-01-20 & \pageref{sec:bern-var} \\ \hline
P302 & bin-var & Variance of the binomial distribution & JoramSoch & 2022-01-20 & \pageref{sec:bin-var} \\ \hline
P303 & bern-varrange & Range of the variance of the Bernoulli distribution & JoramSoch & 2022-01-27 & \pageref{sec:bern-varrange} \\ \hline
P304 & bin-varrange & Range of the variance of the binomial distribution & JoramSoch & 2022-01-27 & \pageref{sec:bin-varrange} \\ \hline
P305 & mlr-mll & Maximum log-likelihood for multiple linear regression & JoramSoch & 2022-02-04 & \pageref{sec:mlr-mll} \\ \hline
P306 & lognorm-med & Median of the log-normal distribution & majapavlo & 2022-02-07 & \pageref{sec:lognorm-med} \\ \hline
P307 & mlr-aic & Akaike information criterion for multiple linear regression & JoramSoch & 2022-02-11 & \pageref{sec:mlr-aic} \\ \hline
P308 & mlr-bic & Bayesian information criterion for multiple linear regression & JoramSoch & 2022-02-11 & \pageref{sec:mlr-bic} \\ \hline
P309 & mlr-aicc & Corrected Akaike information criterion for multiple linear regression & JoramSoch & 2022-02-11 & \pageref{sec:mlr-aicc} \\ \hline
P310 & lognorm-pdf & Probability density function of the log-normal distribution & majapavlo & 2022-02-13 & \pageref{sec:lognorm-pdf} \\ \hline
P311 & lognorm-mode & Mode of the log-normal distribution & majapavlo & 2022-02-13 & \pageref{sec:lognorm-mode} \\ \hline
P312 & mlr-dev & Deviance for multiple linear regression & JoramSoch & 2022-03-01 & \pageref{sec:mlr-dev} \\ \hline
P313 & blr-dic & Deviance information criterion for multiple linear regression & JoramSoch & 2022-03-01 & \pageref{sec:blr-dic} \\ \hline
P317 & mle-bias & Maximum likelihood estimation can result in biased estimates & JoramSoch & 2022-03-18 & \pageref{sec:mle-bias} \\ \hline
P318 & pval-h0 & The p-value follows a uniform distribution under the null hypothesis & JoramSoch & 2022-03-18 & \pageref{sec:pval-h0} \\ \hline
P319 & prob-exh2 & Probability of exhaustive events & JoramSoch & 2022-03-27 & \pageref{sec:prob-exh2} \\ \hline
P320 & slr-olscorr & Parameter estimates for simple linear regression are uncorrelated after mean-centering & JoramSoch & 2022-04-14 & \pageref{sec:slr-olscorr} \\ \hline
P321 & norm-probstd & Probability of normal random variable being within standard deviations from its mean & JoramSoch & 2022-05-08 & \pageref{sec:norm-probstd} \\ \hline
P322 & mult-cov & Covariance matrix of the multinomial distribution & adkipnis & 2022-05-11 & \pageref{sec:mult-cov} \\ \hline
P323 & nw-pdf & Probability density function of the normal-Wishart distribution & JoramSoch & 2022-05-14 & \pageref{sec:nw-pdf} \\ \hline
P324 & ng-nw & Normal-gamma distribution is a special case of normal-Wishart distribution & JoramSoch & 2022-05-20 & \pageref{sec:ng-nw} \\ \hline
P325 & lognorm-cdf & Cumulative distribution function of the log-normal distribution & majapavlo & 2022-06-29 & \pageref{sec:lognorm-cdf} \\ \hline
P326 & lognorm-qf & Quantile function of the log-normal distribution & majapavlo & 2022-07-09 & \pageref{sec:lognorm-qf} \\ \hline
P327 & nw-mean & Mean of the normal-Wishart distribution & JoramSoch & 2022-07-14 & \pageref{sec:nw-mean} \\ \hline
P328 & gam-wish & Gamma distribution is a special case of Wishart distribution & JoramSoch & 2022-07-14 & \pageref{sec:gam-wish} \\ \hline
P329 & mlr-glm & Multiple linear regression is a special case of the general linear model & JoramSoch & 2022-07-21 & \pageref{sec:mlr-glm} \\ \hline
P330 & mvn-matn & Multivariate normal distribution is a special case of matrix-normal distribution & JoramSoch & 2022-07-31 & \pageref{sec:mvn-matn} \\ \hline
P331 & norm-mvn & Normal distribution is a special case of multivariate normal distribution & JoramSoch & 2022-08-19 & \pageref{sec:norm-mvn} \\ \hline
P332 & t-mvt & t-distribution is a special case of multivariate t-distribution & JoramSoch & 2022-08-25 & \pageref{sec:t-mvt} \\ \hline
P333 & mvt-pdf & Probability density function of the multivariate t-distribution & JoramSoch & 2022-09-02 & \pageref{sec:mvt-pdf} \\ \hline
P334 & bern-ent & Entropy of the Bernoulli distribution & JoramSoch & 2022-09-02 & \pageref{sec:bern-ent} \\ \hline
P335 & bin-ent & Entropy of the binomial distribution & JoramSoch & 2022-09-02 & \pageref{sec:bin-ent} \\ \hline
P336 & cat-ent & Entropy of the categorical distribution & JoramSoch & 2022-09-09 & \pageref{sec:cat-ent} \\ \hline
P337 & mult-ent & Entropy of the multinomial distribution & JoramSoch & 2022-09-09 & \pageref{sec:mult-ent} \\ \hline
P338 & cat-cov & Covariance matrix of the categorical distribution & JoramSoch & 2022-09-09 & \pageref{sec:cat-cov} \\ \hline
P339 & mvn-mean & Mean of the multivariate normal distribution & JoramSoch & 2022-09-15 & \pageref{sec:mvn-mean} \\ \hline
P340 & mvn-cov & Covariance matrix of the multivariate normal distribution & JoramSoch & 2022-09-15 & \pageref{sec:mvn-cov} \\ \hline
P341 & matn-mean & Mean of the matrix-normal distribution & JoramSoch & 2022-09-15 & \pageref{sec:matn-mean} \\ \hline
P342 & matn-cov & Covariance matrices of the matrix-normal distribution & JoramSoch & 2022-09-15 & \pageref{sec:matn-cov} \\ \hline
P343 & matn-marg & Marginal distributions for the matrix-normal distribution & JoramSoch & 2022-09-15 & \pageref{sec:matn-marg} \\ \hline
P344 & matn-dent & Differential entropy for the matrix-normal distribution & JoramSoch & 2022-09-22 & \pageref{sec:matn-dent} \\ \hline
P345 & ng-cov & Covariance and variance of the normal-gamma distribution & JoramSoch & 2022-09-22 & \pageref{sec:ng-cov} \\ \hline
P346 & ng-samp & Sampling from the normal-gamma distribution & JoramSoch & 2022-09-22 & \pageref{sec:ng-samp} \\ \hline
P347 & covmat-inv & Invariance of the covariance matrix under addition of constant vector & JoramSoch & 2022-09-22 & \pageref{sec:covmat-inv} \\ \hline
P348 & covmat-scal & Scaling of the covariance matrix upon multiplication with constant matrix & JoramSoch & 2022-09-22 & \pageref{sec:covmat-scal} \\ \hline
P349 & covmat-sum & Covariance matrix of the sum of two random vectors & JoramSoch & 2022-09-26 & \pageref{sec:covmat-sum} \\ \hline
P350 & covmat-symm & Symmetry of the covariance matrix & JoramSoch & 2022-09-26 & \pageref{sec:covmat-symm} \\ \hline
P351 & covmat-psd & Positive semi-definiteness of the covariance matrix & JoramSoch & 2022-09-26 & \pageref{sec:covmat-psd} \\ \hline
P352 & cov-var & Self-covariance equals variance & JoramSoch & 2022-09-26 & \pageref{sec:cov-var} \\ \hline
P353 & cov-symm & Symmetry of the covariance & JoramSoch & 2022-09-26 & \pageref{sec:cov-symm} \\ \hline
P354 & lognorm-mean & Mean of the log-normal distribution & majapavlo & 2022-10-02 & \pageref{sec:lognorm-mean} \\ \hline
P355 & lognorm-var & Variance of the log-normal distribution & majapavlo & 2022-10-02 & \pageref{sec:lognorm-var} \\ \hline
P356 & beta-chi2 & Relationship between chi-squared distribution and beta distribution & JoramSoch & 2022-10-07 & \pageref{sec:beta-chi2} \\ \hline
P357 & betabin-mome & Method of moments for beta-binomial data & JoramSoch & 2022-10-07 & \pageref{sec:betabin-mome} \\ \hline
P358 & bin-margcond & Marginal distribution of a conditional binomial distribution & JoramSoch & 2022-10-07 & \pageref{sec:bin-margcond} \\ \hline
P359 & mean-prodsqr & Square of expectation of product is less than or equal to product of expectation of squares & JoramSoch & 2022-10-11 & \pageref{sec:mean-prodsqr} \\ \hline
P360 & pgf-mean & Probability-generating function is expectation of function of random variable & JoramSoch & 2022-10-11 & \pageref{sec:pgf-mean} \\ \hline
P361 & pgf-zero & Value of the probability-generating function for argument zero & JoramSoch & 2022-10-11 & \pageref{sec:pgf-zero} \\ \hline
P362 & pgf-one & Value of the probability-generating function for argument one & JoramSoch & 2022-10-11 & \pageref{sec:pgf-one} \\ \hline
P363 & bin-pgf & Probability-generating function of the binomial distribution & JoramSoch & 2022-10-11 & \pageref{sec:bin-pgf} \\ \hline
P364 & betabin-pmf & Probability mass function of the beta-binomial distribution & JoramSoch & 2022-10-20 & \pageref{sec:betabin-pmf} \\ \hline
P365 & betabin-pmfitogf & Expression of the probability mass function of the beta-binomial distribution using only the gamma function & JoramSoch & 2022-10-20 & \pageref{sec:betabin-pmfitogf} \\ \hline
P366 & betabin-cdf & Cumulative distribution function of the beta-binomial distribution & JoramSoch & 2022-10-22 & \pageref{sec:betabin-cdf} \\ \hline
P369 & anova1-ols & Ordinary least squares for one-way analysis of variance & JoramSoch & 2022-11-06 & \pageref{sec:anova1-ols} \\ \hline
P370 & anova1-f & F-test for main effect in one-way analysis of variance & JoramSoch & 2022-11-06 & \pageref{sec:anova1-f} \\ \hline
P371 & anova2-ols & Ordinary least squares for two-way analysis of variance & JoramSoch & 2022-11-06 & \pageref{sec:anova2-ols} \\ \hline
P372 & anova2-fme & F-test for main effect in two-way analysis of variance & JoramSoch & 2022-11-10 & \pageref{sec:anova2-fme} \\ \hline
P373 & anova2-fia & F-test for interaction in two-way analysis of variance & JoramSoch & 2022-11-11 & \pageref{sec:anova2-fia} \\ \hline
P374 & anova2-fgm & F-test for grand mean in two-way analysis of variance & JoramSoch & 2022-11-11 & \pageref{sec:anova2-fgm} \\ \hline
P375 & anova1-repara & Reparametrization for one-way analysis of variance & JoramSoch & 2022-11-15 & \pageref{sec:anova1-repara} \\ \hline
P376 & anova1-pss & Partition of sums of squares in one-way analysis of variance & JoramSoch & 2022-11-15 & \pageref{sec:anova1-pss} \\ \hline
P377 & anova1-fols & F-statistic in terms of ordinary least squares estimates in one-way analysis of variance & JoramSoch & 2022-11-15 & \pageref{sec:anova1-fols} \\ \hline
P378 & anova2-cochran & Application of Cochran's theorem to two-way analysis of variance & JoramSoch & 2022-11-16 & \pageref{sec:anova2-cochran} \\ \hline
P379 & anova2-pss & Partition of sums of squares in two-way analysis of variance & JoramSoch & 2022-11-16 & \pageref{sec:anova2-pss} \\ \hline
P380 & anova2-fols & F-statistics in terms of ordinary least squares estimates in two-way analysis of variance & JoramSoch & 2022-11-16 & \pageref{sec:anova2-fols} \\ \hline
P381 & bin-mle & Maximum likelihood estimation for binomial observations & JoramSoch & 2022-11-23 & \pageref{sec:bin-mle} \\ \hline
P382 & bin-mll & Maximum log-likelihood for binomial observations & JoramSoch & 2022-11-24 & \pageref{sec:bin-mll} \\ \hline
P383 & bin-lbf & Log Bayes factor for binomial observations & JoramSoch & 2022-11-25 & \pageref{sec:bin-lbf} \\ \hline
P384 & bin-pp & Posterior probability of the alternative model for binomial observations & JoramSoch & 2022-11-26 & \pageref{sec:bin-pp} \\ \hline
P385 & mult-mle & Maximum likelihood estimation for multinomial observations & JoramSoch & 2022-12-02 & \pageref{sec:mult-mle} \\ \hline
P386 & mult-mll & Maximum log-likelihood for multinomial observations & JoramSoch & 2022-12-02 & \pageref{sec:mult-mll} \\ \hline
P387 & mult-lbf & Log Bayes factor for multinomial observations & JoramSoch & 2022-12-02 & \pageref{sec:mult-lbf} \\ \hline
P388 & mult-pp & Posterior probability of the alternative model for multinomial observations & JoramSoch & 2022-12-02 & \pageref{sec:mult-pp} \\ \hline
P389 & mlr-wlsdist & Distributions of estimated parameters, fitted signal and residuals in multiple linear regression upon weighted least squares & JoramSoch & 2022-12-13 & \pageref{sec:mlr-wlsdist} \\ \hline
P390 & mlr-rssdist & Distribution of residual sum of squares in multiple linear regression with weighted least squares & JoramSoch & 2022-12-13 & \pageref{sec:mlr-rssdist} \\ \hline
P391 & mlr-t & t-test for multiple linear regression using contrast-based inference & JoramSoch & 2022-12-13 & \pageref{sec:mlr-t} \\ \hline
P392 & mlr-f & F-test for multiple linear regression using contrast-based inference & JoramSoch & 2022-12-13 & \pageref{sec:mlr-f} \\ \hline
P393 & mlr-ind & Independence of estimated parameters and residuals in multiple linear regression & JoramSoch & 2022-12-13 & \pageref{sec:mlr-ind} \\ \hline
P394 & mvn-indprod & Independence of products of multivariate normal random vector & JoramSoch & 2022-12-13 & \pageref{sec:mvn-indprod} \\ \hline
P395 & mvn-chi2 & Relationship between multivariate normal distribution and chi-squared distribution & JoramSoch & 2022-12-20 & \pageref{sec:mvn-chi2} \\ \hline
P396 & cuni-var & Variance of the continuous uniform distribution & JoramSoch & 2022-12-20 & \pageref{sec:cuni-var} \\ \hline
P397 & cuni-dent & Differential entropy of the continuous uniform distribution & JoramSoch & 2022-12-20 & \pageref{sec:cuni-dent} \\ \hline
P398 & resvar-biasp & Maximum likelihood estimator of variance in multiple linear regression is biased & JoramSoch & 2022-12-21 & \pageref{sec:resvar-biasp} \\ \hline
P399 & mlr-symm & Projection matrix and residual-forming matrix are symmetric & JoramSoch & 2022-12-22 & \pageref{sec:mlr-symm} \\ \hline
P400 & mlr-olsdist & Distributions of estimated parameters, fitted signal and residuals in multiple linear regression upon ordinary least squares & JoramSoch & 2022-12-23 & \pageref{sec:mlr-olsdist} \\ \hline
P401 & exp-var & Variance of the exponential distribution & majapavlo & 2023-01-23 & \pageref{sec:exp-var} \\ \hline
P402 & exg-pdf & Probability density function of the ex-Gaussian distribution & tomfaulkenberry & 2023-04-18 & \pageref{sec:exg-pdf} \\ \hline
P403 & exp-mgf & Moment-generating function of the exponential distribution & tomfaulkenberry & 2023-04-19 & \pageref{sec:exp-mgf} \\ \hline
P404 & exg-mgf & Moment-generating function of the ex-Gaussian distribution & tomfaulkenberry & 2023-04-19 & \pageref{sec:exg-mgf} \\ \hline
P405 & exg-mean & Mean of the ex-Gaussian distribution & tomfaulkenberry & 2023-04-19 & \pageref{sec:exg-mean} \\ \hline
P406 & exg-var & Variance of the ex-Gaussian distribution & tomfaulkenberry & 2023-04-19 & \pageref{sec:exg-var} \\ \hline
P407 & skew-mean & Partition of skewness into expected values & tomfaulkenberry & 2023-04-20 & \pageref{sec:skew-mean} \\ \hline
P408 & exg-skew & Skewness of the ex-Gaussian distribution & tomfaulkenberry & 2023-04-21 & \pageref{sec:exg-skew} \\ \hline
P409 & exp-skew & Skewness of the exponential distribution & tomfaulkenberry & 2023-04-24 & \pageref{sec:exp-skew} \\ \hline
P410 & duni-ent & Entropy of the discrete uniform distribution & JoramSoch & 2023-08-11 & \pageref{sec:duni-ent} \\ \hline
P411 & duni-maxent & Discrete uniform distribution maximizes entropy for finite support & JoramSoch & 2023-08-18 & \pageref{sec:duni-maxent} \\ \hline
P412 & cuni-maxent & Continuous uniform distribution maximizes differential entropy for fixed range & JoramSoch & 2023-08-25 & \pageref{sec:cuni-maxent} \\ \hline
P413 & post-ind & Combined posterior distributions in terms of individual posterior distributions obtained from conditionally independent data & JoramSoch & 2023-09-01 & \pageref{sec:post-ind} \\ \hline
P416 & bvn-pdf & Probability density function of the bivariate normal distribution & JoramSoch & 2023-09-22 & \pageref{sec:bvn-pdf} \\ \hline
P417 & bvn-pdfcorr & Probability density function of the bivariate normal distribution in terms of correlation coefficient & JoramSoch & 2023-09-29 & \pageref{sec:bvn-pdfcorr} \\ \hline
P418 & mlr-olstr & Ordinary least squares for multiple linear regression with two regressors & JoramSoch & 2023-10-06 & \pageref{sec:mlr-olstr} \\ \hline
P419 & bern-kl & Kullback-Leibler divergence for the Bernoulli distribution & JoramSoch & 2023-10-13 & \pageref{sec:bern-kl} \\ \hline
P420 & bin-kl & Kullback-Leibler divergence for the binomial distribution & JoramSoch & 2023-10-20 & \pageref{sec:bin-kl} \\ \hline
P421 & wald-skew & Skewness of the Wald distribution & tomfaulkenberry & 2023-10-24 & \pageref{sec:wald-skew} \\ \hline
P422 & cuni-kl & Kullback-Leibler divergence for the continuous uniform distribution & JoramSoch & 2023-10-27 & \pageref{sec:cuni-kl} \\ \hline
P423 & wald-mome & Method of moments for Wald-distributed data & tomfaulkenberry & 2023-10-30 & \pageref{sec:wald-mome} \\ \hline
P424 & exg-mome & Method of moments for ex-Gaussian-distributed data & tomfaulkenberry & 2023-10-30 & \pageref{sec:exg-mome} \\ \hline
P425 & duni-kl & Kullback-Leibler divergence for the discrete uniform distribution & JoramSoch & 2023-11-17 & \pageref{sec:duni-kl} \\ \hline
P426 & gam-scal & Scaling of a random variable following the gamma distribution & JoramSoch & 2023-11-24 & \pageref{sec:gam-scal} \\ \hline
P427 & bin-map & Maximum-a-posteriori estimation for binomial observations & JoramSoch & 2023-12-01 & \pageref{sec:bin-map} \\ \hline
P428 & mult-map & Maximum-a-posteriori estimation for multinomial observations & JoramSoch & 2023-12-08 & \pageref{sec:mult-map} \\ \hline
P429 & bin-test & Binomial test & JoramSoch & 2023-12-16 & \pageref{sec:bin-test} \\ \hline
P430 & mult-test & Multinomial test & JoramSoch & 2023-12-23 & \pageref{sec:mult-test} \\ \hline
P431 & blr-anc & Accuracy and complexity for Bayesian linear regression & JoramSoch & 2024-01-12 & \pageref{sec:blr-anc} \\ \hline
P432 & blrkc-prior & Conjugate prior distribution for Bayesian linear regression with known covariance & JoramSoch & 2024-01-19 & \pageref{sec:blrkc-prior} \\ \hline
P433 & blrkc-post & Posterior distribution for Bayesian linear regression with known covariance & JoramSoch & 2024-01-19 & \pageref{sec:blrkc-post} \\ \hline
P434 & blrkc-lme & Log model evidence for Bayesian linear regression with known covariance & JoramSoch & 2024-01-19 & \pageref{sec:blrkc-lme} \\ \hline
P435 & blrkc-anc & Accuracy and complexity for Bayesian linear regression with known covariance & JoramSoch & 2024-01-19 & \pageref{sec:blrkc-anc} \\ \hline
P436 & mvn-mgf & Moment-generating function of the multivariate normal distribution & JoramSoch & 2024-02-16 & \pageref{sec:mvn-mgf} \\ \hline
P437 & gam-mgf & Moment-generating function of the gamma distribution & JoramSoch & 2024-02-23 & \pageref{sec:gam-mgf} \\ \hline
P438 & lpsr-spsr & The log probability scoring rule is a strictly proper scoring rule & KarahanS & 2024-02-28 & \pageref{sec:lpsr-spsr} \\ \hline
P439 & resvar-unbp & Construction of unbiased estimator for variance in multiple linear regression & JoramSoch & 2024-03-08 & \pageref{sec:resvar-unbp} \\ \hline
P440 & rsq-resvar & Expression of RÂ² in terms of residual variances & JoramSoch & 2024-03-08 & \pageref{sec:rsq-resvar} \\ \hline
P441 & rsq-test & Statistical significance test for the coefficient of determinantion based on an omnibus F-test & JoramSoch & 2024-03-08 & \pageref{sec:rsq-test} \\ \hline
P442 & fstat-rsq & Relationship between F-statistic and RÂ² & JoramSoch & 2024-03-15 & \pageref{sec:fstat-rsq} \\ \hline
P443 & fstat-mll & Relationship between F-statistic and maximum log-likelihood & JoramSoch & 2024-03-28 & \pageref{sec:fstat-mll} \\ \hline
P444 & snr-mll & Relationship between signal-to-noise ratio and maximum log-likelihood & JoramSoch & 2024-03-28 & \pageref{sec:snr-mll} \\ \hline
P445 & bsr-spsr & Brier scoring rule is strictly proper scoring rule & KarahanS & 2024-03-28 & \pageref{sec:bsr-spsr} \\ \hline
P446 & blr-posterr & Expression of the noise precision posterior for Bayesian linear regression using prediction and parameter errors & JoramSoch & 2024-04-05 & \pageref{sec:blr-posterr} \\ \hline
P447 & blr-postind & Combined posterior distribution for Bayesian linear regression when analyzing conditionally independent data sets & JoramSoch & 2024-04-12 & \pageref{sec:blr-postind} \\ \hline
P448 & blr-map & Maximum-a-posteriori estimation for Bayesian linear regression & JoramSoch & 2024-04-19 & \pageref{sec:blr-map} \\ \hline
P449 & blr-lbf & Log Bayes factor for Bayesian linear regression & JoramSoch & 2024-04-26 & \pageref{sec:blr-lbf} \\ \hline
P450 & mlr-tsingle & Specific t-test for single regressor in multiple linear regression & JoramSoch & 2024-05-03 & \pageref{sec:mlr-tsingle} \\ \hline
P451 & slr-tint & Statistical test for intercept parameter in simple linear regression model & JoramSoch & 2024-05-10 & \pageref{sec:slr-tint} \\ \hline
P452 & slr-tslo & Statistical test for slope parameter in simple linear regression model & JoramSoch & 2024-05-17 & \pageref{sec:slr-tslo} \\ \hline
P453 & slr-fcomp & Statistical test for comparing simple linear regression models with and without slope parameter & JoramSoch & 2024-05-24 & \pageref{sec:slr-fcomp} \\ \hline
P454 & mlr-fomnibus & Omnibus F-test for multiple regressors in multiple linear regression & JoramSoch & 2024-05-31 & \pageref{sec:mlr-fomnibus} \\ \hline
P455 & glm-llr & Log-likelihood ratio for the general linear model & JoramSoch & 2024-06-07 & \pageref{sec:glm-llr} \\ \hline
P456 & glm-mll & Maximum log-likelihood for the general linear model & JoramSoch & 2024-06-14 & \pageref{sec:glm-mll} \\ \hline
P457 & glm-mi & Mutual information of dependent and independent variables in the general linear model & JoramSoch & 2024-06-21 & \pageref{sec:glm-mi} \\ \hline
P458 & glm-llrmi & Equivalence of log-likelihood ratio and mutual information for the general linear model & JoramSoch & 2024-06-21 & \pageref{sec:glm-llrmi} \\ \hline
P459 & iglm-llrs & Equivalence of log-likelihood ratios for regular and inverse general linear model & JoramSoch & 2024-06-28 & \pageref{sec:iglm-llrs} \\ \hline
P460 & ug-fev & F-test for equality of variances in two independent samples & JoramSoch & 2024-07-05 & \pageref{sec:ug-fev} \\ \hline
P461 & slr-pss & Partition of sums of squares for simple linear regression & JoramSoch & 2024-07-12 & \pageref{sec:slr-pss} \\ \hline
P462 & mlr-ols3 & Ordinary least squares for multiple linear regression & JoramSoch & 2024-07-18 & \pageref{sec:mlr-ols3} \\ \hline
\end{longtable}



\pagebreak
\section{Definition by Number}

\begin{longtable}{|p{1cm}|p{2cm}|p{6.5cm}|p{3cm}|p{2cm}|c|}
\hline
\textbf{ID} & \textbf{Shortcut} & \textbf{Definition} & \textbf{Author} & \textbf{Date} & \textbf{Page} \\ \hline
D1 & mvn & Multivariate normal distribution & JoramSoch & 2020-01-22 & \pageref{sec:mvn} \\ \hline
D2 & mgf & Moment-generating function & JoramSoch & 2020-01-22 & \pageref{sec:mgf} \\ \hline
D3 & cuni & Continuous uniform distribution & JoramSoch & 2020-01-27 & \pageref{sec:cuni} \\ \hline
D4 & norm & Normal distribution & JoramSoch & 2020-01-27 & \pageref{sec:norm} \\ \hline
D5 & ng & Normal-gamma distribution & JoramSoch & 2020-01-27 & \pageref{sec:ng} \\ \hline
D6 & matn & Matrix-normal distribution & JoramSoch & 2020-01-27 & \pageref{sec:matn} \\ \hline
D7 & gam & Gamma distribution & JoramSoch & 2020-02-08 & \pageref{sec:gam} \\ \hline
D8 & exp & Exponential distribution & JoramSoch & 2020-02-08 & \pageref{sec:exp} \\ \hline
D9 & pmf & Probability mass function & JoramSoch & 2020-02-13 & \pageref{sec:pmf} \\ \hline
D10 & pdf & Probability density function & JoramSoch & 2020-02-13 & \pageref{sec:pdf} \\ \hline
D11 & mean & Expected value & JoramSoch & 2020-02-13 & \pageref{sec:mean} \\ \hline
D12 & var & Variance & JoramSoch & 2020-02-13 & \pageref{sec:var} \\ \hline
D13 & cdf & Cumulative distribution function & JoramSoch & 2020-02-17 & \pageref{sec:cdf} \\ \hline
D14 & qf & Quantile function & JoramSoch & 2020-02-17 & \pageref{sec:qf} \\ \hline
D15 & ent & Shannon entropy & JoramSoch & 2020-02-19 & \pageref{sec:ent} \\ \hline
D16 & dent & Differential entropy & JoramSoch & 2020-02-19 & \pageref{sec:dent} \\ \hline
D17 & ent-cond & Conditional entropy & JoramSoch & 2020-02-19 & \pageref{sec:ent-cond} \\ \hline
D18 & ent-joint & Joint entropy & JoramSoch & 2020-02-19 & \pageref{sec:ent-joint} \\ \hline
D19 & mi & Mutual information & JoramSoch & 2020-02-19 & \pageref{sec:mi} \\ \hline
D19 & mi & Mutual information & JoramSoch & 2020-02-19 & \pageref{sec:mi} \\ \hline
D20 & resvar & Residual variance & JoramSoch & 2020-02-25 & \pageref{sec:resvar} \\ \hline
D21 & rsq & Coefficient of determination & JoramSoch & 2020-02-25 & \pageref{sec:rsq} \\ \hline
D22 & snr & Signal-to-noise ratio & JoramSoch & 2020-02-25 & \pageref{sec:snr} \\ \hline
D27 & gm & Generative model & JoramSoch & 2020-03-03 & \pageref{sec:gm} \\ \hline
D28 & lf & Likelihood function & JoramSoch & 2020-03-03 & \pageref{sec:lf} \\ \hline
D28 & lf & Likelihood function & JoramSoch & 2020-03-03 & \pageref{sec:lf} \\ \hline
D29 & prior & Prior distribution & JoramSoch & 2020-03-03 & \pageref{sec:prior} \\ \hline
D30 & fpm & Full probability model & JoramSoch & 2020-03-03 & \pageref{sec:fpm} \\ \hline
D31 & jl & Joint likelihood & JoramSoch & 2020-03-03 & \pageref{sec:jl} \\ \hline
D32 & post & Posterior distribution & JoramSoch & 2020-03-03 & \pageref{sec:post} \\ \hline
D33 & ml & Marginal likelihood & JoramSoch & 2020-03-03 & \pageref{sec:ml} \\ \hline
D34 & dent-cond & Conditional differential entropy & JoramSoch & 2020-03-21 & \pageref{sec:dent-cond} \\ \hline
D35 & dent-joint & Joint differential entropy & JoramSoch & 2020-03-21 & \pageref{sec:dent-joint} \\ \hline
D36 & mlr & Multiple linear regression & JoramSoch & 2020-03-21 & \pageref{sec:mlr} \\ \hline
D37 & tss & Total sum of squares & JoramSoch & 2020-03-21 & \pageref{sec:tss} \\ \hline
D38 & ess & Explained sum of squares & JoramSoch & 2020-03-21 & \pageref{sec:ess} \\ \hline
D39 & rss & Residual sum of squares & JoramSoch & 2020-03-21 & \pageref{sec:rss} \\ \hline
D40 & glm & General linear model & JoramSoch & 2020-03-21 & \pageref{sec:glm} \\ \hline
D41 & poiss-data & Poisson-distributed data & JoramSoch & 2020-03-22 & \pageref{sec:poiss-data} \\ \hline
D42 & poissexp & Poisson distribution with exposure values & JoramSoch & 2020-03-22 & \pageref{sec:poissexp} \\ \hline
D43 & wish & Wishart distribution & JoramSoch & 2020-03-22 & \pageref{sec:wish} \\ \hline
D44 & bern & Bernoulli distribution & JoramSoch & 2020-03-22 & \pageref{sec:bern} \\ \hline
D45 & bin & Binomial distribution & JoramSoch & 2020-03-22 & \pageref{sec:bin} \\ \hline
D46 & cat & Categorical distribution & JoramSoch & 2020-03-22 & \pageref{sec:cat} \\ \hline
D47 & mult & Multinomial distribution & JoramSoch & 2020-03-22 & \pageref{sec:mult} \\ \hline
D48 & prob & Probability & JoramSoch & 2020-05-10 & \pageref{sec:prob} \\ \hline
D49 & prob-joint & Joint probability & JoramSoch & 2020-05-10 & \pageref{sec:prob-joint} \\ \hline
D50 & prob-marg & Law of marginal probability & JoramSoch & 2020-05-10 & \pageref{sec:prob-marg} \\ \hline
D51 & prob-cond & Law of conditional probability & JoramSoch & 2020-05-10 & \pageref{sec:prob-cond} \\ \hline
D52 & kl & Kullback-Leibler divergence & JoramSoch & 2020-05-10 & \pageref{sec:kl} \\ \hline
D53 & beta & Beta distribution & JoramSoch & 2020-05-10 & \pageref{sec:beta} \\ \hline
D54 & dir & Dirichlet distribution & JoramSoch & 2020-05-10 & \pageref{sec:dir} \\ \hline
D55 & dist & Probability distribution & JoramSoch & 2020-05-17 & \pageref{sec:dist} \\ \hline
D56 & dist-joint & Joint probability distribution & JoramSoch & 2020-05-17 & \pageref{sec:dist-joint} \\ \hline
D57 & dist-marg & Marginal probability distribution & JoramSoch & 2020-05-17 & \pageref{sec:dist-marg} \\ \hline
D58 & dist-cond & Conditional probability distribution & JoramSoch & 2020-05-17 & \pageref{sec:dist-cond} \\ \hline
D59 & llf & Log-likelihood function & JoramSoch & 2020-05-17 & \pageref{sec:llf} \\ \hline
D60 & mle & Maximum likelihood estimation & JoramSoch & 2020-05-15 & \pageref{sec:mle} \\ \hline
D61 & mll & Maximum log-likelihood & JoramSoch & 2020-05-15 & \pageref{sec:mll} \\ \hline
D62 & poiss & Poisson distribution & JoramSoch & 2020-05-25 & \pageref{sec:poiss} \\ \hline
D63 & snorm & Standard normal distribution & JoramSoch & 2020-05-26 & \pageref{sec:snorm} \\ \hline
D64 & sgam & Standard gamma distribution & JoramSoch & 2020-05-26 & \pageref{sec:sgam} \\ \hline
D65 & rvar & Random variable & JoramSoch & 2020-05-27 & \pageref{sec:rvar} \\ \hline
D66 & rvec & Random vector & JoramSoch & 2020-05-27 & \pageref{sec:rvec} \\ \hline
D67 & rmat & Random matrix & JoramSoch & 2020-05-27 & \pageref{sec:rmat} \\ \hline
D68 & cgf & Cumulant-generating function & JoramSoch & 2020-05-31 & \pageref{sec:cgf} \\ \hline
D69 & pgf & Probability-generating function & JoramSoch & 2020-05-31 & \pageref{sec:pgf} \\ \hline
D70 & cov & Covariance & JoramSoch & 2020-06-02 & \pageref{sec:cov} \\ \hline
D71 & corr & Correlation & JoramSoch & 2020-06-02 & \pageref{sec:corr} \\ \hline
D72 & covmat & Covariance matrix & JoramSoch & 2020-06-06 & \pageref{sec:covmat} \\ \hline
D73 & corrmat & Correlation matrix & JoramSoch & 2020-06-06 & \pageref{sec:corrmat} \\ \hline
D74 & precmat & Precision matrix & JoramSoch & 2020-06-06 & \pageref{sec:precmat} \\ \hline
D75 & ind & Statistical independence & JoramSoch & 2020-06-06 & \pageref{sec:ind} \\ \hline
D76 & logreg & Logistic regression & JoramSoch & 2020-06-28 & \pageref{sec:logreg} \\ \hline
D77 & beta-data & Beta-distributed data & JoramSoch & 2020-06-28 & \pageref{sec:beta-data} \\ \hline
D78 & bin-data & Binomial observations & JoramSoch & 2020-07-07 & \pageref{sec:bin-data} \\ \hline
D79 & mult-data & Multinomial observations & JoramSoch & 2020-07-07 & \pageref{sec:mult-data} \\ \hline
D81 & emat & Estimation matrix & JoramSoch & 2020-07-22 & \pageref{sec:emat} \\ \hline
D82 & pmat & Projection matrix & JoramSoch & 2020-07-22 & \pageref{sec:pmat} \\ \hline
D83 & rfmat & Residual-forming matrix & JoramSoch & 2020-07-22 & \pageref{sec:rfmat} \\ \hline
D85 & ent-cross & Cross-entropy & JoramSoch & 2020-07-28 & \pageref{sec:ent-cross} \\ \hline
D86 & dent-cross & Differential cross-entropy & JoramSoch & 2020-07-28 & \pageref{sec:dent-cross} \\ \hline
D88 & duni & Discrete uniform distribution & JoramSoch & 2020-07-28 & \pageref{sec:duni} \\ \hline
D90 & mom & Moment & JoramSoch & 2020-08-19 & \pageref{sec:mom} \\ \hline
D91 & fwhm & Full width at half maximum & JoramSoch & 2020-08-19 & \pageref{sec:fwhm} \\ \hline
D94 & std & Standard deviation & JoramSoch & 2020-09-03 & \pageref{sec:std} \\ \hline
D95 & wald & Wald distribution & tomfaulkenberry & 2020-09-04 & \pageref{sec:wald} \\ \hline
D96 & const & Constant & JoramSoch & 2020-09-09 & \pageref{sec:const} \\ \hline
D97 & mom-raw & Raw moment & JoramSoch & 2020-10-08 & \pageref{sec:mom-raw} \\ \hline
D98 & mom-cent & Central moment & JoramSoch & 2020-10-08 & \pageref{sec:mom-cent} \\ \hline
D99 & mom-stand & Standardized moment & JoramSoch & 2020-10-08 & \pageref{sec:mom-stand} \\ \hline
D100 & chi2 & Chi-squared distribution & kjpetrykowski & 2020-10-13 & \pageref{sec:chi2} \\ \hline
D101 & med & Median & JoramSoch & 2020-10-15 & \pageref{sec:med} \\ \hline
D102 & mode & Mode & JoramSoch & 2020-10-15 & \pageref{sec:mode} \\ \hline
D103 & prob-exc & Exceedance probability & JoramSoch & 2020-10-22 & \pageref{sec:prob-exc} \\ \hline
D104 & dir-data & Dirichlet-distributed data & JoramSoch & 2020-10-22 & \pageref{sec:dir-data} \\ \hline
D105 & rvar-disc & Discrete and continuous random variable & JoramSoch & 2020-10-29 & \pageref{sec:rvar-disc} \\ \hline
D106 & rvar-uni & Univariate and multivariate random variable & JoramSoch & 2020-11-06 & \pageref{sec:rvar-uni} \\ \hline
D107 & min & Minimum & JoramSoch & 2020-11-12 & \pageref{sec:min} \\ \hline
D108 & max & Maximum & JoramSoch & 2020-11-12 & \pageref{sec:max} \\ \hline
D109 & rexp & Random experiment & JoramSoch & 2020-11-19 & \pageref{sec:rexp} \\ \hline
D110 & reve & Random event & JoramSoch & 2020-11-19 & \pageref{sec:reve} \\ \hline
D112 & ind-cond & Conditional independence & JoramSoch & 2020-11-19 & \pageref{sec:ind-cond} \\ \hline
D116 & prior-flat & Flat, hard and soft prior distribution & JoramSoch & 2020-12-02 & \pageref{sec:prior-flat} \\ \hline
D117 & prior-uni & Uniform and non-uniform prior distribution & JoramSoch & 2020-12-02 & \pageref{sec:prior-uni} \\ \hline
D118 & prior-inf & Informative and non-informative prior distribution & JoramSoch & 2020-12-02 & \pageref{sec:prior-inf} \\ \hline
D119 & prior-emp & Empirical and theoretical prior distribution & JoramSoch & 2020-12-02 & \pageref{sec:prior-emp} \\ \hline
D120 & prior-conj & Conjugate and non-conjugate prior distribution & JoramSoch & 2020-12-02 & \pageref{sec:prior-conj} \\ \hline
D121 & prior-maxent & Maximum entropy prior distribution & JoramSoch & 2020-12-02 & \pageref{sec:prior-maxent} \\ \hline
D122 & prior-eb & Empirical Bayes prior distribution & JoramSoch & 2020-12-02 & \pageref{sec:prior-eb} \\ \hline
D123 & prior-ref & Reference prior distribution & JoramSoch & 2020-12-02 & \pageref{sec:prior-ref} \\ \hline
D124 & ug & Univariate Gaussian & JoramSoch & 2021-03-03 & \pageref{sec:ug} \\ \hline
D125 & h0 & Null hypothesis & JoramSoch & 2021-03-12 & \pageref{sec:h0} \\ \hline
D126 & h1 & Alternative hypothesis & JoramSoch & 2021-03-12 & \pageref{sec:h1} \\ \hline
D127 & hyp & Statistical hypothesis & JoramSoch & 2021-03-19 & \pageref{sec:hyp} \\ \hline
D128 & hyp-simp & Simple and composite hypothesis & JoramSoch & 2021-03-19 & \pageref{sec:hyp-simp} \\ \hline
D129 & hyp-point & Point and set hypothesis & JoramSoch & 2021-03-19 & \pageref{sec:hyp-point} \\ \hline
D130 & test & Statistical hypothesis test & JoramSoch & 2021-03-19 & \pageref{sec:test} \\ \hline
D131 & tstat & Test statistic & JoramSoch & 2021-03-19 & \pageref{sec:tstat} \\ \hline
D132 & size & Size of a statistical test & JoramSoch & 2021-03-19 & \pageref{sec:size} \\ \hline
D133 & alpha & Significance level & JoramSoch & 2021-03-19 & \pageref{sec:alpha} \\ \hline
D134 & cval & Critical value & JoramSoch & 2021-03-19 & \pageref{sec:cval} \\ \hline
D135 & pval & p-value & JoramSoch & 2021-03-19 & \pageref{sec:pval} \\ \hline
D136 & ugkv & Univariate Gaussian with known variance & JoramSoch & 2021-03-23 & \pageref{sec:ugkv} \\ \hline
D137 & power & Power of a statistical test & JoramSoch & 2021-03-31 & \pageref{sec:power} \\ \hline
D138 & hyp-tail & One-tailed and two-tailed hypothesis & JoramSoch & 2021-03-31 & \pageref{sec:hyp-tail} \\ \hline
D139 & test-tail & One-tailed and two-tailed test & JoramSoch & 2021-03-31 & \pageref{sec:test-tail} \\ \hline
D140 & dist-samp & Sampling distribution & JoramSoch & 2021-03-31 & \pageref{sec:dist-samp} \\ \hline
D141 & cdf-joint & Joint cumulative distribution function & JoramSoch & 2020-04-07 & \pageref{sec:cdf-joint} \\ \hline
D142 & mean-samp & Sample mean & JoramSoch & 2021-04-16 & \pageref{sec:mean-samp} \\ \hline
D143 & var-samp & Sample variance & JoramSoch & 2021-04-16 & \pageref{sec:var-samp} \\ \hline
D144 & cov-samp & Sample covariance & ciaranmci & 2021-04-21 & \pageref{sec:cov-samp} \\ \hline
D145 & prec & Precision & JoramSoch & 2020-04-21 & \pageref{sec:prec} \\ \hline
D146 & f & F-distribution & JoramSoch & 2020-04-21 & \pageref{sec:f} \\ \hline
D147 & t & t-distribution & JoramSoch & 2021-04-21 & \pageref{sec:t} \\ \hline
D148 & mvt & Multivariate t-distribution & JoramSoch & 2020-04-21 & \pageref{sec:mvt} \\ \hline
D149 & eb & Empirical Bayes & JoramSoch & 2021-04-29 & \pageref{sec:eb} \\ \hline
D150 & vb & Variational Bayes & JoramSoch & 2021-04-29 & \pageref{sec:vb} \\ \hline
D151 & mome & Method-of-moments estimation & JoramSoch & 2021-04-29 & \pageref{sec:mome} \\ \hline
D152 & nst & Non-standardized t-distribution & JoramSoch & 2021-05-20 & \pageref{sec:nst} \\ \hline
D153 & covmat-samp & Sample covariance matrix & JoramSoch & 2021-05-20 & \pageref{sec:covmat-samp} \\ \hline
D154 & mean-rvec & Expected value of a random vector & JoramSoch & 2021-07-08 & \pageref{sec:mean-rvec} \\ \hline
D155 & mean-rmat & Expected value of a random matrix & JoramSoch & 2021-07-08 & \pageref{sec:mean-rmat} \\ \hline
D156 & exc & Mutual exclusivity & JoramSoch & 2021-07-23 & \pageref{sec:exc} \\ \hline
D157 & suni & Standard uniform distribution & JoramSoch & 2021-07-23 & \pageref{sec:suni} \\ \hline
D158 & prob-ax & Kolmogorov axioms of probability & JoramSoch & 2021-07-30 & \pageref{sec:prob-ax} \\ \hline
D159 & cf & Characteristic function & JoramSoch & 2021-09-22 & \pageref{sec:cf} \\ \hline
D160 & tglm & Transformed general linear model & JoramSoch & 2021-10-21 & \pageref{sec:tglm} \\ \hline
D161 & iglm & Inverse general linear model & JoramSoch & 2021-10-21 & \pageref{sec:iglm} \\ \hline
D162 & cfm & Corresponding forward model & JoramSoch & 2021-10-21 & \pageref{sec:cfm} \\ \hline
D163 & slr & Simple linear regression & JoramSoch & 2021-10-27 & \pageref{sec:slr} \\ \hline
D164 & regline & Regression line & JoramSoch & 2021-10-27 & \pageref{sec:regline} \\ \hline
D165 & samp-spc & Sample space & JoramSoch & 2021-11-26 & \pageref{sec:samp-spc} \\ \hline
D166 & eve-spc & Event space & JoramSoch & 2021-11-26 & \pageref{sec:eve-spc} \\ \hline
D167 & prob-spc & Probability space & JoramSoch & 2021-11-26 & \pageref{sec:prob-spc} \\ \hline
D168 & corr-samp & Sample correlation coefficient & JoramSoch & 2021-12-14 & \pageref{sec:corr-samp} \\ \hline
D169 & corrmat-samp & Sample correlation matrix & JoramSoch & 2021-12-14 & \pageref{sec:corrmat-samp} \\ \hline
D170 & lognorm & Log-normal distribution & majapavlo & 2022-02-07 & \pageref{sec:lognorm} \\ \hline
D173 & mse & Mean squared error & JoramSoch & 2022-03-27 & \pageref{sec:mse} \\ \hline
D174 & ci & Confidence interval & JoramSoch & 2022-03-27 & \pageref{sec:ci} \\ \hline
D175 & nw & Normal-Wishart distribution & JoramSoch & 2022-05-14 & \pageref{sec:nw} \\ \hline
D176 & covmat-cross & Cross-covariance matrix & JoramSoch & 2022-09-26 & \pageref{sec:covmat-cross} \\ \hline
D177 & betabin & Beta-binomial distribution & JoramSoch & 2022-10-20 & \pageref{sec:betabin} \\ \hline
D178 & betabin-data & Beta-binomial data & JoramSoch & 2022-10-20 & \pageref{sec:betabin-data} \\ \hline
D181 & anova1 & One-way analysis of variance & JoramSoch & 2022-11-06 & \pageref{sec:anova1} \\ \hline
D182 & anova2 & Two-way analysis of variance & JoramSoch & 2022-11-06 & \pageref{sec:anova2} \\ \hline
D183 & trss & Treatment sum of squares & JoramSoch & 2022-12-14 & \pageref{sec:trss} \\ \hline
D184 & iass & Interaction sum of squares & JoramSoch & 2022-12-14 & \pageref{sec:iass} \\ \hline
D185 & tcon & t-contrast for contrast-based inference in multiple linear regression & JoramSoch & 2022-12-16 & \pageref{sec:tcon} \\ \hline
D186 & fcon & F-contrast for contrast-based inference in multiple linear regression & JoramSoch & 2022-12-16 & \pageref{sec:fcon} \\ \hline
D187 & exg & ex-Gaussian distribution & tomfaulkenberry & 2023-04-18 & \pageref{sec:exg} \\ \hline
D188 & skew & Skewness & tomfaulkenberry & 2023-04-20 & \pageref{sec:skew} \\ \hline
D189 & bvn & Bivariate normal distribution & JoramSoch & 2023-09-22 & \pageref{sec:bvn} \\ \hline
D190 & skew-samp & Sample skewness & tomfaulkenberry & 2023-10-30 & \pageref{sec:skew-samp} \\ \hline
D191 & map & Maximum-a-posteriori estimation & JoramSoch & 2023-12-01 & \pageref{sec:map} \\ \hline
D192 & sr & Scoring rule & KarahanS & 2024-02-28 & \pageref{sec:sr} \\ \hline
D193 & psr & Proper scoring rule & KarahanS & 2024-02-28 & \pageref{sec:psr} \\ \hline
D194 & spsr & Strictly proper scoring rule & KarahanS & 2024-02-28 & \pageref{sec:spsr} \\ \hline
D195 & lpsr & Log probability scoring rule & KarahanS & 2024-02-28 & \pageref{sec:lpsr} \\ \hline
D196 & fstat & F-statistic & JoramSoch & 2024-03-15 & \pageref{sec:fstat} \\ \hline
D197 & bsr & Brier scoring rule & KarahanS & 2024-03-23 & \pageref{sec:bsr} \\ \hline
D198 & lr & Likelihood ratio & JoramSoch & 2024-06-14 & \pageref{sec:lr} \\ \hline
D199 & llr & Log-likelihood ratio & JoramSoch & 2024-06-14 & \pageref{sec:llr} \\ \hline
\end{longtable}



\pagebreak
\section{Proof by Topic}

\textbf{A}

$\bullet$ Accuracy and complexity for Bayesian linear regression, \pageref{sec:blr-anc}

$\bullet$ Accuracy and complexity for Bayesian linear regression with known covariance, \pageref{sec:blrkc-anc}

$\bullet$ Accuracy and complexity for the univariate Gaussian, \pageref{sec:ug-anc}

$\bullet$ Accuracy and complexity for the univariate Gaussian with known variance, \pageref{sec:ugkv-anc}

$\bullet$ Addition law of probability, \pageref{sec:prob-add}

$\bullet$ Addition of the differential entropy upon multiplication with a constant, \pageref{sec:dent-add}

$\bullet$ Addition of the differential entropy upon multiplication with invertible matrix, \pageref{sec:dent-addvec}

$\bullet$ Additivity of the Kullback-Leibler divergence for independent distributions, \pageref{sec:kl-add}

$\bullet$ Additivity of the variance for independent random variables, \pageref{sec:var-add}

$\bullet$ Akaike information criterion for multiple linear regression, \pageref{sec:mlr-aic}

$\bullet$ Application of Cochran's theorem to two-way analysis of variance, \pageref{sec:anova2-cochran}


\vspace{1em}
\textbf{B}

$\bullet$ Bayes' rule, \pageref{sec:bayes-rule}

$\bullet$ Bayes' theorem, \pageref{sec:bayes-th}

$\bullet$ Bayesian information criterion for multiple linear regression, \pageref{sec:mlr-bic}

$\bullet$ Best linear unbiased estimator for the inverse general linear model, \pageref{sec:iglm-blue}

$\bullet$ Binomial test, \pageref{sec:bin-test}

$\bullet$ Brier scoring rule is strictly proper scoring rule, \pageref{sec:bsr-spsr}


\vspace{1em}
\textbf{C}

$\bullet$ Characteristic function of a function of a random variable, \pageref{sec:cf-fct}

$\bullet$ Chi-squared distribution is a special case of gamma distribution, \pageref{sec:chi2-gam}

$\bullet$ Combined posterior distribution for Bayesian linear regression when analyzing conditionally independent data sets, \pageref{sec:blr-postind}

$\bullet$ Combined posterior distributions in terms of individual posterior distributions obtained from conditionally independent data, \pageref{sec:post-ind}

$\bullet$ Concavity of the Shannon entropy, \pageref{sec:ent-conc}

$\bullet$ Conditional distributions of the multivariate normal distribution, \pageref{sec:mvn-cond}

$\bullet$ Conditional distributions of the normal-gamma distribution, \pageref{sec:ng-cond}

$\bullet$ Conjugate prior distribution for Bayesian linear regression, \pageref{sec:blr-prior}

$\bullet$ Conjugate prior distribution for Bayesian linear regression with known covariance, \pageref{sec:blrkc-prior}

$\bullet$ Conjugate prior distribution for binomial observations, \pageref{sec:bin-prior}

$\bullet$ Conjugate prior distribution for multinomial observations, \pageref{sec:mult-prior}

$\bullet$ Conjugate prior distribution for multivariate Bayesian linear regression, \pageref{sec:mblr-prior}

$\bullet$ Conjugate prior distribution for Poisson-distributed data, \pageref{sec:poiss-prior}

$\bullet$ Conjugate prior distribution for the Poisson distribution with exposure values, \pageref{sec:poissexp-prior}

$\bullet$ Conjugate prior distribution for the univariate Gaussian, \pageref{sec:ug-prior}

$\bullet$ Conjugate prior distribution for the univariate Gaussian with known variance, \pageref{sec:ugkv-prior}

$\bullet$ Construction of confidence intervals using Wilks' theorem, \pageref{sec:ci-wilks}

$\bullet$ Construction of unbiased estimator for variance, \pageref{sec:resvar-unb}

$\bullet$ Construction of unbiased estimator for variance in multiple linear regression, \pageref{sec:resvar-unbp}

$\bullet$ Continuous uniform distribution maximizes differential entropy for fixed range, \pageref{sec:cuni-maxent}

$\bullet$ Convexity of the cross-entropy, \pageref{sec:entcross-conv}

$\bullet$ Convexity of the Kullback-Leibler divergence, \pageref{sec:kl-conv}

$\bullet$ Corrected Akaike information criterion for multiple linear regression, \pageref{sec:mlr-aicc}

$\bullet$ Correlation always falls between -1 and +1, \pageref{sec:corr-range}

$\bullet$ Correlation coefficient in terms of standard scores, \pageref{sec:corr-z}

$\bullet$ Covariance and variance of the normal-gamma distribution, \pageref{sec:ng-cov}

$\bullet$ Covariance matrices of the matrix-normal distribution, \pageref{sec:matn-cov}

$\bullet$ Covariance matrix of the categorical distribution, \pageref{sec:cat-cov}

$\bullet$ Covariance matrix of the multinomial distribution, \pageref{sec:mult-cov}

$\bullet$ Covariance matrix of the multivariate normal distribution, \pageref{sec:mvn-cov}

$\bullet$ Covariance matrix of the sum of two random vectors, \pageref{sec:covmat-sum}

$\bullet$ Covariance of independent random variables, \pageref{sec:cov-ind}

$\bullet$ Cross-validated log Bayes factor for the univariate Gaussian with known variance, \pageref{sec:ugkv-cvlbf}

$\bullet$ Cross-validated log model evidence for the univariate Gaussian with known variance, \pageref{sec:ugkv-cvlme}

$\bullet$ Cumulative distribution function in terms of probability density function of a continuous random variable, \pageref{sec:cdf-pdf}

$\bullet$ Cumulative distribution function in terms of probability mass function of a discrete random variable, \pageref{sec:cdf-pmf}

$\bullet$ Cumulative distribution function of a strictly decreasing function of a random variable, \pageref{sec:cdf-sdfct}

$\bullet$ Cumulative distribution function of a strictly increasing function of a random variable, \pageref{sec:cdf-sifct}

$\bullet$ Cumulative distribution function of a sum of independent random variables, \pageref{sec:cdf-sumind}

$\bullet$ Cumulative distribution function of the beta distribution, \pageref{sec:beta-cdf}

$\bullet$ Cumulative distribution function of the beta-binomial distribution, \pageref{sec:betabin-cdf}

$\bullet$ Cumulative distribution function of the continuous uniform distribution, \pageref{sec:cuni-cdf}

$\bullet$ Cumulative distribution function of the discrete uniform distribution, \pageref{sec:duni-cdf}

$\bullet$ Cumulative distribution function of the exponential distribution, \pageref{sec:exp-cdf}

$\bullet$ Cumulative distribution function of the gamma distribution, \pageref{sec:gam-cdf}

$\bullet$ Cumulative distribution function of the log-normal distribution, \pageref{sec:lognorm-cdf}

$\bullet$ Cumulative distribution function of the normal distribution, \pageref{sec:norm-cdf}


\vspace{1em}
\textbf{D}

$\bullet$ Derivation of RÂ² and adjusted RÂ², \pageref{sec:rsq-der}

$\bullet$ Deviance for multiple linear regression, \pageref{sec:mlr-dev}

$\bullet$ Deviance information criterion for multiple linear regression, \pageref{sec:blr-dic}

$\bullet$ Differential entropy can be negative, \pageref{sec:dent-neg}

$\bullet$ Differential entropy for the matrix-normal distribution, \pageref{sec:matn-dent}

$\bullet$ Differential entropy of the continuous uniform distribution, \pageref{sec:cuni-dent}

$\bullet$ Differential entropy of the gamma distribution, \pageref{sec:gam-dent}

$\bullet$ Differential entropy of the multivariate normal distribution, \pageref{sec:mvn-dent}

$\bullet$ Differential entropy of the normal distribution, \pageref{sec:norm-dent}

$\bullet$ Differential entropy of the normal-gamma distribution, \pageref{sec:ng-dent}

$\bullet$ Discrete uniform distribution maximizes entropy for finite support, \pageref{sec:duni-maxent}

$\bullet$ Distribution of parameter estimates for simple linear regression, \pageref{sec:slr-olsdist}

$\bullet$ Distribution of residual sum of squares in multiple linear regression with weighted least squares, \pageref{sec:mlr-rssdist}

$\bullet$ Distribution of the inverse general linear model, \pageref{sec:iglm-dist}

$\bullet$ Distribution of the transformed general linear model, \pageref{sec:tglm-dist}

$\bullet$ Distributional transformation using cumulative distribution function, \pageref{sec:cdf-dt}

$\bullet$ Distributions of estimated parameters, fitted signal and residuals in multiple linear regression upon ordinary least squares, \pageref{sec:mlr-olsdist}

$\bullet$ Distributions of estimated parameters, fitted signal and residuals in multiple linear regression upon weighted least squares, \pageref{sec:mlr-wlsdist}


\vspace{1em}
\textbf{E}

$\bullet$ Effects of mean-centering on parameter estimates for simple linear regression, \pageref{sec:slr-meancent}

$\bullet$ Entropy of the Bernoulli distribution, \pageref{sec:bern-ent}

$\bullet$ Entropy of the binomial distribution, \pageref{sec:bin-ent}

$\bullet$ Entropy of the categorical distribution, \pageref{sec:cat-ent}

$\bullet$ Entropy of the discrete uniform distribution, \pageref{sec:duni-ent}

$\bullet$ Entropy of the multinomial distribution, \pageref{sec:mult-ent}

$\bullet$ Equivalence of log-likelihood ratio and mutual information for the general linear model, \pageref{sec:glm-llrmi}

$\bullet$ Equivalence of log-likelihood ratios for regular and inverse general linear model, \pageref{sec:iglm-llrs}

$\bullet$ Equivalence of matrix-normal distribution and multivariate normal distribution, \pageref{sec:matn-mvn}

$\bullet$ Equivalence of parameter estimates from the transformed general linear model, \pageref{sec:tglm-para}

$\bullet$ Exceedance probabilities for the Dirichlet distribution, \pageref{sec:dir-ep}

$\bullet$ Existence of a corresponding forward model, \pageref{sec:cfm-exist}

$\bullet$ Expectation of a quadratic form, \pageref{sec:mean-qf}

$\bullet$ Expectation of parameter estimates for simple linear regression, \pageref{sec:slr-olsmean}

$\bullet$ Expectation of the cross-validated log Bayes factor for the univariate Gaussian with known variance, \pageref{sec:ugkv-cvlbfmean}

$\bullet$ Expectation of the log Bayes factor for the univariate Gaussian with known variance, \pageref{sec:ugkv-lbfmean}

$\bullet$ Expected value of a non-negative random variable, \pageref{sec:mean-nnrvar}

$\bullet$ Expected value of the trace of a matrix, \pageref{sec:mean-tr}

$\bullet$ Expected value of x times ln(x) for a gamma distribution, \pageref{sec:gam-xlogx}

$\bullet$ Exponential distribution is a special case of gamma distribution, \pageref{sec:exp-gam}

$\bullet$ Expression of RÂ² in terms of residual variances, \pageref{sec:rsq-resvar}

$\bullet$ Expression of the cumulative distribution function of the normal distribution without the error function, \pageref{sec:norm-cdfwerf}

$\bullet$ Expression of the noise precision posterior for Bayesian linear regression using prediction and parameter errors, \pageref{sec:blr-posterr}

$\bullet$ Expression of the probability mass function of the beta-binomial distribution using only the gamma function, \pageref{sec:betabin-pmfitogf}

$\bullet$ Extreme points of the probability density function of the normal distribution, \pageref{sec:norm-extr}


\vspace{1em}
\textbf{F}

$\bullet$ F-statistic in terms of ordinary least squares estimates in one-way analysis of variance, \pageref{sec:anova1-fols}

$\bullet$ F-statistics in terms of ordinary least squares estimates in two-way analysis of variance, \pageref{sec:anova2-fols}

$\bullet$ F-test for equality of variances in two independent samples, \pageref{sec:ug-fev}

$\bullet$ F-test for grand mean in two-way analysis of variance, \pageref{sec:anova2-fgm}

$\bullet$ F-test for interaction in two-way analysis of variance, \pageref{sec:anova2-fia}

$\bullet$ F-test for main effect in one-way analysis of variance, \pageref{sec:anova1-f}

$\bullet$ F-test for main effect in two-way analysis of variance, \pageref{sec:anova2-fme}

$\bullet$ F-test for multiple linear regression using contrast-based inference, \pageref{sec:mlr-f}

$\bullet$ First central moment is zero, \pageref{sec:momcent-1st}

$\bullet$ First raw moment is mean, \pageref{sec:momraw-1st}

$\bullet$ Full width at half maximum for the normal distribution, \pageref{sec:norm-fwhm}


\vspace{1em}
\textbf{G}

$\bullet$ Gamma distribution is a special case of Wishart distribution, \pageref{sec:gam-wish}

$\bullet$ Gaussian integral, \pageref{sec:norm-gi}

$\bullet$ Gibbs' inequality, \pageref{sec:gibbs-ineq}


\vspace{1em}
\textbf{I}

$\bullet$ Independence of estimated parameters and residuals in multiple linear regression, \pageref{sec:mlr-ind}

$\bullet$ Independence of products of multivariate normal random vector, \pageref{sec:mvn-indprod}

$\bullet$ Inflection points of the probability density function of the normal distribution, \pageref{sec:norm-infl}

$\bullet$ Invariance of the covariance matrix under addition of constant vector, \pageref{sec:covmat-inv}

$\bullet$ Invariance of the differential entropy under addition of a constant, \pageref{sec:dent-inv}

$\bullet$ Invariance of the Kullback-Leibler divergence under parameter transformation, \pageref{sec:kl-inv}

$\bullet$ Invariance of the variance under addition of a constant, \pageref{sec:var-inv}

$\bullet$ Inverse transformation method using cumulative distribution function, \pageref{sec:cdf-itm}


\vspace{1em}
\textbf{J}

$\bullet$ Joint likelihood is the product of likelihood function and prior density, \pageref{sec:jl-lfnprior}


\vspace{1em}
\textbf{K}

$\bullet$ Kullback-Leibler divergence for the Bernoulli distribution, \pageref{sec:bern-kl}

$\bullet$ Kullback-Leibler divergence for the binomial distribution, \pageref{sec:bin-kl}

$\bullet$ Kullback-Leibler divergence for the continuous uniform distribution, \pageref{sec:cuni-kl}

$\bullet$ Kullback-Leibler divergence for the Dirichlet distribution, \pageref{sec:dir-kl}

$\bullet$ Kullback-Leibler divergence for the discrete uniform distribution, \pageref{sec:duni-kl}

$\bullet$ Kullback-Leibler divergence for the gamma distribution, \pageref{sec:gam-kl}

$\bullet$ Kullback-Leibler divergence for the matrix-normal distribution, \pageref{sec:matn-kl}

$\bullet$ Kullback-Leibler divergence for the multivariate normal distribution, \pageref{sec:mvn-kl}

$\bullet$ Kullback-Leibler divergence for the normal distribution, \pageref{sec:norm-kl}

$\bullet$ Kullback-Leibler divergence for the normal-gamma distribution, \pageref{sec:ng-kl}

$\bullet$ Kullback-Leibler divergence for the Wishart distribution, \pageref{sec:wish-kl}


\vspace{1em}
\textbf{L}

$\bullet$ Law of the unconscious statistician, \pageref{sec:mean-lotus}

$\bullet$ Law of total covariance, \pageref{sec:cov-tot}

$\bullet$ Law of total expectation, \pageref{sec:mean-tot}

$\bullet$ Law of total probability, \pageref{sec:prob-tot}

$\bullet$ Law of total variance, \pageref{sec:var-tot}

$\bullet$ Linear combination of independent normal random variables, \pageref{sec:norm-lincomb}

$\bullet$ Linear transformation theorem for the matrix-normal distribution, \pageref{sec:matn-ltt}

$\bullet$ Linear transformation theorem for the moment-generating function, \pageref{sec:mgf-ltt}

$\bullet$ Linear transformation theorem for the multivariate normal distribution, \pageref{sec:mvn-ltt}

$\bullet$ Linearity of the expected value, \pageref{sec:mean-lin}

$\bullet$ Log Bayes factor for Bayesian linear regression, \pageref{sec:blr-lbf}

$\bullet$ Log Bayes factor for binomial observations, \pageref{sec:bin-lbf}

$\bullet$ Log Bayes factor for multinomial observations, \pageref{sec:mult-lbf}

$\bullet$ Log Bayes factor for the univariate Gaussian with known variance, \pageref{sec:ugkv-lbf}

$\bullet$ Log model evidence for Bayesian linear regression, \pageref{sec:blr-lme}

$\bullet$ Log model evidence for Bayesian linear regression with known covariance, \pageref{sec:blrkc-lme}

$\bullet$ Log model evidence for binomial observations, \pageref{sec:bin-lme}

$\bullet$ Log model evidence for multinomial observations, \pageref{sec:mult-lme}

$\bullet$ Log model evidence for multivariate Bayesian linear regression, \pageref{sec:mblr-lme}

$\bullet$ Log model evidence for Poisson-distributed data, \pageref{sec:poiss-lme}

$\bullet$ Log model evidence for the Poisson distribution with exposure values, \pageref{sec:poissexp-lme}

$\bullet$ Log model evidence for the univariate Gaussian, \pageref{sec:ug-lme}

$\bullet$ Log model evidence for the univariate Gaussian with known variance, \pageref{sec:ugkv-lme}

$\bullet$ Log sum inequality, \pageref{sec:logsum-ineq}

$\bullet$ Log-likelihood ratio for the general linear model, \pageref{sec:glm-llr}

$\bullet$ Log-odds and probability in logistic regression, \pageref{sec:logreg-lonp}

$\bullet$ Logarithmic expectation of the gamma distribution, \pageref{sec:gam-logmean}


\vspace{1em}
\textbf{M}

$\bullet$ Marginal distribution of a conditional binomial distribution, \pageref{sec:bin-margcond}

$\bullet$ Marginal distributions for the matrix-normal distribution, \pageref{sec:matn-marg}

$\bullet$ Marginal distributions of the multivariate normal distribution, \pageref{sec:mvn-marg}

$\bullet$ Marginal distributions of the normal-gamma distribution, \pageref{sec:ng-marg}

$\bullet$ Marginal likelihood is a definite integral of joint likelihood, \pageref{sec:ml-jl}

$\bullet$ Maximum likelihood estimation can result in biased estimates, \pageref{sec:mle-bias}

$\bullet$ Maximum likelihood estimation for binomial observations, \pageref{sec:bin-mle}

$\bullet$ Maximum likelihood estimation for Dirichlet-distributed data, \pageref{sec:dir-mle}

$\bullet$ Maximum likelihood estimation for multinomial observations, \pageref{sec:mult-mle}

$\bullet$ Maximum likelihood estimation for multiple linear regression, \pageref{sec:mlr-mle}

$\bullet$ Maximum likelihood estimation for Poisson-distributed data, \pageref{sec:poiss-mle}

$\bullet$ Maximum likelihood estimation for simple linear regression, \pageref{sec:slr-mle}

$\bullet$ Maximum likelihood estimation for simple linear regression, \pageref{sec:slr-mle2}

$\bullet$ Maximum likelihood estimation for the general linear model, \pageref{sec:glm-mle}

$\bullet$ Maximum likelihood estimation for the Poisson distribution with exposure values, \pageref{sec:poissexp-mle}

$\bullet$ Maximum likelihood estimation for the univariate Gaussian, \pageref{sec:ug-mle}

$\bullet$ Maximum likelihood estimation for the univariate Gaussian with known variance, \pageref{sec:ugkv-mle}

$\bullet$ Maximum likelihood estimator of variance in multiple linear regression is biased, \pageref{sec:resvar-biasp}

$\bullet$ Maximum likelihood estimator of variance is biased, \pageref{sec:resvar-bias}

$\bullet$ Maximum log-likelihood for binomial observations, \pageref{sec:bin-mll}

$\bullet$ Maximum log-likelihood for multinomial observations, \pageref{sec:mult-mll}

$\bullet$ Maximum log-likelihood for multiple linear regression, \pageref{sec:mlr-mll}

$\bullet$ Maximum log-likelihood for the general linear model, \pageref{sec:glm-mll}

$\bullet$ Maximum-a-posteriori estimation for Bayesian linear regression, \pageref{sec:blr-map}

$\bullet$ Maximum-a-posteriori estimation for binomial observations, \pageref{sec:bin-map}

$\bullet$ Maximum-a-posteriori estimation for multinomial observations, \pageref{sec:mult-map}

$\bullet$ Mean of the Bernoulli distribution, \pageref{sec:bern-mean}

$\bullet$ Mean of the beta distribution, \pageref{sec:beta-mean}

$\bullet$ Mean of the binomial distribution, \pageref{sec:bin-mean}

$\bullet$ Mean of the categorical distribution, \pageref{sec:cat-mean}

$\bullet$ Mean of the continuous uniform distribution, \pageref{sec:cuni-mean}

$\bullet$ Mean of the ex-Gaussian distribution, \pageref{sec:exg-mean}

$\bullet$ Mean of the exponential distribution, \pageref{sec:exp-mean}

$\bullet$ Mean of the gamma distribution, \pageref{sec:gam-mean}

$\bullet$ Mean of the log-normal distribution, \pageref{sec:lognorm-mean}

$\bullet$ Mean of the matrix-normal distribution, \pageref{sec:matn-mean}

$\bullet$ Mean of the multinomial distribution, \pageref{sec:mult-mean}

$\bullet$ Mean of the multivariate normal distribution, \pageref{sec:mvn-mean}

$\bullet$ Mean of the normal distribution, \pageref{sec:norm-mean}

$\bullet$ Mean of the normal-gamma distribution, \pageref{sec:ng-mean}

$\bullet$ Mean of the normal-Wishart distribution, \pageref{sec:nw-mean}

$\bullet$ Mean of the Poisson distribution, \pageref{sec:poiss-mean}

$\bullet$ Mean of the Wald distribution, \pageref{sec:wald-mean}

$\bullet$ Median of the continuous uniform distribution, \pageref{sec:cuni-med}

$\bullet$ Median of the exponential distribution, \pageref{sec:exp-med}

$\bullet$ Median of the log-normal distribution, \pageref{sec:lognorm-med}

$\bullet$ Median of the normal distribution, \pageref{sec:norm-med}

$\bullet$ Method of moments for beta-binomial data, \pageref{sec:betabin-mome}

$\bullet$ Method of moments for beta-distributed data, \pageref{sec:beta-mome}

$\bullet$ Method of moments for ex-Gaussian-distributed data, \pageref{sec:exg-mome}

$\bullet$ Method of moments for Wald-distributed data, \pageref{sec:wald-mome}

$\bullet$ Mode of the continuous uniform distribution, \pageref{sec:cuni-med}

$\bullet$ Mode of the exponential distribution, \pageref{sec:exp-mode}

$\bullet$ Mode of the log-normal distribution, \pageref{sec:lognorm-mode}

$\bullet$ Mode of the normal distribution, \pageref{sec:norm-mode}

$\bullet$ Moment in terms of moment-generating function, \pageref{sec:mom-mgf}

$\bullet$ Moment-generating function of a function of a random variable, \pageref{sec:mgf-fct}

$\bullet$ Moment-generating function of linear combination of independent random variables, \pageref{sec:mgf-lincomb}

$\bullet$ Moment-generating function of the beta distribution, \pageref{sec:beta-mgf}

$\bullet$ Moment-generating function of the ex-Gaussian distribution, \pageref{sec:exg-mgf}

$\bullet$ Moment-generating function of the exponential distribution, \pageref{sec:exp-mgf}

$\bullet$ Moment-generating function of the gamma distribution, \pageref{sec:gam-mgf}

$\bullet$ Moment-generating function of the multivariate normal distribution, \pageref{sec:mvn-mgf}

$\bullet$ Moment-generating function of the normal distribution, \pageref{sec:norm-mgf}

$\bullet$ Moment-generating function of the Wald distribution, \pageref{sec:wald-mgf}

$\bullet$ Moments of the chi-squared distribution, \pageref{sec:chi2-mom}

$\bullet$ Monotonicity of probability, \pageref{sec:prob-mon}

$\bullet$ Monotonicity of the expected value, \pageref{sec:mean-mono}

$\bullet$ Multinomial test, \pageref{sec:mult-test}

$\bullet$ Multiple linear regression is a special case of the general linear model, \pageref{sec:mlr-glm}

$\bullet$ Multivariate normal distribution is a special case of matrix-normal distribution, \pageref{sec:mvn-matn}

$\bullet$ Mutual information of dependent and independent variables in the general linear model, \pageref{sec:glm-mi}


\vspace{1em}
\textbf{N}

$\bullet$ Necessary and sufficient condition for independence of multivariate normal random variables, \pageref{sec:mvn-ind}

$\bullet$ Non-invariance of the differential entropy under change of variables, \pageref{sec:dent-noninv}

$\bullet$ (Non-)Multiplicativity of the expected value, \pageref{sec:mean-mult}

$\bullet$ Non-negativity of the expected value, \pageref{sec:mean-nonneg}

$\bullet$ Non-negativity of the Kullback-Leibler divergence, \pageref{sec:kl-nonneg}

$\bullet$ Non-negativity of the Kullback-Leibler divergence, \pageref{sec:kl-nonneg2}

$\bullet$ Non-negativity of the Shannon entropy, \pageref{sec:ent-nonneg}

$\bullet$ Non-negativity of the variance, \pageref{sec:var-nonneg}

$\bullet$ Non-symmetry of the Kullback-Leibler divergence, \pageref{sec:kl-nonsymm}

$\bullet$ Normal distribution is a special case of multivariate normal distribution, \pageref{sec:norm-mvn}

$\bullet$ Normal distribution maximizes differential entropy for fixed variance, \pageref{sec:norm-maxent}

$\bullet$ Normal-gamma distribution is a special case of normal-Wishart distribution, \pageref{sec:ng-nw}


\vspace{1em}
\textbf{O}

$\bullet$ Omnibus F-test for multiple regressors in multiple linear regression, \pageref{sec:mlr-fomnibus}

$\bullet$ One-sample t-test for independent observations, \pageref{sec:ug-ttest1}

$\bullet$ One-sample z-test for independent observations, \pageref{sec:ugkv-ztest1}

$\bullet$ Ordinary least squares for multiple linear regression, \pageref{sec:mlr-ols}

$\bullet$ Ordinary least squares for multiple linear regression, \pageref{sec:mlr-ols2}

$\bullet$ Ordinary least squares for multiple linear regression, \pageref{sec:mlr-ols3}

$\bullet$ Ordinary least squares for multiple linear regression with two regressors, \pageref{sec:mlr-olstr}

$\bullet$ Ordinary least squares for one-way analysis of variance, \pageref{sec:anova1-ols}

$\bullet$ Ordinary least squares for simple linear regression, \pageref{sec:slr-ols}

$\bullet$ Ordinary least squares for simple linear regression, \pageref{sec:slr-ols2}

$\bullet$ Ordinary least squares for the general linear model, \pageref{sec:glm-ols}

$\bullet$ Ordinary least squares for two-way analysis of variance, \pageref{sec:anova2-ols}


\vspace{1em}
\textbf{P}

$\bullet$ Paired t-test for dependent observations, \pageref{sec:ug-ttestp}

$\bullet$ Paired z-test for dependent observations, \pageref{sec:ugkv-ztestp}

$\bullet$ Parameter estimates for simple linear regression are uncorrelated after mean-centering, \pageref{sec:slr-olscorr}

$\bullet$ Parameters of the corresponding forward model, \pageref{sec:cfm-para}

$\bullet$ Partition of a covariance matrix into expected values, \pageref{sec:covmat-mean}

$\bullet$ Partition of covariance into expected values, \pageref{sec:cov-mean}

$\bullet$ Partition of skewness into expected values, \pageref{sec:skew-mean}

$\bullet$ Partition of sums of squares for multiple linear regression, \pageref{sec:mlr-pss}

$\bullet$ Partition of sums of squares for simple linear regression, \pageref{sec:slr-pss}

$\bullet$ Partition of sums of squares in one-way analysis of variance, \pageref{sec:anova1-pss}

$\bullet$ Partition of sums of squares in two-way analysis of variance, \pageref{sec:anova2-pss}

$\bullet$ Partition of the mean squared error into bias and variance, \pageref{sec:mse-bnv}

$\bullet$ Partition of variance into expected values, \pageref{sec:var-mean}

$\bullet$ Positive semi-definiteness of the covariance matrix, \pageref{sec:covmat-psd}

$\bullet$ Posterior credibility region against the omnibus null hypothesis for Bayesian linear regression, \pageref{sec:blr-pcr}

$\bullet$ Posterior density is proportional to joint likelihood, \pageref{sec:post-jl}

$\bullet$ Posterior distribution for Bayesian linear regression, \pageref{sec:blr-post}

$\bullet$ Posterior distribution for Bayesian linear regression with known covariance, \pageref{sec:blrkc-post}

$\bullet$ Posterior distribution for binomial observations, \pageref{sec:bin-post}

$\bullet$ Posterior distribution for multinomial observations, \pageref{sec:mult-post}

$\bullet$ Posterior distribution for multivariate Bayesian linear regression, \pageref{sec:mblr-post}

$\bullet$ Posterior distribution for Poisson-distributed data, \pageref{sec:poiss-post}

$\bullet$ Posterior distribution for the Poisson distribution with exposure values, \pageref{sec:poissexp-post}

$\bullet$ Posterior distribution for the univariate Gaussian, \pageref{sec:ug-post}

$\bullet$ Posterior distribution for the univariate Gaussian with known variance, \pageref{sec:ugkv-post}

$\bullet$ Posterior probability of the alternative hypothesis for Bayesian linear regression, \pageref{sec:blr-pp}

$\bullet$ Posterior probability of the alternative model for binomial observations, \pageref{sec:bin-pp}

$\bullet$ Posterior probability of the alternative model for multinomial observations, \pageref{sec:mult-pp}

$\bullet$ Probability and log-odds in logistic regression, \pageref{sec:logreg-pnlo}

$\bullet$ Probability density function is first derivative of cumulative distribution function, \pageref{sec:pdf-cdf}

$\bullet$ Probability density function of a linear function of a continuous random vector, \pageref{sec:pdf-linfct}

$\bullet$ Probability density function of a strictly decreasing function of a continuous random variable, \pageref{sec:pdf-sdfct}

$\bullet$ Probability density function of a strictly increasing function of a continuous random variable, \pageref{sec:pdf-sifct}

$\bullet$ Probability density function of a sum of independent discrete random variables, \pageref{sec:pdf-sumind}

$\bullet$ Probability density function of an invertible function of a continuous random vector, \pageref{sec:pdf-invfct}

$\bullet$ Probability density function of the beta distribution, \pageref{sec:beta-pdf}

$\bullet$ Probability density function of the bivariate normal distribution, \pageref{sec:bvn-pdf}

$\bullet$ Probability density function of the bivariate normal distribution in terms of correlation coefficient, \pageref{sec:bvn-pdfcorr}

$\bullet$ Probability density function of the chi-squared distribution, \pageref{sec:chi2-pdf}

$\bullet$ Probability density function of the continuous uniform distribution, \pageref{sec:cuni-pdf}

$\bullet$ Probability density function of the Dirichlet distribution, \pageref{sec:dir-pdf}

$\bullet$ Probability density function of the ex-Gaussian distribution, \pageref{sec:exg-pdf}

$\bullet$ Probability density function of the exponential distribution, \pageref{sec:exp-pdf}

$\bullet$ Probability density function of the F-distribution, \pageref{sec:f-pdf}

$\bullet$ Probability density function of the gamma distribution, \pageref{sec:gam-pdf}

$\bullet$ Probability density function of the log-normal distribution, \pageref{sec:lognorm-pdf}

$\bullet$ Probability density function of the matrix-normal distribution, \pageref{sec:matn-pdf}

$\bullet$ Probability density function of the multivariate normal distribution, \pageref{sec:mvn-pdf}

$\bullet$ Probability density function of the multivariate t-distribution, \pageref{sec:mvt-pdf}

$\bullet$ Probability density function of the normal distribution, \pageref{sec:norm-pdf}

$\bullet$ Probability density function of the normal-gamma distribution, \pageref{sec:ng-pdf}

$\bullet$ Probability density function of the normal-Wishart distribution, \pageref{sec:nw-pdf}

$\bullet$ Probability density function of the t-distribution, \pageref{sec:t-pdf}

$\bullet$ Probability density function of the Wald distribution, \pageref{sec:wald-pdf}

$\bullet$ Probability integral transform using cumulative distribution function, \pageref{sec:cdf-pit}

$\bullet$ Probability mass function of a strictly decreasing function of a discrete random variable, \pageref{sec:pmf-sdfct}

$\bullet$ Probability mass function of a strictly increasing function of a discrete random variable, \pageref{sec:pmf-sifct}

$\bullet$ Probability mass function of a sum of independent discrete random variables, \pageref{sec:pmf-sumind}

$\bullet$ Probability mass function of an invertible function of a random vector, \pageref{sec:pmf-invfct}

$\bullet$ Probability mass function of the Bernoulli distribution, \pageref{sec:bern-pmf}

$\bullet$ Probability mass function of the beta-binomial distribution, \pageref{sec:betabin-pmf}

$\bullet$ Probability mass function of the binomial distribution, \pageref{sec:bin-pmf}

$\bullet$ Probability mass function of the categorical distribution, \pageref{sec:cat-pmf}

$\bullet$ Probability mass function of the discrete uniform distribution, \pageref{sec:duni-pmf}

$\bullet$ Probability mass function of the multinomial distribution, \pageref{sec:mult-pmf}

$\bullet$ Probability mass function of the Poisson distribution, \pageref{sec:poiss-pmf}

$\bullet$ Probability of exhaustive events, \pageref{sec:prob-exh}

$\bullet$ Probability of exhaustive events, \pageref{sec:prob-exh2}

$\bullet$ Probability of normal random variable being within standard deviations from its mean, \pageref{sec:norm-probstd}

$\bullet$ Probability of the complement, \pageref{sec:prob-comp}

$\bullet$ Probability of the empty set, \pageref{sec:prob-emp}

$\bullet$ Probability under mutual exclusivity, \pageref{sec:prob-exc}

$\bullet$ Probability under statistical independence, \pageref{sec:prob-ind}

$\bullet$ Probability-generating function is expectation of function of random variable, \pageref{sec:pgf-mean}

$\bullet$ Probability-generating function of the binomial distribution, \pageref{sec:bin-pgf}

$\bullet$ Projection matrix and residual-forming matrix are idempotent, \pageref{sec:mlr-idem}

$\bullet$ Projection matrix and residual-forming matrix are symmetric, \pageref{sec:mlr-symm}

$\bullet$ Projection of a data point to the regression line, \pageref{sec:slr-proj}


\vspace{1em}
\textbf{Q}

$\bullet$ Quantile function is inverse of strictly monotonically increasing cumulative distribution function, \pageref{sec:qf-cdf}

$\bullet$ Quantile function of the continuous uniform distribution, \pageref{sec:cuni-qf}

$\bullet$ Quantile function of the discrete uniform distribution, \pageref{sec:duni-qf}

$\bullet$ Quantile function of the exponential distribution, \pageref{sec:exp-qf}

$\bullet$ Quantile function of the gamma distribution, \pageref{sec:gam-qf}

$\bullet$ Quantile function of the log-normal distribution, \pageref{sec:lognorm-qf}

$\bullet$ Quantile function of the normal distribution, \pageref{sec:norm-qf}


\vspace{1em}
\textbf{R}

$\bullet$ Range of probability, \pageref{sec:prob-range}

$\bullet$ Range of the variance of the Bernoulli distribution, \pageref{sec:bern-varrange}

$\bullet$ Range of the variance of the binomial distribution, \pageref{sec:bin-varrange}

$\bullet$ Relation of continuous Kullback-Leibler divergence to differential entropy, \pageref{sec:kl-dent}

$\bullet$ Relation of continuous mutual information to joint and conditional differential entropy, \pageref{sec:cmi-jcde}

$\bullet$ Relation of continuous mutual information to marginal and conditional differential entropy, \pageref{sec:cmi-mcde}

$\bullet$ Relation of continuous mutual information to marginal and joint differential entropy, \pageref{sec:cmi-mjde}

$\bullet$ Relation of discrete Kullback-Leibler divergence to Shannon entropy, \pageref{sec:kl-ent}

$\bullet$ Relation of mutual information to joint and conditional entropy, \pageref{sec:dmi-jce}

$\bullet$ Relation of mutual information to marginal and conditional entropy, \pageref{sec:dmi-mce}

$\bullet$ Relation of mutual information to marginal and joint entropy, \pageref{sec:dmi-mje}

$\bullet$ Relationship between chi-squared distribution and beta distribution, \pageref{sec:beta-chi2}

$\bullet$ Relationship between coefficient of determination and correlation coefficient in simple linear regression, \pageref{sec:slr-rsq}

$\bullet$ Relationship between correlation coefficient and slope estimate in simple linear regression, \pageref{sec:slr-corr}

$\bullet$ Relationship between covariance and correlation, \pageref{sec:cov-corr}

$\bullet$ Relationship between covariance matrix and correlation matrix, \pageref{sec:covmat-corrmat}

$\bullet$ Relationship between F-statistic and maximum log-likelihood, \pageref{sec:fstat-mll}

$\bullet$ Relationship between F-statistic and RÂ², \pageref{sec:fstat-rsq}

$\bullet$ Relationship between gamma distribution and standard gamma distribution, \pageref{sec:gam-sgam}

$\bullet$ Relationship between gamma distribution and standard gamma distribution, \pageref{sec:gam-sgam2}

$\bullet$ Relationship between multivariate normal distribution and chi-squared distribution, \pageref{sec:mvn-chi2}

$\bullet$ Relationship between multivariate t-distribution and F-distribution, \pageref{sec:mvt-f}

$\bullet$ Relationship between non-standardized t-distribution and t-distribution, \pageref{sec:nst-t}

$\bullet$ Relationship between normal distribution and chi-squared distribution, \pageref{sec:norm-chi2}

$\bullet$ Relationship between normal distribution and standard normal distribution, \pageref{sec:norm-snorm}

$\bullet$ Relationship between normal distribution and standard normal distribution, \pageref{sec:norm-snorm2}

$\bullet$ Relationship between normal distribution and standard normal distribution, \pageref{sec:norm-snorm3}

$\bullet$ Relationship between normal distribution and t-distribution, \pageref{sec:norm-t}

$\bullet$ Relationship between precision matrix and correlation matrix, \pageref{sec:precmat-corrmat}

$\bullet$ Relationship between RÂ² and maximum log-likelihood, \pageref{sec:rsq-mll}

$\bullet$ Relationship between residual variance and sample variance in simple linear regression, \pageref{sec:slr-resvar}

$\bullet$ Relationship between second raw moment, variance and mean, \pageref{sec:momraw-2nd}

$\bullet$ Relationship between signal-to-noise ratio and maximum log-likelihood, \pageref{sec:snr-mll}

$\bullet$ Relationship between signal-to-noise ratio and RÂ², \pageref{sec:snr-rsq}

$\bullet$ Reparametrization for one-way analysis of variance, \pageref{sec:anova1-repara}


\vspace{1em}
\textbf{S}

$\bullet$ Sampling from the matrix-normal distribution, \pageref{sec:matn-samp}

$\bullet$ Sampling from the normal-gamma distribution, \pageref{sec:ng-samp}

$\bullet$ Scaling of a random variable following the gamma distribution, \pageref{sec:gam-scal}

$\bullet$ Scaling of the covariance matrix upon multiplication with constant matrix, \pageref{sec:covmat-scal}

$\bullet$ Scaling of the variance upon multiplication with a constant, \pageref{sec:var-scal}

$\bullet$ Second central moment is variance, \pageref{sec:momcent-2nd}

$\bullet$ Self-covariance equals variance, \pageref{sec:cov-var}

$\bullet$ Simple linear regression is a special case of multiple linear regression, \pageref{sec:slr-mlr}

$\bullet$ Skewness of the ex-Gaussian distribution, \pageref{sec:exg-skew}

$\bullet$ Skewness of the exponential distribution, \pageref{sec:exp-skew}

$\bullet$ Skewness of the Wald distribution, \pageref{sec:wald-skew}

$\bullet$ Specific t-test for single regressor in multiple linear regression, \pageref{sec:mlr-tsingle}

$\bullet$ Square of expectation of product is less than or equal to product of expectation of squares, \pageref{sec:mean-prodsqr}

$\bullet$ Statistical significance test for the coefficient of determinantion based on an omnibus F-test, \pageref{sec:rsq-test}

$\bullet$ Statistical test for comparing simple linear regression models with and without slope parameter, \pageref{sec:slr-fcomp}

$\bullet$ Statistical test for intercept parameter in simple linear regression model, \pageref{sec:slr-tint}

$\bullet$ Statistical test for slope parameter in simple linear regression model, \pageref{sec:slr-tslo}

$\bullet$ Sums of squares for simple linear regression, \pageref{sec:slr-sss}

$\bullet$ Symmetry of the covariance, \pageref{sec:cov-symm}

$\bullet$ Symmetry of the covariance matrix, \pageref{sec:covmat-symm}


\vspace{1em}
\textbf{T}

$\bullet$ t-distribution is a special case of multivariate t-distribution, \pageref{sec:t-mvt}

$\bullet$ t-test for multiple linear regression using contrast-based inference, \pageref{sec:mlr-t}

$\bullet$ The log probability scoring rule is a strictly proper scoring rule, \pageref{sec:lpsr-spsr}

$\bullet$ The p-value follows a uniform distribution under the null hypothesis, \pageref{sec:pval-h0}

$\bullet$ The regression line goes through the center of mass point, \pageref{sec:slr-comp}

$\bullet$ The residuals and the covariate are uncorrelated in simple linear regression, \pageref{sec:slr-rescorr}

$\bullet$ The sum of residuals is zero in simple linear regression, \pageref{sec:slr-ressum}

$\bullet$ Transformation matrices for ordinary least squares, \pageref{sec:mlr-mat}

$\bullet$ Transformation matrices for simple linear regression, \pageref{sec:slr-mat}

$\bullet$ Transposition of a matrix-normal random variable, \pageref{sec:matn-trans}

$\bullet$ Two-sample t-test for independent observations, \pageref{sec:ug-ttest2}

$\bullet$ Two-sample z-test for independent observations, \pageref{sec:ugkv-ztest2}


\vspace{1em}
\textbf{V}

$\bullet$ Value of the probability-generating function for argument one, \pageref{sec:pgf-one}

$\bullet$ Value of the probability-generating function for argument zero, \pageref{sec:pgf-zero}

$\bullet$ Variance of constant is zero, \pageref{sec:var-const}

$\bullet$ Variance of parameter estimates for simple linear regression, \pageref{sec:slr-olsvar}

$\bullet$ Variance of the Bernoulli distribution, \pageref{sec:bern-var}

$\bullet$ Variance of the beta distribution, \pageref{sec:beta-var}

$\bullet$ Variance of the binomial distribution, \pageref{sec:bin-var}

$\bullet$ Variance of the continuous uniform distribution, \pageref{sec:cuni-var}

$\bullet$ Variance of the ex-Gaussian distribution, \pageref{sec:exg-var}

$\bullet$ Variance of the exponential distribution, \pageref{sec:exp-var}

$\bullet$ Variance of the gamma distribution, \pageref{sec:gam-var}

$\bullet$ Variance of the linear combination of two random variables, \pageref{sec:var-lincomb}

$\bullet$ Variance of the log-normal distribution, \pageref{sec:lognorm-var}

$\bullet$ Variance of the normal distribution, \pageref{sec:norm-var}

$\bullet$ Variance of the Poisson distribution, \pageref{sec:poiss-var}

$\bullet$ Variance of the sum of two random variables, \pageref{sec:var-sum}

$\bullet$ Variance of the Wald distribution, \pageref{sec:wald-var}


\vspace{1em}
\textbf{W}

$\bullet$ Weighted least squares for multiple linear regression, \pageref{sec:mlr-wls}

$\bullet$ Weighted least squares for multiple linear regression, \pageref{sec:mlr-wls2}

$\bullet$ Weighted least squares for simple linear regression, \pageref{sec:slr-wls}

$\bullet$ Weighted least squares for simple linear regression, \pageref{sec:slr-wls2}

$\bullet$ Weighted least squares for the general linear model, \pageref{sec:glm-wls}



\pagebreak
\section{Definition by Topic}

\textbf{A}

$\bullet$ Alternative hypothesis, \pageref{sec:h1}


\vspace{1em}
\textbf{B}

$\bullet$ Bernoulli distribution, \pageref{sec:bern}

$\bullet$ Beta distribution, \pageref{sec:beta}

$\bullet$ Beta-binomial data, \pageref{sec:betabin-data}

$\bullet$ Beta-binomial distribution, \pageref{sec:betabin}

$\bullet$ Beta-distributed data, \pageref{sec:beta-data}

$\bullet$ Binomial distribution, \pageref{sec:bin}

$\bullet$ Binomial observations, \pageref{sec:bin-data}

$\bullet$ Bivariate normal distribution, \pageref{sec:bvn}

$\bullet$ Brier scoring rule, \pageref{sec:bsr}


\vspace{1em}
\textbf{C}

$\bullet$ Categorical distribution, \pageref{sec:cat}

$\bullet$ Central moment, \pageref{sec:mom-cent}

$\bullet$ Characteristic function, \pageref{sec:cf}

$\bullet$ Chi-squared distribution, \pageref{sec:chi2}

$\bullet$ Coefficient of determination, \pageref{sec:rsq}

$\bullet$ Conditional differential entropy, \pageref{sec:dent-cond}

$\bullet$ Conditional entropy, \pageref{sec:ent-cond}

$\bullet$ Conditional independence, \pageref{sec:ind-cond}

$\bullet$ Conditional probability distribution, \pageref{sec:dist-cond}

$\bullet$ Confidence interval, \pageref{sec:ci}

$\bullet$ Conjugate and non-conjugate prior distribution, \pageref{sec:prior-conj}

$\bullet$ Constant, \pageref{sec:const}

$\bullet$ Continuous uniform distribution, \pageref{sec:cuni}

$\bullet$ Correlation, \pageref{sec:corr}

$\bullet$ Correlation matrix, \pageref{sec:corrmat}

$\bullet$ Corresponding forward model, \pageref{sec:cfm}

$\bullet$ Covariance, \pageref{sec:cov}

$\bullet$ Covariance matrix, \pageref{sec:covmat}

$\bullet$ Critical value, \pageref{sec:cval}

$\bullet$ Cross-covariance matrix, \pageref{sec:covmat-cross}

$\bullet$ Cross-entropy, \pageref{sec:ent-cross}

$\bullet$ Cumulant-generating function, \pageref{sec:cgf}

$\bullet$ Cumulative distribution function, \pageref{sec:cdf}


\vspace{1em}
\textbf{D}

$\bullet$ Differential cross-entropy, \pageref{sec:dent-cross}

$\bullet$ Differential entropy, \pageref{sec:dent}

$\bullet$ Dirichlet distribution, \pageref{sec:dir}

$\bullet$ Dirichlet-distributed data, \pageref{sec:dir-data}

$\bullet$ Discrete and continuous random variable, \pageref{sec:rvar-disc}

$\bullet$ Discrete uniform distribution, \pageref{sec:duni}


\vspace{1em}
\textbf{E}

$\bullet$ Empirical and theoretical prior distribution, \pageref{sec:prior-emp}

$\bullet$ Empirical Bayes, \pageref{sec:eb}

$\bullet$ Empirical Bayes prior distribution, \pageref{sec:prior-eb}

$\bullet$ Estimation matrix, \pageref{sec:emat}

$\bullet$ Event space, \pageref{sec:eve-spc}

$\bullet$ ex-Gaussian distribution, \pageref{sec:exg}

$\bullet$ Exceedance probability, \pageref{sec:prob-exc}

$\bullet$ Expected value, \pageref{sec:mean}

$\bullet$ Expected value of a random matrix, \pageref{sec:mean-rmat}

$\bullet$ Expected value of a random vector, \pageref{sec:mean-rvec}

$\bullet$ Explained sum of squares, \pageref{sec:ess}

$\bullet$ Exponential distribution, \pageref{sec:exp}


\vspace{1em}
\textbf{F}

$\bullet$ F-contrast for contrast-based inference in multiple linear regression, \pageref{sec:fcon}

$\bullet$ F-distribution, \pageref{sec:f}

$\bullet$ F-statistic, \pageref{sec:fstat}

$\bullet$ Flat, hard and soft prior distribution, \pageref{sec:prior-flat}

$\bullet$ Full probability model, \pageref{sec:fpm}

$\bullet$ Full width at half maximum, \pageref{sec:fwhm}


\vspace{1em}
\textbf{G}

$\bullet$ Gamma distribution, \pageref{sec:gam}

$\bullet$ General linear model, \pageref{sec:glm}

$\bullet$ Generative model, \pageref{sec:gm}


\vspace{1em}
\textbf{I}

$\bullet$ Informative and non-informative prior distribution, \pageref{sec:prior-inf}

$\bullet$ Interaction sum of squares, \pageref{sec:iass}

$\bullet$ Inverse general linear model, \pageref{sec:iglm}


\vspace{1em}
\textbf{J}

$\bullet$ Joint cumulative distribution function, \pageref{sec:cdf-joint}

$\bullet$ Joint differential entropy, \pageref{sec:dent-joint}

$\bullet$ Joint entropy, \pageref{sec:ent-joint}

$\bullet$ Joint likelihood, \pageref{sec:jl}

$\bullet$ Joint probability, \pageref{sec:prob-joint}

$\bullet$ Joint probability distribution, \pageref{sec:dist-joint}


\vspace{1em}
\textbf{K}

$\bullet$ Kolmogorov axioms of probability, \pageref{sec:prob-ax}

$\bullet$ Kullback-Leibler divergence, \pageref{sec:kl}


\vspace{1em}
\textbf{L}

$\bullet$ Law of conditional probability, \pageref{sec:prob-cond}

$\bullet$ Law of marginal probability, \pageref{sec:prob-marg}

$\bullet$ Likelihood function, \pageref{sec:lf}

$\bullet$ Likelihood function, \pageref{sec:lf}

$\bullet$ Likelihood ratio, \pageref{sec:lr}

$\bullet$ Log probability scoring rule, \pageref{sec:lpsr}

$\bullet$ Log-likelihood function, \pageref{sec:llf}

$\bullet$ Log-likelihood ratio, \pageref{sec:llr}

$\bullet$ Log-normal distribution, \pageref{sec:lognorm}

$\bullet$ Logistic regression, \pageref{sec:logreg}


\vspace{1em}
\textbf{M}

$\bullet$ Marginal likelihood, \pageref{sec:ml}

$\bullet$ Marginal probability distribution, \pageref{sec:dist-marg}

$\bullet$ Matrix-normal distribution, \pageref{sec:matn}

$\bullet$ Maximum, \pageref{sec:max}

$\bullet$ Maximum entropy prior distribution, \pageref{sec:prior-maxent}

$\bullet$ Maximum likelihood estimation, \pageref{sec:mle}

$\bullet$ Maximum log-likelihood, \pageref{sec:mll}

$\bullet$ Maximum-a-posteriori estimation, \pageref{sec:map}

$\bullet$ Mean squared error, \pageref{sec:mse}

$\bullet$ Median, \pageref{sec:med}

$\bullet$ Method-of-moments estimation, \pageref{sec:mome}

$\bullet$ Minimum, \pageref{sec:min}

$\bullet$ Mode, \pageref{sec:mode}

$\bullet$ Moment, \pageref{sec:mom}

$\bullet$ Moment-generating function, \pageref{sec:mgf}

$\bullet$ Multinomial distribution, \pageref{sec:mult}

$\bullet$ Multinomial observations, \pageref{sec:mult-data}

$\bullet$ Multiple linear regression, \pageref{sec:mlr}

$\bullet$ Multivariate normal distribution, \pageref{sec:mvn}

$\bullet$ Multivariate t-distribution, \pageref{sec:mvt}

$\bullet$ Mutual exclusivity, \pageref{sec:exc}

$\bullet$ Mutual information, \pageref{sec:mi}

$\bullet$ Mutual information, \pageref{sec:mi}


\vspace{1em}
\textbf{N}

$\bullet$ Non-standardized t-distribution, \pageref{sec:nst}

$\bullet$ Normal distribution, \pageref{sec:norm}

$\bullet$ Normal-gamma distribution, \pageref{sec:ng}

$\bullet$ Normal-Wishart distribution, \pageref{sec:nw}

$\bullet$ Null hypothesis, \pageref{sec:h0}


\vspace{1em}
\textbf{O}

$\bullet$ One-tailed and two-tailed hypothesis, \pageref{sec:hyp-tail}

$\bullet$ One-tailed and two-tailed test, \pageref{sec:test-tail}

$\bullet$ One-way analysis of variance, \pageref{sec:anova1}


\vspace{1em}
\textbf{P}

$\bullet$ p-value, \pageref{sec:pval}

$\bullet$ Point and set hypothesis, \pageref{sec:hyp-point}

$\bullet$ Poisson distribution, \pageref{sec:poiss}

$\bullet$ Poisson distribution with exposure values, \pageref{sec:poissexp}

$\bullet$ Poisson-distributed data, \pageref{sec:poiss-data}

$\bullet$ Posterior distribution, \pageref{sec:post}

$\bullet$ Power of a statistical test, \pageref{sec:power}

$\bullet$ Precision, \pageref{sec:prec}

$\bullet$ Precision matrix, \pageref{sec:precmat}

$\bullet$ Prior distribution, \pageref{sec:prior}

$\bullet$ Probability, \pageref{sec:prob}

$\bullet$ Probability density function, \pageref{sec:pdf}

$\bullet$ Probability distribution, \pageref{sec:dist}

$\bullet$ Probability mass function, \pageref{sec:pmf}

$\bullet$ Probability space, \pageref{sec:prob-spc}

$\bullet$ Probability-generating function, \pageref{sec:pgf}

$\bullet$ Projection matrix, \pageref{sec:pmat}

$\bullet$ Proper scoring rule, \pageref{sec:psr}


\vspace{1em}
\textbf{Q}

$\bullet$ Quantile function, \pageref{sec:qf}


\vspace{1em}
\textbf{R}

$\bullet$ Random event, \pageref{sec:reve}

$\bullet$ Random experiment, \pageref{sec:rexp}

$\bullet$ Random matrix, \pageref{sec:rmat}

$\bullet$ Random variable, \pageref{sec:rvar}

$\bullet$ Random vector, \pageref{sec:rvec}

$\bullet$ Raw moment, \pageref{sec:mom-raw}

$\bullet$ Reference prior distribution, \pageref{sec:prior-ref}

$\bullet$ Regression line, \pageref{sec:regline}

$\bullet$ Residual sum of squares, \pageref{sec:rss}

$\bullet$ Residual variance, \pageref{sec:resvar}

$\bullet$ Residual-forming matrix, \pageref{sec:rfmat}


\vspace{1em}
\textbf{S}

$\bullet$ Sample correlation coefficient, \pageref{sec:corr-samp}

$\bullet$ Sample correlation matrix, \pageref{sec:corrmat-samp}

$\bullet$ Sample covariance, \pageref{sec:cov-samp}

$\bullet$ Sample covariance matrix, \pageref{sec:covmat-samp}

$\bullet$ Sample mean, \pageref{sec:mean-samp}

$\bullet$ Sample skewness, \pageref{sec:skew-samp}

$\bullet$ Sample space, \pageref{sec:samp-spc}

$\bullet$ Sample variance, \pageref{sec:var-samp}

$\bullet$ Sampling distribution, \pageref{sec:dist-samp}

$\bullet$ Scoring rule, \pageref{sec:sr}

$\bullet$ Shannon entropy, \pageref{sec:ent}

$\bullet$ Signal-to-noise ratio, \pageref{sec:snr}

$\bullet$ Significance level, \pageref{sec:alpha}

$\bullet$ Simple and composite hypothesis, \pageref{sec:hyp-simp}

$\bullet$ Simple linear regression, \pageref{sec:slr}

$\bullet$ Size of a statistical test, \pageref{sec:size}

$\bullet$ Skewness, \pageref{sec:skew}

$\bullet$ Standard deviation, \pageref{sec:std}

$\bullet$ Standard gamma distribution, \pageref{sec:sgam}

$\bullet$ Standard normal distribution, \pageref{sec:snorm}

$\bullet$ Standard uniform distribution, \pageref{sec:suni}

$\bullet$ Standardized moment, \pageref{sec:mom-stand}

$\bullet$ Statistical hypothesis, \pageref{sec:hyp}

$\bullet$ Statistical hypothesis test, \pageref{sec:test}

$\bullet$ Statistical independence, \pageref{sec:ind}

$\bullet$ Strictly proper scoring rule, \pageref{sec:spsr}


\vspace{1em}
\textbf{T}

$\bullet$ t-contrast for contrast-based inference in multiple linear regression, \pageref{sec:tcon}

$\bullet$ t-distribution, \pageref{sec:t}

$\bullet$ Test statistic, \pageref{sec:tstat}

$\bullet$ Total sum of squares, \pageref{sec:tss}

$\bullet$ Transformed general linear model, \pageref{sec:tglm}

$\bullet$ Treatment sum of squares, \pageref{sec:trss}

$\bullet$ Two-way analysis of variance, \pageref{sec:anova2}


\vspace{1em}
\textbf{U}

$\bullet$ Uniform and non-uniform prior distribution, \pageref{sec:prior-uni}

$\bullet$ Univariate and multivariate random variable, \pageref{sec:rvar-uni}

$\bullet$ Univariate Gaussian, \pageref{sec:ug}

$\bullet$ Univariate Gaussian with known variance, \pageref{sec:ugkv}


\vspace{1em}
\textbf{V}

$\bullet$ Variance, \pageref{sec:var}

$\bullet$ Variational Bayes, \pageref{sec:vb}


\vspace{1em}
\textbf{W}

$\bullet$ Wald distribution, \pageref{sec:wald}

$\bullet$ Wishart distribution, \pageref{sec:wish}


\end{document}